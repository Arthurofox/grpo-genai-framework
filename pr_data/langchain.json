[
  {
    "number": 30059,
    "title": "core: make with_alisteners() example workable.",
    "body": "**Description:**\r\n\r\n5 fix of example from function with_alisteners() in libs/core/langchain_core/runnables/base.py\r\nReplace incoherent example output with workable example's output.\r\n\r\n1. SyntaxError: unterminated string literal\r\n    print(f\"on start callback starts at {format_t(time.time())}\r\n    correct as\r\n    print(f\"on start callback starts at {format_t(time.time())}\")\r\n\r\n2. SyntaxError: unterminated string literal\r\n    print(f\"on end callback starts at {format_t(time.time())}\r\n    correct as\r\n    print(f\"on end callback starts at {format_t(time.time())}\")\r\n\r\n3. NameError: name 'Runnable' is not defined\r\n    Fix as\r\n    from langchain_core.runnables import Runnable\r\n\r\n4. NameError: name 'asyncio' is not defined\r\n    Fix as\r\n    import asyncio\r\n\r\n5. NameError: name 'format_t' is not defined.\r\n    Implement format_t() as\r\n    from datetime import datetime, timezone\r\n\r\n    def format_t(timestamp: float) -> str:\r\n        return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\r\n\r\n",
    "issue_title": "core: make with_alisteners() example workable.",
    "issue_body": "**Description:**\r\n\r\n5 fix of example from function with_alisteners() in libs/core/langchain_core/runnables/base.py\r\nReplace incoherent example output with workable example's output.\r\n\r\n1. SyntaxError: unterminated string literal\r\n    print(f\"on start callback starts at {format_t(time.time())}\r\n    correct as\r\n    print(f\"on start callback starts at {format_t(time.time())}\")\r\n\r\n2. SyntaxError: unterminated string literal\r\n    print(f\"on end callback starts at {format_t(time.time())}\r\n    correct as\r\n    print(f\"on end callback starts at {format_t(time.time())}\")\r\n\r\n3. NameError: name 'Runnable' is not defined\r\n    Fix as\r\n    from langchain_core.runnables import Runnable\r\n\r\n4. NameError: name 'asyncio' is not defined\r\n    Fix as\r\n    import asyncio\r\n\r\n5. NameError: name 'format_t' is not defined.\r\n    Implement format_t() as\r\n    from datetime import datetime, timezone\r\n\r\n    def format_t(timestamp: float) -> str:\r\n        return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\r\n\r\n",
    "files": [
      {
        "filename": "libs/core/langchain_core/runnables/base.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport collections\nimport contextlib\nimport functools\nimport inspect\nimport threading\nfrom abc import ABC, abstractmethod\nfrom collections.abc import (\n    AsyncGenerator,\n    AsyncIterator,\n    Awaitable,\n    Coroutine,\n    Iterator,\n    Mapping,\n    Sequence,\n)\nfrom concurrent.futures import FIRST_COMPLETED, wait\nfrom contextvars import copy_context\nfrom functools import wraps\nfrom itertools import groupby, tee\nfrom operator import itemgetter\nfrom types import GenericAlias\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    Optional,\n    Protocol,\n    TypeVar,\n    Union,\n    cast,\n    get_type_hints,\n    overload,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, RootModel\nfrom typing_extensions import Literal, get_args, override\n\nfrom langchain_core._api import beta_decorator\nfrom langchain_core.load.serializable import (\n    Serializable,\n    SerializedConstructor,\n    SerializedNotImplemented,\n)\nfrom langchain_core.runnables.config import (\n    RunnableConfig,\n    _set_config_context,\n    acall_func_with_variable_args,\n    call_func_with_variable_args,\n    ensure_config,\n    get_async_callback_manager_for_config,\n    get_callback_manager_for_config,\n    get_config_list,\n    get_executor_for_config,\n    merge_configs,\n    patch_config,\n    run_in_executor,\n)\nfrom langchain_core.runnables.graph import Graph\nfrom langchain_core.runnables.utils import (\n    AddableDict,\n    AnyConfigurableField,\n    ConfigurableField,\n    ConfigurableFieldSpec,\n    Input,\n    Output,\n    accepts_config,\n    accepts_run_manager,\n    asyncio_accepts_context,\n    gated_coro,\n    gather_with_concurrency,\n    get_function_first_arg_dict_keys,\n    get_function_nonlocals,\n    get_lambda_source,\n    get_unique_config_specs,\n    indent_lines_after_first,\n    is_async_callable,\n    is_async_generator,\n)\nfrom langchain_core.utils.aiter import aclosing, atee, py_anext\nfrom langchain_core.utils.iter import safetee\nfrom langchain_core.utils.pydantic import create_model_v2\n\nif TYPE_CHECKING:\n    from langchain_core.callbacks.manager import (\n        AsyncCallbackManagerForChainRun,\n        CallbackManagerForChainRun,\n    )\n    from langchain_core.prompts.base import BasePromptTemplate\n    from langchain_core.runnables.fallbacks import (\n        RunnableWithFallbacks as RunnableWithFallbacksT,\n    )\n    from langchain_core.runnables.schema import StreamEvent\n    from langchain_core.tools import BaseTool\n    from langchain_core.tracers.log_stream import (\n        RunLog,\n        RunLogPatch,\n    )\n    from langchain_core.tracers.root_listeners import AsyncListener\n    from langchain_core.tracers.schemas import Run\n\n\nOther = TypeVar(\"Other\")\n\n\nclass Runnable(Generic[Input, Output], ABC):\n    \"\"\"A unit of work that can be invoked, batched, streamed, transformed and composed.\n\n    Key Methods\n    ===========\n\n    - **invoke/ainvoke**: Transforms a single input into an output.\n    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n    - **stream/astream**: Streams output from a single input as it's produced.\n    - **astream_log**: Streams output and selected intermediate results from an input.\n\n    Built-in optimizations:\n\n    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n      Override to optimize batching.\n\n    - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n      the sync counterpart using asyncio's thread pool.\n      Override for native async.\n\n    All methods accept an optional config argument, which can be used to configure\n    execution, add tags and metadata for tracing and debugging etc.\n\n    Runnables expose schematic information about their input, output and config via\n    the input_schema property, the output_schema property and config_schema method.\n\n    LCEL and Composition\n    ====================\n\n    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables\n    into chains. Any chain constructed this way will automatically have sync, async,\n    batch, and streaming support.\n\n    The main composition primitives are RunnableSequence and RunnableParallel.\n\n    **RunnableSequence** invokes a series of runnables sequentially, with\n    one Runnable's output serving as the next's input. Construct using\n    the `|` operator or by passing a list of runnables to RunnableSequence.\n\n    **RunnableParallel** invokes runnables concurrently, providing the same input\n    to each. Construct it using a dict literal within a sequence or by passing a\n    dict to RunnableParallel.\n\n\n    For example,\n\n    .. code-block:: python\n\n        from langchain_core.runnables import RunnableLambda\n\n        # A RunnableSequence constructed using the `|` operator\n        sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n        sequence.invoke(1) # 4\n        sequence.batch([1, 2, 3]) # [4, 6, 8]\n\n\n        # A sequence that contains a RunnableParallel constructed using a dict literal\n        sequence = RunnableLambda(lambda x: x + 1) | {\n            'mul_2': RunnableLambda(lambda x: x * 2),\n            'mul_5': RunnableLambda(lambda x: x * 5)\n        }\n        sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}\n\n    Standard Methods\n    ================\n\n    All Runnables expose additional methods that can be used to modify their behavior\n    (e.g., add a retry policy, add lifecycle listeners, make them configurable, etc.).\n\n    These methods will work on any Runnable, including Runnable chains constructed\n    by composing other Runnables. See the individual methods for details.\n\n    For example,\n\n    .. code-block:: python\n\n        from langchain_core.runnables import RunnableLambda\n\n        import random\n\n        def add_one(x: int) -> int:\n            return x + 1\n\n\n        def buggy_double(y: int) -> int:\n            \\\"\\\"\\\"Buggy code that will fail 70% of the time\\\"\\\"\\\"\n            if random.random() > 0.3:\n                print('This code failed, and will probably be retried!')  # noqa: T201\n                raise ValueError('Triggered buggy code')\n            return y * 2\n\n        sequence = (\n            RunnableLambda(add_one) |\n            RunnableLambda(buggy_double).with_retry( # Retry on failure\n                stop_after_attempt=10,\n                wait_exponential_jitter=False\n            )\n        )\n\n        print(sequence.input_schema.model_json_schema()) # Show inferred input schema\n        print(sequence.output_schema.model_json_schema()) # Show inferred output schema\n        print(sequence.invoke(2)) # invoke the sequence (note the retry above!!)\n\n    Debugging and tracing\n    =====================\n\n    As the chains get longer, it can be useful to be able to see intermediate results\n    to debug and trace the chain.\n\n    You can set the global debug flag to True to enable debug output for all chains:\n\n        .. code-block:: python\n\n            from langchain_core.globals import set_debug\n            set_debug(True)\n\n    Alternatively, you can pass existing or custom callbacks to any given chain:\n\n        .. code-block:: python\n\n            from langchain_core.tracers import ConsoleCallbackHandler\n\n            chain.invoke(\n                ...,\n                config={'callbacks': [ConsoleCallbackHandler()]}\n            )\n\n    For a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/\n    \"\"\"  # noqa: E501\n\n    name: Optional[str]\n    \"\"\"The name of the Runnable. Used for debugging and tracing.\"\"\"\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        \"\"\"Get the name of the Runnable.\"\"\"\n        if name:\n            name_ = name\n        elif hasattr(self, \"name\") and self.name:\n            name_ = self.name\n        else:\n            # Here we handle a case where the runnable subclass is also a pydantic\n            # model.\n            cls = self.__class__\n            # Then it's a pydantic sub-class, and we have to check\n            # whether it's a generic, and if so recover the original name.\n            if (\n                hasattr(\n                    cls,\n                    \"__pydantic_generic_metadata__\",\n                )\n                and \"origin\" in cls.__pydantic_generic_metadata__\n                and cls.__pydantic_generic_metadata__[\"origin\"] is not None\n            ):\n                name_ = cls.__pydantic_generic_metadata__[\"origin\"].__name__\n            else:\n                name_ = cls.__name__\n\n        if suffix:\n            if name_[0].isupper():\n                return name_ + suffix.title()\n            else:\n                return name_ + \"_\" + suffix.lower()\n        else:\n            return name_\n\n    @property\n    def InputType(self) -> type[Input]:  # noqa: N802\n        \"\"\"The type of input this Runnable accepts specified as a type annotation.\"\"\"\n        # First loop through all parent classes and if any of them is\n        # a pydantic model, we will pick up the generic parameterization\n        # from that model via the __pydantic_generic_metadata__ attribute.\n        for base in self.__class__.mro():\n            if hasattr(base, \"__pydantic_generic_metadata__\"):\n                metadata = base.__pydantic_generic_metadata__\n                if \"args\" in metadata and len(metadata[\"args\"]) == 2:\n                    return metadata[\"args\"][0]\n\n        # If we didn't find a pydantic model in the parent classes,\n        # then loop through __orig_bases__. This corresponds to\n        # Runnables that are not pydantic models.\n        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]\n            type_args = get_args(cls)\n            if type_args and len(type_args) == 2:\n                return type_args[0]\n\n        msg = (\n            f\"Runnable {self.get_name()} doesn't have an inferable InputType. \"\n            \"Override the InputType property to specify the input type.\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def OutputType(self) -> type[Output]:  # noqa: N802\n        \"\"\"The type of output this Runnable produces specified as a type annotation.\"\"\"\n        # First loop through bases -- this will help generic\n        # any pydantic models.\n        for base in self.__class__.mro():\n            if hasattr(base, \"__pydantic_generic_metadata__\"):\n                metadata = base.__pydantic_generic_metadata__\n                if \"args\" in metadata and len(metadata[\"args\"]) == 2:\n                    return metadata[\"args\"][1]\n\n        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]\n            type_args = get_args(cls)\n            if type_args and len(type_args) == 2:\n                return type_args[1]\n\n        msg = (\n            f\"Runnable {self.get_name()} doesn't have an inferable OutputType. \"\n            \"Override the OutputType property to specify the output type.\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def input_schema(self) -> type[BaseModel]:\n        \"\"\"The type of input this Runnable accepts specified as a pydantic model.\"\"\"\n        return self.get_input_schema()\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get a pydantic model that can be used to validate input to the Runnable.\n\n        Runnables that leverage the configurable_fields and configurable_alternatives\n        methods will have a dynamic input schema that depends on which\n        configuration the Runnable is invoked with.\n\n        This method allows to get an input schema for a specific configuration.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A pydantic model that can be used to validate input.\n        \"\"\"\n        root_type = self.InputType\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=root_type,\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    def get_input_jsonschema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the input to the Runnable.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A JSON schema that represents the input to the Runnable.\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                runnable = RunnableLambda(add_one)\n\n                print(runnable.get_input_jsonschema())\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.get_input_schema(config).model_json_schema()\n\n    @property\n    def output_schema(self) -> type[BaseModel]:\n        \"\"\"The type of output this Runnable produces specified as a pydantic model.\"\"\"\n        return self.get_output_schema()\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get a pydantic model that can be used to validate output to the Runnable.\n\n        Runnables that leverage the configurable_fields and configurable_alternatives\n        methods will have a dynamic output schema that depends on which\n        configuration the Runnable is invoked with.\n\n        This method allows to get an output schema for a specific configuration.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A pydantic model that can be used to validate output.\n        \"\"\"\n        root_type = self.OutputType\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    def get_output_jsonschema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the output of the Runnable.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A JSON schema that represents the output of the Runnable.\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                runnable = RunnableLambda(add_one)\n\n                print(runnable.get_output_jsonschema())\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.get_output_schema(config).model_json_schema()\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"List configurable fields for this Runnable.\"\"\"\n        return []\n\n    def config_schema(\n        self, *, include: Optional[Sequence[str]] = None\n    ) -> type[BaseModel]:\n        \"\"\"The type of config this Runnable accepts specified as a pydantic model.\n\n        To mark a field as configurable, see the `configurable_fields`\n        and `configurable_alternatives` methods.\n\n        Args:\n            include: A list of fields to include in the config schema.\n\n        Returns:\n            A pydantic model that can be used to validate config.\n        \"\"\"\n        include = include or []\n        config_specs = self.config_specs\n        configurable = (\n            create_model_v2(  # type: ignore[call-overload]\n                \"Configurable\",\n                field_definitions={\n                    spec.id: (\n                        spec.annotation,\n                        Field(\n                            spec.default, title=spec.name, description=spec.description\n                        ),\n                    )\n                    for spec in config_specs\n                },\n            )\n            if config_specs\n            else None\n        )\n\n        # Many need to create a typed dict instead to implement NotRequired!\n        all_fields = {\n            **({\"configurable\": (configurable, None)} if configurable else {}),\n            **{\n                field_name: (field_type, None)\n                for field_name, field_type in get_type_hints(RunnableConfig).items()\n                if field_name in [i for i in include if i != \"configurable\"]\n            },\n        }\n        model = create_model_v2(  # type: ignore[call-overload]\n            self.get_name(\"Config\"), field_definitions=all_fields\n        )\n        return model\n\n    def get_config_jsonschema(\n        self, *, include: Optional[Sequence[str]] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the config of the Runnable.\n\n        Args:\n            include: A list of fields to include in the config schema.\n\n        Returns:\n            A JSON schema that represents the config of the Runnable.\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.config_schema(include=include).model_json_schema()\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Return a graph representation of this Runnable.\"\"\"\n        graph = Graph()\n        try:\n            input_node = graph.add_node(self.get_input_schema(config))\n        except TypeError:\n            input_node = graph.add_node(create_model_v2(self.get_name(\"Input\")))\n        runnable_node = graph.add_node(\n            self, metadata=config.get(\"metadata\") if config else None\n        )\n        try:\n            output_node = graph.add_node(self.get_output_schema(config))\n        except TypeError:\n            output_node = graph.add_node(create_model_v2(self.get_name(\"Output\")))\n        graph.add_edge(input_node, runnable_node)\n        graph.add_edge(runnable_node, output_node)\n        return graph\n\n    def get_prompts(\n        self, config: Optional[RunnableConfig] = None\n    ) -> list[BasePromptTemplate]:\n        \"\"\"Return a list of prompts used by this Runnable.\"\"\"\n        from langchain_core.prompts.base import BasePromptTemplate\n\n        prompts = []\n        for _, node in self.get_graph(config=config).nodes.items():\n            if isinstance(node.data, BasePromptTemplate):\n                prompts.append(node.data)\n        return prompts\n\n    def __or__(\n        self,\n        other: Union[\n            Runnable[Any, Other],\n            Callable[[Any], Other],\n            Callable[[Iterator[Any]], Iterator[Other]],\n            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],\n        ],\n    ) -> RunnableSerializable[Input, Other]:\n        \"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\n        return RunnableSequence(self, coerce_to_runnable(other))\n\n    def __ror__(\n        self,\n        other: Union[\n            Runnable[Other, Any],\n            Callable[[Other], Any],\n            Callable[[Iterator[Other]], Iterator[Any]],\n            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],\n        ],\n    ) -> RunnableSerializable[Other, Output]:\n        \"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\n        return RunnableSequence(coerce_to_runnable(other), self)\n\n    def pipe(\n        self,\n        *others: Union[Runnable[Any, Other], Callable[[Any], Other]],\n        name: Optional[str] = None,\n    ) -> RunnableSerializable[Input, Other]:\n        \"\"\"Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n\n        Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n\n        Example:\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                def mul_two(x: int) -> int:\n                    return x * 2\n\n                runnable_1 = RunnableLambda(add_one)\n                runnable_2 = RunnableLambda(mul_two)\n                sequence = runnable_1.pipe(runnable_2)\n                # Or equivalently:\n                # sequence = runnable_1 | runnable_2\n                # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n                sequence.invoke(1)\n                await sequence.ainvoke(1)\n                # -> 4\n\n                sequence.batch([1, 2, 3])\n                await sequence.abatch([1, 2, 3])\n                # -> [4, 6, 8]\n        \"\"\"\n        return RunnableSequence(self, *others, name=name)\n\n    def pick(self, keys: Union[str, list[str]]) -> RunnableSerializable[Any, Any]:\n        \"\"\"Pick keys from the output dict of this Runnable.\n\n        Pick single key:\n            .. code-block:: python\n\n                import json\n\n                from langchain_core.runnables import RunnableLambda, RunnableMap\n\n                as_str = RunnableLambda(str)\n                as_json = RunnableLambda(json.loads)\n                chain = RunnableMap(str=as_str, json=as_json)\n\n                chain.invoke(\"[1, 2, 3]\")\n                # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n\n                json_only_chain = chain.pick(\"json\")\n                json_only_chain.invoke(\"[1, 2, 3]\")\n                # -> [1, 2, 3]\n\n        Pick list of keys:\n            .. code-block:: python\n\n                from typing import Any\n\n                import json\n\n                from langchain_core.runnables import RunnableLambda, RunnableMap\n\n                as_str = RunnableLambda(str)\n                as_json = RunnableLambda(json.loads)\n                def as_bytes(x: Any) -> bytes:\n                    return bytes(x, \"utf-8\")\n\n                chain = RunnableMap(\n                    str=as_str,\n                    json=as_json,\n                    bytes=RunnableLambda(as_bytes)\n                )\n\n                chain.invoke(\"[1, 2, 3]\")\n                # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n\n                json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n                json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n                # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n\n        \"\"\"\n        from langchain_core.runnables.passthrough import RunnablePick\n\n        return self | RunnablePick(keys)\n\n    def assign(\n        self,\n        **kwargs: Union[\n            Runnable[dict[str, Any], Any],\n            Callable[[dict[str, Any]], Any],\n            Mapping[\n                str,\n                Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]],\n            ],\n        ],\n    ) -> RunnableSerializable[Any, Any]:\n        \"\"\"Assigns new fields to the dict output of this Runnable.\n        Returns a new Runnable.\n\n        .. code-block:: python\n\n            from langchain_community.llms.fake import FakeStreamingListLLM\n            from langchain_core.output_parsers import StrOutputParser\n            from langchain_core.prompts import SystemMessagePromptTemplate\n            from langchain_core.runnables import Runnable\n            from operator import itemgetter\n\n            prompt = (\n                SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n                + \"{question}\"\n            )\n            llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n            chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n\n            chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n\n            print(chain_with_assign.input_schema.model_json_schema())\n            # {'title': 'PromptInput', 'type': 'object', 'properties':\n            {'question': {'title': 'Question', 'type': 'string'}}}\n            print(chain_with_assign.output_schema.model_json_schema())\n            # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n            {'str': {'title': 'Str',\n            'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n\n        \"\"\"\n        from langchain_core.runnables.passthrough import RunnableAssign\n\n        return self | RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))\n\n    \"\"\" --- Public API --- \"\"\"\n\n    @abstractmethod\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        \"\"\"Transform a single input into an output. Override to implement.\n\n        Args:\n            input: The input to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details.\n\n        Returns:\n            The output of the Runnable.\n        \"\"\"\n\n    async def ainvoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        \"\"\"Default implementation of ainvoke, calls invoke from a thread.\n\n        The default implementation allows usage of async code even if\n        the Runnable did not implement a native async version of invoke.\n\n        Subclasses should override this method if they can run asynchronously.\n        \"\"\"\n        return await run_in_executor(config, self.invoke, input, config, **kwargs)\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Default implementation runs invoke in parallel using a thread pool executor.\n\n        The default implementation of batch works well for IO bound runnables.\n\n        Subclasses should override this method if they can batch more efficiently;\n        e.g., if the underlying Runnable uses an API which supports a batch mode.\n        \"\"\"\n        if not inputs:\n            return []\n\n        configs = get_config_list(config, len(inputs))\n\n        def invoke(input: Input, config: RunnableConfig) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return self.invoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return self.invoke(input, config, **kwargs)\n\n        # If there's only one input, don't bother with the executor\n        if len(inputs) == 1:\n            return cast(list[Output], [invoke(inputs[0], configs[0])])\n\n        with get_executor_for_config(configs[0]) as executor:\n            return cast(list[Output], list(executor.map(invoke, inputs, configs)))\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Output]]: ...\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...\n\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]:\n        \"\"\"Run invoke in parallel on a list of inputs,\n        yielding results as they complete.\n        \"\"\"\n        if not inputs:\n            return\n\n        configs = get_config_list(config, len(inputs))\n\n        def invoke(\n            i: int, input: Input, config: RunnableConfig\n        ) -> tuple[int, Union[Output, Exception]]:\n            if return_exceptions:\n                try:\n                    out: Union[Output, Exception] = self.invoke(input, config, **kwargs)\n                except Exception as e:\n                    out = e\n            else:\n                out = self.invoke(input, config, **kwargs)\n\n            return (i, out)\n\n        if len(inputs) == 1:\n            yield invoke(0, inputs[0], configs[0])\n            return\n\n        with get_executor_for_config(configs[0]) as executor:\n            futures = {\n                executor.submit(invoke, i, input, config)\n                for i, (input, config) in enumerate(zip(inputs, configs))\n            }\n\n            try:\n                while futures:\n                    done, futures = wait(futures, return_when=FIRST_COMPLETED)\n                    while done:\n                        yield done.pop().result()\n            finally:\n                for future in futures:\n                    future.cancel()\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Default implementation runs ainvoke in parallel using asyncio.gather.\n\n        The default implementation of batch works well for IO bound runnables.\n\n        Subclasses should override this method if they can batch more efficiently;\n        e.g., if the underlying Runnable uses an API which supports a batch mode.\n\n        Args:\n            inputs: A list of inputs to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details. Defaults to None.\n            return_exceptions: Whether to return exceptions instead of raising them.\n                Defaults to False.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Returns:\n            A list of outputs from the Runnable.\n        \"\"\"\n        if not inputs:\n            return []\n\n        configs = get_config_list(config, len(inputs))\n\n        async def ainvoke(\n            input: Input, config: RunnableConfig\n        ) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return await self.ainvoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return await self.ainvoke(input, config, **kwargs)\n\n        coros = map(ainvoke, inputs, configs)\n        return await gather_with_concurrency(configs[0].get(\"max_concurrency\"), *coros)\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Output]]: ...\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...\n\n    async def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:\n        \"\"\"Run ainvoke in parallel on a list of inputs,\n        yielding results as they complete.\n\n        Args:\n            inputs: A list of inputs to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details. Defaults to None. Defaults to None.\n            return_exceptions: Whether to return exceptions instead of raising them.\n                Defaults to False.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            A tuple of the index of the input and the output from the Runnable.\n        \"\"\"\n        if not inputs:\n            return\n\n        configs = get_config_list(config, len(inputs))\n        # Get max_concurrency from first config, defaulting to None (unlimited)\n        max_concurrency = configs[0].get(\"max_concurrency\") if configs else None\n        semaphore = asyncio.Semaphore(max_concurrency) if max_concurrency else None\n\n        async def ainvoke_task(\n            i: int, input: Input, config: RunnableConfig\n        ) -> tuple[int, Union[Output, Exception]]:\n            if return_exceptions:\n                try:\n                    out: Union[Output, Exception] = await self.ainvoke(\n                        input, config, **kwargs\n                    )\n                except Exception as e:\n                    out = e\n            else:\n                out = await self.ainvoke(input, config, **kwargs)\n            return (i, out)\n\n        coros = [\n            gated_coro(semaphore, ainvoke_task(i, input, config))\n            if semaphore\n            else ainvoke_task(i, input, config)\n            for i, (input, config) in enumerate(zip(inputs, configs))\n        ]\n\n        for coro in asyncio.as_completed(coros):\n            yield await coro\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Default implementation of stream, which calls invoke.\n        Subclasses should override this method if they support streaming output.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        yield self.invoke(input, config, **kwargs)\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Default implementation of astream, which calls ainvoke.\n        Subclasses should override this method if they support streaming output.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        yield await self.ainvoke(input, config, **kwargs)\n\n    @overload\n    def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: Literal[True] = True,\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[RunLogPatch]: ...\n\n    @overload\n    def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: Literal[False],\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[RunLog]: ...\n\n    async def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: bool = True,\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]:\n        \"\"\"Stream all output from a Runnable, as reported to the callback system.\n        This includes all inner runs of LLMs, Retrievers, Tools, etc.\n\n        Output is streamed as Log objects, which include a list of\n        Jsonpatch ops that describe how the state of the run has changed in each\n        step, and the final state of the run.\n\n        The Jsonpatch ops can be applied in order to construct state.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable.\n            diff: Whether to yield diffs between each step or the current state.\n            with_streamed_output_list: Whether to yield the streamed_output list.\n            include_names: Only include logs with these names.\n            include_types: Only include logs with these types.\n            include_tags: Only include logs with these tags.\n            exclude_names: Exclude logs with these names.\n            exclude_types: Exclude logs with these types.\n            exclude_tags: Exclude logs with these tags.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            A RunLogPatch or RunLog object.\n        \"\"\"\n        from langchain_core.tracers.log_stream import (\n            LogStreamCallbackHandler,\n            _astream_log_implementation,\n        )\n\n        stream = LogStreamCallbackHandler(\n            auto_close=False,\n            include_names=include_names,\n            include_types=include_types,\n            include_tags=include_tags,\n            exclude_names=exclude_names,\n            exclude_types=exclude_types,\n            exclude_tags=exclude_tags,\n            _schema_format=\"original\",\n        )\n\n        # Mypy isn't resolving the overloads here\n        # Likely an issue b/c `self` is being passed through\n        # and it's can't map it to Runnable[Input,Output]?\n        async for item in _astream_log_implementation(  # type: ignore\n            self,\n            input,\n            config,\n            diff=diff,\n            stream=stream,\n            with_streamed_output_list=with_streamed_output_list,\n            **kwargs,\n        ):\n            yield item\n\n    async def astream_events(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        version: Literal[\"v1\", \"v2\"] = \"v2\",\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[StreamEvent]:\n        \"\"\"Generate a stream of events.\n\n        Use to create an iterator over StreamEvents that provide real-time information\n        about the progress of the Runnable, including StreamEvents from intermediate\n        results.\n\n        A StreamEvent is a dictionary with the following schema:\n\n        - ``event``: **str** - Event names are of the\n            format: on_[runnable_type]_(start|stream|end).\n        - ``name``: **str** - The name of the Runnable that generated the event.\n        - ``run_id``: **str** - randomly generated ID associated with the given execution of\n            the Runnable that emitted the event.\n            A child Runnable that gets invoked as part of the execution of a\n            parent Runnable is assigned its own unique ID.\n        - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n            generated the event. The root Runnable will have an empty list.\n            The order of the parent IDs is from the root to the immediate parent.\n            Only available for v2 version of the API. The v1 version of the API\n            will return an empty list.\n        - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n            the event.\n        - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n            that generated the event.\n        - ``data``: **Dict[str, Any]**\n\n\n        Below is a table that illustrates some events that might be emitted by various\n        chains. Metadata fields have been omitted from the table for brevity.\n        Chain definitions have been included after the table.\n\n        **ATTENTION** This reference table is for the V2 version of the schema.\n\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | event                | name             | chunk                           | input                                         | output                                          |\n        +======================+==================+=================================+===============================================+=================================================+\n        | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n\n        In addition to the standard events, users can also dispatch custom events (see example below).\n\n        Custom events will be only be surfaced with in the `v2` version of the API!\n\n        A custom event has following format:\n\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n        | Attribute | Type | Description                                                                                               |\n        +===========+======+===========================================================================================================+\n        | name      | str  | A user defined name for the event.                                                                        |\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n        | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n\n        Here are declarations associated with the standard events shown above:\n\n        `format_docs`:\n\n        .. code-block:: python\n\n            def format_docs(docs: List[Document]) -> str:\n                '''Format the docs.'''\n                return \", \".join([doc.page_content for doc in docs])\n\n            format_docs = RunnableLambda(format_docs)\n\n        `some_tool`:\n\n        .. code-block:: python\n\n            @tool\n            def some_tool(x: int, y: str) -> dict:\n                '''Some_tool.'''\n                return {\"x\": x, \"y\": y}\n\n        `prompt`:\n\n        .. code-block:: python\n\n            template = ChatPromptTemplate.from_messages(\n                [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n            ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            async def reverse(s: str) -> str:\n                return s[::-1]\n\n            chain = RunnableLambda(func=reverse)\n\n            events = [\n                event async for event in chain.astream_events(\"hello\", version=\"v2\")\n            ]\n\n            # will produce the following events (run_id, and parent_ids\n            # has been omitted for brevity):\n            [\n                {\n                    \"data\": {\"input\": \"hello\"},\n                    \"event\": \"on_chain_start\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"chunk\": \"olleh\"},\n                    \"event\": \"on_chain_stream\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"output\": \"olleh\"},\n                    \"event\": \"on_chain_end\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n            ]\n\n\n        Example: Dispatch Custom Event\n\n        .. code-block:: python\n\n            from langchain_core.callbacks.manager import (\n                adispatch_custom_event,\n            )\n            from langchain_core.runnables import RunnableLambda, RunnableConfig\n            import asyncio\n\n\n            async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n                \\\"\\\"\\\"Do something that takes a long time.\\\"\\\"\\\"\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                await adispatch_custom_event(\n                    \"progress_event\",\n                    {\"message\": \"Finished step 1 of 3\"},\n                    config=config # Must be included for python < 3.10\n                )\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                await adispatch_custom_event(\n                    \"progress_event\",\n                    {\"message\": \"Finished step 2 of 3\"},\n                    config=config # Must be included for python < 3.10\n                )\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                return \"Done\"\n\n            slow_thing = RunnableLambda(slow_thing)\n\n            async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n                print(event)\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable.\n            version: The version of the schema to use either `v2` or `v1`.\n                     Users should use `v2`.\n                     `v1` is for backwards compatibility and will be deprecated\n                     in 0.4.0.\n                     No default will be assigned until the API is stabilized.\n                     custom events will only be surfaced in `v2`.\n            include_names: Only include events from runnables with matching names.\n            include_types: Only include events from runnables with matching types.\n            include_tags: Only include events from runnables with matching tags.\n            exclude_names: Exclude events from runnables with matching names.\n            exclude_types: Exclude events from runnables with matching types.\n            exclude_tags: Exclude events from runnables with matching tags.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n                These will be passed to astream_log as this implementation\n                of astream_events is built on top of astream_log.\n\n        Yields:\n            An async stream of StreamEvents.\n\n        Raises:\n            NotImplementedError: If the version is not `v1` or `v2`.\n        \"\"\"  # noqa: E501\n        from langchain_core.tracers.event_stream import (\n            _astream_events_implementation_v1,\n            _astream_events_implementation_v2,\n        )\n\n        if version == \"v2\":\n            event_stream = _astream_events_implementation_v2(\n                self,\n                input,\n                config=config,\n                include_names=include_names,\n                include_types=include_types,\n                include_tags=include_tags,\n                exclude_names=exclude_names,\n                exclude_types=exclude_types,\n                exclude_tags=exclude_tags,\n                **kwargs,\n            )\n        elif version == \"v1\":\n            # First implementation, built on top of astream_log API\n            # This implementation will be deprecated as of 0.2.0\n            event_stream = _astream_events_implementation_v1(\n                self,\n                input,\n                config=config,\n                include_names=include_names,\n                include_types=include_types,\n                include_tags=include_tags,\n                exclude_names=exclude_names,\n                exclude_types=exclude_types,\n                exclude_tags=exclude_tags,\n                **kwargs,\n            )\n        else:\n            msg = 'Only versions \"v1\" and \"v2\" of the schema is currently supported.'\n            raise NotImplementedError(msg)\n\n        async with aclosing(event_stream):\n            async for event in event_stream:\n                yield event\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Default implementation of transform, which buffers input and calls astream.\n\n        Subclasses should override this method if they can start producing output while\n        input is still being generated.\n\n        Args:\n            input: An iterator of inputs to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        final: Input\n        got_first_val = False\n\n        for ichunk in input:\n            # The default implementation of transform is to buffer input and\n            # then call stream.\n            # It'll attempt to gather all input into a single chunk using\n            # the `+` operator.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk,\n            # and we'll iterate until we get to the last chunk.\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if got_first_val:\n            yield from self.stream(final, config, **kwargs)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Default implementation of atransform, which buffers input and calls astream.\n        Subclasses should override this method if they can start producing output while\n        input is still being generated.\n\n        Args:\n            input: An async iterator of inputs to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        final: Input\n        got_first_val = False\n\n        async for ichunk in input:\n            # The default implementation of transform is to buffer input and\n            # then call stream.\n            # It'll attempt to gather all input into a single chunk using\n            # the `+` operator.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk,\n            # and we'll iterate until we get to the last chunk.\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if got_first_val:\n            async for output in self.astream(final, config, **kwargs):\n                yield output\n\n    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:\n        \"\"\"Bind arguments to a Runnable, returning a new Runnable.\n\n        Useful when a Runnable in a chain requires an argument that is not\n        in the output of the previous Runnable or included in the user input.\n\n        Args:\n            kwargs: The arguments to bind to the Runnable.\n\n        Returns:\n            A new Runnable with the arguments bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_community.chat_models import ChatOllama\n            from langchain_core.output_parsers import StrOutputParser\n\n            llm = ChatOllama(model='llama2')\n\n            # Without bind.\n            chain = (\n                llm\n                | StrOutputParser()\n            )\n\n            chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n            # Output is 'One two three four five.'\n\n            # With bind.\n            chain = (\n                llm.bind(stop=[\"three\"])\n                | StrOutputParser()\n            )\n\n            chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n            # Output is 'One two'\n\n        \"\"\"\n        return RunnableBinding(bound=self, kwargs=kwargs, config={})\n\n    def with_config(\n        self,\n        config: Optional[RunnableConfig] = None,\n        # Sadly Unpack is not well-supported by mypy so this will have to be untyped\n        **kwargs: Any,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind config to a Runnable, returning a new Runnable.\n\n        Args:\n            config: The config to bind to the Runnable.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Returns:\n            A new Runnable with the config bound.\n        \"\"\"\n        return RunnableBinding(\n            bound=self,\n            config=cast(\n                RunnableConfig,\n                {**(config or {}), **kwargs},\n            ),  # type: ignore[misc]\n            kwargs={},\n        )\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        on_start: Called before the Runnable starts running, with the Run object.\n        on_end: Called after the Runnable finishes running, with the Run object.\n        on_error: Called if the Runnable throws an error, with the Run object.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n\n        Args:\n            on_start: Called before the Runnable starts running. Defaults to None.\n            on_end: Called after the Runnable finishes running. Defaults to None.\n            on_error: Called if the Runnable throws an error. Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n            from langchain_core.tracers.schemas import Run\n\n            import time\n\n            def test_runnable(time_to_sleep : int):\n                time.sleep(time_to_sleep)\n\n            def fn_start(run_obj: Run):\n                print(\"start_time:\", run_obj.start_time)\n\n            def fn_end(run_obj: Run):\n                print(\"end_time:\", run_obj.end_time)\n\n            chain = RunnableLambda(test_runnable).with_listeners(\n                on_start=fn_start,\n                on_end=fn_end\n            )\n            chain.invoke(2)\n        \"\"\"\n        from langchain_core.tracers.root_listeners import RootListenersTracer\n\n        return RunnableBinding(\n            bound=self,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        RootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n        )\n\n    def with_alisteners(\n        self,\n        *,\n        on_start: Optional[AsyncListener] = None,\n        on_end: Optional[AsyncListener] = None,\n        on_error: Optional[AsyncListener] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n\n        on_start: Asynchronously called before the Runnable starts running.\n        on_end: Asynchronously called after the Runnable finishes running.\n        on_error: Asynchronously called if the Runnable throws an error.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n\n        Args:\n            on_start: Asynchronously called before the Runnable starts running.\n                Defaults to None.\n            on_end: Asynchronously called after the Runnable finishes running.\n                Defaults to None.\n            on_error: Asynchronously called if the Runnable throws an error.\n                Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n            import time\n\n            async def test_runnable(time_to_sleep : int):\n                print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n                await asyncio.sleep(time_to_sleep)\n                print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n\n            async def fn_start(run_obj : Runnable):\n                print(f\"on start callback starts at {format_t(time.time())}\n                await asyncio.sleep(3)\n                print(f\"on start callback ends at {format_t(time.time())}\")\n\n            async def fn_end(run_obj : Runnable):\n                print(f\"on end callback starts at {format_t(time.time())}\n                await asyncio.sleep(2)\n                print(f\"on end callback ends at {format_t(time.time())}\")\n\n            runnable = RunnableLambda(test_runnable).with_alisteners(\n                on_start=fn_start,\n                on_end=fn_end\n            )\n            async def concurrent_runs():\n                await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n\n            asyncio.run(concurrent_runs())\n            Result:\n            on start callback starts at 2024-05-16T14:20:29.637053+00:00\n            on start callback starts at 2024-05-16T14:20:29.637150+00:00\n            on start callback ends at 2024-05-16T14:20:32.638305+00:00\n            on start callback ends at 2024-05-16T14:20:32.638383+00:00\n            Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n            Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n            Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n            on end callback starts at 2024-05-16T14:20:35.640534+00:00\n            Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n            on end callback starts at 2024-05-16T14:20:37.640574+00:00\n            on end callback ends at 2024-05-16T14:20:37.640654+00:00\n            on end callback ends at 2024-05-16T14:20:39.641751+00:00\n\n        \"\"\"\n        from langchain_core.tracers.root_listeners import AsyncRootListenersTracer\n\n        return RunnableBinding(\n            bound=self,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        AsyncRootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n        )\n\n    def with_types(\n        self,\n        *,\n        input_type: Optional[type[Input]] = None,\n        output_type: Optional[type[Output]] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind input and output types to a Runnable, returning a new Runnable.\n\n        Args:\n            input_type: The input type to bind to the Runnable. Defaults to None.\n            output_type: The output type to bind to the Runnable. Defaults to None.\n\n        Returns:\n            A new Runnable with the types bound.\n        \"\"\"\n        return RunnableBinding(\n            bound=self,\n            custom_input_type=input_type,\n            custom_output_type=output_type,\n            kwargs={},\n        )\n\n    def with_retry(\n        self,\n        *,\n        retry_if_exception_type: tuple[type[BaseException], ...] = (Exception,),\n        wait_exponential_jitter: bool = True,\n        stop_after_attempt: int = 3,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Create a new Runnable that retries the original Runnable on exceptions.\n\n        Args:\n            retry_if_exception_type: A tuple of exception types to retry on.\n                Defaults to (Exception,).\n            wait_exponential_jitter: Whether to add jitter to the wait\n                time between retries. Defaults to True.\n            stop_after_attempt: The maximum number of attempts to make before\n                giving up. Defaults to 3.\n\n        Returns:\n            A new Runnable that retries the original Runnable on exceptions.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            count = 0\n\n\n            def _lambda(x: int) -> None:\n                global count\n                count = count + 1\n                if x == 1:\n                    raise ValueError(\"x is 1\")\n                else:\n                     pass\n\n\n            runnable = RunnableLambda(_lambda)\n            try:\n                runnable.with_retry(\n                    stop_after_attempt=2,\n                    retry_if_exception_type=(ValueError,),\n                ).invoke(1)\n            except ValueError:\n                pass\n\n            assert (count == 2)\n\n\n        Args:\n            retry_if_exception_type: A tuple of exception types to retry on\n            wait_exponential_jitter: Whether to add jitter to the wait time\n                                     between retries\n            stop_after_attempt: The maximum number of attempts to make before giving up\n\n        Returns:\n            A new Runnable that retries the original Runnable on exceptions.\n        \"\"\"\n        from langchain_core.runnables.retry import RunnableRetry\n\n        return RunnableRetry(\n            bound=self,\n            kwargs={},\n            config={},\n            retry_exception_types=retry_if_exception_type,\n            wait_exponential_jitter=wait_exponential_jitter,\n            max_attempt_number=stop_after_attempt,\n        )\n\n    def map(self) -> Runnable[list[Input], list[Output]]:\n        \"\"\"Return a new Runnable that maps a list of inputs to a list of outputs,\n        by calling invoke() with each input.\n\n        Returns:\n            A new Runnable that maps a list of inputs to a list of outputs.\n\n        Example:\n\n            .. code-block:: python\n\n                    from langchain_core.runnables import RunnableLambda\n\n                    def _lambda(x: int) -> int:\n                        return x + 1\n\n                    runnable = RunnableLambda(_lambda)\n                    print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n        \"\"\"\n        return RunnableEach(bound=self)\n\n    def with_fallbacks(\n        self,\n        fallbacks: Sequence[Runnable[Input, Output]],\n        *,\n        exceptions_to_handle: tuple[type[BaseException], ...] = (Exception,),\n        exception_key: Optional[str] = None,\n    ) -> RunnableWithFallbacksT[Input, Output]:\n        \"\"\"Add fallbacks to a Runnable, returning a new Runnable.\n\n        The new Runnable will try the original Runnable, and then each fallback\n        in order, upon failures.\n\n        Args:\n            fallbacks: A sequence of runnables to try if the original Runnable fails.\n            exceptions_to_handle: A tuple of exception types to handle.\n                Defaults to (Exception,).\n            exception_key: If string is specified then handled exceptions will be passed\n                to fallbacks as part of the input under the specified key. If None,\n                exceptions will not be passed to fallbacks. If used, the base Runnable\n                and its fallbacks must accept a dictionary as input. Defaults to None.\n\n        Returns:\n            A new Runnable that will try the original Runnable, and then each\n            fallback in order, upon failures.\n\n        Example:\n\n            .. code-block:: python\n\n                from typing import Iterator\n\n                from langchain_core.runnables import RunnableGenerator\n\n\n                def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n                    raise ValueError()\n                    yield \"\"\n\n\n                def _generate(input: Iterator) -> Iterator[str]:\n                    yield from \"foo bar\"\n\n\n                runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n                    [RunnableGenerator(_generate)]\n                    )\n                print(''.join(runnable.stream({}))) #foo bar\n\n        Args:\n            fallbacks: A sequence of runnables to try if the original Runnable fails.\n            exceptions_to_handle: A tuple of exception types to handle.\n            exception_key: If string is specified then handled exceptions will be passed\n                to fallbacks as part of the input under the specified key. If None,\n                exceptions will not be passed to fallbacks. If used, the base Runnable\n                and its fallbacks must accept a dictionary as input.\n\n        Returns:\n            A new Runnable that will try the original Runnable, and then each\n            fallback in order, upon failures.\n\n        \"\"\"\n        from langchain_core.runnables.fallbacks import RunnableWithFallbacks\n\n        return RunnableWithFallbacks(\n            runnable=self,\n            fallbacks=fallbacks,\n            exceptions_to_handle=exceptions_to_handle,\n            exception_key=exception_key,\n        )\n\n    \"\"\" --- Helper methods for Subclasses --- \"\"\"\n\n    def _call_with_config(\n        self,\n        func: Union[\n            Callable[[Input], Output],\n            Callable[[Input, CallbackManagerForChainRun], Output],\n            Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\n        ],\n        input: Input,\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        serialized: Optional[dict[str, Any]] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        config = ensure_config(config)\n        callback_manager = get_callback_manager_for_config(config)\n        run_manager = callback_manager.on_chain_start(\n            serialized,\n            input,\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            output = cast(\n                Output,\n                context.run(\n                    call_func_with_variable_args,  # type: ignore[arg-type]\n                    func,  # type: ignore[arg-type]\n                    input,  # type: ignore[arg-type]\n                    config,\n                    run_manager,\n                    **kwargs,\n                ),\n            )\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(output)\n            return output\n\n    async def _acall_with_config(\n        self,\n        func: Union[\n            Callable[[Input], Awaitable[Output]],\n            Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n            Callable[\n                [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                Awaitable[Output],\n            ],\n        ],\n        input: Input,\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        serialized: Optional[dict[str, Any]] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement ainvoke() in subclasses.\n        \"\"\"\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        run_manager = await callback_manager.on_chain_start(\n            serialized,\n            input,\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            coro = acall_func_with_variable_args(\n                func, input, config, run_manager, **kwargs\n            )\n            if asyncio_accepts_context():\n                output: Output = await asyncio.create_task(coro, context=context)  # type: ignore\n            else:\n                output = await coro\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(output)\n            return output\n\n    def _batch_with_config(\n        self,\n        func: Union[\n            Callable[[list[Input]], list[Union[Exception, Output]]],\n            Callable[\n                [list[Input], list[CallbackManagerForChainRun]],\n                list[Union[Exception, Output]],\n            ],\n            Callable[\n                [list[Input], list[CallbackManagerForChainRun], list[RunnableConfig]],\n                list[Union[Exception, Output]],\n            ],\n        ],\n        input: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        if not input:\n            return []\n\n        configs = get_config_list(config, len(input))\n        callback_managers = [get_callback_manager_for_config(c) for c in configs]\n        run_managers = [\n            callback_manager.on_chain_start(\n                None,\n                input,\n                run_type=run_type,\n                name=config.get(\"run_name\") or self.get_name(),\n                run_id=config.pop(\"run_id\", None),\n            )\n            for callback_manager, input, config in zip(\n                callback_managers, input, configs\n            )\n        ]\n        try:\n            if accepts_config(func):\n                kwargs[\"config\"] = [\n                    patch_config(c, callbacks=rm.get_child())\n                    for c, rm in zip(configs, run_managers)\n                ]\n            if accepts_run_manager(func):\n                kwargs[\"run_manager\"] = run_managers\n            output = func(input, **kwargs)  # type: ignore[call-arg]\n        except BaseException as e:\n            for run_manager in run_managers:\n                run_manager.on_chain_error(e)\n            if return_exceptions:\n                return cast(list[Output], [e for _ in input])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            for run_manager, out in zip(run_managers, output):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    run_manager.on_chain_error(out)\n                else:\n                    run_manager.on_chain_end(out)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], output)\n            else:\n                raise first_exception\n\n    async def _abatch_with_config(\n        self,\n        func: Union[\n            Callable[[list[Input]], Awaitable[list[Union[Exception, Output]]]],\n            Callable[\n                [list[Input], list[AsyncCallbackManagerForChainRun]],\n                Awaitable[list[Union[Exception, Output]]],\n            ],\n            Callable[\n                [\n                    list[Input],\n                    list[AsyncCallbackManagerForChainRun],\n                    list[RunnableConfig],\n                ],\n                Awaitable[list[Union[Exception, Output]]],\n            ],\n        ],\n        input: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        if not input:\n            return []\n\n        configs = get_config_list(config, len(input))\n        callback_managers = [get_async_callback_manager_for_config(c) for c in configs]\n        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(\n            *(\n                callback_manager.on_chain_start(\n                    None,\n                    input,\n                    run_type=run_type,\n                    name=config.get(\"run_name\") or self.get_name(),\n                    run_id=config.pop(\"run_id\", None),\n                )\n                for callback_manager, input, config in zip(\n                    callback_managers, input, configs\n                )\n            )\n        )\n        try:\n            if accepts_config(func):\n                kwargs[\"config\"] = [\n                    patch_config(c, callbacks=rm.get_child())\n                    for c, rm in zip(configs, run_managers)\n                ]\n            if accepts_run_manager(func):\n                kwargs[\"run_manager\"] = run_managers\n            output = await func(input, **kwargs)  # type: ignore[call-arg]\n        except BaseException as e:\n            await asyncio.gather(\n                *(run_manager.on_chain_error(e) for run_manager in run_managers)\n            )\n            if return_exceptions:\n                return cast(list[Output], [e for _ in input])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            coros: list[Awaitable[None]] = []\n            for run_manager, out in zip(run_managers, output):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    coros.append(run_manager.on_chain_error(out))\n                else:\n                    coros.append(run_manager.on_chain_end(out))\n            await asyncio.gather(*coros)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], output)\n            else:\n                raise first_exception\n\n    def _transform_stream_with_config(\n        self,\n        input: Iterator[Input],\n        transformer: Union[\n            Callable[[Iterator[Input]], Iterator[Output]],\n            Callable[[Iterator[Input], CallbackManagerForChainRun], Iterator[Output]],\n            Callable[\n                [\n                    Iterator[Input],\n                    CallbackManagerForChainRun,\n                    RunnableConfig,\n                ],\n                Iterator[Output],\n            ],\n        ],\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Helper method to transform an Iterator of Input values into an Iterator of\n        Output values, with callbacks.\n        Use this to implement `stream()` or `transform()` in Runnable subclasses.\n        \"\"\"\n        # Mixin that is used by both astream log and astream events implementation\n        from langchain_core.tracers._streaming import _StreamingCallbackHandler\n\n        # tee the input so we can iterate over it twice\n        input_for_tracing, input_for_transform = tee(input, 2)\n        # Start the input iterator to ensure the input Runnable starts before this one\n        final_input: Optional[Input] = next(input_for_tracing, None)\n        final_input_supported = True\n        final_output: Optional[Output] = None\n        final_output_supported = True\n\n        config = ensure_config(config)\n        callback_manager = get_callback_manager_for_config(config)\n        run_manager = callback_manager.on_chain_start(\n            None,\n            {\"input\": \"\"},\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            if accepts_config(transformer):\n                kwargs[\"config\"] = child_config\n            if accepts_run_manager(transformer):\n                kwargs[\"run_manager\"] = run_manager\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            iterator = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]\n            if stream_handler := next(\n                (\n                    cast(_StreamingCallbackHandler, h)\n                    for h in run_manager.handlers\n                    # instance check OK here, it's a mixin\n                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]\n                ),\n                None,\n            ):\n                # populates streamed_output in astream_log() output if needed\n                iterator = stream_handler.tap_output_iter(run_manager.run_id, iterator)\n            try:\n                while True:\n                    chunk: Output = context.run(next, iterator)  # type: ignore\n                    yield chunk\n                    if final_output_supported:\n                        if final_output is None:\n                            final_output = chunk\n                        else:\n                            try:\n                                final_output = final_output + chunk  # type: ignore\n                            except TypeError:\n                                final_output = chunk\n                                final_output_supported = False\n                    else:\n                        final_output = chunk\n            except (StopIteration, GeneratorExit):\n                pass\n            for ichunk in input_for_tracing:\n                if final_input_supported:\n                    if final_input is None:\n                        final_input = ichunk\n                    else:\n                        try:\n                            final_input = final_input + ichunk  # type: ignore\n                        except TypeError:\n                            final_input = ichunk\n                            final_input_supported = False\n                else:\n                    final_input = ichunk\n        except BaseException as e:\n            run_manager.on_chain_error(e, inputs=final_input)\n            raise\n        else:\n            run_manager.on_chain_end(final_output, inputs=final_input)\n\n    async def _atransform_stream_with_config(\n        self,\n        input: AsyncIterator[Input],\n        transformer: Union[\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n            Callable[\n                [AsyncIterator[Input], AsyncCallbackManagerForChainRun],\n                AsyncIterator[Output],\n            ],\n            Callable[\n                [\n                    AsyncIterator[Input],\n                    AsyncCallbackManagerForChainRun,\n                    RunnableConfig,\n                ],\n                AsyncIterator[Output],\n            ],\n        ],\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Helper method to transform an Async Iterator of Input values into an Async\n        Iterator of Output values, with callbacks.\n        Use this to implement `astream()` or `atransform()` in Runnable subclasses.\n        \"\"\"\n        # Mixin that is used by both astream log and astream events implementation\n        from langchain_core.tracers._streaming import _StreamingCallbackHandler\n\n        # tee the input so we can iterate over it twice\n        input_for_tracing, input_for_transform = atee(input, 2)\n        # Start the input iterator to ensure the input Runnable starts before this one\n        final_input: Optional[Input] = await py_anext(input_for_tracing, None)\n        final_input_supported = True\n        final_output: Optional[Output] = None\n        final_output_supported = True\n\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            {\"input\": \"\"},\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            if accepts_config(transformer):\n                kwargs[\"config\"] = child_config\n            if accepts_run_manager(transformer):\n                kwargs[\"run_manager\"] = run_manager\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            iterator_ = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]\n\n            if stream_handler := next(\n                (\n                    cast(_StreamingCallbackHandler, h)\n                    for h in run_manager.handlers\n                    # instance check OK here, it's a mixin\n                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]\n                ),\n                None,\n            ):\n                # populates streamed_output in astream_log() output if needed\n                iterator = stream_handler.tap_output_aiter(\n                    run_manager.run_id, iterator_\n                )\n            else:\n                iterator = iterator_\n            try:\n                while True:\n                    if asyncio_accepts_context():\n                        chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]\n                            py_anext(iterator),  # type: ignore[arg-type]\n                            context=context,\n                        )\n                    else:\n                        chunk = cast(Output, await py_anext(iterator))\n                    yield chunk\n                    if final_output_supported:\n                        if final_output is None:\n                            final_output = chunk\n                        else:\n                            try:\n                                final_output = final_output + chunk  # type: ignore\n                            except TypeError:\n                                final_output = chunk\n                                final_output_supported = False\n                    else:\n                        final_output = chunk\n            except StopAsyncIteration:\n                pass\n            async for ichunk in input_for_tracing:\n                if final_input_supported:\n                    if final_input is None:\n                        final_input = ichunk\n                    else:\n                        try:\n                            final_input = final_input + ichunk  # type: ignore[operator]\n                        except TypeError:\n                            final_input = ichunk\n                            final_input_supported = False\n                else:\n                    final_input = ichunk\n        except BaseException as e:\n            await run_manager.on_chain_error(e, inputs=final_input)\n            raise\n        else:\n            await run_manager.on_chain_end(final_output, inputs=final_input)\n        finally:\n            if iterator_ is not None and hasattr(iterator_, \"aclose\"):\n                await iterator_.aclose()\n\n    @beta_decorator.beta(message=\"This API is in beta and may change in the future.\")\n    def as_tool(\n        self,\n        args_schema: Optional[type[BaseModel]] = None,\n        *,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        arg_types: Optional[dict[str, type]] = None,\n    ) -> BaseTool:\n        \"\"\"Create a BaseTool from a Runnable.\n\n        ``as_tool`` will instantiate a BaseTool with a name, description, and\n        ``args_schema`` from a Runnable. Where possible, schemas are inferred\n        from ``runnable.get_input_schema``. Alternatively (e.g., if the\n        Runnable takes a dict as input and the specific dict keys are not typed),\n        the schema can be specified directly with ``args_schema``. You can also\n        pass ``arg_types`` to just specify the required arguments and their types.\n\n        Args:\n            args_schema: The schema for the tool. Defaults to None.\n            name: The name of the tool. Defaults to None.\n            description: The description of the tool. Defaults to None.\n            arg_types: A dictionary of argument names to types. Defaults to None.\n\n        Returns:\n            A BaseTool instance.\n\n        Typed dict input:\n\n        .. code-block:: python\n\n            from typing import List\n            from typing_extensions import TypedDict\n            from langchain_core.runnables import RunnableLambda\n\n            class Args(TypedDict):\n                a: int\n                b: List[int]\n\n            def f(x: Args) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool()\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        ``dict`` input, specifying schema via ``args_schema``:\n\n        .. code-block:: python\n\n            from typing import Any, Dict, List\n            from pydantic import BaseModel, Field\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: Dict[str, Any]) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            class FSchema(BaseModel):\n                \\\"\\\"\\\"Apply a function to an integer and list of integers.\\\"\\\"\\\"\n\n                a: int = Field(..., description=\"Integer\")\n                b: List[int] = Field(..., description=\"List of ints\")\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool(FSchema)\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        ``dict`` input, specifying schema via ``arg_types``:\n\n        .. code-block:: python\n\n            from typing import Any, Dict, List\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: Dict[str, Any]) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        String input:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: str) -> str:\n                return x + \"a\"\n\n            def g(x: str) -> str:\n                return x + \"z\"\n\n            runnable = RunnableLambda(f) | g\n            as_tool = runnable.as_tool()\n            as_tool.invoke(\"b\")\n\n        .. versionadded:: 0.2.14\n        \"\"\"\n        # Avoid circular import\n        from langchain_core.tools import convert_runnable_to_tool\n\n        return convert_runnable_to_tool(\n            self,\n            args_schema=args_schema,\n            name=name,\n            description=description,\n            arg_types=arg_types,\n        )\n\n\nclass RunnableSerializable(Serializable, Runnable[Input, Output]):\n    \"\"\"Runnable that can be serialized to JSON.\"\"\"\n\n    name: Optional[str] = None\n\n    model_config = ConfigDict(\n        # Suppress warnings from pydantic protected namespaces\n        # (e.g., `model_`)\n        protected_namespaces=(),\n    )\n\n    def to_json(self) -> Union[SerializedConstructor, SerializedNotImplemented]:\n        \"\"\"Serialize the Runnable to JSON.\n\n        Returns:\n            A JSON-serializable representation of the Runnable.\n        \"\"\"\n        dumped = super().to_json()\n        with contextlib.suppress(Exception):\n            dumped[\"name\"] = self.get_name()\n        return dumped\n\n    def configurable_fields(\n        self, **kwargs: AnyConfigurableField\n    ) -> RunnableSerializable[Input, Output]:\n        \"\"\"Configure particular Runnable fields at runtime.\n\n        Args:\n            **kwargs: A dictionary of ConfigurableField instances to configure.\n\n        Returns:\n            A new Runnable with the fields configured.\n\n        .. code-block:: python\n\n            from langchain_core.runnables import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            model = ChatOpenAI(max_tokens=20).configurable_fields(\n                max_tokens=ConfigurableField(\n                    id=\"output_token_number\",\n                    name=\"Max tokens in the output\",\n                    description=\"The maximum number of tokens in the output\",\n                )\n            )\n\n            # max_tokens = 20\n            print(\n                \"max_tokens_20: \",\n                model.invoke(\"tell me something about chess\").content\n            )\n\n            # max_tokens = 200\n            print(\"max_tokens_200: \", model.with_config(\n                configurable={\"output_token_number\": 200}\n                ).invoke(\"tell me something about chess\").content\n            )\n        \"\"\"\n        from langchain_core.runnables.configurable import RunnableConfigurableFields\n\n        for key in kwargs:\n            if key not in self.model_fields:\n                msg = (\n                    f\"Configuration key {key} not found in {self}: \"\n                    f\"available keys are {self.model_fields.keys()}\"\n                )\n                raise ValueError(msg)\n\n        return RunnableConfigurableFields(default=self, fields=kwargs)\n\n    def configurable_alternatives(\n        self,\n        which: ConfigurableField,\n        *,\n        default_key: str = \"default\",\n        prefix_keys: bool = False,\n        **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],\n    ) -> RunnableSerializable[Input, Output]:\n        \"\"\"Configure alternatives for Runnables that can be set at runtime.\n\n        Args:\n            which: The ConfigurableField instance that will be used to select the\n                alternative.\n            default_key: The default key to use if no alternative is selected.\n                Defaults to \"default\".\n            prefix_keys: Whether to prefix the keys with the ConfigurableField id.\n                Defaults to False.\n            **kwargs: A dictionary of keys to Runnable instances or callables that\n                return Runnable instances.\n\n        Returns:\n            A new Runnable with the alternatives configured.\n\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n            from langchain_core.runnables.utils import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            model = ChatAnthropic(\n                model_name=\"claude-3-sonnet-20240229\"\n            ).configurable_alternatives(\n                ConfigurableField(id=\"llm\"),\n                default_key=\"anthropic\",\n                openai=ChatOpenAI()\n            )\n\n            # uses the default model ChatAnthropic\n            print(model.invoke(\"which organization created you?\").content)\n\n            # uses ChatOpenAI\n            print(\n                model.with_config(\n                    configurable={\"llm\": \"openai\"}\n                ).invoke(\"which organization created you?\").content\n            )\n        \"\"\"\n        from langchain_core.runnables.configurable import (\n            RunnableConfigurableAlternatives,\n        )\n\n        return RunnableConfigurableAlternatives(\n            which=which,\n            default=self,\n            alternatives=kwargs,\n            default_key=default_key,\n            prefix_keys=prefix_keys,\n        )\n\n\ndef _seq_input_schema(\n    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]\n) -> type[BaseModel]:\n    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick\n\n    first = steps[0]\n    if len(steps) == 1:\n        return first.get_input_schema(config)\n    elif isinstance(first, RunnableAssign):\n        next_input_schema = _seq_input_schema(steps[1:], config)\n        if not issubclass(next_input_schema, RootModel):\n            # it's a dict as expected\n            return create_model_v2(  # type: ignore[call-overload]\n                \"RunnableSequenceInput\",\n                field_definitions={\n                    k: (v.annotation, v.default)\n                    for k, v in next_input_schema.model_fields.items()\n                    if k not in first.mapper.steps__\n                },\n            )\n    elif isinstance(first, RunnablePick):\n        return _seq_input_schema(steps[1:], config)\n\n    return first.get_input_schema(config)\n\n\ndef _seq_output_schema(\n    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]\n) -> type[BaseModel]:\n    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick\n\n    last = steps[-1]\n    if len(steps) == 1:\n        return last.get_input_schema(config)\n    elif isinstance(last, RunnableAssign):\n        mapper_output_schema = last.mapper.get_output_schema(config)\n        prev_output_schema = _seq_output_schema(steps[:-1], config)\n        if not issubclass(prev_output_schema, RootModel):\n            # it's a dict as expected\n            return create_model_v2(  # type: ignore[call-overload]\n                \"RunnableSequenceOutput\",\n                field_definitions={\n                    **{\n                        k: (v.annotation, v.default)\n                        for k, v in prev_output_schema.model_fields.items()\n                    },\n                    **{\n                        k: (v.annotation, v.default)\n                        for k, v in mapper_output_schema.model_fields.items()\n                    },\n                },\n            )\n    elif isinstance(last, RunnablePick):\n        prev_output_schema = _seq_output_schema(steps[:-1], config)\n        if not issubclass(prev_output_schema, RootModel):\n            # it's a dict as expected\n            if isinstance(last.keys, list):\n                return create_model_v2(  # type: ignore[call-overload]\n                    \"RunnableSequenceOutput\",\n                    field_definitions={\n                        k: (v.annotation, v.default)\n                        for k, v in prev_output_schema.model_fields.items()\n                        if k in last.keys\n                    },\n                )\n            else:\n                field = prev_output_schema.model_fields[last.keys]\n                return create_model_v2(  # type: ignore[call-overload]\n                    \"RunnableSequenceOutput\", root=(field.annotation, field.default)\n                )\n\n    return last.get_output_schema(config)\n\n\nclass RunnableSequence(RunnableSerializable[Input, Output]):\n    \"\"\"Sequence of Runnables, where the output of each is the input of the next.\n\n    **RunnableSequence** is the most important composition operator in LangChain\n    as it is used in virtually every chain.\n\n    A RunnableSequence can be instantiated directly or more commonly by using the `|`\n    operator where either the left or right operands (or both) must be a Runnable.\n\n    Any RunnableSequence automatically supports sync, async, batch.\n\n    The default implementations of `batch` and `abatch` utilize threadpools and\n    asyncio gather and will be faster than naive invocation of invoke or ainvoke\n    for IO bound Runnables.\n\n    Batching is implemented by invoking the batch method on each component of the\n    RunnableSequence in order.\n\n    A RunnableSequence preserves the streaming properties of its components, so if all\n    components of the sequence implement a `transform` method -- which\n    is the method that implements the logic to map a streaming input to a streaming\n    output -- then the sequence will be able to stream input to output!\n\n    If any component of the sequence does not implement transform then the\n    streaming will only begin after this component is run. If there are\n    multiple blocking components, streaming begins after the last one.\n\n    Please note: RunnableLambdas do not support `transform` by default! So if\n        you need to use a RunnableLambdas be careful about where you place them in a\n        RunnableSequence (if you need to use the .stream()/.astream() methods).\n\n        If you need arbitrary logic and need streaming, you can subclass\n        Runnable, and implement `transform` for whatever logic you need.\n\n    Here is a simple example that uses simple functions to illustrate the use of\n    RunnableSequence:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            def mul_two(x: int) -> int:\n                return x * 2\n\n            runnable_1 = RunnableLambda(add_one)\n            runnable_2 = RunnableLambda(mul_two)\n            sequence = runnable_1 | runnable_2\n            # Or equivalently:\n            # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n            sequence.invoke(1)\n            await sequence.ainvoke(1)\n\n            sequence.batch([1, 2, 3])\n            await sequence.abatch([1, 2, 3])\n\n    Here's an example that uses streams JSON output generated by an LLM:\n\n        .. code-block:: python\n\n            from langchain_core.output_parsers.json import SimpleJsonOutputParser\n            from langchain_openai import ChatOpenAI\n\n            prompt = PromptTemplate.from_template(\n                'In JSON format, give me a list of {topic} and their '\n                'corresponding names in French, Spanish and in a '\n                'Cat Language.'\n            )\n\n            model = ChatOpenAI()\n            chain = prompt | model | SimpleJsonOutputParser()\n\n            async for chunk in chain.astream({'topic': 'colors'}):\n                print('-')  # noqa: T201\n                print(chunk, sep='', flush=True)  # noqa: T201\n    \"\"\"\n\n    # The steps are broken into first, middle and last, solely for type checking\n    # purposes. It allows specifying the `Input` on the first type, the `Output` of\n    # the last type.\n    first: Runnable[Input, Any]\n    \"\"\"The first Runnable in the sequence.\"\"\"\n    middle: list[Runnable[Any, Any]] = Field(default_factory=list)\n    \"\"\"The middle Runnables in the sequence.\"\"\"\n    last: Runnable[Any, Output]\n    \"\"\"The last Runnable in the sequence.\"\"\"\n\n    def __init__(\n        self,\n        *steps: RunnableLike,\n        name: Optional[str] = None,\n        first: Optional[Runnable[Any, Any]] = None,\n        middle: Optional[list[Runnable[Any, Any]]] = None,\n        last: Optional[Runnable[Any, Any]] = None,\n    ) -> None:\n        \"\"\"Create a new RunnableSequence.\n\n        Args:\n            steps: The steps to include in the sequence.\n            name: The name of the Runnable. Defaults to None.\n            first: The first Runnable in the sequence. Defaults to None.\n            middle: The middle Runnables in the sequence. Defaults to None.\n            last: The last Runnable in the sequence. Defaults to None.\n\n        Raises:\n            ValueError: If the sequence has less than 2 steps.\n        \"\"\"\n        steps_flat: list[Runnable] = []\n        if not steps and first is not None and last is not None:\n            steps_flat = [first] + (middle or []) + [last]\n        for step in steps:\n            if isinstance(step, RunnableSequence):\n                steps_flat.extend(step.steps)\n            else:\n                steps_flat.append(coerce_to_runnable(step))\n        if len(steps_flat) < 2:\n            msg = f\"RunnableSequence must have at least 2 steps, got {len(steps_flat)}\"\n            raise ValueError(msg)\n        super().__init__(  # type: ignore[call-arg]\n            first=steps_flat[0],\n            middle=list(steps_flat[1:-1]),\n            last=steps_flat[-1],\n            name=name,\n        )\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    @property\n    def steps(self) -> list[Runnable[Any, Any]]:\n        \"\"\"All the Runnables that make up the sequence in order.\n\n        Returns:\n            A list of Runnables.\n        \"\"\"\n        return [self.first] + self.middle + [self.last]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Check if the object is serializable.\n\n        Returns:\n            True if the object is serializable, False otherwise.\n                Defaults to True.\n        \"\"\"\n        return True\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    @override\n    def InputType(self) -> type[Input]:\n        \"\"\"The type of the input to the Runnable.\"\"\"\n        return self.first.InputType\n\n    @property\n    @override\n    def OutputType(self) -> type[Output]:\n        \"\"\"The type of the output of the Runnable.\"\"\"\n        return self.last.OutputType\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the input schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema of the Runnable.\n        \"\"\"\n        return _seq_input_schema(self.steps, config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the output schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The output schema of the Runnable.\n        \"\"\"\n        return _seq_output_schema(self.steps, config)\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"Get the config specs of the Runnable.\n\n        Returns:\n            The config specs of the Runnable.\n        \"\"\"\n        from langchain_core.beta.runnables.context import (\n            CONTEXT_CONFIG_PREFIX,\n            _key_from_id,\n        )\n\n        # get all specs\n        all_specs = [\n            (spec, idx)\n            for idx, step in enumerate(self.steps)\n            for spec in step.config_specs\n        ]\n        # calculate context dependencies\n        specs_by_pos = groupby(\n            [tup for tup in all_specs if tup[0].id.startswith(CONTEXT_CONFIG_PREFIX)],\n            itemgetter(1),\n        )\n        next_deps: set[str] = set()\n        deps_by_pos: dict[int, set[str]] = {}\n        for pos, specs in specs_by_pos:\n            deps_by_pos[pos] = next_deps\n            next_deps = next_deps | {spec[0].id for spec in specs}\n        # assign context dependencies\n        for pos, (spec, idx) in enumerate(all_specs):\n            if spec.id.startswith(CONTEXT_CONFIG_PREFIX):\n                all_specs[pos] = (\n                    ConfigurableFieldSpec(\n                        id=spec.id,\n                        annotation=spec.annotation,\n                        name=spec.name,\n                        default=spec.default,\n                        description=spec.description,\n                        is_shared=spec.is_shared,\n                        dependencies=[\n                            d\n                            for d in deps_by_pos[idx]\n                            if _key_from_id(d) != _key_from_id(spec.id)\n                        ]\n                        + (spec.dependencies or []),\n                    ),\n                    idx,\n                )\n\n        return get_unique_config_specs(spec for spec, _ in all_specs)\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Get the graph representation of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The graph representation of the Runnable.\n\n        Raises:\n            ValueError: If a Runnable has no first or last node.\n        \"\"\"\n        from langchain_core.runnables.graph import Graph\n\n        graph = Graph()\n        for step in self.steps:\n            current_last_node = graph.last_node()\n            step_graph = step.get_graph(config)\n            if step is not self.first:\n                step_graph.trim_first_node()\n            if step is not self.last:\n                step_graph.trim_last_node()\n            step_first_node, _ = graph.extend(step_graph)\n            if not step_first_node:\n                msg = f\"Runnable {step} has no first node\"\n                raise ValueError(msg)\n            if current_last_node:\n                graph.add_edge(current_last_node, step_first_node)\n\n        return graph\n\n    def __repr__(self) -> str:\n        return \"\\n| \".join(\n            repr(s) if i == 0 else indent_lines_after_first(repr(s), \"| \")\n            for i, s in enumerate(self.steps)\n        )\n\n    def __or__(\n        self,\n        other: Union[\n            Runnable[Any, Other],\n            Callable[[Any], Other],\n            Callable[[Iterator[Any]], Iterator[Other]],\n            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],\n        ],\n    ) -> RunnableSerializable[Input, Other]:\n        if isinstance(other, RunnableSequence):\n            return RunnableSequence(\n                self.first,\n                *self.middle,\n                self.last,\n                other.first,\n                *other.middle,\n                other.last,\n                name=self.name or other.name,\n            )\n        else:\n            return RunnableSequence(\n                self.first,\n                *self.middle,\n                self.last,\n                coerce_to_runnable(other),\n                name=self.name,\n            )\n\n    def __ror__(\n        self,\n        other: Union[\n            Runnable[Other, Any],\n            Callable[[Other], Any],\n            Callable[[Iterator[Other]], Iterator[Any]],\n            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],\n        ],\n    ) -> RunnableSerializable[Other, Output]:\n        if isinstance(other, RunnableSequence):\n            return RunnableSequence(\n                other.first,\n                *other.middle,\n                other.last,\n                self.first,\n                *self.middle,\n                self.last,\n                name=other.name or self.name,\n            )\n        else:\n            return RunnableSequence(\n                coerce_to_runnable(other),\n                self.first,\n                *self.middle,\n                self.last,\n                name=self.name,\n            )\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        from langchain_core.beta.runnables.context import config_with_context\n\n        # setup callbacks and context\n        config = config_with_context(ensure_config(config), self.steps)\n        callback_manager = get_callback_manager_for_config(config)\n        # start the root run\n        run_manager = callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        # invoke all steps in sequence\n        try:\n            for i, step in enumerate(self.steps):\n                # mark each step as a child run\n                config = patch_config(\n                    config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n                )\n                context = copy_context()\n                context.run(_set_config_context, config)\n                if i == 0:\n                    input = context.run(step.invoke, input, config, **kwargs)\n                else:\n                    input = context.run(step.invoke, input, config)\n        # finish the root run\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(input)\n            return cast(Output, input)\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n\n        # setup callbacks and context\n        config = aconfig_with_context(ensure_config(config), self.steps)\n        callback_manager = get_async_callback_manager_for_config(config)\n        # start the root run\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        # invoke all steps in sequence\n        try:\n            for i, step in enumerate(self.steps):\n                # mark each step as a child run\n                config = patch_config(\n                    config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n                )\n                context = copy_context()\n                context.run(_set_config_context, config)\n                if i == 0:\n                    part = functools.partial(step.ainvoke, input, config, **kwargs)\n                else:\n                    part = functools.partial(step.ainvoke, input, config)\n                if asyncio_accepts_context():\n                    input = await asyncio.create_task(part(), context=context)  # type: ignore\n                else:\n                    input = await asyncio.create_task(part())\n        # finish the root run\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(input)\n            return cast(Output, input)\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        from langchain_core.beta.runnables.context import config_with_context\n        from langchain_core.callbacks.manager import CallbackManager\n\n        if not inputs:\n            return []\n\n        # setup callbacks and context\n        configs = [\n            config_with_context(c, self.steps)\n            for c in get_config_list(config, len(inputs))\n        ]\n        callback_managers = [\n            CallbackManager.configure(\n                inheritable_callbacks=config.get(\"callbacks\"),\n                local_callbacks=None,\n                verbose=False,\n                inheritable_tags=config.get(\"tags\"),\n                local_tags=None,\n                inheritable_metadata=config.get(\"metadata\"),\n                local_metadata=None,\n            )\n            for config in configs\n        ]\n        # start the root runs, one per input\n        run_managers = [\n            cm.on_chain_start(\n                None,\n                input,\n                name=config.get(\"run_name\") or self.get_name(),\n                run_id=config.pop(\"run_id\", None),\n            )\n            for cm, input, config in zip(callback_managers, inputs, configs)\n        ]\n\n        # invoke\n        try:\n            if return_exceptions:\n                # Track which inputs (by index) failed so far\n                # If an input has failed it will be present in this map,\n                # and the value will be the exception that was raised.\n                failed_inputs_map: dict[int, Exception] = {}\n                for stepidx, step in enumerate(self.steps):\n                    # Assemble the original indexes of the remaining inputs\n                    # (i.e. the ones that haven't failed yet)\n                    remaining_idxs = [\n                        i for i in range(len(configs)) if i not in failed_inputs_map\n                    ]\n                    # Invoke the step on the remaining inputs\n                    inputs = step.batch(\n                        [\n                            inp\n                            for i, inp in zip(remaining_idxs, inputs)\n                            if i not in failed_inputs_map\n                        ],\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config,\n                                callbacks=rm.get_child(f\"seq:step:{stepidx + 1}\"),\n                            )\n                            for i, (rm, config) in enumerate(zip(run_managers, configs))\n                            if i not in failed_inputs_map\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if stepidx == 0 else {}),\n                    )\n                    # If an input failed, add it to the map\n                    for i, inp in zip(remaining_idxs, inputs):\n                        if isinstance(inp, Exception):\n                            failed_inputs_map[i] = inp\n                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]\n                    # If all inputs have failed, stop processing\n                    if len(failed_inputs_map) == len(configs):\n                        break\n\n                # Reassemble the outputs, inserting Exceptions for failed inputs\n                inputs_copy = inputs.copy()\n                inputs = []\n                for i in range(len(configs)):\n                    if i in failed_inputs_map:\n                        inputs.append(cast(Input, failed_inputs_map[i]))\n                    else:\n                        inputs.append(inputs_copy.pop(0))\n            else:\n                for i, step in enumerate(self.steps):\n                    inputs = step.batch(\n                        inputs,\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config, callbacks=rm.get_child(f\"seq:step:{i + 1}\")\n                            )\n                            for rm, config in zip(run_managers, configs)\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if i == 0 else {}),\n                    )\n\n        # finish the root runs\n        except BaseException as e:\n            for rm in run_managers:\n                rm.on_chain_error(e)\n            if return_exceptions:\n                return cast(list[Output], [e for _ in inputs])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            for run_manager, out in zip(run_managers, inputs):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    run_manager.on_chain_error(out)\n                else:\n                    run_manager.on_chain_end(out)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], inputs)\n            else:\n                raise first_exception\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n        from langchain_core.callbacks.manager import AsyncCallbackManager\n\n        if not inputs:\n            return []\n\n        # setup callbacks and context\n        configs = [\n            aconfig_with_context(c, self.steps)\n            for c in get_config_list(config, len(inputs))\n        ]\n        callback_managers = [\n            AsyncCallbackManager.configure(\n                inheritable_callbacks=config.get(\"callbacks\"),\n                local_callbacks=None,\n                verbose=False,\n                inheritable_tags=config.get(\"tags\"),\n                local_tags=None,\n                inheritable_metadata=config.get(\"metadata\"),\n                local_metadata=None,\n            )\n            for config in configs\n        ]\n        # start the root runs, one per input\n        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(\n            *(\n                cm.on_chain_start(\n                    None,\n                    input,\n                    name=config.get(\"run_name\") or self.get_name(),\n                    run_id=config.pop(\"run_id\", None),\n                )\n                for cm, input, config in zip(callback_managers, inputs, configs)\n            )\n        )\n\n        # invoke .batch() on each step\n        # this uses batching optimizations in Runnable subclasses, like LLM\n        try:\n            if return_exceptions:\n                # Track which inputs (by index) failed so far\n                # If an input has failed it will be present in this map,\n                # and the value will be the exception that was raised.\n                failed_inputs_map: dict[int, Exception] = {}\n                for stepidx, step in enumerate(self.steps):\n                    # Assemble the original indexes of the remaining inputs\n                    # (i.e. the ones that haven't failed yet)\n                    remaining_idxs = [\n                        i for i in range(len(configs)) if i not in failed_inputs_map\n                    ]\n                    # Invoke the step on the remaining inputs\n                    inputs = await step.abatch(\n                        [\n                            inp\n                            for i, inp in zip(remaining_idxs, inputs)\n                            if i not in failed_inputs_map\n                        ],\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config,\n                                callbacks=rm.get_child(f\"seq:step:{stepidx + 1}\"),\n                            )\n                            for i, (rm, config) in enumerate(zip(run_managers, configs))\n                            if i not in failed_inputs_map\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if stepidx == 0 else {}),\n                    )\n                    # If an input failed, add it to the map\n                    for i, inp in zip(remaining_idxs, inputs):\n                        if isinstance(inp, Exception):\n                            failed_inputs_map[i] = inp\n                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]\n                    # If all inputs have failed, stop processing\n                    if len(failed_inputs_map) == len(configs):\n                        break\n\n                # Reassemble the outputs, inserting Exceptions for failed inputs\n                inputs_copy = inputs.copy()\n                inputs = []\n                for i in range(len(configs)):\n                    if i in failed_inputs_map:\n                        inputs.append(cast(Input, failed_inputs_map[i]))\n                    else:\n                        inputs.append(inputs_copy.pop(0))\n            else:\n                for i, step in enumerate(self.steps):\n                    inputs = await step.abatch(\n                        inputs,\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config, callbacks=rm.get_child(f\"seq:step:{i + 1}\")\n                            )\n                            for rm, config in zip(run_managers, configs)\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if i == 0 else {}),\n                    )\n        # finish the root runs\n        except BaseException as e:\n            await asyncio.gather(*(rm.on_chain_error(e) for rm in run_managers))\n            if return_exceptions:\n                return cast(list[Output], [e for _ in inputs])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            coros: list[Awaitable[None]] = []\n            for run_manager, out in zip(run_managers, inputs):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    coros.append(run_manager.on_chain_error(out))\n                else:\n                    coros.append(run_manager.on_chain_end(out))\n            await asyncio.gather(*coros)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], inputs)\n            else:\n                raise first_exception\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        from langchain_core.beta.runnables.context import config_with_context\n\n        steps = [self.first] + self.middle + [self.last]\n        config = config_with_context(config, self.steps)\n\n        # transform the input stream of each step with the next\n        # steps that don't natively support transforming an input stream will\n        # buffer input in memory until all available, and then start emitting output\n        final_pipeline = cast(Iterator[Output], input)\n        for idx, step in enumerate(steps):\n            config = patch_config(\n                config, callbacks=run_manager.get_child(f\"seq:step:{idx + 1}\")\n            )\n            if idx == 0:\n                final_pipeline = step.transform(final_pipeline, config, **kwargs)\n            else:\n                final_pipeline = step.transform(final_pipeline, config)\n\n        yield from final_pipeline\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n\n        steps = [self.first] + self.middle + [self.last]\n        config = aconfig_with_context(config, self.steps)\n\n        # stream the last steps\n        # transform the input stream of each step with the next\n        # steps that don't natively support transforming an input stream will\n        # buffer input in memory until all available, and then start emitting output\n        final_pipeline = cast(AsyncIterator[Output], input)\n        for idx, step in enumerate(steps):\n            config = patch_config(\n                config,\n                callbacks=run_manager.get_child(f\"seq:step:{idx + 1}\"),\n            )\n            if idx == 0:\n                final_pipeline = step.atransform(final_pipeline, config, **kwargs)\n            else:\n                final_pipeline = step.atransform(final_pipeline, config)\n        async for output in final_pipeline:\n            yield output\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self._transform_stream_with_config(\n            input,\n            self._transform,\n            patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),\n            **kwargs,\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self.transform(iter([input]), config, **kwargs)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for chunk in self._atransform_stream_with_config(\n            input,\n            self._atransform,\n            patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),\n            **kwargs,\n        ):\n            yield chunk\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\nclass RunnableParallel(RunnableSerializable[Input, dict[str, Any]]):\n    \"\"\"Runnable that runs a mapping of Runnables in parallel, and returns a mapping\n    of their outputs.\n\n    RunnableParallel is one of the two main composition primitives for the LCEL,\n    alongside RunnableSequence. It invokes Runnables concurrently, providing the same\n    input to each.\n\n    A RunnableParallel can be instantiated directly or by using a dict literal within a\n    sequence.\n\n    Here is a simple example that uses functions to illustrate the use of\n    RunnableParallel:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            def mul_two(x: int) -> int:\n                return x * 2\n\n            def mul_three(x: int) -> int:\n                return x * 3\n\n            runnable_1 = RunnableLambda(add_one)\n            runnable_2 = RunnableLambda(mul_two)\n            runnable_3 = RunnableLambda(mul_three)\n\n            sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n                \"mul_two\": runnable_2,\n                \"mul_three\": runnable_3,\n            }\n            # Or equivalently:\n            # sequence = runnable_1 | RunnableParallel(\n            #     {\"mul_two\": runnable_2, \"mul_three\": runnable_3}\n            # )\n            # Also equivalently:\n            # sequence = runnable_1 | RunnableParallel(\n            #     mul_two=runnable_2,\n            #     mul_three=runnable_3,\n            # )\n\n            sequence.invoke(1)\n            await sequence.ainvoke(1)\n\n            sequence.batch([1, 2, 3])\n            await sequence.abatch([1, 2, 3])\n\n    RunnableParallel makes it easy to run Runnables in parallel. In the below example,\n    we simultaneously stream output from two different Runnables:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.runnables import RunnableParallel\n            from langchain_openai import ChatOpenAI\n\n            model = ChatOpenAI()\n            joke_chain = (\n                ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n                | model\n            )\n            poem_chain = (\n                ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n                | model\n            )\n\n            runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)\n\n            # Display stream\n            output = {key: \"\" for key, _ in runnable.output_schema()}\n            for chunk in runnable.stream({\"topic\": \"bear\"}):\n                for key in chunk:\n                    output[key] = output[key] + chunk[key].content\n                print(output)  # noqa: T201\n    \"\"\"\n\n    steps__: Mapping[str, Runnable[Input, Any]]\n\n    def __init__(\n        self,\n        steps__: Optional[\n            Mapping[\n                str,\n                Union[\n                    Runnable[Input, Any],\n                    Callable[[Input], Any],\n                    Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],\n                ],\n            ]\n        ] = None,\n        **kwargs: Union[\n            Runnable[Input, Any],\n            Callable[[Input], Any],\n            Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],\n        ],\n    ) -> None:\n        merged = {**steps__} if steps__ is not None else {}\n        merged.update(kwargs)\n        super().__init__(  # type: ignore[call-arg]\n            steps__={key: coerce_to_runnable(r) for key, r in merged.items()}\n        )\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        \"\"\"Get the name of the Runnable.\n\n        Args:\n            suffix: The suffix to use. Defaults to None.\n            name: The name to use. Defaults to None.\n\n        Returns:\n            The name of the Runnable.\n        \"\"\"\n        name = name or self.name or f\"RunnableParallel<{','.join(self.steps__.keys())}>\"\n        return super().get_name(suffix, name=name)\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        \"\"\"The type of the input to the Runnable.\"\"\"\n        for step in self.steps__.values():\n            if step.InputType:\n                return step.InputType\n\n        return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the input schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema of the Runnable.\n        \"\"\"\n        if all(\n            s.get_input_schema(config).model_json_schema().get(\"type\", \"object\")\n            == \"object\"\n            for s in self.steps__.values()\n        ):\n            # This is correct, but pydantic typings/mypy don't think so.\n            return create_model_v2(  # type: ignore[call-overload]\n                self.get_name(\"Input\"),\n                field_definitions={\n                    k: (v.annotation, v.default)\n                    for step in self.steps__.values()\n                    for k, v in step.get_input_schema(config).model_fields.items()\n                    if k != \"__root__\"\n                },\n            )\n\n        return super().get_input_schema(config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the output schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The output schema of the Runnable.\n        \"\"\"\n        fields = {k: (v.OutputType, ...) for k, v in self.steps__.items()}\n        return create_model_v2(self.get_name(\"Output\"), field_definitions=fields)\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"Get the config specs of the Runnable.\n\n        Returns:\n            The config specs of the Runnable.\n        \"\"\"\n        return get_unique_config_specs(\n            spec for step in self.steps__.values() for spec in step.config_specs\n        )\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Get the graph representation of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The graph representation of the Runnable.\n\n        Raises:\n            ValueError: If a Runnable has no first or last node.\n        \"\"\"\n        from langchain_core.runnables.graph import Graph\n\n        graph = Graph()\n        input_node = graph.add_node(self.get_input_schema(config))\n        output_node = graph.add_node(self.get_output_schema(config))\n        for step in self.steps__.values():\n            step_graph = step.get_graph()\n            step_graph.trim_first_node()\n            step_graph.trim_last_node()\n            if not step_graph:\n                graph.add_edge(input_node, output_node)\n            else:\n                step_first_node, step_last_node = graph.extend(step_graph)\n                if not step_first_node:\n                    msg = f\"Runnable {step} has no first node\"\n                    raise ValueError(msg)\n                if not step_last_node:\n                    msg = f\"Runnable {step} has no last node\"\n                    raise ValueError(msg)\n                graph.add_edge(input_node, step_first_node)\n                graph.add_edge(step_last_node, output_node)\n\n        return graph\n\n    def __repr__(self) -> str:\n        map_for_repr = \",\\n  \".join(\n            f\"{k}: {indent_lines_after_first(repr(v), '  ' + k + ': ')}\"\n            for k, v in self.steps__.items()\n        )\n        return \"{\\n  \" + map_for_repr + \"\\n}\"\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> dict[str, Any]:\n        from langchain_core.callbacks.manager import CallbackManager\n\n        # setup callbacks\n        config = ensure_config(config)\n        callback_manager = CallbackManager.configure(\n            inheritable_callbacks=config.get(\"callbacks\"),\n            local_callbacks=None,\n            verbose=False,\n            inheritable_tags=config.get(\"tags\"),\n            local_tags=None,\n            inheritable_metadata=config.get(\"metadata\"),\n            local_metadata=None,\n        )\n        # start the root run\n        run_manager = callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        def _invoke_step(\n            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n                # mark each step as a child run\n                callbacks=run_manager.get_child(f\"map:key:{key}\"),\n            )\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            return context.run(\n                step.invoke,\n                input,\n                child_config,\n            )\n\n        # gather results from all steps\n        try:\n            # copy to avoid issues from the caller mutating the steps during invoke()\n            steps = dict(self.steps__)\n\n            with get_executor_for_config(config) as executor:\n                futures = [\n                    executor.submit(_invoke_step, step, input, config, key)\n                    for key, step in steps.items()\n                ]\n                output = {key: future.result() for key, future in zip(steps, futures)}\n        # finish the root run\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(output)\n            return output\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> dict[str, Any]:\n        # setup callbacks\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        # start the root run\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        async def _ainvoke_step(\n            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n                callbacks=run_manager.get_child(f\"map:key:{key}\"),\n            )\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            if asyncio_accepts_context():\n                return await asyncio.create_task(  # type: ignore\n                    step.ainvoke(input, child_config), context=context\n                )\n            else:\n                return await asyncio.create_task(step.ainvoke(input, child_config))\n\n        # gather results from all steps\n        try:\n            # copy to avoid issues from the caller mutating the steps during invoke()\n            steps = dict(self.steps__)\n            results = await asyncio.gather(\n                *(\n                    _ainvoke_step(\n                        step,\n                        input,\n                        # mark each step as a child run\n                        config,\n                        key,\n                    )\n                    for key, step in steps.items()\n                )\n            )\n            output = dict(zip(steps, results))\n        # finish the root run\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(output)\n            return output\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n    ) -> Iterator[AddableDict]:\n        # Shallow copy steps to ignore mutations while in progress\n        steps = dict(self.steps__)\n        # Each step gets a copy of the input iterator,\n        # which is consumed in parallel in a separate thread.\n        input_copies = list(safetee(input, len(steps), lock=threading.Lock()))\n        with get_executor_for_config(config) as executor:\n            # Create the transform() generator for each step\n            named_generators = [\n                (\n                    name,\n                    step.transform(\n                        input_copies.pop(),\n                        patch_config(\n                            config, callbacks=run_manager.get_child(f\"map:key:{name}\")\n                        ),\n                    ),\n                )\n                for name, step in steps.items()\n            ]\n            # Start the first iteration of each generator\n            futures = {\n                executor.submit(next, generator): (step_name, generator)\n                for step_name, generator in named_generators\n            }\n            # Yield chunks from each as they become available,\n            # and start the next iteration of that generator that yielded it.\n            # When all generators are exhausted, stop.\n            while futures:\n                completed_futures, _ = wait(futures, return_when=FIRST_COMPLETED)\n                for future in completed_futures:\n                    (step_name, generator) = futures.pop(future)\n                    try:\n                        chunk = AddableDict({step_name: future.result()})\n                        yield chunk\n                        futures[executor.submit(next, generator)] = (\n                            step_name,\n                            generator,\n                        )\n                    except StopIteration:\n                        pass\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[dict[str, Any]]:\n        yield from self._transform_stream_with_config(\n            input, self._transform, config, **kwargs\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[dict[str, Any]]:\n        yield from self.transform(iter([input]), config)\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n    ) -> AsyncIterator[AddableDict]:\n        # Shallow copy steps to ignore mutations while in progress\n        steps = dict(self.steps__)\n        # Each step gets a copy of the input iterator,\n        # which is consumed in parallel in a separate thread.\n        input_copies = list(atee(input, len(steps), lock=asyncio.Lock()))\n        # Create the transform() generator for each step\n        named_generators = [\n            (\n                name,\n                step.atransform(\n                    input_copies.pop(),\n                    patch_config(\n                        config, callbacks=run_manager.get_child(f\"map:key:{name}\")\n                    ),\n                ),\n            )\n            for name, step in steps.items()\n        ]\n\n        # Wrap in a coroutine to satisfy linter\n        async def get_next_chunk(generator: AsyncIterator) -> Optional[Output]:\n            return await py_anext(generator)\n\n        # Start the first iteration of each generator\n        tasks = {\n            asyncio.create_task(get_next_chunk(generator)): (step_name, generator)\n            for step_name, generator in named_generators\n        }\n        # Yield chunks from each as they become available,\n        # and start the next iteration of the generator that yielded it.\n        # When all generators are exhausted, stop.\n        while tasks:\n            completed_tasks, _ = await asyncio.wait(\n                tasks, return_when=asyncio.FIRST_COMPLETED\n            )\n            for task in completed_tasks:\n                (step_name, generator) = tasks.pop(task)\n                try:\n                    chunk = AddableDict({step_name: task.result()})\n                    yield chunk\n                    new_task = asyncio.create_task(get_next_chunk(generator))\n                    tasks[new_task] = (step_name, generator)\n                except StopAsyncIteration:\n                    pass\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        async for chunk in self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        ):\n            yield chunk\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[dict[str, Any]]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config):\n            yield chunk\n\n\n# We support both names\nRunnableMap = RunnableParallel\n\n\nclass RunnableGenerator(Runnable[Input, Output]):\n    \"\"\"Runnable that runs a generator function.\n\n    RunnableGenerators can be instantiated directly or by using a generator within\n    a sequence.\n\n    RunnableGenerators can be used to implement custom behavior, such as custom output\n    parsers, while preserving streaming capabilities. Given a generator function with\n    a signature Iterator[A] -> Iterator[B], wrapping it in a RunnableGenerator allows\n    it to emit output chunks as soon as they are streamed in from the previous step.\n\n    Note that if a generator function has a signature A -> Iterator[B], such that it\n    requires its input from the previous step to be completed before emitting chunks\n    (e.g., most LLMs need the entire prompt available to start generating), it can\n    instead be wrapped in a RunnableLambda.\n\n    Here is an example to show the basic mechanics of a RunnableGenerator:\n\n        .. code-block:: python\n\n            from typing import Any, AsyncIterator, Iterator\n\n            from langchain_core.runnables import RunnableGenerator\n\n\n            def gen(input: Iterator[Any]) -> Iterator[str]:\n                for token in [\"Have\", \" a\", \" nice\", \" day\"]:\n                    yield token\n\n\n            runnable = RunnableGenerator(gen)\n            runnable.invoke(None)  # \"Have a nice day\"\n            list(runnable.stream(None))  # [\"Have\", \" a\", \" nice\", \" day\"]\n            runnable.batch([None, None])  # [\"Have a nice day\", \"Have a nice day\"]\n\n\n            # Async version:\n            async def agen(input: AsyncIterator[Any]) -> AsyncIterator[str]:\n                for token in [\"Have\", \" a\", \" nice\", \" day\"]:\n                    yield token\n\n            runnable = RunnableGenerator(agen)\n            await runnable.ainvoke(None)  # \"Have a nice day\"\n            [p async for p in runnable.astream(None)] # [\"Have\", \" a\", \" nice\", \" day\"]\n\n    RunnableGenerator makes it easy to implement custom behavior within a streaming\n    context. Below we show an example:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.runnables import RunnableGenerator, RunnableLambda\n            from langchain_openai import ChatOpenAI\n            from langchain_core.output_parsers import StrOutputParser\n\n\n            model = ChatOpenAI()\n            chant_chain = (\n                ChatPromptTemplate.from_template(\"Give me a 3 word chant about {topic}\")\n                | model\n                | StrOutputParser()\n            )\n\n            def character_generator(input: Iterator[str]) -> Iterator[str]:\n                for token in input:\n                    if \",\" in token or \".\" in token:\n                        yield \"\ud83d\udc4f\" + token\n                    else:\n                        yield token\n\n\n            runnable = chant_chain | character_generator\n            assert type(runnable.last) is RunnableGenerator\n            \"\".join(runnable.stream({\"topic\": \"waste\"})) # Reduce\ud83d\udc4f, Reuse\ud83d\udc4f, Recycle\ud83d\udc4f.\n\n            # Note that RunnableLambda can be used to delay streaming of one step in a\n            # sequence until the previous step is finished:\n            def reverse_generator(input: str) -> Iterator[str]:\n                # Yield characters of input in reverse order.\n                for character in input[::-1]:\n                    yield character\n\n            runnable = chant_chain | RunnableLambda(reverse_generator)\n            \"\".join(runnable.stream({\"topic\": \"waste\"}))  # \".elcycer ,esuer ,ecudeR\"\n    \"\"\"\n\n    def __init__(\n        self,\n        transform: Union[\n            Callable[[Iterator[Input]], Iterator[Output]],\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n        ],\n        atransform: Optional[\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]]\n        ] = None,\n        *,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Initialize a RunnableGenerator.\n\n        Args:\n            transform: The transform function.\n            atransform: The async transform function. Defaults to None.\n\n        Raises:\n            TypeError: If the transform is not a generator function.\n        \"\"\"\n        if atransform is not None:\n            self._atransform = atransform\n            func_for_name: Callable = atransform\n\n        if is_async_generator(transform):\n            self._atransform = transform  # type: ignore[assignment]\n            func_for_name = transform\n        elif inspect.isgeneratorfunction(transform):\n            self._transform = transform\n            func_for_name = transform\n        else:\n            msg = (\n                \"Expected a generator function type for `transform`.\"\n                f\"Instead got an unsupported type: {type(transform)}\"\n            )\n            raise TypeError(msg)\n\n        try:\n            self.name = name or func_for_name.__name__\n        except AttributeError:\n            self.name = \"RunnableGenerator\"\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        func = getattr(self, \"_transform\", None) or self._atransform\n        try:\n            params = inspect.signature(func).parameters\n            first_param = next(iter(params.values()), None)\n            if first_param and first_param.annotation != inspect.Parameter.empty:\n                return getattr(first_param.annotation, \"__args__\", (Any,))[0]\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable generator, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.InputType\n\n        func = getattr(self, \"_transform\", None) or self._atransform\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        func = getattr(self, \"_transform\", None) or self._atransform\n        try:\n            sig = inspect.signature(func)\n            return (\n                getattr(sig.return_annotation, \"__args__\", (Any,))[0]\n                if sig.return_annotation != inspect.Signature.empty\n                else Any\n            )\n        except ValueError:\n            return Any\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable generator, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.OutputType\n        func = getattr(self, \"_transform\", None) or self._atransform\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, RunnableGenerator):\n            if hasattr(self, \"_transform\") and hasattr(other, \"_transform\"):\n                return self._transform == other._transform\n            elif hasattr(self, \"_atransform\") and hasattr(other, \"_atransform\"):\n                return self._atransform == other._atransform\n            else:\n                return False\n        else:\n            return False\n\n    def __repr__(self) -> str:\n        return f\"RunnableGenerator({self.name})\"\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        if not hasattr(self, \"_transform\"):\n            msg = f\"{repr(self)} only supports async methods.\"\n            raise NotImplementedError(msg)\n        return self._transform_stream_with_config(\n            input,\n            self._transform,  # type: ignore[arg-type]\n            config,\n            **kwargs,  # type: ignore[arg-type]\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        final: Optional[Output] = None\n        for output in self.stream(input, config, **kwargs):\n            final = output if final is None else final + output  # type: ignore[operator]\n        return cast(Output, final)\n\n    def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        if not hasattr(self, \"_atransform\"):\n            msg = f\"{repr(self)} only supports sync methods.\"\n            raise NotImplementedError(msg)\n\n        return self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        )\n\n    def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        return self.atransform(input_aiter(), config, **kwargs)\n\n    async def ainvoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        final: Optional[Output] = None\n        async for output in self.astream(input, config, **kwargs):\n            final = output if final is None else final + output  # type: ignore[operator]\n        return cast(Output, final)\n\n\nclass RunnableLambda(Runnable[Input, Output]):\n    \"\"\"RunnableLambda converts a python callable into a Runnable.\n\n    Wrapping a callable in a RunnableLambda makes the callable usable\n    within either a sync or async context.\n\n    RunnableLambda can be composed as any other Runnable and provides\n    seamless integration with LangChain tracing.\n\n    ``RunnableLambda`` is best suited for code that does not need to support\n    streaming. If you need to support streaming (i.e., be able to operate\n    on chunks of inputs and yield chunks of outputs), use ``RunnableGenerator``\n    instead.\n\n    Note that if a ``RunnableLambda`` returns an instance of ``Runnable``, that\n    instance is invoked (or streamed) during execution.\n\n    Examples:\n\n        .. code-block:: python\n\n            # This is a RunnableLambda\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            runnable = RunnableLambda(add_one)\n\n            runnable.invoke(1) # returns 2\n            runnable.batch([1, 2, 3]) # returns [2, 3, 4]\n\n            # Async is supported by default by delegating to the sync implementation\n            await runnable.ainvoke(1) # returns 2\n            await runnable.abatch([1, 2, 3]) # returns [2, 3, 4]\n\n\n            # Alternatively, can provide both synd and sync implementations\n            async def add_one_async(x: int) -> int:\n                return x + 1\n\n            runnable = RunnableLambda(add_one, afunc=add_one_async)\n            runnable.invoke(1) # Uses add_one\n            await runnable.ainvoke(1) # Uses add_one_async\n    \"\"\"\n\n    def __init__(\n        self,\n        func: Union[\n            Union[\n                Callable[[Input], Output],\n                Callable[[Input], Iterator[Output]],\n                Callable[[Input, RunnableConfig], Output],\n                Callable[[Input, CallbackManagerForChainRun], Output],\n                Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\n            ],\n            Union[\n                Callable[[Input], Awaitable[Output]],\n                Callable[[Input], AsyncIterator[Output]],\n                Callable[[Input, RunnableConfig], Awaitable[Output]],\n                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n                Callable[\n                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                    Awaitable[Output],\n                ],\n            ],\n        ],\n        afunc: Optional[\n            Union[\n                Callable[[Input], Awaitable[Output]],\n                Callable[[Input], AsyncIterator[Output]],\n                Callable[[Input, RunnableConfig], Awaitable[Output]],\n                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n                Callable[\n                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                    Awaitable[Output],\n                ],\n            ]\n        ] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Create a RunnableLambda from a callable, and async callable or both.\n\n        Accepts both sync and async variants to allow providing efficient\n        implementations for sync and async execution.\n\n        Args:\n            func: Either sync or async callable\n            afunc: An async callable that takes an input and returns an output.\n                Defaults to None.\n            name: The name of the Runnable. Defaults to None.\n\n        Raises:\n            TypeError: If the func is not a callable type.\n            TypeError: If both func and afunc are provided.\n        \"\"\"\n        if afunc is not None:\n            self.afunc = afunc\n            func_for_name: Callable = afunc\n\n        if is_async_callable(func) or is_async_generator(func):\n            if afunc is not None:\n                msg = (\n                    \"Func was provided as a coroutine function, but afunc was \"\n                    \"also provided. If providing both, func should be a regular \"\n                    \"function to avoid ambiguity.\"\n                )\n                raise TypeError(msg)\n            self.afunc = func\n            func_for_name = func\n        elif callable(func):\n            self.func = cast(Callable[[Input], Output], func)\n            func_for_name = func\n        else:\n            msg = (\n                \"Expected a callable type for `func`.\"\n                f\"Instead got an unsupported type: {type(func)}\"\n            )\n            raise TypeError(msg)\n\n        try:\n            if name is not None:\n                self.name = name\n            elif func_for_name.__name__ != \"<lambda>\":\n                self.name = func_for_name.__name__\n        except AttributeError:\n            pass\n\n        self._repr: Optional[str] = None\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        \"\"\"The type of the input to this Runnable.\"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n        try:\n            params = inspect.signature(func).parameters\n            first_param = next(iter(params.values()), None)\n            if first_param and first_param.annotation != inspect.Parameter.empty:\n                return first_param.annotation\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"The pydantic schema for the input to this Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema for this Runnable.\n        \"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n\n        if isinstance(func, itemgetter):\n            # This is terrible, but afaict it's not possible to access _items\n            # on itemgetter objects, so we have to parse the repr\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\n            if all(\n                item[0] == \"'\" and item[-1] == \"'\" and len(item) > 2 for item in items\n            ):\n                fields = {item[1:-1]: (Any, ...) for item in items}\n                # It's a dict, lol\n                return create_model_v2(self.get_name(\"Input\"), field_definitions=fields)\n            else:\n                module = getattr(func, \"__module__\", None)\n                return create_model_v2(\n                    self.get_name(\"Input\"),\n                    root=list[Any],\n                    # To create the schema, we need to provide the module\n                    # where the underlying function is defined.\n                    # This allows pydantic to resolve type annotations appropriately.\n                    module_name=module,\n                )\n\n        if self.InputType != Any:\n            return super().get_input_schema(config)\n\n        if dict_keys := get_function_first_arg_dict_keys(func):\n            return create_model_v2(\n                self.get_name(\"Input\"),\n                field_definitions=dict.fromkeys(dict_keys, (Any, ...)),\n            )\n\n        return super().get_input_schema(config)\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        \"\"\"The type of the output of this Runnable as a type annotation.\n\n        Returns:\n            The type of the output of this Runnable.\n        \"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n        try:\n            sig = inspect.signature(func)\n            if sig.return_annotation != inspect.Signature.empty:\n                # unwrap iterator types\n                if getattr(sig.return_annotation, \"__origin__\", None) in (\n                    collections.abc.Iterator,\n                    collections.abc.AsyncIterator,\n                ):\n                    return getattr(sig.return_annotation, \"__args__\", (Any,))[0]\n                return sig.return_annotation\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable lambda, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.OutputType\n        func = getattr(self, \"func\", None) or self.afunc\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    @functools.cached_property\n    def deps(self) -> list[Runnable]:\n        \"\"\"The dependencies of this Runnable.\n\n        Returns:\n            The dependencies of this Runnable. If the function has nonlocal\n            variables that are Runnables, they are considered dependencies.\n        \"\"\"\n        if hasattr(self, \"func\"):\n            objects = get_function_nonlocals(self.func)\n        elif hasattr(self, \"afunc\"):\n            objects = get_function_nonlocals(self.afunc)\n        else:\n            objects = []\n\n        deps: list[Runnable] = []\n        for obj in objects:\n            if isinstance(obj, Runnable):\n                deps.append(obj)\n            elif isinstance(getattr(obj, \"__self__\", None), Runnable):\n                deps.append(obj.__self__)\n        return deps\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return get_unique_config_specs(\n            spec for dep in self.deps for spec in dep.config_specs\n        )\n\n    def get_graph(self, config: RunnableConfig | None = None) -> Graph:\n        if deps := self.deps:\n            graph = Graph()\n            input_node = graph.add_node(self.get_input_schema(config))\n            output_node = graph.add_node(self.get_output_schema(config))\n            for dep in deps:\n                dep_graph = dep.get_graph()\n                dep_graph.trim_first_node()\n                dep_graph.trim_last_node()\n                if not dep_graph:\n                    graph.add_edge(input_node, output_node)\n                else:\n                    dep_first_node, dep_last_node = graph.extend(dep_graph)\n                    if not dep_first_node:\n                        msg = f\"Runnable {dep} has no first node\"\n                        raise ValueError(msg)\n                    if not dep_last_node:\n                        msg = f\"Runnable {dep} has no last node\"\n                        raise ValueError(msg)\n                    graph.add_edge(input_node, dep_first_node)\n                    graph.add_edge(dep_last_node, output_node)\n        else:\n            graph = super().get_graph(config)\n\n        return graph\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, RunnableLambda):\n            if hasattr(self, \"func\") and hasattr(other, \"func\"):\n                return self.func == other.func\n            elif hasattr(self, \"afunc\") and hasattr(other, \"afunc\"):\n                return self.afunc == other.afunc\n            else:\n                return False\n        else:\n            return False\n\n    def __repr__(self) -> str:\n        \"\"\"A string representation of this Runnable.\"\"\"\n        if self._repr is None:\n            if hasattr(self, \"func\") and isinstance(self.func, itemgetter):\n                self._repr = f\"RunnableLambda({str(self.func)[len('operator.') :]})\"\n            elif hasattr(self, \"func\"):\n                self._repr = f\"RunnableLambda({get_lambda_source(self.func) or '...'})\"\n            elif hasattr(self, \"afunc\"):\n                self._repr = (\n                    f\"RunnableLambda(afunc={get_lambda_source(self.afunc) or '...'})\"\n                )\n            else:\n                self._repr = \"RunnableLambda(...)\"\n        return self._repr\n\n    def _invoke(\n        self,\n        input: Input,\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Output:\n        if inspect.isgeneratorfunction(self.func):\n            output: Optional[Output] = None\n            for chunk in call_func_with_variable_args(\n                cast(Callable[[Input], Iterator[Output]], self.func),\n                input,\n                config,\n                run_manager,\n                **kwargs,\n            ):\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk  # type: ignore[operator]\n                    except TypeError:\n                        output = chunk\n        else:\n            output = call_func_with_variable_args(\n                self.func, input, config, run_manager, **kwargs\n            )\n        # If the output is a Runnable, invoke it\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {input}.\"\n                )\n                raise RecursionError(msg)\n            output = output.invoke(\n                input,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            )\n        return cast(Output, output)\n\n    async def _ainvoke(\n        self,\n        input: Input,\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Output:\n        if hasattr(self, \"afunc\"):\n            afunc = self.afunc\n        else:\n            if inspect.isgeneratorfunction(self.func):\n\n                def func(\n                    input: Input,\n                    run_manager: AsyncCallbackManagerForChainRun,\n                    config: RunnableConfig,\n                    **kwargs: Any,\n                ) -> Output:\n                    output: Optional[Output] = None\n                    for chunk in call_func_with_variable_args(\n                        cast(Callable[[Input], Iterator[Output]], self.func),\n                        input,\n                        config,\n                        run_manager.get_sync(),\n                        **kwargs,\n                    ):\n                        if output is None:\n                            output = chunk\n                        else:\n                            try:\n                                output = output + chunk  # type: ignore[operator]\n                            except TypeError:\n                                output = chunk\n                    return cast(Output, output)\n\n            else:\n\n                def func(\n                    input: Input,\n                    run_manager: AsyncCallbackManagerForChainRun,\n                    config: RunnableConfig,\n                    **kwargs: Any,\n                ) -> Output:\n                    return call_func_with_variable_args(\n                        self.func, input, config, run_manager.get_sync(), **kwargs\n                    )\n\n            @wraps(func)\n            async def f(*args: Any, **kwargs: Any) -> Any:\n                return await run_in_executor(config, func, *args, **kwargs)\n\n            afunc = f\n\n        if is_async_generator(afunc):\n            output: Optional[Output] = None\n            async with aclosing(\n                cast(\n                    AsyncGenerator[Any, Any],\n                    acall_func_with_variable_args(\n                        cast(Callable, afunc),\n                        input,\n                        config,\n                        run_manager,\n                        **kwargs,\n                    ),\n                )\n            ) as stream:\n                async for chunk in cast(\n                    AsyncIterator[Output],\n                    stream,\n                ):\n                    if output is None:\n                        output = chunk\n                    else:\n                        try:\n                            output = output + chunk  # type: ignore[operator]\n                        except TypeError:\n                            output = chunk\n        else:\n            output = await acall_func_with_variable_args(\n                cast(Callable, afunc), input, config, run_manager, **kwargs\n            )\n        # If the output is a Runnable, invoke it\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {input}.\"\n                )\n                raise RecursionError(msg)\n            output = await output.ainvoke(\n                input,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            )\n        return cast(Output, output)\n\n    def _config(\n        self, config: Optional[RunnableConfig], callable: Callable[..., Any]\n    ) -> RunnableConfig:\n        return ensure_config(config)\n\n    def invoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Invoke this Runnable synchronously.\n\n        Args:\n            input: The input to this Runnable.\n            config: The config to use. Defaults to None.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            The output of this Runnable.\n\n        Raises:\n            TypeError: If the Runnable is a coroutine function.\n        \"\"\"\n        if hasattr(self, \"func\"):\n            return self._call_with_config(\n                self._invoke,\n                input,\n                self._config(config, self.func),\n                **kwargs,\n            )\n        else:\n            msg = (\n                \"Cannot invoke a coroutine function synchronously.\"\n                \"Use `ainvoke` instead.\"\n            )\n            raise TypeError(msg)\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Invoke this Runnable asynchronously.\n\n        Args:\n            input: The input to this Runnable.\n            config: The config to use. Defaults to None.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            The output of this Runnable.\n        \"\"\"\n        the_func = self.afunc if hasattr(self, \"afunc\") else self.func\n        return await self._acall_with_config(\n            self._ainvoke,\n            input,\n            self._config(config, the_func),\n            **kwargs,\n        )\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        final: Input\n        got_first_val = False\n        for ichunk in input:\n            # By definitions, RunnableLambdas consume all input before emitting output.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk.\n            # So we'll iterate until we get to the last chunk!\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if inspect.isgeneratorfunction(self.func):\n            output: Optional[Output] = None\n            for chunk in call_func_with_variable_args(\n                self.func, cast(Input, final), config, run_manager, **kwargs\n            ):\n                yield chunk\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk\n                    except TypeError:\n                        output = chunk\n        else:\n            output = call_func_with_variable_args(\n                self.func, cast(Input, final), config, run_manager, **kwargs\n            )\n\n        # If the output is a Runnable, use its stream output\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {final}.\"\n                )\n                raise RecursionError(msg)\n            for chunk in output.stream(\n                final,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            ):\n                yield chunk\n        elif not inspect.isgeneratorfunction(self.func):\n            # Otherwise, just yield it\n            yield cast(Output, output)\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        if hasattr(self, \"func\"):\n            yield from self._transform_stream_with_config(\n                input,\n                self._transform,\n                self._config(config, self.func),\n                **kwargs,\n            )\n        else:\n            msg = (\n                \"Cannot stream a coroutine function synchronously.\"\n                \"Use `astream` instead.\"\n            )\n            raise TypeError(msg)\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        final: Input\n        got_first_val = False\n        async for ichunk in input:\n            # By definitions, RunnableLambdas consume all input before emitting output.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk.\n            # So we'll iterate until we get to the last chunk!\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if hasattr(self, \"afunc\"):\n            afunc = self.afunc\n        else:\n            if inspect.isgeneratorfunction(self.func):\n                msg = (\n                    \"Cannot stream from a generator function asynchronously.\"\n                    \"Use .stream() instead.\"\n                )\n                raise TypeError(msg)\n\n            def func(\n                input: Input,\n                run_manager: AsyncCallbackManagerForChainRun,\n                config: RunnableConfig,\n                **kwargs: Any,\n            ) -> Output:\n                return call_func_with_variable_args(\n                    self.func, input, config, run_manager.get_sync(), **kwargs\n                )\n\n            @wraps(func)\n            async def f(*args: Any, **kwargs: Any) -> Any:\n                return await run_in_executor(config, func, *args, **kwargs)\n\n            afunc = f\n\n        if is_async_generator(afunc):\n            output: Optional[Output] = None\n            async for chunk in cast(\n                AsyncIterator[Output],\n                acall_func_with_variable_args(\n                    cast(Callable, afunc),\n                    cast(Input, final),\n                    config,\n                    run_manager,\n                    **kwargs,\n                ),\n            ):\n                yield chunk\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk  # type: ignore[operator]\n                    except TypeError:\n                        output = chunk\n        else:\n            output = await acall_func_with_variable_args(\n                cast(Callable, afunc), cast(Input, final), config, run_manager, **kwargs\n            )\n\n        # If the output is a Runnable, use its astream output\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {final}.\"\n                )\n                raise RecursionError(msg)\n            async for chunk in output.astream(\n                final,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            ):\n                yield chunk\n        elif not is_async_generator(afunc):\n            # Otherwise, just yield it\n            yield cast(Output, output)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for output in self._atransform_stream_with_config(\n            input,\n            self._atransform,\n            self._config(config, self.afunc if hasattr(self, \"afunc\") else self.func),\n            **kwargs,\n        ):\n            yield output\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\nclass RunnableEachBase(RunnableSerializable[list[Input], list[Output]]):\n    \"\"\"Runnable that delegates calls to another Runnable\n    with each element of the input sequence.\n\n    Use only if creating a new RunnableEach subclass with different __init__ args.\n\n    See documentation for RunnableEach for more details.\n    \"\"\"\n\n    bound: Runnable[Input, Output]\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        return list[self.bound.InputType]  # type: ignore[name-defined]\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=(\n                list[self.bound.get_input_schema(config)],  # type: ignore\n                None,\n            ),\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    @property\n    @override\n    def OutputType(self) -> type[list[Output]]:\n        return list[self.bound.OutputType]  # type: ignore[name-defined]\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        schema = self.bound.get_output_schema(config)\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=list[schema],  # type: ignore[valid-type]\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return self.bound.config_specs\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        return self.bound.get_graph(config)\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def _invoke(\n        self,\n        inputs: list[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> list[Output]:\n        configs = [\n            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs\n        ]\n        return self.bound.batch(inputs, configs, **kwargs)\n\n    def invoke(\n        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> list[Output]:\n        return self._call_with_config(self._invoke, input, config, **kwargs)\n\n    async def _ainvoke(\n        self,\n        inputs: list[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> list[Output]:\n        configs = [\n            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs\n        ]\n        return await self.bound.abatch(inputs, configs, **kwargs)\n\n    async def ainvoke(\n        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> list[Output]:\n        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)\n\n    async def astream_events(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[StreamEvent]:\n        for _ in range(1):\n            msg = \"RunnableEach does not support astream_events yet.\"\n            raise NotImplementedError(msg)\n            yield\n\n\nclass RunnableEach(RunnableEachBase[Input, Output]):\n    \"\"\"Runnable that delegates calls to another Runnable\n    with each element of the input sequence.\n\n    It allows you to call multiple inputs with the bounded Runnable.\n\n    RunnableEach makes it easy to run multiple inputs for the Runnable.\n    In the below example, we associate and run three inputs\n    with a Runnable:\n\n        .. code-block:: python\n\n            from langchain_core.runnables.base import RunnableEach\n            from langchain_openai import ChatOpenAI\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.output_parsers import StrOutputParser\n            prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about\n            {topic}\")\n            model = ChatOpenAI()\n            output_parser = StrOutputParser()\n            runnable = prompt | model | output_parser\n            runnable_each = RunnableEach(bound=runnable)\n            output = runnable_each.invoke([{'topic':'Computer Science'},\n                                        {'topic':'Art'},\n                                        {'topic':'Biology'}])\n            print(output)  # noqa: T201\n    \"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        name = name or self.name or f\"RunnableEach<{self.bound.get_name()}>\"\n        return super().get_name(suffix, name=name)\n\n    def bind(self, **kwargs: Any) -> RunnableEach[Input, Output]:\n        return RunnableEach(bound=self.bound.bind(**kwargs))\n\n    def with_config(\n        self, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> RunnableEach[Input, Output]:\n        return RunnableEach(bound=self.bound.with_config(config, **kwargs))\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> RunnableEach[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called before the Runnable starts running, with the Run object.\n                Defaults to None.\n            on_end: Called after the Runnable finishes running, with the Run object.\n                Defaults to None.\n            on_error: Called if the Runnable throws an error, with the Run object.\n                Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n        \"\"\"\n        return RunnableEach(\n            bound=self.bound.with_listeners(\n                on_start=on_start, on_end=on_end, on_error=on_error\n            )\n        )\n\n    def with_alisteners(\n        self,\n        *,\n        on_start: Optional[AsyncListener] = None,\n        on_end: Optional[AsyncListener] = None,\n        on_error: Optional[AsyncListener] = None,\n    ) -> RunnableEach[Input, Output]:\n        \"\"\"Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called asynchronously before the Runnable starts running,\n                      with the Run object. Defaults to None.\n            on_end: Called asynchronously after the Runnable finishes running,\n                    with the Run object. Defaults to None.\n            on_error: Called asynchronously if the Runnable throws an error,\n                    with the Run object. Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n        \"\"\"\n        return RunnableEach(\n            bound=self.bound.with_alisteners(\n                on_start=on_start, on_end=on_end, on_error=on_error\n            )\n        )\n\n\nclass RunnableBindingBase(RunnableSerializable[Input, Output]):\n    \"\"\"Runnable that delegates calls to another Runnable with a set of kwargs.\n\n    Use only if creating a new RunnableBinding subclass with different __init__ args.\n\n    See documentation for RunnableBinding for more details.\n    \"\"\"\n\n    bound: Runnable[Input, Output]\n    \"\"\"The underlying Runnable that this Runnable delegates to.\"\"\"\n\n    kwargs: Mapping[str, Any] = Field(default_factory=dict)\n    \"\"\"kwargs to pass to the underlying Runnable when running.\n\n    For example, when the Runnable binding is invoked the underlying\n    Runnable will be invoked with the same input but with these additional\n    kwargs.\n    \"\"\"\n\n    config: RunnableConfig = Field(default_factory=RunnableConfig)  # type: ignore\n    \"\"\"The config to bind to the underlying Runnable.\"\"\"\n\n    config_factories: list[Callable[[RunnableConfig], RunnableConfig]] = Field(\n        default_factory=list\n    )\n    \"\"\"The config factories to bind to the underlying Runnable.\"\"\"\n\n    # Union[Type[Input], BaseModel] + things like List[str]\n    custom_input_type: Optional[Any] = None\n    \"\"\"Override the input type of the underlying Runnable with a custom type.\n\n    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n    \"\"\"\n    # Union[Type[Output], BaseModel] + things like List[str]\n    custom_output_type: Optional[Any] = None\n    \"\"\"Override the output type of the underlying Runnable with a custom type.\n\n    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    def __init__(\n        self,\n        *,\n        bound: Runnable[Input, Output],\n        kwargs: Optional[Mapping[str, Any]] = None,\n        config: Optional[RunnableConfig] = None,\n        config_factories: Optional[\n            list[Callable[[RunnableConfig], RunnableConfig]]\n        ] = None,\n        custom_input_type: Optional[Union[type[Input], BaseModel]] = None,\n        custom_output_type: Optional[Union[type[Output], BaseModel]] = None,\n        **other_kwargs: Any,\n    ) -> None:\n        \"\"\"Create a RunnableBinding from a Runnable and kwargs.\n\n        Args:\n            bound: The underlying Runnable that this Runnable delegates calls to.\n            kwargs: optional kwargs to pass to the underlying Runnable, when running\n                    the underlying Runnable (e.g., via `invoke`, `batch`,\n                    `transform`, or `stream` or async variants)\n                    Defaults to None.\n            config: optional config to bind to the underlying Runnable.\n                    Defaults to None.\n            config_factories: optional list of config factories to apply to the\n                    config before binding to the underlying Runnable.\n                    Defaults to None.\n            custom_input_type: Specify to override the input type of the underlying\n                               Runnable with a custom type. Defaults to None.\n            custom_output_type: Specify to override the output type of the underlying\n                Runnable with a custom type. Defaults to None.\n            **other_kwargs: Unpacked into the base class.\n        \"\"\"\n        super().__init__(  # type: ignore[call-arg]\n            bound=bound,\n            kwargs=kwargs or {},\n            config=config or {},\n            config_factories=config_factories or [],\n            custom_input_type=custom_input_type,\n            custom_output_type=custom_output_type,\n            **other_kwargs,\n        )\n        # if we don't explicitly set config to the TypedDict here,\n        # the pydantic init above will strip out any of the \"extra\"\n        # fields even though total=False on the typed dict.\n        self.config = config or {}\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        return self.bound.get_name(suffix, name=name)\n\n    @property\n    @override\n    def InputType(self) -> type[Input]:\n        return (\n            cast(type[Input], self.custom_input_type)\n            if self.custom_input_type is not None\n            else self.bound.InputType\n        )\n\n    @property\n    @override\n    def OutputType(self) -> type[Output]:\n        return (\n            cast(type[Output], self.custom_output_type)\n            if self.custom_output_type is not None\n            else self.bound.OutputType\n        )\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        if self.custom_input_type is not None:\n            return super().get_input_schema(config)\n        return self.bound.get_input_schema(merge_configs(self.config, config))\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        if self.custom_output_type is not None:\n            return super().get_output_schema(config)\n        return self.bound.get_output_schema(merge_configs(self.config, config))\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return self.bound.config_specs\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        return self.bound.get_graph(self._merge_configs(config))\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:\n        config = merge_configs(self.config, *configs)\n        return merge_configs(config, *(f(config) for f in self.config_factories))\n\n    def invoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        return self.bound.invoke(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        return await self.bound.ainvoke(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if isinstance(config, list):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        return self.bound.batch(\n            inputs,\n            configs,\n            return_exceptions=return_exceptions,\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if isinstance(config, list):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        return await self.bound.abatch(\n            inputs,\n            configs,\n            return_exceptions=return_exceptions,\n            **{**self.kwargs, **kwargs},\n        )\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Output]]: ...\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...\n\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]:\n        if isinstance(config, Sequence):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        # lol mypy\n        if return_exceptions:\n            yield from self.bound.batch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            )\n        else:\n            yield from self.bound.batch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            )\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Output]]: ...\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...\n\n    async def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:\n        if isinstance(config, Sequence):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        if return_exceptions:\n            async for item in self.bound.abatch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            ):\n                yield item\n        else:\n            async for item in self.bound.abatch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            ):\n                yield item\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self.bound.stream(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for item in self.bound.astream(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        ):\n            yield item\n\n    async def astream_events(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[StreamEvent]:\n        async for item in self.bound.astream_events(\n            input, self._merge_configs(config), **{**self.kwargs, **kwargs}\n        ):\n            yield item\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        yield from self.bound.transform(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        async for item in self.bound.atransform(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        ):\n            yield item\n\n\nRunnableBindingBase.model_rebuild()\n\n\nclass RunnableBinding(RunnableBindingBase[Input, Output]):\n    \"\"\"Wrap a Runnable with additional functionality.\n\n    A RunnableBinding can be thought of as a \"runnable decorator\" that\n    preserves the essential features of Runnable; i.e., batching, streaming,\n    and async support, while adding additional functionality.\n\n    Any class that inherits from Runnable can be bound to a `RunnableBinding`.\n    Runnables expose a standard set of methods for creating `RunnableBindings`\n    or sub-classes of `RunnableBindings` (e.g., `RunnableRetry`,\n    `RunnableWithFallbacks`) that add additional functionality.\n\n    These methods include:\n\n    - ``bind``: Bind kwargs to pass to the underlying Runnable when running it.\n    - ``with_config``: Bind config to pass to the underlying Runnable when running it.\n    - ``with_listeners``:  Bind lifecycle listeners to the underlying Runnable.\n    - ``with_types``: Override the input and output types of the underlying Runnable.\n    - ``with_retry``: Bind a retry policy to the underlying Runnable.\n    - ``with_fallbacks``: Bind a fallback policy to the underlying Runnable.\n\n    Example:\n    `bind`: Bind kwargs to pass to the underlying Runnable when running it.\n\n        .. code-block:: python\n\n            # Create a Runnable binding that invokes the ChatModel with the\n            # additional kwarg `stop=['-']` when running it.\n            from langchain_community.chat_models import ChatOpenAI\n            model = ChatOpenAI()\n            model.invoke('Say \"Parrot-MAGIC\"', stop=['-']) # Should return `Parrot`\n            # Using it the easy way via `bind` method which returns a new\n            # RunnableBinding\n            runnable_binding = model.bind(stop=['-'])\n            runnable_binding.invoke('Say \"Parrot-MAGIC\"') # Should return `Parrot`\n\n        Can also be done by instantiating a RunnableBinding directly (not recommended):\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableBinding\n            runnable_binding = RunnableBinding(\n                bound=model,\n                kwargs={'stop': ['-']} # <-- Note the additional kwargs\n            )\n            runnable_binding.invoke('Say \"Parrot-MAGIC\"') # Should return `Parrot`\n    \"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:\n        \"\"\"Bind additional kwargs to a Runnable, returning a new Runnable.\n\n        Args:\n            **kwargs: The kwargs to bind to the Runnable.\n\n        Returns:\n            A new Runnable with the same type and config as the original,\n            but with the additional kwargs bound.\n        \"\"\"\n        return self.__class__(\n            bound=self.bound,\n            config=self.config,\n            kwargs={**self.kwargs, **kwargs},\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_config(\n        self,\n        config: Optional[RunnableConfig] = None,\n        # Sadly Unpack is not well supported by mypy so this will have to be untyped\n        **kwargs: Any,\n    ) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=cast(RunnableConfig, {**self.config, **(config or {}), **kwargs}),\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called before the Runnable starts running, with the Run object.\n                Defaults to None.\n            on_end: Called after the Runnable finishes running, with the Run object.\n                Defaults to None.\n            on_error: Called if the Runnable throws an error, with the Run object.\n                Defaults to None.\n\n        Returns:\n            The Runnable object contains information about the run, including its id,\n            type, input, output, error, start_time, end_time, and any tags or metadata\n            added to the run.\n        \"\"\"\n        from langchain_core.tracers.root_listeners import RootListenersTracer\n\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=self.config,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        RootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_types(\n        self,\n        input_type: Optional[Union[type[Input], BaseModel]] = None,\n        output_type: Optional[Union[type[Output], BaseModel]] = None,\n    ) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=self.config,\n            custom_input_type=(\n                input_type if input_type is not None else self.custom_input_type\n            ),\n            custom_output_type=(\n                output_type if output_type is not None else self.custom_output_type\n            ),\n        )\n\n    def with_retry(self, **kwargs: Any) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound.with_retry(**kwargs),\n            kwargs=self.kwargs,\n            config=self.config,\n        )\n\n    def __getattr__(self, name: str) -> Any:\n        attr = getattr(self.bound, name)\n\n        if callable(attr) and (\n            config_param := inspect.signature(attr).parameters.get(\"config\")\n        ):\n            if config_param.kind == inspect.Parameter.KEYWORD_ONLY:\n\n                @wraps(attr)\n                def wrapper(*args: Any, **kwargs: Any) -> Any:\n                    return attr(\n                        *args,\n                        config=merge_configs(self.config, kwargs.pop(\"config\", None)),\n                        **kwargs,\n                    )\n\n                return wrapper\n            elif config_param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                idx = list(inspect.signature(attr).parameters).index(\"config\")\n\n                @wraps(attr)\n                def wrapper(*args: Any, **kwargs: Any) -> Any:\n                    if len(args) >= idx + 1:\n                        argsl = list(args)\n                        argsl[idx] = merge_configs(self.config, argsl[idx])\n                        return attr(*argsl, **kwargs)\n                    else:\n                        return attr(\n                            *args,\n                            config=merge_configs(\n                                self.config, kwargs.pop(\"config\", None)\n                            ),\n                            **kwargs,\n                        )\n\n                return wrapper\n\n        return attr\n\n\nclass _RunnableCallableSync(Protocol[Input, Output]):\n    def __call__(self, __in: Input, *, config: RunnableConfig) -> Output: ...\n\n\nclass _RunnableCallableAsync(Protocol[Input, Output]):\n    def __call__(self, __in: Input, *, config: RunnableConfig) -> Awaitable[Output]: ...\n\n\nclass _RunnableCallableIterator(Protocol[Input, Output]):\n    def __call__(\n        self, __in: Iterator[Input], *, config: RunnableConfig\n    ) -> Iterator[Output]: ...\n\n\nclass _RunnableCallableAsyncIterator(Protocol[Input, Output]):\n    def __call__(\n        self, __in: AsyncIterator[Input], *, config: RunnableConfig\n    ) -> AsyncIterator[Output]: ...\n\n\nRunnableLike = Union[\n    Runnable[Input, Output],\n    Callable[[Input], Output],\n    Callable[[Input], Awaitable[Output]],\n    Callable[[Iterator[Input]], Iterator[Output]],\n    Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n    _RunnableCallableSync[Input, Output],\n    _RunnableCallableAsync[Input, Output],\n    _RunnableCallableIterator[Input, Output],\n    _RunnableCallableAsyncIterator[Input, Output],\n    Mapping[str, Any],\n]\n\n\ndef coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:\n    \"\"\"Coerce a Runnable-like object into a Runnable.\n\n    Args:\n        thing: A Runnable-like object.\n\n    Returns:\n        A Runnable.\n\n    Raises:\n        TypeError: If the object is not Runnable-like.\n    \"\"\"\n    if isinstance(thing, Runnable):\n        return thing\n    elif is_async_generator(thing) or inspect.isgeneratorfunction(thing):\n        return RunnableGenerator(thing)\n    elif callable(thing):\n        return RunnableLambda(cast(Callable[[Input], Output], thing))\n    elif isinstance(thing, dict):\n        return cast(Runnable[Input, Output], RunnableParallel(thing))\n    else:\n        msg = (\n            f\"Expected a Runnable, callable or dict.\"\n            f\"Instead got an unsupported type: {type(thing)}\"\n        )\n        raise TypeError(msg)\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Coroutine[Any, Any, Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Iterator[Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], AsyncIterator[Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Output],\n) -> Runnable[Input, Output]: ...\n\n\ndef chain(\n    func: Union[\n        Callable[[Input], Output],\n        Callable[[Input], Iterator[Output]],\n        Callable[[Input], Coroutine[Any, Any, Output]],\n        Callable[[Input], AsyncIterator[Output]],\n    ],\n) -> Runnable[Input, Output]:\n    \"\"\"Decorate a function to make it a Runnable.\n    Sets the name of the Runnable to the name of the function.\n    Any runnables called by the function will be traced as dependencies.\n\n    Args:\n        func: A callable.\n\n    Returns:\n        A Runnable.\n\n    Example:\n\n    .. code-block:: python\n\n        from langchain_core.runnables import chain\n        from langchain_core.prompts import PromptTemplate\n        from langchain_openai import OpenAI\n\n        @chain\n        def my_func(fields):\n            prompt = PromptTemplate(\"Hello, {name}!\")\n            llm = OpenAI()\n            formatted = prompt.invoke(**fields)\n\n            for chunk in llm.stream(formatted):\n                yield chunk\n    \"\"\"\n    return RunnableLambda(func)\n",
        "patch": "@@ -1644,21 +1644,26 @@ def with_alisteners(\n \n         .. code-block:: python\n \n-            from langchain_core.runnables import RunnableLambda\n+            from langchain_core.runnables import RunnableLambda, Runnable\n+            from datetime import datetime, timezone\n             import time\n+            import asyncio\n+\n+            def format_t(timestamp: float) -> str:\n+                return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n \n             async def test_runnable(time_to_sleep : int):\n                 print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n                 await asyncio.sleep(time_to_sleep)\n                 print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n \n             async def fn_start(run_obj : Runnable):\n-                print(f\"on start callback starts at {format_t(time.time())}\n+                print(f\"on start callback starts at {format_t(time.time())}\")\n                 await asyncio.sleep(3)\n                 print(f\"on start callback ends at {format_t(time.time())}\")\n \n             async def fn_end(run_obj : Runnable):\n-                print(f\"on end callback starts at {format_t(time.time())}\n+                print(f\"on end callback starts at {format_t(time.time())}\")\n                 await asyncio.sleep(2)\n                 print(f\"on end callback ends at {format_t(time.time())}\")\n \n@@ -1671,18 +1676,18 @@ async def concurrent_runs():\n \n             asyncio.run(concurrent_runs())\n             Result:\n-            on start callback starts at 2024-05-16T14:20:29.637053+00:00\n-            on start callback starts at 2024-05-16T14:20:29.637150+00:00\n-            on start callback ends at 2024-05-16T14:20:32.638305+00:00\n-            on start callback ends at 2024-05-16T14:20:32.638383+00:00\n-            Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n-            Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n-            Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n-            on end callback starts at 2024-05-16T14:20:35.640534+00:00\n-            Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n-            on end callback starts at 2024-05-16T14:20:37.640574+00:00\n-            on end callback ends at 2024-05-16T14:20:37.640654+00:00\n-            on end callback ends at 2024-05-16T14:20:39.641751+00:00\n+            on start callback starts at 2025-03-01T07:05:22.875378+00:00\n+            on start callback starts at 2025-03-01T07:05:22.875495+00:00\n+            on start callback ends at 2025-03-01T07:05:25.878862+00:00\n+            on start callback ends at 2025-03-01T07:05:25.878947+00:00\n+            Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n+            Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n+            Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n+            on end callback starts at 2025-03-01T07:05:27.882360+00:00\n+            Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n+            on end callback starts at 2025-03-01T07:05:28.882428+00:00\n+            on end callback ends at 2025-03-01T07:05:29.883893+00:00\n+            on end callback ends at 2025-03-01T07:05:30.884831+00:00\n \n         \"\"\"\n         from langchain_core.tracers.root_listeners import AsyncRootListenersTracer"
      }
    ]
  },
  {
    "number": 30060,
    "title": "Update ascend.py",
    "body": "add batch_size to fix oom when embed large amount texts\r\n\r\nThank you for contributing to LangChain!\r\n\r\n- [ ] **PR title**: \"package: description\"\r\n  - Where \"package\" is whichever of langchain, community, core, etc. is being modified. Use \"docs: ...\" for purely docs changes, \"infra: ...\" for CI changes.\r\n  - Example: \"community: add foobar LLM\"\r\n\r\n\r\n- [ ] **PR message**: ***Delete this entire checklist*** and replace with\r\n    - **Description:** a description of the change\r\n    - **Issue:** the issue # it fixes, if applicable\r\n    - **Dependencies:** any dependencies required for this change\r\n    - **Twitter handle:** if your PR gets announced, and you'd like a mention, we'll gladly shout you out!\r\n\r\n\r\n- [ ] **Add tests and docs**: If you're adding a new integration, please include\r\n  1. a test for the integration, preferably unit tests that do not rely on network access,\r\n  2. an example notebook showing its use. It lives in `docs/docs/integrations` directory.\r\n\r\n\r\n- [ ] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n\r\nAdditional guidelines:\r\n- Make sure optional dependencies are imported within a function.\r\n- Please do not add dependencies to pyproject.toml files (even optional ones) unless they are required for unit tests.\r\n- Most PRs should not touch more than one package.\r\n- Changes should be backwards compatible.\r\n- If you are adding something to community, do not re-import it in langchain.\r\n\r\nIf no one reviews your PR within a few days, please @-mention one of baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\r\n",
    "issue_title": "Update ascend.py",
    "issue_body": "add batch_size to fix oom when embed large amount texts\r\n\r\nThank you for contributing to LangChain!\r\n\r\n- [ ] **PR title**: \"package: description\"\r\n  - Where \"package\" is whichever of langchain, community, core, etc. is being modified. Use \"docs: ...\" for purely docs changes, \"infra: ...\" for CI changes.\r\n  - Example: \"community: add foobar LLM\"\r\n\r\n\r\n- [ ] **PR message**: ***Delete this entire checklist*** and replace with\r\n    - **Description:** a description of the change\r\n    - **Issue:** the issue # it fixes, if applicable\r\n    - **Dependencies:** any dependencies required for this change\r\n    - **Twitter handle:** if your PR gets announced, and you'd like a mention, we'll gladly shout you out!\r\n\r\n\r\n- [ ] **Add tests and docs**: If you're adding a new integration, please include\r\n  1. a test for the integration, preferably unit tests that do not rely on network access,\r\n  2. an example notebook showing its use. It lives in `docs/docs/integrations` directory.\r\n\r\n\r\n- [ ] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n\r\nAdditional guidelines:\r\n- Make sure optional dependencies are imported within a function.\r\n- Please do not add dependencies to pyproject.toml files (even optional ones) unless they are required for unit tests.\r\n- Most PRs should not touch more than one package.\r\n- Changes should be backwards compatible.\r\n- If you are adding something to community, do not re-import it in langchain.\r\n\r\nIf no one reviews your PR within a few days, please @-mention one of baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/embeddings/ascend.py",
        "content_before": "import os\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain_core.embeddings import Embeddings\nfrom pydantic import BaseModel, ConfigDict, model_validator\n\n\nclass AscendEmbeddings(Embeddings, BaseModel):\n    \"\"\"\n    Ascend NPU accelerate Embedding model\n\n    Please ensure that you have installed CANN and torch_npu.\n\n    Example:\n\n    from langchain_community.embeddings import AscendEmbeddings\n    model = AscendEmbeddings(model_path=<path_to_model>,\n        device_id=0,\n        query_instruction=\"Represent this sentence for searching relevant passages: \"\n    )\n    \"\"\"\n\n    \"\"\"model path\"\"\"\n    model_path: str\n    \"\"\"Ascend NPU device id.\"\"\"\n    device_id: int = 0\n    \"\"\"Unstruntion to used for embedding query.\"\"\"\n    query_instruction: str = \"\"\n    \"\"\"Unstruntion to used for embedding document.\"\"\"\n    document_instruction: str = \"\"\n    use_fp16: bool = True\n    pooling_method: Optional[str] = \"cls\"\n    model: Any\n    tokenizer: Any\n\n    model_config = ConfigDict(protected_namespaces=())\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        try:\n            from transformers import AutoModel, AutoTokenizer\n        except ImportError as e:\n            raise ImportError(\n                \"Unable to import transformers, please install with \"\n                \"`pip install -U transformers`.\"\n            ) from e\n        try:\n            self.model = AutoModel.from_pretrained(self.model_path).npu().eval()\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        except Exception as e:\n            raise Exception(\n                f\"Failed to load model [self.model_path], due to following error:{e}\"\n            )\n\n        if self.use_fp16:\n            self.model.half()\n        self.encode([f\"warmup {i} times\" for i in range(10)])\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_environment(cls, values: Dict) -> Any:\n        if \"model_path\" not in values:\n            raise ValueError(\"model_path is required\")\n        if not os.access(values[\"model_path\"], os.F_OK):\n            raise FileNotFoundError(\n                f\"Unable to find valid model path in [{values['model_path']}]\"\n            )\n        try:\n            import torch_npu\n        except ImportError:\n            raise ModuleNotFoundError(\"torch_npu not found, please install torch_npu\")\n        except Exception as e:\n            raise e\n        try:\n            torch_npu.npu.set_device(values[\"device_id\"])\n        except Exception as e:\n            raise Exception(f\"set device failed due to {e}\")\n        return values\n\n    def encode(self, sentences: Any) -> Any:\n        inputs = self.tokenizer(\n            sentences,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n            max_length=512,\n        )\n        try:\n            import torch\n        except ImportError as e:\n            raise ImportError(\n                \"Unable to import torch, please install with `pip install -U torch`.\"\n            ) from e\n        last_hidden_state = self.model(\n            inputs.input_ids.npu(), inputs.attention_mask.npu(), return_dict=True\n        ).last_hidden_state\n        tmp = self.pooling(last_hidden_state, inputs[\"attention_mask\"].npu())\n        embeddings = torch.nn.functional.normalize(tmp, dim=-1)\n        return embeddings.cpu().detach().numpy()\n\n    def pooling(self, last_hidden_state: Any, attention_mask: Any = None) -> Any:\n        try:\n            import torch\n        except ImportError as e:\n            raise ImportError(\n                \"Unable to import torch, please install with `pip install -U torch`.\"\n            ) from e\n        if self.pooling_method == \"cls\":\n            return last_hidden_state[:, 0]\n        elif self.pooling_method == \"mean\":\n            s = torch.sum(\n                last_hidden_state * attention_mask.unsqueeze(-1).float(), dim=-1\n            )\n            d = attention_mask.sum(dim=1, keepdim=True).float()\n            return s / d\n        else:\n            raise NotImplementedError(\n                f\"Pooling method [{self.pooling_method}] not implemented\"\n            )\n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        return self.encode([self.document_instruction + text for text in texts])\n\n    def embed_query(self, text: str) -> List[float]:\n        return self.encode([self.query_instruction + text])[0]\n",
        "patch": "@@ -30,6 +30,7 @@ class AscendEmbeddings(Embeddings, BaseModel):\n     document_instruction: str = \"\"\n     use_fp16: bool = True\n     pooling_method: Optional[str] = \"cls\"\n+    batch_size: int = 32\n     model: Any\n     tokenizer: Any\n \n@@ -119,7 +120,18 @@ def pooling(self, last_hidden_state: Any, attention_mask: Any = None) -> Any:\n             )\n \n     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n-        return self.encode([self.document_instruction + text for text in texts])\n+        try:\n+            import numpy as np\n+        except ImportError as e:\n+            raise ImportError(\n+                \"Unable to import numpy, please install with `pip install -U numpy`.\"\n+            ) from e\n+        embedding_list = []\n+        for i in range(0, len(texts), self.batch_size):\n+            texts_ = texts[i : i + self.batch_size]\n+            emb = self.encode([self.document_instruction + text for text in texts_])\n+            embedding_list.append(emb)\n+        return np.concatenate(embedding_list)\n \n     def embed_query(self, text: str) -> List[float]:\n         return self.encode([self.query_instruction + text])[0]"
      }
    ]
  },
  {
    "number": 30047,
    "title": "anthropic[patch]: allow structured output when thinking is enabled",
    "body": "Structured output will currently always raise a BadRequestError when Claude 3.7 Sonnet's `thinking` is enabled, because we rely on forced tool use for structured output and this feature is not supported when `thinking` is enabled.\r\n\r\nHere we:\r\n- Emit a warning if `with_structured_output` is called when `thinking` is enabled.\r\n- Raise `OutputParserException` if no tool calls are generated.\r\n\r\nThis is arguably preferable to raising an error in all cases.\r\n\r\n```python\r\nfrom langchain_anthropic import ChatAnthropic\r\nfrom pydantic import BaseModel\r\n\r\n\r\nclass Person(BaseModel):\r\n    name: str\r\n    age: int\r\n\r\n\r\nllm = ChatAnthropic(\r\n    model=\"claude-3-7-sonnet-latest\",\r\n    max_tokens=5_000,\r\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\r\n)\r\nstructured_llm = llm.with_structured_output(Person)  # <-- this generates a warning\r\n```\r\n\r\n```python\r\nstructured_llm.invoke(\"Alice is 30.\")  # <-- works\r\n```\r\n\r\n```python\r\nstructured_llm.invoke(\"Hello!\")  # <-- raises OutputParserException\r\n```",
    "issue_title": "anthropic[patch]: allow structured output when thinking is enabled",
    "issue_body": "Structured output will currently always raise a BadRequestError when Claude 3.7 Sonnet's `thinking` is enabled, because we rely on forced tool use for structured output and this feature is not supported when `thinking` is enabled.\r\n\r\nHere we:\r\n- Emit a warning if `with_structured_output` is called when `thinking` is enabled.\r\n- Raise `OutputParserException` if no tool calls are generated.\r\n\r\nThis is arguably preferable to raising an error in all cases.\r\n\r\n```python\r\nfrom langchain_anthropic import ChatAnthropic\r\nfrom pydantic import BaseModel\r\n\r\n\r\nclass Person(BaseModel):\r\n    name: str\r\n    age: int\r\n\r\n\r\nllm = ChatAnthropic(\r\n    model=\"claude-3-7-sonnet-latest\",\r\n    max_tokens=5_000,\r\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\r\n)\r\nstructured_llm = llm.with_structured_output(Person)  # <-- this generates a warning\r\n```\r\n\r\n```python\r\nstructured_llm.invoke(\"Alice is 30.\")  # <-- works\r\n```\r\n\r\n```python\r\nstructured_llm.invoke(\"Hello!\")  # <-- raises OutputParserException\r\n```",
    "files": [
      {
        "filename": "libs/partners/anthropic/langchain_anthropic/chat_models.py",
        "content_before": "import copy\nimport re\nimport warnings\nfrom functools import cached_property\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport anthropic\nfrom langchain_core._api import beta, deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.ai import InputTokenDetails, UsageMetadata\nfrom langchain_core.messages.tool import tool_call_chunk as create_tool_call_chunk\nfrom langchain_core.output_parsers import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n)\nfrom langchain_core.output_parsers.base import OutputParserLike\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableMap,\n    RunnablePassthrough,\n)\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import (\n    from_env,\n    get_pydantic_field_names,\n    secret_from_env,\n)\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import is_basemodel_subclass\nfrom langchain_core.utils.utils import _build_model_kwargs\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SecretStr,\n    model_validator,\n)\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom langchain_anthropic.output_parsers import extract_tool_calls\n\n_message_type_lookups = {\n    \"human\": \"user\",\n    \"ai\": \"assistant\",\n    \"AIMessageChunk\": \"assistant\",\n    \"HumanMessageChunk\": \"user\",\n}\n\n\ndef _format_image(image_url: str) -> Dict:\n    \"\"\"\n    Formats an image of format data:image/jpeg;base64,{b64_string}\n    to a dict for anthropic api\n\n    {\n      \"type\": \"base64\",\n      \"media_type\": \"image/jpeg\",\n      \"data\": \"/9j/4AAQSkZJRg...\",\n    }\n\n    And throws an error if it's not a b64 image\n    \"\"\"\n    regex = r\"^data:(?P<media_type>image/.+);base64,(?P<data>.+)$\"\n    match = re.match(regex, image_url)\n    if match is None:\n        raise ValueError(\n            \"Anthropic only supports base64-encoded images currently.\"\n            \" Example: data:image/png;base64,'/9j/4AAQSk'...\"\n        )\n    return {\n        \"type\": \"base64\",\n        \"media_type\": match.group(\"media_type\"),\n        \"data\": match.group(\"data\"),\n    }\n\n\ndef _merge_messages(\n    messages: Sequence[BaseMessage],\n) -> List[Union[SystemMessage, AIMessage, HumanMessage]]:\n    \"\"\"Merge runs of human/tool messages into single human messages with content blocks.\"\"\"  # noqa: E501\n    merged: list = []\n    for curr in messages:\n        if isinstance(curr, ToolMessage):\n            if (\n                isinstance(curr.content, list)\n                and curr.content\n                and all(\n                    isinstance(block, dict) and block.get(\"type\") == \"tool_result\"\n                    for block in curr.content\n                )\n            ):\n                curr = HumanMessage(curr.content)  # type: ignore[misc]\n            else:\n                curr = HumanMessage(  # type: ignore[misc]\n                    [\n                        {\n                            \"type\": \"tool_result\",\n                            \"content\": curr.content,\n                            \"tool_use_id\": curr.tool_call_id,\n                            \"is_error\": curr.status == \"error\",\n                        }\n                    ]\n                )\n        last = merged[-1] if merged else None\n        if any(\n            all(isinstance(m, c) for m in (curr, last))\n            for c in (SystemMessage, HumanMessage)\n        ):\n            if isinstance(cast(BaseMessage, last).content, str):\n                new_content: List = [\n                    {\"type\": \"text\", \"text\": cast(BaseMessage, last).content}\n                ]\n            else:\n                new_content = copy.copy(cast(list, cast(BaseMessage, last).content))\n            if isinstance(curr.content, str):\n                new_content.append({\"type\": \"text\", \"text\": curr.content})\n            else:\n                new_content.extend(curr.content)\n            merged[-1] = curr.model_copy(update={\"content\": new_content})\n        else:\n            merged.append(curr)\n    return merged\n\n\ndef _format_messages(\n    messages: List[BaseMessage],\n) -> Tuple[Union[str, List[Dict], None], List[Dict]]:\n    \"\"\"Format messages for anthropic.\"\"\"\n\n    \"\"\"\n    [\n                {\n                    \"role\": _message_type_lookups[m.type],\n                    \"content\": [_AnthropicMessageContent(text=m.content).model_dump()],\n                }\n                for m in messages\n            ]\n    \"\"\"\n    system: Union[str, List[Dict], None] = None\n    formatted_messages: List[Dict] = []\n\n    merged_messages = _merge_messages(messages)\n    for i, message in enumerate(merged_messages):\n        if message.type == \"system\":\n            if system is not None:\n                raise ValueError(\"Received multiple non-consecutive system messages.\")\n            elif isinstance(message.content, list):\n                system = [\n                    (\n                        block\n                        if isinstance(block, dict)\n                        else {\"type\": \"text\", \"text\": block}\n                    )\n                    for block in message.content\n                ]\n            else:\n                system = message.content\n            continue\n\n        role = _message_type_lookups[message.type]\n        content: Union[str, List]\n\n        if not isinstance(message.content, str):\n            # parse as dict\n            assert isinstance(\n                message.content, list\n            ), \"Anthropic message content must be str or list of dicts\"\n\n            # populate content\n            content = []\n            for block in message.content:\n                if isinstance(block, str):\n                    content.append({\"type\": \"text\", \"text\": block})\n                elif isinstance(block, dict):\n                    if \"type\" not in block:\n                        raise ValueError(\"Dict content block must have a type key\")\n                    elif block[\"type\"] == \"image_url\":\n                        # convert format\n                        source = _format_image(block[\"image_url\"][\"url\"])\n                        content.append({\"type\": \"image\", \"source\": source})\n                    elif block[\"type\"] == \"tool_use\":\n                        # If a tool_call with the same id as a tool_use content block\n                        # exists, the tool_call is preferred.\n                        if isinstance(message, AIMessage) and block[\"id\"] in [\n                            tc[\"id\"] for tc in message.tool_calls\n                        ]:\n                            overlapping = [\n                                tc\n                                for tc in message.tool_calls\n                                if tc[\"id\"] == block[\"id\"]\n                            ]\n                            content.extend(\n                                _lc_tool_calls_to_anthropic_tool_use_blocks(overlapping)\n                            )\n                        else:\n                            block.pop(\"text\", None)\n                            content.append(block)\n                    elif block[\"type\"] == \"text\":\n                        text = block.get(\"text\", \"\")\n                        # Only add non-empty strings for now as empty ones are not\n                        # accepted.\n                        # https://github.com/anthropics/anthropic-sdk-python/issues/461\n                        if text.strip():\n                            content.append(\n                                {\n                                    k: v\n                                    for k, v in block.items()\n                                    if k in (\"type\", \"text\", \"cache_control\")\n                                }\n                            )\n                    elif block[\"type\"] == \"thinking\":\n                        content.append(\n                            {\n                                k: v\n                                for k, v in block.items()\n                                if k\n                                in (\"type\", \"thinking\", \"cache_control\", \"signature\")\n                            }\n                        )\n                    elif block[\"type\"] == \"redacted_thinking\":\n                        content.append(\n                            {\n                                k: v\n                                for k, v in block.items()\n                                if k in (\"type\", \"cache_control\", \"data\")\n                            }\n                        )\n                    elif block[\"type\"] == \"tool_result\":\n                        tool_content = _format_messages(\n                            [HumanMessage(block[\"content\"])]\n                        )[1][0][\"content\"]\n                        content.append({**block, **{\"content\": tool_content}})\n                    else:\n                        content.append(block)\n                else:\n                    raise ValueError(\n                        f\"Content blocks must be str or dict, instead was: \"\n                        f\"{type(block)}\"\n                    )\n        else:\n            content = message.content\n\n        # Ensure all tool_calls have a tool_use content block\n        if isinstance(message, AIMessage) and message.tool_calls:\n            content = content or []\n            content = (\n                [{\"type\": \"text\", \"text\": message.content}]\n                if isinstance(content, str) and content\n                else content\n            )\n            tool_use_ids = [\n                cast(dict, block)[\"id\"]\n                for block in content\n                if cast(dict, block)[\"type\"] == \"tool_use\"\n            ]\n            missing_tool_calls = [\n                tc for tc in message.tool_calls if tc[\"id\"] not in tool_use_ids\n            ]\n            cast(list, content).extend(\n                _lc_tool_calls_to_anthropic_tool_use_blocks(missing_tool_calls)\n            )\n\n        formatted_messages.append({\"role\": role, \"content\": content})\n    return system, formatted_messages\n\n\nclass ChatAnthropic(BaseChatModel):\n    \"\"\"Anthropic chat models.\n\n    See https://docs.anthropic.com/en/docs/models-overview for a list of the latest models.\n\n    Setup:\n        Install ``langchain-anthropic`` and set environment variable ``ANTHROPIC_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-anthropic\n            export ANTHROPIC_API_KEY=\"your-api-key\"\n\n    Key init args \u2014 completion params:\n        model: str\n            Name of Anthropic model to use. E.g. \"claude-3-sonnet-20240229\".\n        temperature: float\n            Sampling temperature. Ranges from 0.0 to 1.0.\n        max_tokens: int\n            Max number of tokens to generate.\n\n    Key init args \u2014 client params:\n        timeout: Optional[float]\n            Timeout for requests.\n        max_retries: int\n            Max number of retries if a request fails.\n        api_key: Optional[str]\n            Anthropic API key. If not passed in will be read from env var ANTHROPIC_API_KEY.\n        base_url: Optional[str]\n            Base URL for API requests. Only specify if using a proxy or service\n            emulator.\n\n    See full list of supported init args and their descriptions in the params section.\n\n    Instantiate:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(\n                model=\"claude-3-sonnet-20240229\",\n                temperature=0,\n                max_tokens=1024,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # base_url=\"...\",\n                # other params...\n            )\n\n    **NOTE**: Any param which is not explicitly supported will be passed directly to the\n    ``anthropic.Anthropic.messages.create(...)`` API every time to the model is\n    invoked. For example:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n            import anthropic\n\n            ChatAnthropic(..., extra_headers={}).invoke(...)\n\n            # results in underlying API call of:\n\n            anthropic.Anthropic(..).messages.create(..., extra_headers={})\n\n            # which is also equivalent to:\n\n            ChatAnthropic(...).invoke(..., extra_headers={})\n\n    Invoke:\n        .. code-block:: python\n\n            messages = [\n                (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Stream:\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            AIMessageChunk(content='J', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=\"'\", id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='a', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ime', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' la', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' programm', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ation', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='.', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n        .. code-block:: python\n\n            AIMessageChunk(content=\"J'aime la programmation.\", id='run-b34faef0-882f-4869-a19c-ed2b856e6361')\n\n    Async:\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Tool calling:\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n            ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [{'name': 'GetWeather',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01KzpPEAgzura7hpBqwHbWdo'},\n             {'name': 'GetWeather',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JtgbVGVJbiSwtZk3Uycezx'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01429aygngesudV9nTbCKGuw'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JPktyd44tVMeBcPPnFSEJG'}]\n\n        See ``ChatAnthropic.bind_tools()`` method for more.\n\n    Structured output:\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        .. code-block:: python\n\n            Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)\n\n        See ``ChatAnthropic.with_structured_output()`` for more.\n\n    Image input:\n        .. code-block:: python\n\n            import base64\n            import httpx\n            from langchain_core.messages import HumanMessage\n\n            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n            message = HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                    },\n                ],\n            )\n            ai_msg = llm.invoke([message])\n            ai_msg.content\n\n        .. code-block:: python\n\n            \"The image depicts a sunny day with a partly cloudy sky. The sky is a brilliant blue color with scattered white clouds drifting across. The lighting and cloud patterns suggest pleasant, mild weather conditions. The scene shows a grassy field or meadow with a wooden boardwalk trail leading through it, indicating an outdoor setting on a nice day well-suited for enjoying nature.\"\n\n    Extended thinking:\n        Claude 3.7 Sonnet supports an\n        `extended thinking <https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking>`_\n        feature, which will output the step-by-step reasoning process that led to its\n        final answer.\n\n        To use it, specify the `thinking` parameter when initializing `ChatAnthropic`.\n        It can also be passed in as a kwarg during invocation.\n\n        You will need to specify a token budget to use this feature. See usage example:\n\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(\n                model=\"claude-3-7-sonnet-latest\",\n                max_tokens=5000,\n                thinking={\"type\": \"enabled\", \"budget_tokens\": 2000},\n            )\n\n            response = llm.invoke(\"What is the cube root of 50.653?\")\n            response.content\n\n        .. code-block:: python\n\n            [{'signature': '...', 'thinking': \"To find the cube root of 50.653...\", 'type': 'thinking'}, {'text': 'The cube root of 50.653 is ...', 'type': 'text'}]\n\n    Citations:\n        Anthropic supports a\n        `citations <https://docs.anthropic.com/en/docs/build-with-claude/citations>`_\n        feature that lets Claude attach context to its answers based on source\n        documents supplied by the user. When\n        `document content blocks <https://docs.anthropic.com/en/docs/build-with-claude/citations#document-types>`_\n        with ``\"citations\": {\"enabled\": True}`` are included in a query, Claude may\n        generate citations in its response.\n\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"document\",\n                            \"source\": {\n                                \"type\": \"text\",\n                                \"media_type\": \"text/plain\",\n                                \"data\": \"The grass is green. The sky is blue.\",\n                            },\n                            \"title\": \"My Document\",\n                            \"context\": \"This is a trustworthy document.\",\n                            \"citations\": {\"enabled\": True},\n                        },\n                        {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n                    ],\n                }\n            ]\n            response = llm.invoke(messages)\n            response.content\n\n        .. code-block:: python\n\n            [{'text': 'Based on the document, ', 'type': 'text'},\n            {'text': 'the grass is green',\n            'type': 'text',\n            'citations': [{'type': 'char_location',\n                'cited_text': 'The grass is green. ',\n                'document_index': 0,\n                'document_title': 'My Document',\n                'start_char_index': 0,\n                'end_char_index': 20}]},\n            {'text': ', and ', 'type': 'text'},\n            {'text': 'the sky is blue',\n            'type': 'text',\n            'citations': [{'type': 'char_location',\n                'cited_text': 'The sky is blue.',\n                'document_index': 0,\n                'document_title': 'My Document',\n                'start_char_index': 20,\n                'end_char_index': 36}]},\n            {'text': '.', 'type': 'text'}]\n\n    Token usage:\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        Message chunks containing token usage will be included during streaming by\n        default:\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        These can be disabled by setting ``stream_usage=False`` in the stream method,\n        or by setting ``stream_usage=False`` when initializing ChatAnthropic.\n\n    Response metadata\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n        .. code-block:: python\n\n            {'id': 'msg_013xU6FHEGEq76aP4RgFerVT',\n             'model': 'claude-3-sonnet-20240229',\n             'stop_reason': 'end_turn',\n             'stop_sequence': None,\n             'usage': {'input_tokens': 25, 'output_tokens': 11}}\n\n    \"\"\"  # noqa: E501\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n    )\n\n    model: str = Field(alias=\"model_name\")\n    \"\"\"Model name to use.\"\"\"\n\n    max_tokens: int = Field(default=1024, alias=\"max_tokens_to_sample\")\n    \"\"\"Denotes the number of tokens to predict per generation.\"\"\"\n\n    temperature: Optional[float] = None\n    \"\"\"A non-negative float that tunes the degree of randomness in generation.\"\"\"\n\n    top_k: Optional[int] = None\n    \"\"\"Number of most likely tokens to consider at each step.\"\"\"\n\n    top_p: Optional[float] = None\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n\n    default_request_timeout: Optional[float] = Field(None, alias=\"timeout\")\n    \"\"\"Timeout for requests to Anthropic Completion API.\"\"\"\n\n    # sdk default = 2: https://github.com/anthropics/anthropic-sdk-python?tab=readme-ov-file#retries\n    max_retries: int = 2\n    \"\"\"Number of retries allowed for requests sent to the Anthropic Completion API.\"\"\"\n\n    stop_sequences: Optional[List[str]] = Field(None, alias=\"stop\")\n    \"\"\"Default stop sequences.\"\"\"\n\n    anthropic_api_url: Optional[str] = Field(\n        alias=\"base_url\",\n        default_factory=from_env(\n            [\"ANTHROPIC_API_URL\", \"ANTHROPIC_BASE_URL\"],\n            default=\"https://api.anthropic.com\",\n        ),\n    )\n    \"\"\"Base URL for API requests. Only specify if using a proxy or service emulator.\n\n    If a value isn't passed in, will attempt to read the value first from\n    ANTHROPIC_API_URL and if that is not set, ANTHROPIC_BASE_URL.\n    If neither are set, the default value of 'https://api.anthropic.com' will\n    be used.\n    \"\"\"\n\n    anthropic_api_key: SecretStr = Field(\n        alias=\"api_key\",\n        default_factory=secret_from_env(\"ANTHROPIC_API_KEY\", default=\"\"),\n    )\n\n    \"\"\"Automatically read from env var `ANTHROPIC_API_KEY` if not provided.\"\"\"\n\n    default_headers: Optional[Mapping[str, str]] = None\n    \"\"\"Headers to pass to the Anthropic clients, will be used for every API call.\"\"\"\n\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n\n    streaming: bool = False\n    \"\"\"Whether to use streaming or not.\"\"\"\n\n    stream_usage: bool = True\n    \"\"\"Whether to include usage metadata in streaming output. If True, additional\n    message chunks will be generated during the stream including usage metadata.\n    \"\"\"\n\n    thinking: Optional[Dict[str, Any]] = Field(default=None)\n    \"\"\"Parameters for Claude reasoning,\n    e.g., ``{\"type\": \"enabled\", \"budget_tokens\": 10_000}``\"\"\"\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"anthropic-chat\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"anthropic_api_key\": \"ANTHROPIC_API_KEY\"}\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"anthropic\"]\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"model_kwargs\": self.model_kwargs,\n            \"streaming\": self.streaming,\n            \"max_retries\": self.max_retries,\n            \"default_request_timeout\": self.default_request_timeout,\n            \"thinking\": self.thinking,\n        }\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"anthropic\",\n            ls_model_name=self.model,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict) -> Any:\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @cached_property\n    def _client_params(self) -> Dict[str, Any]:\n        client_params: Dict[str, Any] = {\n            \"api_key\": self.anthropic_api_key.get_secret_value(),\n            \"base_url\": self.anthropic_api_url,\n            \"max_retries\": self.max_retries,\n            \"default_headers\": (self.default_headers or None),\n        }\n        # value <= 0 indicates the param should be ignored. None is a meaningful value\n        # for Anthropic client and treated differently than not specifying the param at\n        # all.\n        if self.default_request_timeout is None or self.default_request_timeout > 0:\n            client_params[\"timeout\"] = self.default_request_timeout\n\n        return client_params\n\n    @cached_property\n    def _client(self) -> anthropic.Client:\n        return anthropic.Client(**self._client_params)\n\n    @cached_property\n    def _async_client(self) -> anthropic.AsyncClient:\n        return anthropic.AsyncClient(**self._client_params)\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Dict,\n    ) -> Dict:\n        messages = self._convert_input(input_).to_messages()\n        system, formatted_messages = _format_messages(messages)\n        payload = {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"messages\": formatted_messages,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"stop_sequences\": stop or self.stop_sequences,\n            \"system\": system,\n            **self.model_kwargs,\n            **kwargs,\n        }\n        if self.thinking is not None:\n            payload[\"thinking\"] = self.thinking\n        return {k: v for k, v in payload.items() if v is not None}\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = self._client.messages.create(**payload)\n        coerce_content_to_string = (\n            not _tools_in_params(payload)\n            and not _documents_in_params(payload)\n            and not _thinking_in_params(payload)\n        )\n        for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = await self._async_client.messages.create(**payload)\n        coerce_content_to_string = (\n            not _tools_in_params(payload)\n            and not _documents_in_params(payload)\n            and not _thinking_in_params(payload)\n        )\n        async for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    await run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    def _format_output(self, data: Any, **kwargs: Any) -> ChatResult:\n        data_dict = data.model_dump()\n        content = data_dict[\"content\"]\n\n        # Remove citations if they are None - introduced in anthropic sdk 0.45\n        for block in content:\n            if (\n                isinstance(block, dict)\n                and \"citations\" in block\n                and block[\"citations\"] is None\n            ):\n                block.pop(\"citations\")\n            if (\n                isinstance(block, dict)\n                and block.get(\"type\") == \"thinking\"\n                and \"text\" in block\n                and block[\"text\"] is None\n            ):\n                block.pop(\"text\")\n\n        llm_output = {\n            k: v for k, v in data_dict.items() if k not in (\"content\", \"role\", \"type\")\n        }\n        if (\n            len(content) == 1\n            and content[0][\"type\"] == \"text\"\n            and not content[0].get(\"citations\")\n        ):\n            msg = AIMessage(content=content[0][\"text\"])\n        elif any(block[\"type\"] == \"tool_use\" for block in content):\n            tool_calls = extract_tool_calls(content)\n            msg = AIMessage(\n                content=content,\n                tool_calls=tool_calls,\n            )\n        else:\n            msg = AIMessage(content=content)\n        msg.usage_metadata = _create_usage_metadata(data.usage)\n        return ChatResult(\n            generations=[ChatGeneration(message=msg)],\n            llm_output=llm_output,\n        )\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = self._client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = await self._async_client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[\n            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n        ] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        r\"\"\"Bind tool-like objects to this chat model.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports Anthropic format tool schemas and any tool definition handled\n                by :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call. Options are:\n\n                - name of the tool as a string or as dict ``{\"type\": \"tool\", \"name\": \"<<tool_name>>\"}``: calls corresponding tool;\n                - ``\"auto\"``, ``{\"type: \"auto\"}``, or None: automatically selects a tool (including no tool);\n                - ``\"any\"`` or ``{\"type: \"any\"}``: force at least one tool to be called;\n            parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n                Defaults to ``None`` (no specification, which allows parallel tool use).\n\n                .. versionadded:: 0.3.2\n            kwargs: Any additional parameters are passed directly to\n                :meth:`~langchain_anthropic.chat_models.ChatAnthropic.bind`.\n\n        Example:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n                # -> AIMessage(\n                #     content=[\n                #         {'text': '<thinking>\\nBased on the user\\'s question, the relevant function to call is GetWeather, which requires the \"location\" parameter.\\n\\nThe user has directly specified the location as \"San Francisco\". Since San Francisco is a well known city, I can reasonably infer they mean San Francisco, CA without needing the state specified.\\n\\nAll the required parameters are provided, so I can proceed with the API call.\\n</thinking>', 'type': 'text'},\n                #         {'text': None, 'type': 'tool_use', 'id': 'toolu_01SCgExKzQ7eqSkMHfygvYuu', 'name': 'GetWeather', 'input': {'location': 'San Francisco, CA'}}\n                #     ],\n                #     response_metadata={'id': 'msg_01GM3zQtoFv8jGQMW7abLnhi', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 487, 'output_tokens': 145}},\n                #     id='run-87b1331e-9251-4a68-acef-f0a018b639cc-0'\n                # )\n\n        Example \u2014 force tool call with tool_choice 'any':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"any\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n\n        Example \u2014 force specific tool call with tool_choice '<name_of_tool>':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"GetWeather\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n        Example \u2014 cache specific tools:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic, convert_to_anthropic_tool\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n                # We'll convert our pydantic class to the anthropic tool format\n                # before passing to bind_tools so that we can set the 'cache_control'\n                # field on our tool.\n                cached_price_tool = convert_to_anthropic_tool(GetPrice)\n                # Currently the only supported \"cache_control\" value is\n                # {\"type\": \"ephemeral\"}.\n                cached_price_tool[\"cache_control\"] = {\"type\": \"ephemeral\"}\n\n                # We need to pass in extra headers to enable use of the beta cache\n                # control API.\n                llm = ChatAnthropic(\n                    model=\"claude-3-5-sonnet-20240620\",\n                    temperature=0,\n                    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n                )\n                llm_with_tools = llm.bind_tools([GetWeather, cached_price_tool])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n            This outputs:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': \"Certainly! I can help you find out the current weather in San Francisco. To get this information, I'll use the GetWeather function. Let me fetch that data for you right away.\", 'type': 'text'}, {'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_01Xg7Wr5inFWgBxE5jH9rpRo', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 96, 'cache_creation_input_tokens': 1470, 'cache_read_input_tokens': 0}}, id='run-b36a5b54-5d69-470e-a1b0-b932d00b089e-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 96, 'total_tokens': 267})\n\n            If we invoke the tool again, we can see that the \"usage\" information in the AIMessage.response_metadata shows that we had a cache hit:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': 'To get the current weather in San Francisco, I can use the GetWeather function. Let me check that for you.', 'type': 'text'}, {'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_016RfWHrRvW6DAGCdwB6Ac64', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 82, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 1470}}, id='run-88b1f825-dcb7-4277-ac27-53df55d22001-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 82, 'total_tokens': 253})\n\n        \"\"\"  # noqa: E501\n        formatted_tools = [convert_to_anthropic_tool(tool) for tool in tools]\n        if not tool_choice:\n            pass\n        elif isinstance(tool_choice, dict):\n            kwargs[\"tool_choice\"] = tool_choice\n        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n        elif isinstance(tool_choice, str):\n            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n        else:\n            raise ValueError(\n                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n                f\"str, or None.\"\n            )\n\n        if parallel_tool_calls is not None:\n            disable_parallel_tool_use = not parallel_tool_calls\n            if \"tool_choice\" in kwargs:\n                kwargs[\"tool_choice\"][\"disable_parallel_tool_use\"] = (\n                    disable_parallel_tool_use\n                )\n            else:\n                kwargs[\"tool_choice\"] = {\n                    \"type\": \"auto\",\n                    \"disable_parallel_tool_use\": disable_parallel_tool_use,\n                }\n\n        return self.bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Union[Dict, type],\n        *,\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema: The output schema. Can be passed in as:\n\n                - an Anthropic tool schema,\n                - an OpenAI function/tool schema,\n                - a JSON Schema,\n                - a TypedDict class,\n                - or a Pydantic class.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            kwargs: Additional keyword arguments are ignored.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`~langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: Pydantic schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example:  Pydantic schema (include_raw=True):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: Dict schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n\n                schema = {\n                    \"name\": \"AnswerWithJustification\",\n                    \"description\": \"An answer to the user question along with justification for the answer.\",\n                    \"input_schema\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"answer\": {\"type\": \"string\"},\n                            \"justification\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"answer\", \"justification\"]\n                    }\n                }\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(schema)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. versionchanged:: 0.1.22\n\n                Added support for TypedDict class as `schema`.\n\n        \"\"\"  # noqa: E501\n        formatted_tool = convert_to_anthropic_tool(schema)\n        tool_name = formatted_tool[\"name\"]\n        llm = self.bind_tools(\n            [schema],\n            tool_choice=tool_name,\n            structured_output_format={\"kwargs\": {}, \"schema\": formatted_tool},\n        )\n        if isinstance(schema, type) and is_basemodel_subclass(schema):\n            output_parser: OutputParserLike = PydanticToolsParser(\n                tools=[schema], first_tool_only=True\n            )\n        else:\n            output_parser = JsonOutputKeyToolsParser(\n                key_name=tool_name, first_tool_only=True\n            )\n\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    @beta()\n    def get_num_tokens_from_messages(\n        self,\n        messages: List[BaseMessage],\n        tools: Optional[\n            Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]\n        ] = None,\n    ) -> int:\n        \"\"\"Count tokens in a sequence of input messages.\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n\n        Basic usage:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage, SystemMessage\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                messages = [\n                    SystemMessage(content=\"You are a scientist\"),\n                    HumanMessage(content=\"Hello, Claude\"),\n                ]\n                llm.get_num_tokens_from_messages(messages)\n\n            .. code-block:: none\n\n                14\n\n        Pass tool schemas:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage\n                from langchain_core.tools import tool\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                @tool(parse_docstring=True)\n                def get_weather(location: str) -> str:\n                    \\\"\\\"\\\"Get the current weather in a given location\n\n                    Args:\n                        location: The city and state, e.g. San Francisco, CA\n                    \\\"\\\"\\\"\n                    return \"Sunny\"\n\n                messages = [\n                    HumanMessage(content=\"What's the weather like in San Francisco?\"),\n                ]\n                llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n\n            .. code-block:: none\n\n                403\n\n        .. versionchanged:: 0.3.0\n\n                Uses Anthropic's token counting API to count tokens in messages. See:\n                https://docs.anthropic.com/en/docs/build-with-claude/token-counting\n        \"\"\"\n        formatted_system, formatted_messages = _format_messages(messages)\n        kwargs: Dict[str, Any] = {}\n        if isinstance(formatted_system, str):\n            kwargs[\"system\"] = formatted_system\n        if tools:\n            kwargs[\"tools\"] = [convert_to_anthropic_tool(tool) for tool in tools]\n\n        response = self._client.beta.messages.count_tokens(\n            betas=[\"token-counting-2024-11-01\"],\n            model=self.model,\n            messages=formatted_messages,  # type: ignore[arg-type]\n            **kwargs,\n        )\n        return response.input_tokens\n\n\nclass AnthropicTool(TypedDict):\n    \"\"\"Anthropic tool definition.\"\"\"\n\n    name: str\n    description: str\n    input_schema: Dict[str, Any]\n    cache_control: NotRequired[Dict[str, str]]\n\n\ndef convert_to_anthropic_tool(\n    tool: Union[Dict[str, Any], Type, Callable, BaseTool],\n) -> AnthropicTool:\n    \"\"\"Convert a tool-like object to an Anthropic tool definition.\"\"\"\n    # already in Anthropic tool format\n    if isinstance(tool, dict) and all(\n        k in tool for k in (\"name\", \"description\", \"input_schema\")\n    ):\n        anthropic_formatted = AnthropicTool(tool)  # type: ignore\n    else:\n        oai_formatted = convert_to_openai_tool(tool)[\"function\"]\n        anthropic_formatted = AnthropicTool(\n            name=oai_formatted[\"name\"],\n            description=oai_formatted[\"description\"],\n            input_schema=oai_formatted[\"parameters\"],\n        )\n    return anthropic_formatted\n\n\ndef _tools_in_params(params: dict) -> bool:\n    return \"tools\" in params or (\n        \"extra_body\" in params and params[\"extra_body\"].get(\"tools\")\n    )\n\n\ndef _thinking_in_params(params: dict) -> bool:\n    return params.get(\"thinking\", {}).get(\"type\") == \"enabled\"\n\n\ndef _documents_in_params(params: dict) -> bool:\n    for message in params.get(\"messages\", []):\n        if isinstance(message.get(\"content\"), list):\n            for block in message[\"content\"]:\n                if (\n                    isinstance(block, dict)\n                    and block.get(\"type\") == \"document\"\n                    and block.get(\"citations\", {}).get(\"enabled\")\n                ):\n                    return True\n    return False\n\n\nclass _AnthropicToolUse(TypedDict):\n    type: Literal[\"tool_use\"]\n    name: str\n    input: dict\n    id: str\n\n\ndef _lc_tool_calls_to_anthropic_tool_use_blocks(\n    tool_calls: List[ToolCall],\n) -> List[_AnthropicToolUse]:\n    blocks = []\n    for tool_call in tool_calls:\n        blocks.append(\n            _AnthropicToolUse(\n                type=\"tool_use\",\n                name=tool_call[\"name\"],\n                input=tool_call[\"args\"],\n                id=cast(str, tool_call[\"id\"]),\n            )\n        )\n    return blocks\n\n\ndef _make_message_chunk_from_anthropic_event(\n    event: anthropic.types.RawMessageStreamEvent,\n    *,\n    stream_usage: bool = True,\n    coerce_content_to_string: bool,\n) -> Optional[AIMessageChunk]:\n    \"\"\"Convert Anthropic event to AIMessageChunk.\n\n    Note that not all events will result in a message chunk. In these cases\n    we return None.\n    \"\"\"\n    message_chunk: Optional[AIMessageChunk] = None\n    # See https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/streaming/_messages.py  # noqa: E501\n    if event.type == \"message_start\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.message.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\" if coerce_content_to_string else [],\n            usage_metadata=usage_metadata,\n        )\n    elif (\n        event.type == \"content_block_start\"\n        and event.content_block is not None\n        and event.content_block.type in (\"tool_use\", \"document\", \"redacted_thinking\")\n    ):\n        if coerce_content_to_string:\n            warnings.warn(\"Received unexpected tool content block.\")\n        content_block = event.content_block.model_dump()\n        content_block[\"index\"] = event.index\n        if event.content_block.type == \"tool_use\":\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=event.content_block.id,\n                name=event.content_block.name,\n                args=\"\",\n            )\n            tool_call_chunks = [tool_call_chunk]\n        else:\n            tool_call_chunks = []\n        message_chunk = AIMessageChunk(\n            content=[content_block],\n            tool_call_chunks=tool_call_chunks,  # type: ignore\n        )\n    elif event.type == \"content_block_delta\":\n        if event.delta.type in (\"text_delta\", \"citations_delta\"):\n            if coerce_content_to_string and hasattr(event.delta, \"text\"):\n                text = event.delta.text\n                message_chunk = AIMessageChunk(content=text)\n            else:\n                content_block = event.delta.model_dump()\n                content_block[\"index\"] = event.index\n                content_block[\"type\"] = \"text\"\n                if \"citation\" in content_block:\n                    content_block[\"citations\"] = [content_block.pop(\"citation\")]\n                message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"thinking_delta\":\n            content_block = event.delta.model_dump()\n            if \"text\" in content_block and content_block[\"text\"] is None:\n                content_block.pop(\"text\")\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"thinking\"\n            message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"signature_delta\":\n            content_block = event.delta.model_dump()\n            if \"text\" in content_block and content_block[\"text\"] is None:\n                content_block.pop(\"text\")\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"thinking\"\n            message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"input_json_delta\":\n            content_block = event.delta.model_dump()\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"tool_use\"\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=None,\n                name=None,\n                args=event.delta.partial_json,\n            )\n            message_chunk = AIMessageChunk(\n                content=[content_block],\n                tool_call_chunks=[tool_call_chunk],  # type: ignore\n            )\n    elif event.type == \"message_delta\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\",\n            usage_metadata=usage_metadata,\n            response_metadata={\n                \"stop_reason\": event.delta.stop_reason,\n                \"stop_sequence\": event.delta.stop_sequence,\n            },\n        )\n    else:\n        pass\n\n    return message_chunk\n\n\n@deprecated(since=\"0.1.0\", removal=\"1.0.0\", alternative=\"ChatAnthropic\")\nclass ChatAnthropicMessages(ChatAnthropic):\n    pass\n\n\ndef _create_usage_metadata(anthropic_usage: BaseModel) -> UsageMetadata:\n    input_token_details: Dict = {\n        \"cache_read\": getattr(anthropic_usage, \"cache_read_input_tokens\", None),\n        \"cache_creation\": getattr(anthropic_usage, \"cache_creation_input_tokens\", None),\n    }\n\n    # Anthropic input_tokens exclude cached token counts.\n    input_tokens = (\n        getattr(anthropic_usage, \"input_tokens\", 0)\n        + (input_token_details[\"cache_read\"] or 0)\n        + (input_token_details[\"cache_creation\"] or 0)\n    )\n    output_tokens = getattr(anthropic_usage, \"output_tokens\", 0)\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=input_tokens + output_tokens,\n        input_token_details=InputTokenDetails(\n            **{k: v for k, v in input_token_details.items() if v is not None}\n        ),\n    )\n",
        "patch": "@@ -26,6 +26,7 @@\n     AsyncCallbackManagerForLLMRun,\n     CallbackManagerForLLMRun,\n )\n+from langchain_core.exceptions import OutputParserException\n from langchain_core.language_models import LanguageModelInput\n from langchain_core.language_models.chat_models import (\n     BaseChatModel,\n@@ -83,6 +84,15 @@\n }\n \n \n+class AnthropicTool(TypedDict):\n+    \"\"\"Anthropic tool definition.\"\"\"\n+\n+    name: str\n+    description: str\n+    input_schema: Dict[str, Any]\n+    cache_control: NotRequired[Dict[str, str]]\n+\n+\n def _format_image(image_url: str) -> Dict:\n     \"\"\"\n     Formats an image of format data:image/jpeg;base64,{b64_string}\n@@ -952,6 +962,31 @@ async def _agenerate(\n         data = await self._async_client.messages.create(**payload)\n         return self._format_output(data, **kwargs)\n \n+    def _get_llm_for_structured_output_when_thinking_is_enabled(\n+        self,\n+        schema: Union[Dict, type],\n+        formatted_tool: AnthropicTool,\n+    ) -> Runnable[LanguageModelInput, BaseMessage]:\n+        thinking_admonition = (\n+            \"Anthropic structured output relies on forced tool calling, \"\n+            \"which is not supported when `thinking` is enabled. This method will raise \"\n+            \"langchain_core.exceptions.OutputParserException if tool calls are not \"\n+            \"generated. Consider disabling `thinking` or adjust your prompt to ensure \"\n+            \"the tool is called.\"\n+        )\n+        warnings.warn(thinking_admonition)\n+        llm = self.bind_tools(\n+            [schema],\n+            structured_output_format={\"kwargs\": {}, \"schema\": formatted_tool},\n+        )\n+\n+        def _raise_if_no_tool_calls(message: AIMessage) -> AIMessage:\n+            if not message.tool_calls:\n+                raise OutputParserException(thinking_admonition)\n+            return message\n+\n+        return llm | _raise_if_no_tool_calls\n+\n     def bind_tools(\n         self,\n         tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n@@ -1249,11 +1284,17 @@ class AnswerWithJustification(BaseModel):\n         \"\"\"  # noqa: E501\n         formatted_tool = convert_to_anthropic_tool(schema)\n         tool_name = formatted_tool[\"name\"]\n-        llm = self.bind_tools(\n-            [schema],\n-            tool_choice=tool_name,\n-            structured_output_format={\"kwargs\": {}, \"schema\": formatted_tool},\n-        )\n+        if self.thinking is not None and self.thinking.get(\"type\") == \"enabled\":\n+            llm = self._get_llm_for_structured_output_when_thinking_is_enabled(\n+                schema, formatted_tool\n+            )\n+        else:\n+            llm = self.bind_tools(\n+                [schema],\n+                tool_choice=tool_name,\n+                structured_output_format={\"kwargs\": {}, \"schema\": formatted_tool},\n+            )\n+\n         if isinstance(schema, type) and is_basemodel_subclass(schema):\n             output_parser: OutputParserLike = PydanticToolsParser(\n                 tools=[schema], first_tool_only=True\n@@ -1356,15 +1397,6 @@ def get_weather(location: str) -> str:\n         return response.input_tokens\n \n \n-class AnthropicTool(TypedDict):\n-    \"\"\"Anthropic tool definition.\"\"\"\n-\n-    name: str\n-    description: str\n-    input_schema: Dict[str, Any]\n-    cache_control: NotRequired[Dict[str, str]]\n-\n-\n def convert_to_anthropic_tool(\n     tool: Union[Dict[str, Any], Type, Callable, BaseTool],\n ) -> AnthropicTool:"
      },
      {
        "filename": "libs/partners/anthropic/tests/integration_tests/test_chat_models.py",
        "content_before": "\"\"\"Test ChatAnthropic chat model.\"\"\"\n\nimport json\nfrom base64 import b64encode\nfrom typing import List, Optional\n\nimport pytest\nimport requests\nfrom langchain_core.callbacks import CallbackManager\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    HumanMessage,\n    SystemMessage,\n    ToolMessage,\n)\nfrom langchain_core.outputs import ChatGeneration, LLMResult\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\nfrom langchain_anthropic import ChatAnthropic, ChatAnthropicMessages\nfrom tests.unit_tests._utils import FakeCallbackHandler\n\nMODEL_NAME = \"claude-3-5-haiku-latest\"\nIMAGE_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n\n\ndef test_stream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    for token in llm.stream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n\nasync def test_astream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    async for token in llm.astream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n    # Check expected raw API output\n    async_client = llm._async_client\n    params: dict = {\n        \"model\": MODEL_NAME,\n        \"max_tokens\": 1024,\n        \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}],\n        \"temperature\": 0.0,\n    }\n    stream = await async_client.messages.create(**params, stream=True)\n    async for event in stream:\n        if event.type == \"message_start\":\n            assert event.message.usage.input_tokens > 1\n            # Note: this single output token included in message start event\n            # does not appear to contribute to overall output token counts. It\n            # is excluded from the total token count.\n            assert event.message.usage.output_tokens == 1\n        elif event.type == \"message_delta\":\n            assert event.usage.output_tokens > 1\n        else:\n            pass\n\n\nasync def test_stream_usage() -> None:\n    \"\"\"Test usage metadata can be excluded.\"\"\"\n    model = ChatAnthropic(model_name=MODEL_NAME, stream_usage=False)  # type: ignore[call-arg]\n    async for token in model.astream(\"hi\"):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n    # check we override with kwarg\n    model = ChatAnthropic(model_name=MODEL_NAME)  # type: ignore[call-arg]\n    assert model.stream_usage\n    async for token in model.astream(\"hi\", stream_usage=False):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n\n\nasync def test_abatch() -> None:\n    \"\"\"Test streaming tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_abatch_tags() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch(\n        [\"I'm Pickle Rick\", \"I'm not Pickle Rick\"], config={\"tags\": [\"foo\"]}\n    )\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_async_tool_use() -> None:\n    llm = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = await llm_with_tools.ainvoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    first = True\n    chunks = []  # type: ignore\n    async for chunk in llm_with_tools.astream(\n        \"what's the weather in san francisco, ca\"\n    ):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_call_chunks, list)\n    assert len(gathered.tool_call_chunks) == 1\n    tool_call_chunk = gathered.tool_call_chunks[0]\n    assert tool_call_chunk[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call_chunk[\"args\"], str)\n    assert \"location\" in json.loads(tool_call_chunk[\"args\"])\n\n\ndef test_batch() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.batch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_ainvoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.ainvoke(\"I'm Pickle Rick\", config={\"tags\": [\"foo\"]})\n    assert isinstance(result.content, str)\n\n\ndef test_invoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.invoke(\"I'm Pickle Rick\", config=dict(tags=[\"foo\"]))\n    assert isinstance(result.content, str)\n\n\ndef test_system_invoke() -> None:\n    \"\"\"Test invoke tokens with a system message\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are an expert cartographer. If asked, you are a cartographer. \"\n                \"STAY IN CHARACTER\",\n            ),\n            (\"human\", \"Are you a mathematician?\"),\n        ]\n    )\n\n    chain = prompt | llm\n\n    result = chain.invoke({})\n    assert isinstance(result.content, str)\n\n\ndef test_anthropic_call() -> None:\n    \"\"\"Test valid call to anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.invoke([message])\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n\n\ndef test_anthropic_generate() -> None:\n    \"\"\"Test generate method of anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    chat_messages: List[List[BaseMessage]] = [\n        [HumanMessage(content=\"How many toes do dogs have?\")]\n    ]\n    messages_copy = [messages.copy() for messages in chat_messages]\n    result: LLMResult = chat.generate(chat_messages)\n    assert isinstance(result, LLMResult)\n    for response in result.generations[0]:\n        assert isinstance(response, ChatGeneration)\n        assert isinstance(response.text, str)\n        assert response.text == response.message.content\n    assert chat_messages == messages_copy\n\n\ndef test_anthropic_streaming() -> None:\n    \"\"\"Test streaming tokens from anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.stream([message])\n    for token in response:\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n\n\ndef test_anthropic_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    message = HumanMessage(content=\"Write me a sentence with 10 words.\")\n    for token in chat.stream([message]):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\nasync def test_anthropic_async_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    chat_messages: List[BaseMessage] = [\n        HumanMessage(content=\"How many toes do dogs have?\")\n    ]\n    async for token in chat.astream(chat_messages):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\ndef test_anthropic_multimodal() -> None:\n    \"\"\"Test that multimodal inputs are handled correctly.\"\"\"\n    chat = ChatAnthropic(model=IMAGE_MODEL_NAME)\n    messages: list[BaseMessage] = [\n        HumanMessage(\n            content=[\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        # langchain logo\n                        \"url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAMCAggHCQgGCQgICAcICAgICAgICAYICAgHDAgHCAgICAgIBggICAgICAgICBYICAgICwkKCAgNDQoIDggICQgBAwQEBgUGCgYGCBALCg0QCg0NEA0KCg8LDQoKCgoLDgoQDQoLDQoKCg4NDQ0NDgsQDw0OCg4NDQ4NDQoJDg8OCP/AABEIALAAsAMBEQACEQEDEQH/xAAdAAEAAgEFAQAAAAAAAAAAAAAABwgJAQIEBQYD/8QANBAAAgIBAwIDBwQCAgIDAAAAAQIAAwQFERIIEwYhMQcUFyJVldQjQVGBcZEJMzJiFRYk/8QAGwEBAAMAAwEAAAAAAAAAAAAAAAQFBgEDBwL/xAA5EQACAQIDBQQJBAIBBQAAAAAAAQIDEQQhMQVBUWGREhRxgRMVIjJSU8HR8CNyobFCguEGJGKi4v/aAAwDAQACEQMRAD8ApfJplBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBAEAQBANl16qOTEKB6kkAD+z5Tkcj0On+z7Ub1FlOmanejeavj6dqV6kfsQ1OK4IP8AIM6pVYR1kuqJdLCV6qvCnJ/6v66nL+Ems/RNc+y63+BOvvFL411O/wBW4r5T6D4Saz9E1z7Lrf4Ed4pfGuo9W4r5T6D4Saz9E1z7Lrf4Ed4pfGuo9W4r5T6D4Saz9E1z7Lrf4Ed4pfGuo9W4r5T6D4Saz9E1z7Lrf4Ed4pfGuo9W4r5T6D4Saz9E1z7Lrf4Ed4pfGuo9W4r5T6D4Saz9E1z7Lrf4Ed4pfGuo9W4r5T6D4Saz9E1z7Lrf4Ed4pfGuo9W4r5T6HE1D2e6lQpsu0zU6EXzZ8jTtSoUD9yWuxUAA/kmdkasJaSXVHRVwlekrzpyX+r+mh56m9WHJSGU+hUgg/wBjynaRORvnAEAQBAEAQBAEAQCbennpVzfER95LHE0tX4tlsnJr2B2srw6yQLCpBQ3Me1W+4/VZLKlh4jFRo5ay4cPH7f0XWA2XUxft37MONs34ffRcy/Xsu6bdG0UK2Nh1tkAbHMyAt+Wx2HIi11/SDcQe3jrTXv6IJRVcRUqe88uC0Nxhdn0MMv0458XnJ+e7wVlyJPJkYsTSAIAgCAIAgCAIBqDAIx9qHTbo2tBmycOtcgjYZmOBRlqdjxJtQDuhdye3ette/qhkmliKlP3XlwehXYrZ9DEr9SOfFZS6rXwd1yKCdQ3Srm+HT7yGOXpbPxXLVOLUMTtXXmVgkVliQgvU9qx9h+kz11Ne4fFRrZaS4cfD7f2YfH7LqYT279qHHevH76PlvhKTClEAQBAEAQBAJp6WOn0+I80i7mumYnF8x1LIbSSe3iV2DYq13ElnQ8q6gdijWUuIeKxHoY5e89PuXWy8D3qp7S9iOvN/D9+XiZRNN06uiuvHqrSqmpFrqqrVUrrrUBUREUBVVVAAUAAATNNtu7PR4xUUoxVkskloktxyCZwfRj26jetHPtzrMXSM4Uabj7Vrfj10O2ZdsDbb3bqrCKEYmpeyED8Hs53LZVwvsPg4qN6kbt+OS8t5hdobYqOo44edorK6SzfmtFpz14H16f8Arkz6cmrD1e9crBvsFZy3ropvxC2yo7NTXXXbjhtuXcTmisz91hX2yr4KLjemrNbuPXeMDtuoqihiGnF/5ZJx55ZNceF76GQSUJuhAEAQBAEAhb239WWl+H391s7mXnbAnExu2WqUjdWyLHda6Qw2IXdrCCGFZX5pMo4WdXNZLiyoxm1KOFfZl7UuCtdeN2kvzcRB4d/5JMV7OOVpWRRSWAFmPk1ZTKN9uT1PRi+QHnsj2H12DHYGXLZzS9mV3zVvuVFL/qGDlapSaXFST6qyfS/3tb4M8a4up49WoYlyZGLcCUsTf1B2ZGVgHrsRgVNbqrIwIYAjaVc4Sg+zJWZqaVWFWCnB3T0/PodnqOnV312Y9taW02o1dtViq9dlbAq6OjAqyspIKkEEGfKbTuj7lFSTjJXTyaejXAxd9U/T6fDmYBTzbTMvm+G7FnNRBHcxLLDuWankCrueVlRG5dq7nOlwuI9NHP3lr9zzjamA7rU9n3Jacn8P25eBC0mFKIAgCAIBtdwASfQDc/4nIbsZXulr2ZDR9HwsYpxybqxmZe4Xl71cquyMR69hO3jg+fy0r5n1OWxNX0lRvdovBflz1DZuG7vh4xtZtXl+55vpp5EsyKWZ5X2seH783TdRwsZgmVk4OVRQzMUUXPRYle7gEoCxA5gEqDvsdp2U5KM03omv7I+Ig6lKUIuzaaXmigPtb6HNQ0bEytTGXjZeLiKlhWuu6rINPMLbY1bFqkXHQ908b7CyK+wUqFe+pY2FSSjZpvnl+MwmJ2JVw9OVTtqUYq+Sadt+WaVtd9+W+uLLv5HzB8j/AIlgZ8yRdGfUXXq2JXpGTZtquFUE+cnfMxU2Wu9CzEvaicEsG+/MdzYLbsmexmHdOXaS9l/w+H2PQ9kY9V6apyftxVtdUtJc3x58iykrjQCAIAgFdurzqbPh+lMHFKHVspC6FuLLh427Icp0O4d2ZWREb5WZLGbktJrssMJhvSu8vdX8vh9zP7X2i8LBRp27b46Rj8Vt73JebyVnCfSz0jNqh/8AsGsrZZRcxuoxrms7ua7HmcvLYkOaXJ5Ctjvkb8n/AE+K3TcVi+x+nS6rdyX33eJTbL2S636+JTaeaTveTf8AlLlwjv35ZFmfHnSnoWo47Yo0/FxLOBWnJw8ejHuobb5GVqkUOqnY9qwOjDyI9CKyGKqwd+03ybdjS19mYarHs+jSe5pJNdP6KudBPiTIwNYz/D1jA1WJk91AWKLqGJctDWVg+QFlfdQtsGcVY+//AFgSzx0VKmqi5dJK/wCeZm9iVJ0sRPDye6WWdu1BpXWeV78M8uGd/wCURuCJuqX2YjWNHzMYJyyaKzmYm3Hl71SrOqKW8h307mOT5fLc3mPUSsNV9HUT3aPwf5crNpYbvGHlG2azj+5Zrrp5mKFHBAI9CNx/iak8vTubpwBAEAQDtPCekLk5WHiON0yczFx3H8pbkVVMP7VyJ8zfZi3wTfRHdRh26kI8ZRXk5IzREf6mPPXTSAIB1/iPQa8yjIwrVD05NFuPYrAFWrsrat1YHyIKsRsf2nMXZpo+ZR7UXF77rqYW2xHrJqsHG2smu1T6rapKWKf8OCP6mxvfNHj1nH2XqsnfW6yOVpGr241teVRY9ORS4sqtrPF67B6Mp/2NiCGBIIYMQeGlJWaujsp1JU5KcHZrQyZdK/U3X4ipONdwq1fGQNkVL5JkVbhfe8cE/wDgWKq1e5NFjKD8ttLPm8ThnSd17r0+35qej7N2hHFQs8prVfVcv6J4kIuBAKtdWnV8uj89I090fVeP/wCi8hXq05CvIcg26PmMpDCpgVqUrZaCGqrussLhPSe3P3f7/wCOf4s9tTaXd16On77/APXn48EU58OYl+RremrrRyHbJzdPbI9+LvZZjW21vUlgs5FMe4OqmshVrrscca9jtcSaVKXotydrcVr58zH04znioLFXd3G/a17L08E3u5vJEveGeobX/Cuq2YmttbbjX3NflUu7ZC1VW2OTlaZZuzDHrIbbGXZOFbV9qmwfLElh6Venelqsl4rc+fP6FtT2hicHiHDEu8W7u+ii8lKObtHL3fH/AC1tn1AdReJ4exVvJW/MyEJwcVWG9x2G1zkb8MVNwTbt83kqhmYCVVDDyqytot7/ADeanG46GFh2nm37q4/8c/qVr/4/fZ9k5Obm+J7+Xa430V2soVcrNuuW3LtT+RQUNZKjj3L2QHlRYqWOPqJRVJcvJJWRnth4epKpLE1FqnZ8XJ3b8MuG/LQvdKQ2ZqB/qAYXfFmkLjZWZiINkxszKx0H8JVkW1KP6VAJsIPtRT4pPqjyKtDsVJx4SkvJSdjq59HSIAgCAdp4T1dcbKw8tzsmNmYuQ5/hKsiq1j/SoTPma7UWuKa6o7qM+xUhLhKL8lJXM0RP+pjz100gCAIBjA6x/Y9ZpGq35KofcdSssy8ewA8Vvcl8rHJ3OzrazXAeQNVq8d+3Zx0mDrKpTS3rLy3P6HnG18I6FdzS9mWa/c9V9fPkQTJxRnf+AfHeRpOXj6pjHa/GsDhd+K2p6W0WHY/p31lqidiVDchsyqR8VIKpFxlo/wAv5EjD15UKiqw1X8revMy++DfFtOo4uNqNDcsfKprvrJ8iFZQeLD1Dod0KnzVlI/aZKcXCTi9UerUqkasFOLumk14M8T1L+0uzRdHzdRp8skKlGO2wPC+6xKUt2PkezzN3E7g8NtjvO7D01UqKL03+CzIe0MQ8Ph5VI66Lxbsv7Ks9D3ThTqG/iXOBvSvJsGHTae4L8lWDXZ2QzMzXMt7MoWzzNyW2PzPaYWeNxDj+nDLLPw4dPsZ7Y+CVb/ua3tO7tfitZPzyS5XJS6zOlu3XAmrYSh9Rpq7N2OzKozMYF3RUZyEXIqZ325lVtVyrMOFUjYPEql7MtP6f2J+1tmvE2qU/fWWusfo1/P8AVWfbjruoWabpFGrl/wD5Wq/UOyMhO3mV6QFxaU98BCuzW5dNxW2wcraqeZawku1pQjFVJOn7uWmna1y8uhmMdUqOhSjiPfTlr73o0rXfi1k96V7nq/YP0n6lr99OdqgysfS6qqKw2QbK8rKx6kWrHxcdG2toxlrUA3lU+Q71c3ta+rpr4qFJONOzlnpom9/N8vpkTMBsyriZKeITUEla+rSyUbapLyvzeZkT0fR6saqvFprSmilFrqqrUJXXWo2VEUABVUDbYSgbbd3qbyMVFWSskcucH0ag/wCoBhd8WauuTlZmWh3TIzMrIQ/yluRbap/tXBmwguzFLgkuiPIq0+3UnLjKT8nJ2Orn0dIgCAIBtdAQQfQjY/4nIauZXulr2nDWNHw8kvyyaKxh5e/Hl71SqozsF8h307eQB5fLcvkPQZbE0vR1Gt2q8H+WPUNm4nvGHjK92spfuWT66+ZLMilmIAgHm/aL4ExtVxL9PyaVvptRtkb1WwA9uyths1dqNsRYhDKf39Z905uElKLszor0YVoOE1dP86mH7R/DORdi5OeKz2sI4iZZIKtU+Q11dPJSvl+rS1ZBIKsyDY7krrXJKSjxvbyzPKY0ZuMprSNlLim21p4rPh1t6fA9ieq34Ka1RhW5OA7XKbMcC6ypq7DU/doT9cLyBPNK7ECglmT0nW60FLsN2fPnnroSI4KvKl6aMLxz0zeTavbW3hfy3Wq/4+fbVQKbPDd9wW7vWZGnK2wW2l17l9FTehsS0W5PA/M62uV5CqzhV4+i7+kS5Px4/T8z02wcXHsvDyed24+DzaXg7u3PLLSderP2f3arombi0KXyEFWVVWBu1jU2pc1SD93sqWxAP3dlkHC1FCqm9NOuRd7ToOvhpwjrk14xadv4K7dEPU5gYOI2iZ+RXiql1l2Hk2fJjtVae5ZVbaSUrsW42WB7O2jpYqg8k+exxuGnKXbgr8eOWXmUGxtpUqdP0FV9m12m9Gm72/8AFp8dfEmb22dZmlaXjv7nk42pag4K0U49q3U1t5fqZV1LFErTfl2g4st/8VCjnZXDo4Oc37ScVvv9L/iLXG7Xo0IfpyU57kndeLa0X8vRcq59OnsAzPFWY3iTVmezBa3uMbQOWo2qdhSibcUwa+IrPEBSq9pB/wBjV2GIrxoR9HT1/r/6M/s7A1MbU7ziHeN75/5tbuUF/Oml28h0oDfCAIBE/VL7TRo+j5uSr8cm6s4eJtx5e9XKyK6hvJuwncyCPP5aW8j6GVhqXpKiW7V+C/LFZtLE93w8pXzeUf3PJdNfIxQIgAAHoBsP8TUnl6VjdOAIAgCAIBNPSx1BHw5mE3c20zL4JmIoZjUQT28uusblmp5EMiDlZUTsHaulDDxWH9NHL3lp9i62Xj+61Pa9yWvJ/F9+XgZRNN1Ku+uvIqsS2m1FsqtrZXrsrYBkdHUlWVlIIYEggzNNNOzPR4yUkpRd081bRp7zkTg+jUQCH9Q8FeJjnNdVrmImmPx/QfTKXuqAVOXa2ZeTO5tAe29hWq1bpeS8lKdLs2cH2v3Zfn5kVjpYr0t1VXY4djNaaZ+OumWpGh9j2vaVi6pp+NVpep4+ouxQXY9ZzMnKybbGy8rVbNsHENdKMdiot2Raa0pbtjud/pac5RlK6a4PJJaJasivD4inCcIdmSle11m3JttyeStn/RJ/sG8A6no2LgaTaultiY+MwuuxmzUyDlFue4rek1XGxmd3yWspLvuwoTnskevONSTkr58bafm7dxJuDpVaNONOXZsln2b6+evjv4I6jVejTRLMp9TqTLw8xrRkV24eVZT7vkcuZtorKvUjM25KMj1+Z2RdzOxYuoo9l2a5rVcOJGnsnDubqxTjLVOMmrPilnG/k1yJxrXYAbkkADkdtyf5OwA3Pr5AD+APSQi5K7e1zod0nVrnzanu07KtZnuOMK3x7rWO7WPjuNlsY7sWoenmzMzB2YtLCljZ012XmuevUoMVsWhXk5puEnra1m+Nnl0tffmeY8Df8dum49iXZmZkZ4Q79gImJjv/AALQj23Mv/qt6BvRuQJU9lTaE5K0Vb+X9iNQ2BRg71JOfKyUemb/AJ/gtXhYSVIlNaLXVWqpXWiqqIigBURVACqoAAUAAASrbvmzTpJKy0PtByIBx9R1KuiuzItsSqmpGsttsZUrrrUFnd3YhVVVBJYkAATlJt2R8ykopyk7JZtvRJbzF31T9QR8R5gNPNdMxOSYaMGQ2kkdzLsrOxVruICo45V1AbhGsuQaXC4f0Mc/eev2PONqY7vVT2fcjpzfxfbl4kLSYUogCAIAgCAIBNvTz1VZvh0+7FTl6Wz8mxGfi1DE72WYdhBFZYkuaGHasfc/os9lrQ8RhY1s9JcePj9/7LrAbUnhPYt2ocN68Pto+W+/fsv6ktG1oKuNmVrkEbnDyCKMtTsOQFTkd0LuB3KGtr39HMoquHqU/eWXFaG4wu0KGJX6cs+DykvJ6+KuuZJxEjFiaQBAEAQBAEAQBANQIBGHtR6ktG0UMuTmVtkAbjDxyt+Wx2PEGpG/SDcSO5kNTXv6uJJpYepV91ZcXoV2K2hQwy/UlnwWcn5bvF2XMoL1DdVWb4iPuwU4mlq/JcRX5NewO9dmZYABYVIDilR2q32P6rJXat7h8LGjnrLjw8Pv/Rh8ftSpi/Yt2YcL5vx+2i5kJSYUogCAIAgCAIAgCAbLqFYcWAZT6hgCD/R8pyOZ6HT/AGg6lQorp1PU6EXyVMfUdSoUD9gFpykAA/gCdUqUJaxXREuli69JWhUkv9n9Tl/FvWfreufetb/PnX3el8C6Hf6yxXzX1Hxb1n63rn3rW/z47vS+BdB6yxXzX1Hxb1n63rn3rW/z47vS+BdB6yxXzX1Hxb1n63rn3rW/z47vS+BdB6yxXzX1Hxb1n63rn3rW/wA+O70vgXQessV819R8W9Z+t65961v8+O70vgXQessV819R8W9Z+t65961v8+O70vgXQessV819R8W9Z+t65961v8+O70vgXQessV819Tiah7QdRvU13anqd6N5MmRqOpXqR+4K3ZTgg/wROyNKEdIrojoqYuvVVp1JP/Z/TU89TQqjioCgegAAA/oeU7SJzN84AgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgCAIAgH/9k=\",  # noqa: E501\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is this a logo for?\"},\n            ]\n        )\n    ]\n    response = chat.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n    num_tokens = chat.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n\ndef test_streaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = llm.generate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\nasync def test_astreaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = await llm.agenerate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\ndef test_tool_use() -> None:\n    llm = ChatAnthropic(model=MODEL_NAME)\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = llm_with_tools.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    input = \"how are you? what's the weather in san francisco, ca\"\n    first = True\n    chunks = []  # type: ignore\n    for chunk in llm_with_tools.stream(input):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered.content, list)\n    assert len(gathered.content) == 2\n    tool_use_block = None\n    for content_block in gathered.content:\n        assert isinstance(content_block, dict)\n        if content_block[\"type\"] == \"tool_use\":\n            tool_use_block = content_block\n            break\n    assert tool_use_block is not None\n    assert tool_use_block[\"name\"] == \"get_weather\"\n    assert \"location\" in json.loads(tool_use_block[\"partial_json\"])\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_calls, list)\n    assert len(gathered.tool_calls) == 1\n    tool_call = gathered.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n    assert tool_call[\"id\"] is not None\n\n    # Test passing response back to model\n    stream = llm_with_tools.stream(\n        [\n            input,\n            gathered,\n            ToolMessage(content=\"sunny and warm\", tool_call_id=tool_call[\"id\"]),\n        ]\n    )\n    chunks = []  # type: ignore\n    first = True\n    for chunk in stream:\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n\n\nclass GenerateUsername(BaseModel):\n    \"Get a username based on someone's name and hair color.\"\n\n    name: str\n    hair_color: str\n\n\ndef test_disable_parallel_tool_calling() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n    llm_with_tools = llm.bind_tools([GenerateUsername], parallel_tool_calls=False)\n    result = llm_with_tools.invoke(\n        \"Use the GenerateUsername tool to generate user names for:\\n\\n\"\n        \"Sally with green hair\\n\"\n        \"Bob with blue hair\"\n    )\n    assert isinstance(result, AIMessage)\n    assert len(result.tool_calls) == 1\n\n\ndef test_anthropic_with_empty_text_block() -> None:\n    \"\"\"Anthropic SDK can return an empty text block.\"\"\"\n\n    @tool\n    def type_letter(letter: str) -> str:\n        \"\"\"Type the given letter.\"\"\"\n        return \"OK\"\n\n    model = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0).bind_tools(\n        [type_letter]\n    )\n\n    messages = [\n        SystemMessage(\n            content=\"Repeat the given string using the provided tools. Do not write \"\n            \"anything else or provide any explanations. For example, \"\n            \"if the string is 'abc', you must print the \"\n            \"letters 'a', 'b', and 'c' one at a time and in that order. \"\n        ),\n        HumanMessage(content=\"dog\"),\n        AIMessage(\n            content=[\n                {\"text\": \"\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"letter\": \"d\"},\n                    \"name\": \"type_letter\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"type_letter\",\n                    \"args\": {\"letter\": \"d\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"OK\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n\n    model.invoke(messages)\n\n\ndef test_with_structured_output() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-opus-20240229\",\n    )\n\n    structured_llm = llm.with_structured_output(\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather report for a city\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\"location\": {\"type\": \"string\"}},\n            },\n        }\n    )\n    response = structured_llm.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, dict)\n    assert response[\"location\"]\n\n\ndef test_get_num_tokens_from_messages() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n    # Test simple case\n    messages = [\n        SystemMessage(content=\"You are a scientist\"),\n        HumanMessage(content=\"Hello, Claude\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n    # Test tool use\n    @tool(parse_docstring=True)\n    def get_weather(location: str) -> str:\n        \"\"\"Get the current weather in a given location\n\n        Args:\n            location: The city and state, e.g. San Francisco, CA\n        \"\"\"\n        return \"Sunny\"\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n        AIMessage(\n            content=[\n                {\"text\": \"Let's see.\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"location\": \"SF\"},\n                    \"name\": \"get_weather\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"get_weather\",\n                    \"args\": {\"location\": \"SF\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"Sunny\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n\nclass GetWeather(BaseModel):\n    \"\"\"Get the current weather in a given location\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n@pytest.mark.parametrize(\"tool_choice\", [\"GetWeather\", \"auto\", \"any\"])\ndef test_anthropic_bind_tools_tool_choice(tool_choice: str) -> None:\n    chat_model = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n    chat_model_with_tools = chat_model.bind_tools([GetWeather], tool_choice=tool_choice)\n    response = chat_model_with_tools.invoke(\"what's the weather in ny and la\")\n    assert isinstance(response, AIMessage)\n\n\ndef test_pdf_document_input() -> None:\n    url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n    data = b64encode(requests.get(url).content).decode()\n\n    result = ChatAnthropic(model=IMAGE_MODEL_NAME).invoke(\n        [\n            HumanMessage(\n                [\n                    \"summarize this document\",\n                    {\n                        \"type\": \"document\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"data\": data,\n                            \"media_type\": \"application/pdf\",\n                        },\n                    },\n                ]\n            )\n        ]\n    )\n    assert isinstance(result, AIMessage)\n    assert isinstance(result.content, str)\n    assert len(result.content) > 0\n\n\ndef test_citations() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"content\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": \"The grass is green\"},\n                            {\"type\": \"text\", \"text\": \"The sky is blue\"},\n                        ],\n                    },\n                    \"citations\": {\"enabled\": True},\n                },\n                {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n            ],\n        }\n    ]\n    response = llm.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert any(\"citations\" in block for block in response.content)\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(messages):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    assert any(\"citations\" in block for block in full.content)\n    assert not any(\"citation\" in block for block in full.content)\n\n\ndef test_thinking() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-7-sonnet-latest\",\n        max_tokens=5_000,\n        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n    )\n    response = llm.invoke(\"Hello\")\n    assert any(\"thinking\" in block for block in response.content)\n    for block in response.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"thinking\":\n            assert set(block.keys()) == {\"type\", \"thinking\", \"signature\"}\n            assert block[\"thinking\"] and isinstance(block[\"thinking\"], str)\n            assert block[\"signature\"] and isinstance(block[\"signature\"], str)\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(\"Hello\"):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    assert any(\"thinking\" in block for block in full.content)\n    for block in full.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"thinking\":\n            assert set(block.keys()) == {\"type\", \"thinking\", \"signature\", \"index\"}\n            assert block[\"thinking\"] and isinstance(block[\"thinking\"], str)\n            assert block[\"signature\"] and isinstance(block[\"signature\"], str)\n\n\ndef test_redacted_thinking() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-7-sonnet-latest\",\n        max_tokens=5_000,\n        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n    )\n    query = \"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"  # noqa: E501\n\n    response = llm.invoke(query)\n    has_reasoning = False\n    for block in response.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"redacted_thinking\":\n            has_reasoning = True\n            assert set(block.keys()) == {\"type\", \"data\"}\n            assert block[\"data\"] and isinstance(block[\"data\"], str)\n    assert has_reasoning\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(query):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    stream_has_reasoning = False\n    for block in full.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"redacted_thinking\":\n            stream_has_reasoning = True\n            assert set(block.keys()) == {\"type\", \"data\", \"index\"}\n            assert block[\"data\"] and isinstance(block[\"data\"], str)\n    assert stream_has_reasoning\n",
        "patch": "@@ -6,7 +6,9 @@\n \n import pytest\n import requests\n+from anthropic import BadRequestError\n from langchain_core.callbacks import CallbackManager\n+from langchain_core.exceptions import OutputParserException\n from langchain_core.messages import (\n     AIMessage,\n     AIMessageChunk,\n@@ -725,3 +727,39 @@ def test_redacted_thinking() -> None:\n             assert set(block.keys()) == {\"type\", \"data\", \"index\"}\n             assert block[\"data\"] and isinstance(block[\"data\"], str)\n     assert stream_has_reasoning\n+\n+\n+def test_structured_output_thinking_enabled() -> None:\n+    llm = ChatAnthropic(\n+        model=\"claude-3-7-sonnet-latest\",\n+        max_tokens=5_000,\n+        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n+    )\n+    with pytest.warns(match=\"structured output\"):\n+        structured_llm = llm.with_structured_output(GenerateUsername)\n+    query = \"Generate a username for Sally with green hair\"\n+    response = structured_llm.invoke(query)\n+    assert isinstance(response, GenerateUsername)\n+\n+    with pytest.raises(OutputParserException):\n+        structured_llm.invoke(\"Hello\")\n+\n+    # Test streaming\n+    for chunk in structured_llm.stream(query):\n+        assert isinstance(chunk, GenerateUsername)\n+\n+\n+def test_structured_output_thinking_force_tool_use() -> None:\n+    # Structured output currently relies on forced tool use, which is not supported\n+    # when `thinking` is enabled. When this test fails, it means that the feature\n+    # is supported and the workarounds in `with_structured_output` should be removed.\n+    llm = ChatAnthropic(\n+        model=\"claude-3-7-sonnet-latest\",\n+        max_tokens=5_000,\n+        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n+    ).bind_tools(\n+        [GenerateUsername],\n+        tool_choice=\"GenerateUsername\",\n+    )\n+    with pytest.raises(BadRequestError):\n+        llm.invoke(\"Generate a username for Sally with green hair\")"
      }
    ]
  },
  {
    "number": 29338,
    "title": "core: Add ruff rules PTH (pathlib)",
    "body": "See https://docs.astral.sh/ruff/rules/#flake8-use-pathlib-pth",
    "issue_title": "core: Add ruff rules PTH (pathlib)",
    "issue_body": "See https://docs.astral.sh/ruff/rules/#flake8-use-pathlib-pth",
    "files": [
      {
        "filename": "libs/core/langchain_core/callbacks/file.py",
        "content_before": "\"\"\"Callback Handler that writes to a file.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Any, Optional, TextIO, cast\n\nfrom langchain_core.callbacks import BaseCallbackHandler\nfrom langchain_core.utils.input import print_text\n\nif TYPE_CHECKING:\n    from langchain_core.agents import AgentAction, AgentFinish\n\n\nclass FileCallbackHandler(BaseCallbackHandler):\n    \"\"\"Callback Handler that writes to a file.\n\n    Parameters:\n        filename: The file to write to.\n        mode: The mode to open the file in. Defaults to \"a\".\n        color: The color to use for the text.\n    \"\"\"\n\n    def __init__(\n        self, filename: str, mode: str = \"a\", color: Optional[str] = None\n    ) -> None:\n        \"\"\"Initialize callback handler.\n\n        Args:\n            filename: The filename to write to.\n            mode: The mode to open the file in. Defaults to \"a\".\n            color: The color to use for the text. Defaults to None.\n        \"\"\"\n        self.file = cast(TextIO, open(filename, mode, encoding=\"utf-8\"))  # noqa: SIM115\n        self.color = color\n\n    def __del__(self) -> None:\n        \"\"\"Destructor to cleanup when done.\"\"\"\n        self.file.close()\n\n    def on_chain_start(\n        self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out that we are entering a chain.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized chain.\n            inputs (Dict[str, Any]): The inputs to the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if \"name\" in kwargs:\n            name = kwargs[\"name\"]\n        else:\n            if serialized:\n                name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n            else:\n                name = \"<unknown>\"\n        print_text(\n            f\"\\n\\n\\033[1m> Entering new {name} chain...\\033[0m\",\n            end=\"\\n\",\n            file=self.file,\n        )\n\n    def on_chain_end(self, outputs: dict[str, Any], **kwargs: Any) -> None:\n        \"\"\"Print out that we finished a chain.\n\n        Args:\n            outputs (Dict[str, Any]): The outputs of the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(\"\\n\\033[1m> Finished chain.\\033[0m\", end=\"\\n\", file=self.file)\n\n    def on_agent_action(\n        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Run on agent action.\n\n        Args:\n            action (AgentAction): The agent action.\n            color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(action.log, color=color or self.color, file=self.file)\n\n    def on_tool_end(\n        self,\n        output: str,\n        color: Optional[str] = None,\n        observation_prefix: Optional[str] = None,\n        llm_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"If not the final action, print out observation.\n\n        Args:\n           output (str): The output to print.\n           color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n           observation_prefix (Optional[str], optional): The observation prefix.\n            Defaults to None.\n           llm_prefix (Optional[str], optional): The LLM prefix.\n                Defaults to None.\n           **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if observation_prefix is not None:\n            print_text(f\"\\n{observation_prefix}\", file=self.file)\n        print_text(output, color=color or self.color, file=self.file)\n        if llm_prefix is not None:\n            print_text(f\"\\n{llm_prefix}\", file=self.file)\n\n    def on_text(\n        self, text: str, color: Optional[str] = None, end: str = \"\", **kwargs: Any\n    ) -> None:\n        \"\"\"Run when the agent ends.\n\n        Args:\n           text (str): The text to print.\n           color (Optional[str], optional): The color to use for the text.\n            Defaults to None.\n           end (str, optional): The end character. Defaults to \"\".\n           **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(text, color=color or self.color, end=end, file=self.file)\n\n    def on_agent_finish(\n        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n    ) -> None:\n        \"\"\"Run on the agent end.\n\n        Args:\n            finish (AgentFinish): The agent finish.\n            color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(finish.log, color=color or self.color, end=\"\\n\", file=self.file)\n",
        "patch": "@@ -2,6 +2,7 @@\n \n from __future__ import annotations\n \n+from pathlib import Path\n from typing import TYPE_CHECKING, Any, Optional, TextIO, cast\n \n from langchain_core.callbacks import BaseCallbackHandler\n@@ -30,7 +31,7 @@ def __init__(\n             mode: The mode to open the file in. Defaults to \"a\".\n             color: The color to use for the text. Defaults to None.\n         \"\"\"\n-        self.file = cast(TextIO, open(filename, mode, encoding=\"utf-8\"))  # noqa: SIM115\n+        self.file = cast(TextIO, Path(filename).open(mode, encoding=\"utf-8\"))  # noqa: SIM115\n         self.color = color\n \n     def __del__(self) -> None:"
      },
      {
        "filename": "libs/core/langchain_core/documents/base.py",
        "content_before": "from __future__ import annotations\n\nimport contextlib\nimport mimetypes\nfrom io import BufferedReader, BytesIO\nfrom pathlib import PurePath\nfrom typing import TYPE_CHECKING, Any, Literal, Optional, Union, cast\n\nfrom pydantic import ConfigDict, Field, field_validator, model_validator\n\nfrom langchain_core.load.serializable import Serializable\n\nif TYPE_CHECKING:\n    from collections.abc import Generator\n\nPathLike = Union[str, PurePath]\n\n\nclass BaseMedia(Serializable):\n    \"\"\"Use to represent media content.\n\n    Media objects can be used to represent raw data, such as text or binary data.\n\n    LangChain Media objects allow associating metadata and an optional identifier\n    with the content.\n\n    The presence of an ID and metadata make it easier to store, index, and search\n    over the content in a structured way.\n    \"\"\"\n\n    # The ID field is optional at the moment.\n    # It will likely become required in a future major release after\n    # it has been adopted by enough vectorstore implementations.\n    id: Optional[str] = None\n    \"\"\"An optional identifier for the document.\n\n    Ideally this should be unique across the document collection and formatted\n    as a UUID, but this will not be enforced.\n\n    .. versionadded:: 0.2.11\n    \"\"\"\n\n    metadata: dict = Field(default_factory=dict)\n    \"\"\"Arbitrary metadata associated with the content.\"\"\"\n\n    @field_validator(\"id\", mode=\"before\")\n    def cast_id_to_str(cls, id_value: Any) -> Optional[str]:\n        if id_value is not None:\n            return str(id_value)\n        else:\n            return id_value\n\n\nclass Blob(BaseMedia):\n    \"\"\"Blob represents raw data by either reference or value.\n\n    Provides an interface to materialize the blob in different representations, and\n    help to decouple the development of data loaders from the downstream parsing of\n    the raw data.\n\n    Inspired by: https://developer.mozilla.org/en-US/docs/Web/API/Blob\n\n    Example: Initialize a blob from in-memory data\n\n        .. code-block:: python\n\n            from langchain_core.documents import Blob\n\n            blob = Blob.from_data(\"Hello, world!\")\n\n            # Read the blob as a string\n            print(blob.as_string())\n\n            # Read the blob as bytes\n            print(blob.as_bytes())\n\n            # Read the blob as a byte stream\n            with blob.as_bytes_io() as f:\n                print(f.read())\n\n    Example: Load from memory and specify mime-type and metadata\n\n        .. code-block:: python\n\n            from langchain_core.documents import Blob\n\n            blob = Blob.from_data(\n                data=\"Hello, world!\",\n                mime_type=\"text/plain\",\n                metadata={\"source\": \"https://example.com\"}\n            )\n\n    Example: Load the blob from a file\n\n        .. code-block:: python\n\n            from langchain_core.documents import Blob\n\n            blob = Blob.from_path(\"path/to/file.txt\")\n\n            # Read the blob as a string\n            print(blob.as_string())\n\n            # Read the blob as bytes\n            print(blob.as_bytes())\n\n            # Read the blob as a byte stream\n            with blob.as_bytes_io() as f:\n                print(f.read())\n    \"\"\"\n\n    data: Union[bytes, str, None] = None\n    \"\"\"Raw data associated with the blob.\"\"\"\n    mimetype: Optional[str] = None\n    \"\"\"MimeType not to be confused with a file extension.\"\"\"\n    encoding: str = \"utf-8\"\n    \"\"\"Encoding to use if decoding the bytes into a string.\n\n    Use utf-8 as default encoding, if decoding to string.\n    \"\"\"\n    path: Optional[PathLike] = None\n    \"\"\"Location where the original content was found.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        frozen=True,\n    )\n\n    @property\n    def source(self) -> Optional[str]:\n        \"\"\"The source location of the blob as string if known otherwise none.\n\n        If a path is associated with the blob, it will default to the path location.\n\n        Unless explicitly set via a metadata field called \"source\", in which\n        case that value will be used instead.\n        \"\"\"\n        if self.metadata and \"source\" in self.metadata:\n            return cast(Optional[str], self.metadata[\"source\"])\n        return str(self.path) if self.path else None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_blob_is_valid(cls, values: dict[str, Any]) -> Any:\n        \"\"\"Verify that either data or path is provided.\"\"\"\n        if \"data\" not in values and \"path\" not in values:\n            msg = \"Either data or path must be provided\"\n            raise ValueError(msg)\n        return values\n\n    def as_string(self) -> str:\n        \"\"\"Read data as a string.\"\"\"\n        if self.data is None and self.path:\n            with open(str(self.path), encoding=self.encoding) as f:\n                return f.read()\n        elif isinstance(self.data, bytes):\n            return self.data.decode(self.encoding)\n        elif isinstance(self.data, str):\n            return self.data\n        else:\n            msg = f\"Unable to get string for blob {self}\"\n            raise ValueError(msg)\n\n    def as_bytes(self) -> bytes:\n        \"\"\"Read data as bytes.\"\"\"\n        if isinstance(self.data, bytes):\n            return self.data\n        elif isinstance(self.data, str):\n            return self.data.encode(self.encoding)\n        elif self.data is None and self.path:\n            with open(str(self.path), \"rb\") as f:\n                return f.read()\n        else:\n            msg = f\"Unable to get bytes for blob {self}\"\n            raise ValueError(msg)\n\n    @contextlib.contextmanager\n    def as_bytes_io(self) -> Generator[Union[BytesIO, BufferedReader], None, None]:\n        \"\"\"Read data as a byte stream.\"\"\"\n        if isinstance(self.data, bytes):\n            yield BytesIO(self.data)\n        elif self.data is None and self.path:\n            with open(str(self.path), \"rb\") as f:\n                yield f\n        else:\n            msg = f\"Unable to convert blob {self}\"\n            raise NotImplementedError(msg)\n\n    @classmethod\n    def from_path(\n        cls,\n        path: PathLike,\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        guess_type: bool = True,\n        metadata: Optional[dict] = None,\n    ) -> Blob:\n        \"\"\"Load the blob from a path like object.\n\n        Args:\n            path: path like object to file to be read\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            guess_type: If True, the mimetype will be guessed from the file extension,\n                        if a mime-type was not provided\n            metadata: Metadata to associate with the blob\n\n        Returns:\n            Blob instance\n        \"\"\"\n        if mime_type is None and guess_type:\n            _mimetype = mimetypes.guess_type(path)[0] if guess_type else None\n        else:\n            _mimetype = mime_type\n        # We do not load the data immediately, instead we treat the blob as a\n        # reference to the underlying data.\n        return cls(\n            data=None,\n            mimetype=_mimetype,\n            encoding=encoding,\n            path=path,\n            metadata=metadata if metadata is not None else {},\n        )\n\n    @classmethod\n    def from_data(\n        cls,\n        data: Union[str, bytes],\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        path: Optional[str] = None,\n        metadata: Optional[dict] = None,\n    ) -> Blob:\n        \"\"\"Initialize the blob from in-memory data.\n\n        Args:\n            data: the in-memory data associated with the blob\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            path: if provided, will be set as the source from which the data came\n            metadata: Metadata to associate with the blob\n\n        Returns:\n            Blob instance\n        \"\"\"\n        return cls(\n            data=data,\n            mimetype=mime_type,\n            encoding=encoding,\n            path=path,\n            metadata=metadata if metadata is not None else {},\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Define the blob representation.\"\"\"\n        str_repr = f\"Blob {id(self)}\"\n        if self.source:\n            str_repr += f\" {self.source}\"\n        return str_repr\n\n\nclass Document(BaseMedia):\n    \"\"\"Class for storing a piece of text and associated metadata.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.documents import Document\n\n            document = Document(\n                page_content=\"Hello, world!\",\n                metadata={\"source\": \"https://example.com\"}\n            )\n    \"\"\"\n\n    page_content: str\n    \"\"\"String text.\"\"\"\n    type: Literal[\"Document\"] = \"Document\"\n\n    def __init__(self, page_content: str, **kwargs: Any) -> None:\n        \"\"\"Pass page_content in as positional or named arg.\"\"\"\n        # my-py is complaining that page_content is not defined on the base class.\n        # Here, we're relying on pydantic base class to handle the validation.\n        super().__init__(page_content=page_content, **kwargs)  # type: ignore[call-arg]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this class is serializable.\"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"document\"]\n\n    def __str__(self) -> str:\n        \"\"\"Override __str__ to restrict it to page_content and metadata.\"\"\"\n        # The format matches pydantic format for __str__.\n        #\n        # The purpose of this change is to make sure that user code that\n        # feeds Document objects directly into prompts remains unchanged\n        # due to the addition of the id field (or any other fields in the future).\n        #\n        # This override will likely be removed in the future in favor of\n        # a more general solution of formatting content directly inside the prompts.\n        if self.metadata:\n            return f\"page_content='{self.page_content}' metadata={self.metadata}\"\n        else:\n            return f\"page_content='{self.page_content}'\"\n",
        "patch": "@@ -3,7 +3,7 @@\n import contextlib\n import mimetypes\n from io import BufferedReader, BytesIO\n-from pathlib import PurePath\n+from pathlib import Path, PurePath\n from typing import TYPE_CHECKING, Any, Literal, Optional, Union, cast\n \n from pydantic import ConfigDict, Field, field_validator, model_validator\n@@ -151,8 +151,7 @@ def check_blob_is_valid(cls, values: dict[str, Any]) -> Any:\n     def as_string(self) -> str:\n         \"\"\"Read data as a string.\"\"\"\n         if self.data is None and self.path:\n-            with open(str(self.path), encoding=self.encoding) as f:\n-                return f.read()\n+            return Path(self.path).read_text(encoding=self.encoding)\n         elif isinstance(self.data, bytes):\n             return self.data.decode(self.encoding)\n         elif isinstance(self.data, str):\n@@ -168,8 +167,7 @@ def as_bytes(self) -> bytes:\n         elif isinstance(self.data, str):\n             return self.data.encode(self.encoding)\n         elif self.data is None and self.path:\n-            with open(str(self.path), \"rb\") as f:\n-                return f.read()\n+            return Path(self.path).read_bytes()\n         else:\n             msg = f\"Unable to get bytes for blob {self}\"\n             raise ValueError(msg)\n@@ -180,7 +178,7 @@ def as_bytes_io(self) -> Generator[Union[BytesIO, BufferedReader], None, None]:\n         if isinstance(self.data, bytes):\n             yield BytesIO(self.data)\n         elif self.data is None and self.path:\n-            with open(str(self.path), \"rb\") as f:\n+            with Path(self.path).open(\"rb\") as f:\n                 yield f\n         else:\n             msg = f\"Unable to convert blob {self}\""
      },
      {
        "filename": "libs/core/langchain_core/language_models/llms.py",
        "content_before": "\"\"\"Base interface for large language models to expose.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport logging\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import AsyncIterator, Iterator, Sequence\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Optional,\n    Union,\n    cast,\n)\n\nimport yaml\nfrom pydantic import ConfigDict, Field, model_validator\nfrom tenacity import (\n    RetryCallState,\n    before_sleep_log,\n    retry,\n    retry_base,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\nfrom typing_extensions import override\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.callbacks import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForLLMRun,\n    BaseCallbackManager,\n    CallbackManager,\n    CallbackManagerForLLMRun,\n    Callbacks,\n)\nfrom langchain_core.globals import get_llm_cache\nfrom langchain_core.language_models.base import (\n    BaseLanguageModel,\n    LangSmithParams,\n    LanguageModelInput,\n)\nfrom langchain_core.load import dumpd\nfrom langchain_core.messages import (\n    AIMessage,\n    BaseMessage,\n    convert_to_messages,\n    get_buffer_string,\n)\nfrom langchain_core.outputs import Generation, GenerationChunk, LLMResult, RunInfo\nfrom langchain_core.prompt_values import ChatPromptValue, PromptValue, StringPromptValue\nfrom langchain_core.runnables import RunnableConfig, ensure_config, get_config_list\nfrom langchain_core.runnables.config import run_in_executor\n\nif TYPE_CHECKING:\n    import uuid\n\nlogger = logging.getLogger(__name__)\n\n\n@functools.lru_cache\ndef _log_error_once(msg: str) -> None:\n    \"\"\"Log an error once.\"\"\"\n    logger.error(msg)\n\n\ndef create_base_retry_decorator(\n    error_types: list[type[BaseException]],\n    max_retries: int = 1,\n    run_manager: Optional[\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n    ] = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Create a retry decorator for a given LLM and provided\n     a list of error types.\n\n    Args:\n        error_types: List of error types to retry on.\n        max_retries: Number of retries. Default is 1.\n        run_manager: Callback manager for the run. Default is None.\n\n    Returns:\n        A retry decorator.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    _logging = before_sleep_log(logger, logging.WARNING)\n\n    def _before_sleep(retry_state: RetryCallState) -> None:\n        _logging(retry_state)\n        if run_manager:\n            if isinstance(run_manager, AsyncCallbackManagerForLLMRun):\n                coro = run_manager.on_retry(retry_state)\n                try:\n                    loop = asyncio.get_event_loop()\n                    if loop.is_running():\n                        loop.create_task(coro)\n                    else:\n                        asyncio.run(coro)\n                except Exception as e:\n                    _log_error_once(f\"Error in on_retry: {e}\")\n            else:\n                run_manager.on_retry(retry_state)\n\n    min_seconds = 4\n    max_seconds = 10\n    # Wait 2^x * 1 second between each retry starting with\n    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards\n    retry_instance: retry_base = retry_if_exception_type(error_types[0])\n    for error in error_types[1:]:\n        retry_instance = retry_instance | retry_if_exception_type(error)\n    return retry(\n        reraise=True,\n        stop=stop_after_attempt(max_retries),\n        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),\n        retry=retry_instance,\n        before_sleep=_before_sleep,\n    )\n\n\ndef _resolve_cache(cache: Union[BaseCache, bool, None]) -> Optional[BaseCache]:\n    \"\"\"Resolve the cache.\"\"\"\n    if isinstance(cache, BaseCache):\n        llm_cache = cache\n    elif cache is None:\n        llm_cache = get_llm_cache()\n    elif cache is True:\n        llm_cache = get_llm_cache()\n        if llm_cache is None:\n            msg = (\n                \"No global cache was configured. Use `set_llm_cache`.\"\n                \"to set a global cache if you want to use a global cache.\"\n                \"Otherwise either pass a cache object or set cache to False/None\"\n            )\n            raise ValueError(msg)\n    elif cache is False:\n        llm_cache = None\n    else:\n        msg = f\"Unsupported cache value {cache}\"\n        raise ValueError(msg)\n    return llm_cache\n\n\ndef get_prompts(\n    params: dict[str, Any],\n    prompts: list[str],\n    cache: Optional[Union[BaseCache, bool, None]] = None,\n) -> tuple[dict[int, list], str, list[int], list[str]]:\n    \"\"\"Get prompts that are already cached.\n\n    Args:\n        params: Dictionary of parameters.\n        prompts: List of prompts.\n        cache: Cache object. Default is None.\n\n    Returns:\n        A tuple of existing prompts, llm_string, missing prompt indexes,\n            and missing prompts.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_string = str(sorted(params.items()))\n    missing_prompts = []\n    missing_prompt_idxs = []\n    existing_prompts = {}\n\n    llm_cache = _resolve_cache(cache)\n    for i, prompt in enumerate(prompts):\n        if llm_cache:\n            cache_val = llm_cache.lookup(prompt, llm_string)\n            if isinstance(cache_val, list):\n                existing_prompts[i] = cache_val\n            else:\n                missing_prompts.append(prompt)\n                missing_prompt_idxs.append(i)\n    return existing_prompts, llm_string, missing_prompt_idxs, missing_prompts\n\n\nasync def aget_prompts(\n    params: dict[str, Any],\n    prompts: list[str],\n    cache: Optional[Union[BaseCache, bool, None]] = None,\n) -> tuple[dict[int, list], str, list[int], list[str]]:\n    \"\"\"Get prompts that are already cached. Async version.\n\n    Args:\n        params: Dictionary of parameters.\n        prompts: List of prompts.\n        cache: Cache object. Default is None.\n\n    Returns:\n        A tuple of existing prompts, llm_string, missing prompt indexes,\n            and missing prompts.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_string = str(sorted(params.items()))\n    missing_prompts = []\n    missing_prompt_idxs = []\n    existing_prompts = {}\n    llm_cache = _resolve_cache(cache)\n    for i, prompt in enumerate(prompts):\n        if llm_cache:\n            cache_val = await llm_cache.alookup(prompt, llm_string)\n            if isinstance(cache_val, list):\n                existing_prompts[i] = cache_val\n            else:\n                missing_prompts.append(prompt)\n                missing_prompt_idxs.append(i)\n    return existing_prompts, llm_string, missing_prompt_idxs, missing_prompts\n\n\ndef update_cache(\n    cache: Union[BaseCache, bool, None],\n    existing_prompts: dict[int, list],\n    llm_string: str,\n    missing_prompt_idxs: list[int],\n    new_results: LLMResult,\n    prompts: list[str],\n) -> Optional[dict]:\n    \"\"\"Update the cache and get the LLM output.\n\n    Args:\n        cache: Cache object.\n        existing_prompts: Dictionary of existing prompts.\n        llm_string: LLM string.\n        missing_prompt_idxs: List of missing prompt indexes.\n        new_results: LLMResult object.\n        prompts: List of prompts.\n\n    Returns:\n        LLM output.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_cache = _resolve_cache(cache)\n    for i, result in enumerate(new_results.generations):\n        existing_prompts[missing_prompt_idxs[i]] = result\n        prompt = prompts[missing_prompt_idxs[i]]\n        if llm_cache is not None:\n            llm_cache.update(prompt, llm_string, result)\n    llm_output = new_results.llm_output\n    return llm_output\n\n\nasync def aupdate_cache(\n    cache: Union[BaseCache, bool, None],\n    existing_prompts: dict[int, list],\n    llm_string: str,\n    missing_prompt_idxs: list[int],\n    new_results: LLMResult,\n    prompts: list[str],\n) -> Optional[dict]:\n    \"\"\"Update the cache and get the LLM output. Async version.\n\n    Args:\n        cache: Cache object.\n        existing_prompts: Dictionary of existing prompts.\n        llm_string: LLM string.\n        missing_prompt_idxs: List of missing prompt indexes.\n        new_results: LLMResult object.\n        prompts: List of prompts.\n\n    Returns:\n        LLM output.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_cache = _resolve_cache(cache)\n    for i, result in enumerate(new_results.generations):\n        existing_prompts[missing_prompt_idxs[i]] = result\n        prompt = prompts[missing_prompt_idxs[i]]\n        if llm_cache:\n            await llm_cache.aupdate(prompt, llm_string, result)\n    llm_output = new_results.llm_output\n    return llm_output\n\n\nclass BaseLLM(BaseLanguageModel[str], ABC):\n    \"\"\"Base LLM abstract interface.\n\n    It should take in a prompt and return a string.\n    \"\"\"\n\n    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)\n    \"\"\"[DEPRECATED]\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def raise_deprecation(cls, values: dict) -> Any:\n        \"\"\"Raise deprecation warning if callback_manager is used.\"\"\"\n        if values.get(\"callback_manager\") is not None:\n            warnings.warn(\n                \"callback_manager is deprecated. Please use callbacks instead.\",\n                DeprecationWarning,\n                stacklevel=5,\n            )\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\n        return values\n\n    @functools.cached_property\n    def _serialized(self) -> dict[str, Any]:\n        return dumpd(self)\n\n    # --- Runnable methods ---\n\n    @property\n    @override\n    def OutputType(self) -> type[str]:\n        \"\"\"Get the input type for this runnable.\"\"\"\n        return str\n\n    def _convert_input(self, input: LanguageModelInput) -> PromptValue:\n        if isinstance(input, PromptValue):\n            return input\n        elif isinstance(input, str):\n            return StringPromptValue(text=input)\n        elif isinstance(input, Sequence):\n            return ChatPromptValue(messages=convert_to_messages(input))\n        else:\n            msg = (\n                f\"Invalid input type {type(input)}. \"\n                \"Must be a PromptValue, str, or list of BaseMessages.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n\n    def _get_ls_params(\n        self,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        # get default provider from class name\n        default_provider = self.__class__.__name__\n        default_provider = default_provider.removesuffix(\"LLM\")\n        default_provider = default_provider.lower()\n\n        ls_params = LangSmithParams(ls_provider=default_provider, ls_model_type=\"llm\")\n        if stop:\n            ls_params[\"ls_stop\"] = stop\n\n        # model\n        if hasattr(self, \"model\") and isinstance(self.model, str):\n            ls_params[\"ls_model_name\"] = self.model\n        elif hasattr(self, \"model_name\") and isinstance(self.model_name, str):\n            ls_params[\"ls_model_name\"] = self.model_name\n\n        # temperature\n        if \"temperature\" in kwargs and isinstance(kwargs[\"temperature\"], float):\n            ls_params[\"ls_temperature\"] = kwargs[\"temperature\"]\n        elif hasattr(self, \"temperature\") and isinstance(self.temperature, float):\n            ls_params[\"ls_temperature\"] = self.temperature\n\n        # max_tokens\n        if \"max_tokens\" in kwargs and isinstance(kwargs[\"max_tokens\"], int):\n            ls_params[\"ls_max_tokens\"] = kwargs[\"max_tokens\"]\n        elif hasattr(self, \"max_tokens\") and isinstance(self.max_tokens, int):\n            ls_params[\"ls_max_tokens\"] = self.max_tokens\n\n        return ls_params\n\n    def invoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> str:\n        config = ensure_config(config)\n        return (\n            self.generate_prompt(\n                [self._convert_input(input)],\n                stop=stop,\n                callbacks=config.get(\"callbacks\"),\n                tags=config.get(\"tags\"),\n                metadata=config.get(\"metadata\"),\n                run_name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                **kwargs,\n            )\n            .generations[0][0]\n            .text\n        )\n\n    async def ainvoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> str:\n        config = ensure_config(config)\n        llm_result = await self.agenerate_prompt(\n            [self._convert_input(input)],\n            stop=stop,\n            callbacks=config.get(\"callbacks\"),\n            tags=config.get(\"tags\"),\n            metadata=config.get(\"metadata\"),\n            run_name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            **kwargs,\n        )\n        return llm_result.generations[0][0].text\n\n    def batch(\n        self,\n        inputs: list[LanguageModelInput],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Any,\n    ) -> list[str]:\n        if not inputs:\n            return []\n\n        config = get_config_list(config, len(inputs))\n        max_concurrency = config[0].get(\"max_concurrency\")\n\n        if max_concurrency is None:\n            try:\n                llm_result = self.generate_prompt(\n                    [self._convert_input(input) for input in inputs],\n                    callbacks=[c.get(\"callbacks\") for c in config],\n                    tags=[c.get(\"tags\") for c in config],\n                    metadata=[c.get(\"metadata\") for c in config],\n                    run_name=[c.get(\"run_name\") for c in config],\n                    **kwargs,\n                )\n                return [g[0].text for g in llm_result.generations]\n            except Exception as e:\n                if return_exceptions:\n                    return cast(list[str], [e for _ in inputs])\n                else:\n                    raise\n        else:\n            batches = [\n                inputs[i : i + max_concurrency]\n                for i in range(0, len(inputs), max_concurrency)\n            ]\n            config = [{**c, \"max_concurrency\": None} for c in config]  # type: ignore[misc]\n            return [\n                output\n                for i, batch in enumerate(batches)\n                for output in self.batch(\n                    batch,\n                    config=config[i * max_concurrency : (i + 1) * max_concurrency],\n                    return_exceptions=return_exceptions,\n                    **kwargs,\n                )\n            ]\n\n    async def abatch(\n        self,\n        inputs: list[LanguageModelInput],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Any,\n    ) -> list[str]:\n        if not inputs:\n            return []\n        config = get_config_list(config, len(inputs))\n        max_concurrency = config[0].get(\"max_concurrency\")\n\n        if max_concurrency is None:\n            try:\n                llm_result = await self.agenerate_prompt(\n                    [self._convert_input(input) for input in inputs],\n                    callbacks=[c.get(\"callbacks\") for c in config],\n                    tags=[c.get(\"tags\") for c in config],\n                    metadata=[c.get(\"metadata\") for c in config],\n                    run_name=[c.get(\"run_name\") for c in config],\n                    **kwargs,\n                )\n                return [g[0].text for g in llm_result.generations]\n            except Exception as e:\n                if return_exceptions:\n                    return cast(list[str], [e for _ in inputs])\n                else:\n                    raise\n        else:\n            batches = [\n                inputs[i : i + max_concurrency]\n                for i in range(0, len(inputs), max_concurrency)\n            ]\n            config = [{**c, \"max_concurrency\": None} for c in config]  # type: ignore[misc]\n            return [\n                output\n                for i, batch in enumerate(batches)\n                for output in await self.abatch(\n                    batch,\n                    config=config[i * max_concurrency : (i + 1) * max_concurrency],\n                    return_exceptions=return_exceptions,\n                    **kwargs,\n                )\n            ]\n\n    def stream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> Iterator[str]:\n        if type(self)._stream == BaseLLM._stream:\n            # model doesn't implement streaming, so use default implementation\n            yield self.invoke(input, config=config, stop=stop, **kwargs)\n        else:\n            prompt = self._convert_input(input).to_string()\n            config = ensure_config(config)\n            params = self.dict()\n            params[\"stop\"] = stop\n            params = {**params, **kwargs}\n            options = {\"stop\": stop}\n            inheritable_metadata = {\n                **(config.get(\"metadata\") or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n            }\n            callback_manager = CallbackManager.configure(\n                config.get(\"callbacks\"),\n                self.callbacks,\n                self.verbose,\n                config.get(\"tags\"),\n                self.tags,\n                inheritable_metadata,\n                self.metadata,\n            )\n            (run_manager,) = callback_manager.on_llm_start(\n                self._serialized,\n                [prompt],\n                invocation_params=params,\n                options=options,\n                name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                batch_size=1,\n            )\n            generation: Optional[GenerationChunk] = None\n            try:\n                for chunk in self._stream(\n                    prompt, stop=stop, run_manager=run_manager, **kwargs\n                ):\n                    yield chunk.text\n                    if generation is None:\n                        generation = chunk\n                    else:\n                        generation += chunk\n            except BaseException as e:\n                run_manager.on_llm_error(\n                    e,\n                    response=LLMResult(\n                        generations=[[generation]] if generation else []\n                    ),\n                )\n                raise\n\n            if generation is None:\n                err = ValueError(\"No generation chunks were returned\")\n                run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n                raise err\n\n            run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n\n    async def astream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[str]:\n        if (\n            type(self)._astream is BaseLLM._astream\n            and type(self)._stream is BaseLLM._stream\n        ):\n            yield await self.ainvoke(input, config=config, stop=stop, **kwargs)\n            return\n\n        prompt = self._convert_input(input).to_string()\n        config = ensure_config(config)\n        params = self.dict()\n        params[\"stop\"] = stop\n        params = {**params, **kwargs}\n        options = {\"stop\": stop}\n        inheritable_metadata = {\n            **(config.get(\"metadata\") or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n        }\n        callback_manager = AsyncCallbackManager.configure(\n            config.get(\"callbacks\"),\n            self.callbacks,\n            self.verbose,\n            config.get(\"tags\"),\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n        (run_manager,) = await callback_manager.on_llm_start(\n            self._serialized,\n            [prompt],\n            invocation_params=params,\n            options=options,\n            name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            batch_size=1,\n        )\n        generation: Optional[GenerationChunk] = None\n        try:\n            async for chunk in self._astream(\n                prompt,\n                stop=stop,\n                run_manager=run_manager,\n                **kwargs,\n            ):\n                yield chunk.text\n                if generation is None:\n                    generation = chunk\n                else:\n                    generation += chunk\n        except BaseException as e:\n            await run_manager.on_llm_error(\n                e,\n                response=LLMResult(generations=[[generation]] if generation else []),\n            )\n            raise\n\n        if generation is None:\n            err = ValueError(\"No generation chunks were returned\")\n            await run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n            raise err\n\n        await run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n\n    # --- Custom methods ---\n\n    @abstractmethod\n    def _generate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Run the LLM on the given prompts.\"\"\"\n\n    async def _agenerate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Run the LLM on the given prompts.\"\"\"\n        return await run_in_executor(\n            None,\n            self._generate,\n            prompts,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n    def _stream(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[GenerationChunk]:\n        \"\"\"Stream the LLM on the given prompt.\n\n        This method should be overridden by subclasses that support streaming.\n\n        If not implemented, the default behavior of calls to stream will be to\n        fallback to the non-streaming version of the model and return\n        the output as a single chunk.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An iterator of GenerationChunks.\n        \"\"\"\n        raise NotImplementedError\n\n    async def _astream(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[GenerationChunk]:\n        \"\"\"An async version of the _stream method.\n\n        The default implementation uses the synchronous _stream method and wraps it in\n        an async iterator. Subclasses that need to provide a true async implementation\n        should override this method.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An async iterator of GenerationChunks.\n        \"\"\"\n        iterator = await run_in_executor(\n            None,\n            self._stream,\n            prompt,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n        done = object()\n        while True:\n            item = await run_in_executor(\n                None,\n                next,\n                iterator,\n                done,  # type: ignore[call-arg, arg-type]\n            )\n            if item is done:\n                break\n            yield item  # type: ignore[misc]\n\n    def generate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_strings = [p.to_string() for p in prompts]\n        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n\n    async def agenerate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_strings = [p.to_string() for p in prompts]\n        return await self.agenerate(\n            prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n        )\n\n    def _generate_helper(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]],\n        run_managers: list[CallbackManagerForLLMRun],\n        new_arg_supported: bool,\n        **kwargs: Any,\n    ) -> LLMResult:\n        try:\n            output = (\n                self._generate(\n                    prompts,\n                    stop=stop,\n                    # TODO: support multiple run managers\n                    run_manager=run_managers[0] if run_managers else None,\n                    **kwargs,\n                )\n                if new_arg_supported\n                else self._generate(prompts, stop=stop)\n            )\n        except BaseException as e:\n            for run_manager in run_managers:\n                run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n            raise\n        flattened_outputs = output.flatten()\n        for manager, flattened_output in zip(run_managers, flattened_outputs):\n            manager.on_llm_end(flattened_output)\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\n\n    def generate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        *,\n        tags: Optional[Union[list[str], list[list[str]]]] = None,\n        metadata: Optional[Union[dict[str, Any], list[dict[str, Any]]]] = None,\n        run_name: Optional[Union[str, list[str]]] = None,\n        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Pass a sequence of prompts to a model and return generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            prompts: List of string prompts.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            tags: List of tags to associate with each prompt. If provided, the length\n                of the list must match the length of the prompts list.\n            metadata: List of metadata dictionaries to associate with each prompt. If\n                provided, the length of the list must match the length of the prompts\n                list.\n            run_name: List of run names to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            run_id: List of run IDs to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        if not isinstance(prompts, list):\n            msg = (\n                \"Argument 'prompts' is expected to be of type List[str], received\"\n                f\" argument of type {type(prompts)}.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n        # Create callback managers\n        if isinstance(metadata, list):\n            metadata = [\n                {\n                    **(meta or {}),\n                    **self._get_ls_params(stop=stop, **kwargs),\n                }\n                for meta in metadata\n            ]\n        elif isinstance(metadata, dict):\n            metadata = {\n                **(metadata or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n            }\n        else:\n            pass\n        if (\n            isinstance(callbacks, list)\n            and callbacks\n            and (\n                isinstance(callbacks[0], (list, BaseCallbackManager))\n                or callbacks[0] is None\n            )\n        ):\n            # We've received a list of callbacks args to apply to each input\n            if len(callbacks) != len(prompts):\n                msg = \"callbacks must be the same length as prompts\"\n                raise ValueError(msg)\n            if tags is not None and not (\n                isinstance(tags, list) and len(tags) == len(prompts)\n            ):\n                msg = \"tags must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if metadata is not None and not (\n                isinstance(metadata, list) and len(metadata) == len(prompts)\n            ):\n                msg = \"metadata must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if run_name is not None and not (\n                isinstance(run_name, list) and len(run_name) == len(prompts)\n            ):\n                msg = \"run_name must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            callbacks = cast(list[Callbacks], callbacks)\n            tags_list = cast(list[Optional[list[str]]], tags or ([None] * len(prompts)))\n            metadata_list = cast(\n                list[Optional[dict[str, Any]]], metadata or ([{}] * len(prompts))\n            )\n            run_name_list = run_name or cast(\n                list[Optional[str]], ([None] * len(prompts))\n            )\n            callback_managers = [\n                CallbackManager.configure(\n                    callback,\n                    self.callbacks,\n                    self.verbose,\n                    tag,\n                    self.tags,\n                    meta,\n                    self.metadata,\n                )\n                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)\n            ]\n        else:\n            # We've received a single callbacks arg to apply to all inputs\n            callback_managers = [\n                CallbackManager.configure(\n                    cast(Callbacks, callbacks),\n                    self.callbacks,\n                    self.verbose,\n                    cast(list[str], tags),\n                    self.tags,\n                    cast(dict[str, Any], metadata),\n                    self.metadata,\n                )\n            ] * len(prompts)\n            run_name_list = [cast(Optional[str], run_name)] * len(prompts)\n        run_ids_list = self._get_run_ids_list(run_id, prompts)\n        params = self.dict()\n        params[\"stop\"] = stop\n        options = {\"stop\": stop}\n        (\n            existing_prompts,\n            llm_string,\n            missing_prompt_idxs,\n            missing_prompts,\n        ) = get_prompts(params, prompts, self.cache)\n        new_arg_supported = inspect.signature(self._generate).parameters.get(\n            \"run_manager\"\n        )\n        if (self.cache is None and get_llm_cache() is None) or self.cache is False:\n            run_managers = [\n                callback_manager.on_llm_start(\n                    self._serialized,\n                    [prompt],\n                    invocation_params=params,\n                    options=options,\n                    name=run_name,\n                    batch_size=len(prompts),\n                    run_id=run_id_,\n                )[0]\n                for callback_manager, prompt, run_name, run_id_ in zip(\n                    callback_managers, prompts, run_name_list, run_ids_list\n                )\n            ]\n            output = self._generate_helper(\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            return output\n        if len(missing_prompts) > 0:\n            run_managers = [\n                callback_managers[idx].on_llm_start(\n                    self._serialized,\n                    [prompts[idx]],\n                    invocation_params=params,\n                    options=options,\n                    name=run_name_list[idx],\n                    batch_size=len(missing_prompts),\n                )[0]\n                for idx in missing_prompt_idxs\n            ]\n            new_results = self._generate_helper(\n                missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            llm_output = update_cache(\n                self.cache,\n                existing_prompts,\n                llm_string,\n                missing_prompt_idxs,\n                new_results,\n                prompts,\n            )\n            run_info = (\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\n                if run_managers\n                else None\n            )\n        else:\n            llm_output = {}\n            run_info = None\n        generations = [existing_prompts[i] for i in range(len(prompts))]\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)\n\n    @staticmethod\n    def _get_run_ids_list(\n        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]], prompts: list\n    ) -> list:\n        if run_id is None:\n            return [None] * len(prompts)\n        if isinstance(run_id, list):\n            if len(run_id) != len(prompts):\n                msg = (\n                    \"Number of manually provided run_id's does not match batch length.\"\n                    f\" {len(run_id)} != {len(prompts)}\"\n                )\n                raise ValueError(msg)\n            return run_id\n        return [run_id] + [None] * (len(prompts) - 1)\n\n    async def _agenerate_helper(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]],\n        run_managers: list[AsyncCallbackManagerForLLMRun],\n        new_arg_supported: bool,\n        **kwargs: Any,\n    ) -> LLMResult:\n        try:\n            output = (\n                await self._agenerate(\n                    prompts,\n                    stop=stop,\n                    run_manager=run_managers[0] if run_managers else None,\n                    **kwargs,\n                )\n                if new_arg_supported\n                else await self._agenerate(prompts, stop=stop)\n            )\n        except BaseException as e:\n            await asyncio.gather(\n                *[\n                    run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n                    for run_manager in run_managers\n                ]\n            )\n            raise\n        flattened_outputs = output.flatten()\n        await asyncio.gather(\n            *[\n                run_manager.on_llm_end(flattened_output)\n                for run_manager, flattened_output in zip(\n                    run_managers, flattened_outputs\n                )\n            ]\n        )\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\n\n    async def agenerate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        *,\n        tags: Optional[Union[list[str], list[list[str]]]] = None,\n        metadata: Optional[Union[dict[str, Any], list[dict[str, Any]]]] = None,\n        run_name: Optional[Union[str, list[str]]] = None,\n        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Asynchronously pass a sequence of prompts to a model and return generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            prompts: List of string prompts.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            tags: List of tags to associate with each prompt. If provided, the length\n                of the list must match the length of the prompts list.\n            metadata: List of metadata dictionaries to associate with each prompt. If\n                provided, the length of the list must match the length of the prompts\n                list.\n            run_name: List of run names to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            run_id: List of run IDs to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        if isinstance(metadata, list):\n            metadata = [\n                {\n                    **(meta or {}),\n                    **self._get_ls_params(stop=stop, **kwargs),\n                }\n                for meta in metadata\n            ]\n        elif isinstance(metadata, dict):\n            metadata = {\n                **(metadata or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n            }\n        else:\n            pass\n        # Create callback managers\n        if isinstance(callbacks, list) and (\n            isinstance(callbacks[0], (list, BaseCallbackManager))\n            or callbacks[0] is None\n        ):\n            # We've received a list of callbacks args to apply to each input\n            if len(callbacks) != len(prompts):\n                msg = \"callbacks must be the same length as prompts\"\n                raise ValueError(msg)\n            if tags is not None and not (\n                isinstance(tags, list) and len(tags) == len(prompts)\n            ):\n                msg = \"tags must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if metadata is not None and not (\n                isinstance(metadata, list) and len(metadata) == len(prompts)\n            ):\n                msg = \"metadata must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if run_name is not None and not (\n                isinstance(run_name, list) and len(run_name) == len(prompts)\n            ):\n                msg = \"run_name must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            callbacks = cast(list[Callbacks], callbacks)\n            tags_list = cast(list[Optional[list[str]]], tags or ([None] * len(prompts)))\n            metadata_list = cast(\n                list[Optional[dict[str, Any]]], metadata or ([{}] * len(prompts))\n            )\n            run_name_list = run_name or cast(\n                list[Optional[str]], ([None] * len(prompts))\n            )\n            callback_managers = [\n                AsyncCallbackManager.configure(\n                    callback,\n                    self.callbacks,\n                    self.verbose,\n                    tag,\n                    self.tags,\n                    meta,\n                    self.metadata,\n                )\n                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)\n            ]\n        else:\n            # We've received a single callbacks arg to apply to all inputs\n            callback_managers = [\n                AsyncCallbackManager.configure(\n                    cast(Callbacks, callbacks),\n                    self.callbacks,\n                    self.verbose,\n                    cast(list[str], tags),\n                    self.tags,\n                    cast(dict[str, Any], metadata),\n                    self.metadata,\n                )\n            ] * len(prompts)\n            run_name_list = [cast(Optional[str], run_name)] * len(prompts)\n        run_ids_list = self._get_run_ids_list(run_id, prompts)\n        params = self.dict()\n        params[\"stop\"] = stop\n        options = {\"stop\": stop}\n        (\n            existing_prompts,\n            llm_string,\n            missing_prompt_idxs,\n            missing_prompts,\n        ) = await aget_prompts(params, prompts, self.cache)\n\n        # Verify whether the cache is set, and if the cache is set,\n        # verify whether the cache is available.\n        new_arg_supported = inspect.signature(self._agenerate).parameters.get(\n            \"run_manager\"\n        )\n        if (self.cache is None and get_llm_cache() is None) or self.cache is False:\n            run_managers = await asyncio.gather(\n                *[\n                    callback_manager.on_llm_start(\n                        self._serialized,\n                        [prompt],\n                        invocation_params=params,\n                        options=options,\n                        name=run_name,\n                        batch_size=len(prompts),\n                        run_id=run_id_,\n                    )\n                    for callback_manager, prompt, run_name, run_id_ in zip(\n                        callback_managers, prompts, run_name_list, run_ids_list\n                    )\n                ]\n            )\n            run_managers = [r[0] for r in run_managers]  # type: ignore[misc]\n            output = await self._agenerate_helper(\n                prompts,\n                stop,\n                run_managers,  # type: ignore[arg-type]\n                bool(new_arg_supported),\n                **kwargs,  # type: ignore[arg-type]\n            )\n            return output\n        if len(missing_prompts) > 0:\n            run_managers = await asyncio.gather(\n                *[\n                    callback_managers[idx].on_llm_start(\n                        self._serialized,\n                        [prompts[idx]],\n                        invocation_params=params,\n                        options=options,\n                        name=run_name_list[idx],\n                        batch_size=len(missing_prompts),\n                    )\n                    for idx in missing_prompt_idxs\n                ]\n            )\n            run_managers = [r[0] for r in run_managers]  # type: ignore[misc]\n            new_results = await self._agenerate_helper(\n                missing_prompts,\n                stop,\n                run_managers,  # type: ignore[arg-type]\n                bool(new_arg_supported),\n                **kwargs,  # type: ignore[arg-type]\n            )\n            llm_output = await aupdate_cache(\n                self.cache,\n                existing_prompts,\n                llm_string,\n                missing_prompt_idxs,\n                new_results,\n                prompts,\n            )\n            run_info = (\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]  # type: ignore[attr-defined]\n                if run_managers\n                else None\n            )\n        else:\n            llm_output = {}\n            run_info = None\n        generations = [existing_prompts[i] for i in range(len(prompts))]\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def __call__(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Check Cache and run the LLM on the given prompt and input.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            tags: List of tags to associate with the prompt.\n            metadata: Metadata to associate with the prompt.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            The generated text.\n\n        Raises:\n            ValueError: If the prompt is not a string.\n        \"\"\"\n        if not isinstance(prompt, str):\n            msg = (\n                \"Argument `prompt` is expected to be a string. Instead found \"\n                f\"{type(prompt)}. If you want to run the LLM on multiple prompts, use \"\n                \"`generate` instead.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n        return (\n            self.generate(\n                [prompt],\n                stop=stop,\n                callbacks=callbacks,\n                tags=tags,\n                metadata=metadata,\n                **kwargs,\n            )\n            .generations[0][0]\n            .text\n        )\n\n    async def _call_async(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\n        result = await self.agenerate(\n            [prompt],\n            stop=stop,\n            callbacks=callbacks,\n            tags=tags,\n            metadata=metadata,\n            **kwargs,\n        )\n        return result.generations[0][0].text\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        return self(text, stop=_stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        text = get_buffer_string(messages)\n        _stop = None if stop is None else list(stop)\n        content = self(text, stop=_stop, **kwargs)\n        return AIMessage(content=content)\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        return await self._call_async(text, stop=_stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        text = get_buffer_string(messages)\n        _stop = None if stop is None else list(stop)\n        content = await self._call_async(text, stop=_stop, **kwargs)\n        return AIMessage(content=content)\n\n    def __str__(self) -> str:\n        \"\"\"Get a string representation of the object for printing.\"\"\"\n        cls_name = f\"\\033[1m{self.__class__.__name__}\\033[0m\"\n        return f\"{cls_name}\\nParams: {self._identifying_params}\"\n\n    @property\n    @abstractmethod\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n\n    def dict(self, **kwargs: Any) -> dict:\n        \"\"\"Return a dictionary of the LLM.\"\"\"\n        starter_dict = dict(self._identifying_params)\n        starter_dict[\"_type\"] = self._llm_type\n        return starter_dict\n\n    def save(self, file_path: Union[Path, str]) -> None:\n        \"\"\"Save the LLM.\n\n        Args:\n            file_path: Path to file to save the LLM to.\n\n        Raises:\n            ValueError: If the file path is not a string or Path object.\n\n        Example:\n        .. code-block:: python\n\n            llm.save(file_path=\"path/llm.yaml\")\n        \"\"\"\n        # Convert file to Path object.\n        save_path = Path(file_path) if isinstance(file_path, str) else file_path\n\n        directory_path = save_path.parent\n        directory_path.mkdir(parents=True, exist_ok=True)\n\n        # Fetch dictionary to save\n        prompt_dict = self.dict()\n\n        if save_path.suffix == \".json\":\n            with open(file_path, \"w\") as f:\n                json.dump(prompt_dict, f, indent=4)\n        elif save_path.suffix.endswith((\".yaml\", \".yml\")):\n            with open(file_path, \"w\") as f:\n                yaml.dump(prompt_dict, f, default_flow_style=False)\n        else:\n            msg = f\"{save_path} must be json or yaml\"\n            raise ValueError(msg)\n\n\nclass LLM(BaseLLM):\n    \"\"\"Simple interface for implementing a custom LLM.\n\n    You should subclass this class and implement the following:\n\n    - `_call` method: Run the LLM on the given prompt and input (used by `invoke`).\n    - `_identifying_params` property: Return a dictionary of the identifying parameters\n        This is critical for caching and tracing purposes. Identifying parameters\n        is a dict that identifies the LLM.\n        It should mostly include a `model_name`.\n\n    Optional: Override the following methods to provide more optimizations:\n\n    - `_acall`: Provide a native async version of the `_call` method.\n        If not provided, will delegate to the synchronous version using\n        `run_in_executor`. (Used by `ainvoke`).\n    - `_stream`: Stream the LLM on the given prompt and input.\n        `stream` will use `_stream` if provided, otherwise it\n        use `_call` and output will arrive in one chunk.\n    - `_astream`: Override to provide a native async version of the `_stream` method.\n        `astream` will use `_astream` if provided, otherwise it will implement\n        a fallback behavior that will use `_stream` if `_stream` is implemented,\n        and use `_acall` if `_stream` is not implemented.\n\n    Please see the following guide for more information on how to\n    implement a custom LLM:\n\n    https://python.langchain.com/docs/how_to/custom_llm/\n    \"\"\"\n\n    @abstractmethod\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Run the LLM on the given input.\n\n        Override this method to implement the LLM logic.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of the stop substrings.\n                If stop tokens are not supported consider raising NotImplementedError.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            The model output as a string. SHOULD NOT include the prompt.\n        \"\"\"\n\n    async def _acall(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Async version of the _call method.\n\n        The default implementation delegates to the synchronous _call method using\n        `run_in_executor`. Subclasses that need to provide a true async implementation\n        should override this method to reduce the overhead of using `run_in_executor`.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of the stop substrings.\n                If stop tokens are not supported consider raising NotImplementedError.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            The model output as a string. SHOULD NOT include the prompt.\n        \"\"\"\n        return await run_in_executor(\n            None,\n            self._call,\n            prompt,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n    def _generate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n        # TODO: add caching here.\n        generations = []\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\n        for prompt in prompts:\n            text = (\n                self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n                if new_arg_supported\n                else self._call(prompt, stop=stop, **kwargs)\n            )\n            generations.append([Generation(text=text)])\n        return LLMResult(generations=generations)\n\n    async def _agenerate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Async run the LLM on the given prompt and input.\"\"\"\n        generations = []\n        new_arg_supported = inspect.signature(self._acall).parameters.get(\"run_manager\")\n        for prompt in prompts:\n            text = (\n                await self._acall(prompt, stop=stop, run_manager=run_manager, **kwargs)\n                if new_arg_supported\n                else await self._acall(prompt, stop=stop, **kwargs)\n            )\n            generations.append([Generation(text=text)])\n        return LLMResult(generations=generations)\n",
        "patch": "@@ -1402,7 +1402,7 @@ def save(self, file_path: Union[Path, str]) -> None:\n             llm.save(file_path=\"path/llm.yaml\")\n         \"\"\"\n         # Convert file to Path object.\n-        save_path = Path(file_path) if isinstance(file_path, str) else file_path\n+        save_path = Path(file_path)\n \n         directory_path = save_path.parent\n         directory_path.mkdir(parents=True, exist_ok=True)\n@@ -1411,10 +1411,10 @@ def save(self, file_path: Union[Path, str]) -> None:\n         prompt_dict = self.dict()\n \n         if save_path.suffix == \".json\":\n-            with open(file_path, \"w\") as f:\n+            with save_path.open(\"w\") as f:\n                 json.dump(prompt_dict, f, indent=4)\n         elif save_path.suffix.endswith((\".yaml\", \".yml\")):\n-            with open(file_path, \"w\") as f:\n+            with save_path.open(\"w\") as f:\n                 yaml.dump(prompt_dict, f, default_flow_style=False)\n         else:\n             msg = f\"{save_path} must be json or yaml\""
      },
      {
        "filename": "libs/core/langchain_core/prompts/base.py",
        "content_before": "from __future__ import annotations\n\nimport contextlib\nimport json\nimport typing\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Mapping\nfrom functools import cached_property\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    Optional,\n    TypeVar,\n    Union,\n)\n\nimport yaml\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\nfrom typing_extensions import Self, override\n\nfrom langchain_core.exceptions import ErrorCode, create_message\nfrom langchain_core.load import dumpd\nfrom langchain_core.output_parsers.base import BaseOutputParser\nfrom langchain_core.prompt_values import (\n    ChatPromptValueConcrete,\n    PromptValue,\n    StringPromptValue,\n)\nfrom langchain_core.runnables import RunnableConfig, RunnableSerializable\nfrom langchain_core.runnables.config import ensure_config\nfrom langchain_core.utils.pydantic import create_model_v2\n\nif TYPE_CHECKING:\n    from langchain_core.documents import Document\n\n\nFormatOutputType = TypeVar(\"FormatOutputType\")\n\n\nclass BasePromptTemplate(\n    RunnableSerializable[dict, PromptValue], Generic[FormatOutputType], ABC\n):\n    \"\"\"Base class for all prompt templates, returning a prompt.\"\"\"\n\n    input_variables: list[str]\n    \"\"\"A list of the names of the variables whose values are required as inputs to the\n    prompt.\"\"\"\n    optional_variables: list[str] = Field(default=[])\n    \"\"\"optional_variables: A list of the names of the variables for placeholder\n       or MessagePlaceholder that are optional. These variables are auto inferred\n       from the prompt and user need not provide them.\"\"\"\n    input_types: typing.Dict[str, Any] = Field(default_factory=dict, exclude=True)  # noqa: UP006\n    \"\"\"A dictionary of the types of the variables the prompt template expects.\n    If not provided, all variables are assumed to be strings.\"\"\"\n    output_parser: Optional[BaseOutputParser] = None\n    \"\"\"How to parse the output of calling an LLM on this formatted prompt.\"\"\"\n    partial_variables: Mapping[str, Any] = Field(default_factory=dict)\n    \"\"\"A dictionary of the partial variables the prompt template carries.\n\n    Partial variables populate the template so that you don't need to\n    pass them in every time you call the prompt.\"\"\"\n    metadata: Optional[typing.Dict[str, Any]] = None  # noqa: UP006\n    \"\"\"Metadata to be used for tracing.\"\"\"\n    tags: Optional[list[str]] = None\n    \"\"\"Tags to be used for tracing.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_variable_names(self) -> Self:\n        \"\"\"Validate variable names do not include restricted names.\"\"\"\n        if \"stop\" in self.input_variables:\n            msg = (\n                \"Cannot have an input variable named 'stop', as it is used internally,\"\n                \" please rename.\"\n            )\n            raise ValueError(\n                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n            )\n        if \"stop\" in self.partial_variables:\n            msg = (\n                \"Cannot have an partial variable named 'stop', as it is used \"\n                \"internally, please rename.\"\n            )\n            raise ValueError(\n                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n            )\n\n        overall = set(self.input_variables).intersection(self.partial_variables)\n        if overall:\n            msg = f\"Found overlapping input and partial variables: {overall}\"\n            raise ValueError(\n                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n            )\n        return self\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\n        Returns [\"langchain\", \"schema\", \"prompt_template\"].\n        \"\"\"\n        return [\"langchain\", \"schema\", \"prompt_template\"]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this class is serializable.\n        Returns True.\n        \"\"\"\n        return True\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @cached_property\n    def _serialized(self) -> dict[str, Any]:\n        return dumpd(self)\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        \"\"\"Return the output type of the prompt.\"\"\"\n        return Union[StringPromptValue, ChatPromptValueConcrete]\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the input schema for the prompt.\n\n        Args:\n            config: RunnableConfig, configuration for the prompt.\n\n        Returns:\n            Type[BaseModel]: The input schema for the prompt.\n        \"\"\"\n        # This is correct, but pydantic typings/mypy don't think so.\n        required_input_variables = {\n            k: (self.input_types.get(k, str), ...) for k in self.input_variables\n        }\n        optional_input_variables = {\n            k: (self.input_types.get(k, str), None) for k in self.optional_variables\n        }\n        return create_model_v2(\n            \"PromptInput\",\n            field_definitions={**required_input_variables, **optional_input_variables},\n        )\n\n    def _validate_input(self, inner_input: Any) -> dict:\n        if not isinstance(inner_input, dict):\n            if len(self.input_variables) == 1:\n                var_name = self.input_variables[0]\n                inner_input = {var_name: inner_input}\n\n            else:\n                msg = (\n                    f\"Expected mapping type as input to {self.__class__.__name__}. \"\n                    f\"Received {type(inner_input)}.\"\n                )\n                raise TypeError(\n                    create_message(\n                        message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT\n                    )\n                )\n        missing = set(self.input_variables).difference(inner_input)\n        if missing:\n            msg = (\n                f\"Input to {self.__class__.__name__} is missing variables {missing}. \"\n                f\" Expected: {self.input_variables}\"\n                f\" Received: {list(inner_input.keys())}\"\n            )\n            example_key = missing.pop()\n            msg += (\n                f\"\\nNote: if you intended {{{example_key}}} to be part of the string\"\n                \" and not a variable, please escape it with double curly braces like: \"\n                f\"'{{{{{example_key}}}}}'.\"\n            )\n            raise KeyError(\n                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n            )\n        return inner_input\n\n    def _format_prompt_with_error_handling(self, inner_input: dict) -> PromptValue:\n        _inner_input = self._validate_input(inner_input)\n        return self.format_prompt(**_inner_input)\n\n    async def _aformat_prompt_with_error_handling(\n        self, inner_input: dict\n    ) -> PromptValue:\n        _inner_input = self._validate_input(inner_input)\n        return await self.aformat_prompt(**_inner_input)\n\n    def invoke(\n        self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> PromptValue:\n        \"\"\"Invoke the prompt.\n\n        Args:\n            input: Dict, input to the prompt.\n            config: RunnableConfig, configuration for the prompt.\n\n        Returns:\n            PromptValue: The output of the prompt.\n        \"\"\"\n        config = ensure_config(config)\n        if self.metadata:\n            config[\"metadata\"] = {**config[\"metadata\"], **self.metadata}\n        if self.tags:\n            config[\"tags\"] = config[\"tags\"] + self.tags\n        return self._call_with_config(\n            self._format_prompt_with_error_handling,\n            input,\n            config,\n            run_type=\"prompt\",\n            serialized=self._serialized,\n        )\n\n    async def ainvoke(\n        self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> PromptValue:\n        \"\"\"Async invoke the prompt.\n\n        Args:\n            input: Dict, input to the prompt.\n            config: RunnableConfig, configuration for the prompt.\n\n        Returns:\n            PromptValue: The output of the prompt.\n        \"\"\"\n        config = ensure_config(config)\n        if self.metadata:\n            config[\"metadata\"].update(self.metadata)\n        if self.tags:\n            config[\"tags\"].extend(self.tags)\n        return await self._acall_with_config(\n            self._aformat_prompt_with_error_handling,\n            input,\n            config,\n            run_type=\"prompt\",\n            serialized=self._serialized,\n        )\n\n    @abstractmethod\n    def format_prompt(self, **kwargs: Any) -> PromptValue:\n        \"\"\"Create Prompt Value.\n\n        Args:\n            kwargs: Any arguments to be passed to the prompt template.\n\n        Returns:\n            PromptValue: The output of the prompt.\n        \"\"\"\n\n    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:\n        \"\"\"Async create Prompt Value.\n\n        Args:\n            kwargs: Any arguments to be passed to the prompt template.\n\n        Returns:\n            PromptValue: The output of the prompt.\n        \"\"\"\n        return self.format_prompt(**kwargs)\n\n    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:\n        \"\"\"Return a partial of the prompt template.\n\n        Args:\n            kwargs: Union[str, Callable[[], str]], partial variables to set.\n\n        Returns:\n            BasePromptTemplate: A partial of the prompt template.\n        \"\"\"\n        prompt_dict = self.__dict__.copy()\n        prompt_dict[\"input_variables\"] = list(\n            set(self.input_variables).difference(kwargs)\n        )\n        prompt_dict[\"partial_variables\"] = {**self.partial_variables, **kwargs}\n        return type(self)(**prompt_dict)\n\n    def _merge_partial_and_user_variables(self, **kwargs: Any) -> dict[str, Any]:\n        # Get partial params:\n        partial_kwargs = {\n            k: v if not callable(v) else v() for k, v in self.partial_variables.items()\n        }\n        return {**partial_kwargs, **kwargs}\n\n    @abstractmethod\n    def format(self, **kwargs: Any) -> FormatOutputType:\n        \"\"\"Format the prompt with the inputs.\n\n        Args:\n            kwargs: Any arguments to be passed to the prompt template.\n\n        Returns:\n            A formatted string.\n\n        Example:\n\n        .. code-block:: python\n\n            prompt.format(variable1=\"foo\")\n        \"\"\"\n\n    async def aformat(self, **kwargs: Any) -> FormatOutputType:\n        \"\"\"Async format the prompt with the inputs.\n\n        Args:\n            kwargs: Any arguments to be passed to the prompt template.\n\n        Returns:\n            A formatted string.\n\n        Example:\n\n        .. code-block:: python\n\n            await prompt.aformat(variable1=\"foo\")\n        \"\"\"\n        return self.format(**kwargs)\n\n    @property\n    def _prompt_type(self) -> str:\n        \"\"\"Return the prompt type key.\"\"\"\n        raise NotImplementedError\n\n    def dict(self, **kwargs: Any) -> dict:\n        \"\"\"Return dictionary representation of prompt.\n\n        Args:\n            kwargs: Any additional arguments to pass to the dictionary.\n\n        Returns:\n            Dict: Dictionary representation of the prompt.\n\n        Raises:\n            NotImplementedError: If the prompt type is not implemented.\n        \"\"\"\n        prompt_dict = super().model_dump(**kwargs)\n        with contextlib.suppress(NotImplementedError):\n            prompt_dict[\"_type\"] = self._prompt_type\n        return prompt_dict\n\n    def save(self, file_path: Union[Path, str]) -> None:\n        \"\"\"Save the prompt.\n\n        Args:\n            file_path: Path to directory to save prompt to.\n\n        Raises:\n            ValueError: If the prompt has partial variables.\n            ValueError: If the file path is not json or yaml.\n            NotImplementedError: If the prompt type is not implemented.\n\n        Example:\n        .. code-block:: python\n\n            prompt.save(file_path=\"path/prompt.yaml\")\n        \"\"\"\n        if self.partial_variables:\n            msg = \"Cannot save prompt with partial variables.\"\n            raise ValueError(msg)\n\n        # Fetch dictionary to save\n        prompt_dict = self.dict()\n        if \"_type\" not in prompt_dict:\n            msg = f\"Prompt {self} does not support saving.\"\n            raise NotImplementedError(msg)\n\n        # Convert file to Path object.\n        save_path = Path(file_path) if isinstance(file_path, str) else file_path\n\n        directory_path = save_path.parent\n        directory_path.mkdir(parents=True, exist_ok=True)\n\n        if save_path.suffix == \".json\":\n            with open(file_path, \"w\") as f:\n                json.dump(prompt_dict, f, indent=4)\n        elif save_path.suffix.endswith((\".yaml\", \".yml\")):\n            with open(file_path, \"w\") as f:\n                yaml.dump(prompt_dict, f, default_flow_style=False)\n        else:\n            msg = f\"{save_path} must be json or yaml\"\n            raise ValueError(msg)\n\n\ndef _get_document_info(doc: Document, prompt: BasePromptTemplate[str]) -> dict:\n    base_info = {\"page_content\": doc.page_content, **doc.metadata}\n    missing_metadata = set(prompt.input_variables).difference(base_info)\n    if len(missing_metadata) > 0:\n        required_metadata = [\n            iv for iv in prompt.input_variables if iv != \"page_content\"\n        ]\n        msg = (\n            f\"Document prompt requires documents to have metadata variables: \"\n            f\"{required_metadata}. Received document with missing metadata: \"\n            f\"{list(missing_metadata)}.\"\n        )\n        raise ValueError(\n            create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n        )\n    return {k: base_info[k] for k in prompt.input_variables}\n\n\ndef format_document(doc: Document, prompt: BasePromptTemplate[str]) -> str:\n    \"\"\"Format a document into a string based on a prompt template.\n\n    First, this pulls information from the document from two sources:\n\n    1. page_content:\n        This takes the information from the `document.page_content`\n        and assigns it to a variable named `page_content`.\n    2. metadata:\n        This takes information from `document.metadata` and assigns\n        it to variables of the same name.\n\n    Those variables are then passed into the `prompt` to produce a formatted string.\n\n    Args:\n        doc: Document, the page_content and metadata will be used to create\n            the final string.\n        prompt: BasePromptTemplate, will be used to format the page_content\n            and metadata into the final string.\n\n    Returns:\n        string of the document formatted.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_core.documents import Document\n            from langchain_core.prompts import PromptTemplate\n\n            doc = Document(page_content=\"This is a joke\", metadata={\"page\": \"1\"})\n            prompt = PromptTemplate.from_template(\"Page {page}: {page_content}\")\n            format_document(doc, prompt)\n            >>> \"Page 1: This is a joke\"\n    \"\"\"\n    return prompt.format(**_get_document_info(doc, prompt))\n\n\nasync def aformat_document(doc: Document, prompt: BasePromptTemplate[str]) -> str:\n    \"\"\"Async format a document into a string based on a prompt template.\n\n    First, this pulls information from the document from two sources:\n\n    1. page_content:\n        This takes the information from the `document.page_content`\n        and assigns it to a variable named `page_content`.\n    2. metadata:\n        This takes information from `document.metadata` and assigns\n        it to variables of the same name.\n\n    Those variables are then passed into the `prompt` to produce a formatted string.\n\n    Args:\n        doc: Document, the page_content and metadata will be used to create\n            the final string.\n        prompt: BasePromptTemplate, will be used to format the page_content\n            and metadata into the final string.\n\n    Returns:\n        string of the document formatted.\n    \"\"\"\n    return await prompt.aformat(**_get_document_info(doc, prompt))\n",
        "patch": "@@ -368,16 +368,16 @@ def save(self, file_path: Union[Path, str]) -> None:\n             raise NotImplementedError(msg)\n \n         # Convert file to Path object.\n-        save_path = Path(file_path) if isinstance(file_path, str) else file_path\n+        save_path = Path(file_path)\n \n         directory_path = save_path.parent\n         directory_path.mkdir(parents=True, exist_ok=True)\n \n         if save_path.suffix == \".json\":\n-            with open(file_path, \"w\") as f:\n+            with save_path.open(\"w\") as f:\n                 json.dump(prompt_dict, f, indent=4)\n         elif save_path.suffix.endswith((\".yaml\", \".yml\")):\n-            with open(file_path, \"w\") as f:\n+            with save_path.open(\"w\") as f:\n                 yaml.dump(prompt_dict, f, default_flow_style=False)\n         else:\n             msg = f\"{save_path} must be json or yaml\""
      },
      {
        "filename": "libs/core/langchain_core/prompts/chat.py",
        "content_before": "\"\"\"Chat prompt template.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import (\n    TYPE_CHECKING,\n    Annotated,\n    Any,\n    Optional,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\n\nfrom pydantic import (\n    Field,\n    PositiveInt,\n    SkipValidation,\n    model_validator,\n)\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.load import Serializable\nfrom langchain_core.messages import (\n    AIMessage,\n    AnyMessage,\n    BaseMessage,\n    ChatMessage,\n    HumanMessage,\n    SystemMessage,\n    convert_to_messages,\n)\nfrom langchain_core.messages.base import get_msg_title_repr\nfrom langchain_core.prompt_values import ChatPromptValue, ImageURL, PromptValue\nfrom langchain_core.prompts.base import BasePromptTemplate\nfrom langchain_core.prompts.image import ImagePromptTemplate\nfrom langchain_core.prompts.prompt import PromptTemplate\nfrom langchain_core.prompts.string import (\n    PromptTemplateFormat,\n    StringPromptTemplate,\n    get_template_variables,\n)\nfrom langchain_core.utils import get_colored_text\nfrom langchain_core.utils.interactive_env import is_interactive_env\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n    from pathlib import Path\n\n\nclass BaseMessagePromptTemplate(Serializable, ABC):\n    \"\"\"Base class for message prompt templates.\"\"\"\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether or not the class is serializable.\n        Returns: True.\n        \"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    @abstractmethod\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs. Should return a list of BaseMessages.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format messages from kwargs.\n        Should return a list of BaseMessages.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return self.format_messages(**kwargs)\n\n    @property\n    @abstractmethod\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variables.\n        \"\"\"\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        raise NotImplementedError\n\n    def pretty_print(self) -> None:\n        \"\"\"Print a human-readable representation.\"\"\"\n        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201\n\n    def __add__(self, other: Any) -> ChatPromptTemplate:\n        \"\"\"Combine two prompt templates.\n\n        Args:\n            other: Another prompt template.\n\n        Returns:\n            Combined prompt template.\n        \"\"\"\n        prompt = ChatPromptTemplate(messages=[self])  # type: ignore[call-arg]\n        return prompt + other\n\n\nclass MessagesPlaceholder(BaseMessagePromptTemplate):\n    \"\"\"Prompt template that assumes variable is already list of messages.\n\n    A placeholder which can be used to pass in a list of messages.\n\n    Direct usage:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import MessagesPlaceholder\n\n            prompt = MessagesPlaceholder(\"history\")\n            prompt.format_messages() # raises KeyError\n\n            prompt = MessagesPlaceholder(\"history\", optional=True)\n            prompt.format_messages() # returns empty list []\n\n            prompt.format_messages(\n                history=[\n                    (\"system\", \"You are an AI assistant.\"),\n                    (\"human\", \"Hello!\"),\n                ]\n            )\n            # -> [\n            #     SystemMessage(content=\"You are an AI assistant.\"),\n            #     HumanMessage(content=\"Hello!\"),\n            # ]\n\n    Building a prompt with chat history:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n            prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", \"You are a helpful assistant.\"),\n                    MessagesPlaceholder(\"history\"),\n                    (\"human\", \"{question}\")\n                ]\n            )\n            prompt.invoke(\n               {\n                   \"history\": [(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n                   \"question\": \"now multiply that by 4\"\n               }\n            )\n            # -> ChatPromptValue(messages=[\n            #     SystemMessage(content=\"You are a helpful assistant.\"),\n            #     HumanMessage(content=\"what's 5 + 2\"),\n            #     AIMessage(content=\"5 + 2 is 7\"),\n            #     HumanMessage(content=\"now multiply that by 4\"),\n            # ])\n\n    Limiting the number of messages:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import MessagesPlaceholder\n\n            prompt = MessagesPlaceholder(\"history\", n_messages=1)\n\n            prompt.format_messages(\n                history=[\n                    (\"system\", \"You are an AI assistant.\"),\n                    (\"human\", \"Hello!\"),\n                ]\n            )\n            # -> [\n            #     HumanMessage(content=\"Hello!\"),\n            # ]\n    \"\"\"\n\n    variable_name: str\n    \"\"\"Name of variable to use as messages.\"\"\"\n\n    optional: bool = False\n    \"\"\"If True format_messages can be called with no arguments and will return an empty\n        list. If False then a named argument with name `variable_name` must be passed\n        in, even if the value is an empty list.\"\"\"\n\n    n_messages: Optional[PositiveInt] = None\n    \"\"\"Maximum number of messages to include. If None, then will include all.\n    Defaults to None.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    def __init__(\n        self, variable_name: str, *, optional: bool = False, **kwargs: Any\n    ) -> None:\n        # mypy can't detect the init which is defined in the parent class\n        # b/c these are BaseModel classes.\n        super().__init__(  # type: ignore\n            variable_name=variable_name, optional=optional, **kwargs\n        )\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessage.\n\n        Raises:\n            ValueError: If variable is not a list of messages.\n        \"\"\"\n        value = (\n            kwargs.get(self.variable_name, [])\n            if self.optional\n            else kwargs[self.variable_name]\n        )\n        if not isinstance(value, list):\n            msg = (\n                f\"variable {self.variable_name} should be a list of base messages, \"\n                f\"got {value} of type {type(value)}\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n        value = convert_to_messages(value)\n        if self.n_messages:\n            value = value[-self.n_messages :]\n        return value\n\n    @property\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variable names.\n        \"\"\"\n        return [self.variable_name] if not self.optional else []\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        var = \"{\" + self.variable_name + \"}\"\n        if html:\n            title = get_msg_title_repr(\"Messages Placeholder\", bold=True)\n            var = get_colored_text(var, \"yellow\")\n        else:\n            title = get_msg_title_repr(\"Messages Placeholder\")\n        return f\"{title}\\n\\n{var}\"\n\n\nMessagePromptTemplateT = TypeVar(\n    \"MessagePromptTemplateT\", bound=\"BaseStringMessagePromptTemplate\"\n)\n\"\"\"Type variable for message prompt templates.\"\"\"\n\n\nclass BaseStringMessagePromptTemplate(BaseMessagePromptTemplate, ABC):\n    \"\"\"Base class for message prompt templates that use a string prompt template.\"\"\"\n\n    prompt: StringPromptTemplate\n    \"\"\"String prompt template.\"\"\"\n    additional_kwargs: dict = Field(default_factory=dict)\n    \"\"\"Additional keyword arguments to pass to the prompt template.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    @classmethod\n    def from_template(\n        cls: type[MessagePromptTemplateT],\n        template: str,\n        template_format: PromptTemplateFormat = \"f-string\",\n        partial_variables: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> MessagePromptTemplateT:\n        \"\"\"Create a class from a string template.\n\n        Args:\n            template: a template.\n            template_format: format of the template. Defaults to \"f-string\".\n            partial_variables: A dictionary of variables that can be used to partially\n                               fill in the template. For example, if the template is\n                              `\"{variable1} {variable2}\"`, and `partial_variables` is\n                              `{\"variable1\": \"foo\"}`, then the final prompt will be\n                              `\"foo {variable2}\"`.\n                              Defaults to None.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        prompt = PromptTemplate.from_template(\n            template,\n            template_format=template_format,\n            partial_variables=partial_variables,\n        )\n        return cls(prompt=prompt, **kwargs)\n\n    @classmethod\n    def from_template_file(\n        cls: type[MessagePromptTemplateT],\n        template_file: Union[str, Path],\n        input_variables: list[str],\n        **kwargs: Any,\n    ) -> MessagePromptTemplateT:\n        \"\"\"Create a class from a template file.\n\n        Args:\n            template_file: path to a template file. String or Path.\n            input_variables: list of input variables.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        prompt = PromptTemplate.from_file(template_file, input_variables)\n        return cls(prompt=prompt, **kwargs)\n\n    @abstractmethod\n    def format(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n\n    async def aformat(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Async format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        return self.format(**kwargs)\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [self.format(**kwargs)]\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [await self.aformat(**kwargs)]\n\n    @property\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variable names.\n        \"\"\"\n        return self.prompt.input_variables\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        # TODO: Handle partials\n        title = self.__class__.__name__.replace(\"MessagePromptTemplate\", \" Message\")\n        title = get_msg_title_repr(title, bold=html)\n        return f\"{title}\\n\\n{self.prompt.pretty_repr(html=html)}\"\n\n\nclass ChatMessagePromptTemplate(BaseStringMessagePromptTemplate):\n    \"\"\"Chat message prompt template.\"\"\"\n\n    role: str\n    \"\"\"Role of the message.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    def format(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        text = self.prompt.format(**kwargs)\n        return ChatMessage(\n            content=text, role=self.role, additional_kwargs=self.additional_kwargs\n        )\n\n    async def aformat(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Async format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        text = await self.prompt.aformat(**kwargs)\n        return ChatMessage(\n            content=text, role=self.role, additional_kwargs=self.additional_kwargs\n        )\n\n\n_StringImageMessagePromptTemplateT = TypeVar(\n    \"_StringImageMessagePromptTemplateT\", bound=\"_StringImageMessagePromptTemplate\"\n)\n\n\nclass _TextTemplateParam(TypedDict, total=False):\n    text: Union[str, dict]\n\n\nclass _ImageTemplateParam(TypedDict, total=False):\n    image_url: Union[str, dict]\n\n\nclass _StringImageMessagePromptTemplate(BaseMessagePromptTemplate):\n    \"\"\"Human message prompt template. This is a message sent from the user.\"\"\"\n\n    prompt: Union[\n        StringPromptTemplate, list[Union[StringPromptTemplate, ImagePromptTemplate]]\n    ]\n    \"\"\"Prompt template.\"\"\"\n    additional_kwargs: dict = Field(default_factory=dict)\n    \"\"\"Additional keyword arguments to pass to the prompt template.\"\"\"\n\n    _msg_class: type[BaseMessage]\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    @classmethod\n    def from_template(\n        cls: type[_StringImageMessagePromptTemplateT],\n        template: Union[str, list[Union[str, _TextTemplateParam, _ImageTemplateParam]]],\n        template_format: PromptTemplateFormat = \"f-string\",\n        *,\n        partial_variables: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> _StringImageMessagePromptTemplateT:\n        \"\"\"Create a class from a string template.\n\n        Args:\n            template: a template.\n            template_format: format of the template.\n                Options are: 'f-string', 'mustache', 'jinja2'. Defaults to \"f-string\".\n            partial_variables: A dictionary of variables that can be used too partially.\n                Defaults to None.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n\n        Raises:\n            ValueError: If the template is not a string or list of strings.\n        \"\"\"\n        if isinstance(template, str):\n            prompt: Union[StringPromptTemplate, list] = PromptTemplate.from_template(\n                template,\n                template_format=template_format,\n                partial_variables=partial_variables,\n            )\n            return cls(prompt=prompt, **kwargs)\n        elif isinstance(template, list):\n            if (partial_variables is not None) and len(partial_variables) > 0:\n                msg = \"Partial variables are not supported for list of templates.\"\n                raise ValueError(msg)\n            prompt = []\n            for tmpl in template:\n                if isinstance(tmpl, str) or isinstance(tmpl, dict) and \"text\" in tmpl:\n                    if isinstance(tmpl, str):\n                        text: str = tmpl\n                    else:\n                        text = cast(_TextTemplateParam, tmpl)[\"text\"]  # type: ignore[assignment]\n                    prompt.append(\n                        PromptTemplate.from_template(\n                            text, template_format=template_format\n                        )\n                    )\n                elif isinstance(tmpl, dict) and \"image_url\" in tmpl:\n                    img_template = cast(_ImageTemplateParam, tmpl)[\"image_url\"]\n                    input_variables = []\n                    if isinstance(img_template, str):\n                        vars = get_template_variables(img_template, template_format)\n                        if vars:\n                            if len(vars) > 1:\n                                msg = (\n                                    \"Only one format variable allowed per image\"\n                                    f\" template.\\nGot: {vars}\"\n                                    f\"\\nFrom: {tmpl}\"\n                                )\n                                raise ValueError(msg)\n                            input_variables = [vars[0]]\n                        img_template = {\"url\": img_template}\n                        img_template_obj = ImagePromptTemplate(\n                            input_variables=input_variables,\n                            template=img_template,\n                            template_format=template_format,\n                        )\n                    elif isinstance(img_template, dict):\n                        img_template = dict(img_template)\n                        for key in [\"url\", \"path\", \"detail\"]:\n                            if key in img_template:\n                                input_variables.extend(\n                                    get_template_variables(\n                                        img_template[key], template_format\n                                    )\n                                )\n                        img_template_obj = ImagePromptTemplate(\n                            input_variables=input_variables,\n                            template=img_template,\n                            template_format=template_format,\n                        )\n                    else:\n                        msg = f\"Invalid image template: {tmpl}\"\n                        raise ValueError(msg)\n                    prompt.append(img_template_obj)\n                else:\n                    msg = f\"Invalid template: {tmpl}\"\n                    raise ValueError(msg)\n            return cls(prompt=prompt, **kwargs)\n        else:\n            msg = f\"Invalid template: {template}\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @classmethod\n    def from_template_file(\n        cls: type[_StringImageMessagePromptTemplateT],\n        template_file: Union[str, Path],\n        input_variables: list[str],\n        **kwargs: Any,\n    ) -> _StringImageMessagePromptTemplateT:\n        \"\"\"Create a class from a template file.\n\n        Args:\n            template_file: path to a template file. String or Path.\n            input_variables: list of input variables.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        with open(str(template_file)) as f:\n            template = f.read()\n        return cls.from_template(template, input_variables=input_variables, **kwargs)\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [self.format(**kwargs)]\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [await self.aformat(**kwargs)]\n\n    @property\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variable names.\n        \"\"\"\n        prompts = self.prompt if isinstance(self.prompt, list) else [self.prompt]\n        input_variables = [iv for prompt in prompts for iv in prompt.input_variables]\n        return input_variables\n\n    def format(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        if isinstance(self.prompt, StringPromptTemplate):\n            text = self.prompt.format(**kwargs)\n            return self._msg_class(\n                content=text, additional_kwargs=self.additional_kwargs\n            )\n        else:\n            content: list = []\n            for prompt in self.prompt:\n                inputs = {var: kwargs[var] for var in prompt.input_variables}\n                if isinstance(prompt, StringPromptTemplate):\n                    formatted: Union[str, ImageURL] = prompt.format(**inputs)\n                    content.append({\"type\": \"text\", \"text\": formatted})\n                elif isinstance(prompt, ImagePromptTemplate):\n                    formatted = prompt.format(**inputs)\n                    content.append({\"type\": \"image_url\", \"image_url\": formatted})\n            return self._msg_class(\n                content=content, additional_kwargs=self.additional_kwargs\n            )\n\n    async def aformat(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Async format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        if isinstance(self.prompt, StringPromptTemplate):\n            text = await self.prompt.aformat(**kwargs)\n            return self._msg_class(\n                content=text, additional_kwargs=self.additional_kwargs\n            )\n        else:\n            content: list = []\n            for prompt in self.prompt:\n                inputs = {var: kwargs[var] for var in prompt.input_variables}\n                if isinstance(prompt, StringPromptTemplate):\n                    formatted: Union[str, ImageURL] = await prompt.aformat(**inputs)\n                    content.append({\"type\": \"text\", \"text\": formatted})\n                elif isinstance(prompt, ImagePromptTemplate):\n                    formatted = await prompt.aformat(**inputs)\n                    content.append({\"type\": \"image_url\", \"image_url\": formatted})\n            return self._msg_class(\n                content=content, additional_kwargs=self.additional_kwargs\n            )\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        # TODO: Handle partials\n        title = self.__class__.__name__.replace(\"MessagePromptTemplate\", \" Message\")\n        title = get_msg_title_repr(title, bold=html)\n        prompts = self.prompt if isinstance(self.prompt, list) else [self.prompt]\n        prompt_reprs = \"\\n\\n\".join(prompt.pretty_repr(html=html) for prompt in prompts)\n        return f\"{title}\\n\\n{prompt_reprs}\"\n\n\nclass HumanMessagePromptTemplate(_StringImageMessagePromptTemplate):\n    \"\"\"Human message prompt template. This is a message sent from the user.\"\"\"\n\n    _msg_class: type[BaseMessage] = HumanMessage\n\n\nclass AIMessagePromptTemplate(_StringImageMessagePromptTemplate):\n    \"\"\"AI message prompt template. This is a message sent from the AI.\"\"\"\n\n    _msg_class: type[BaseMessage] = AIMessage\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n\nclass SystemMessagePromptTemplate(_StringImageMessagePromptTemplate):\n    \"\"\"System message prompt template.\n    This is a message that is not sent to the user.\n    \"\"\"\n\n    _msg_class: type[BaseMessage] = SystemMessage\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n\nclass BaseChatPromptTemplate(BasePromptTemplate, ABC):\n    \"\"\"Base class for chat prompt templates.\"\"\"\n\n    @property\n    def lc_attributes(self) -> dict:\n        \"\"\"Return a list of attribute names that should be included in the\n        serialized kwargs. These attributes must be accepted by the\n        constructor.\n        \"\"\"\n        return {\"input_variables\": self.input_variables}\n\n    def format(self, **kwargs: Any) -> str:\n        \"\"\"Format the chat template into a string.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            formatted string.\n        \"\"\"\n        return self.format_prompt(**kwargs).to_string()\n\n    async def aformat(self, **kwargs: Any) -> str:\n        \"\"\"Async format the chat template into a string.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            formatted string.\n        \"\"\"\n        return (await self.aformat_prompt(**kwargs)).to_string()\n\n    def format_prompt(self, **kwargs: Any) -> PromptValue:\n        \"\"\"Format prompt. Should return a PromptValue.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            PromptValue.\n        \"\"\"\n        messages = self.format_messages(**kwargs)\n        return ChatPromptValue(messages=messages)\n\n    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:\n        \"\"\"Async format prompt. Should return a PromptValue.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            PromptValue.\n        \"\"\"\n        messages = await self.aformat_messages(**kwargs)\n        return ChatPromptValue(messages=messages)\n\n    @abstractmethod\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format kwargs into a list of messages.\"\"\"\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format kwargs into a list of messages.\"\"\"\n        return self.format_messages(**kwargs)\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        raise NotImplementedError\n\n    def pretty_print(self) -> None:\n        \"\"\"Print a human-readable representation.\"\"\"\n        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201\n\n\nMessageLike = Union[BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate]\n\nMessageLikeRepresentation = Union[\n    MessageLike,\n    tuple[\n        Union[str, type],\n        Union[str, list[dict], list[object]],\n    ],\n    str,\n    dict,\n]\n\n\nclass ChatPromptTemplate(BaseChatPromptTemplate):\n    \"\"\"Prompt template for chat models.\n\n    Use to create flexible templated prompts for chat models.\n\n    Examples:\n\n        .. versionchanged:: 0.2.24\n\n            You can pass any Message-like formats supported by\n            ``ChatPromptTemplate.from_messages()`` directly to ``ChatPromptTemplate()``\n            init.\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n\n            template = ChatPromptTemplate([\n                (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n                (\"human\", \"Hello, how are you doing?\"),\n                (\"ai\", \"I'm doing well, thanks!\"),\n                (\"human\", \"{user_input}\"),\n            ])\n\n            prompt_value = template.invoke(\n                {\n                    \"name\": \"Bob\",\n                    \"user_input\": \"What is your name?\"\n                }\n            )\n            # Output:\n            # ChatPromptValue(\n            #    messages=[\n            #        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n            #        HumanMessage(content='Hello, how are you doing?'),\n            #        AIMessage(content=\"I'm doing well, thanks!\"),\n            #        HumanMessage(content='What is your name?')\n            #    ]\n            #)\n\n    Messages Placeholder:\n\n        .. code-block:: python\n\n            # In addition to Human/AI/Tool/Function messages,\n            # you can initialize the template with a MessagesPlaceholder\n            # either using the class directly or with the shorthand tuple syntax:\n\n            template = ChatPromptTemplate([\n                (\"system\", \"You are a helpful AI bot.\"),\n                # Means the template will receive an optional list of messages under\n                # the \"conversation\" key\n                (\"placeholder\", \"{conversation}\")\n                # Equivalently:\n                # MessagesPlaceholder(variable_name=\"conversation\", optional=True)\n            ])\n\n            prompt_value = template.invoke(\n                {\n                    \"conversation\": [\n                        (\"human\", \"Hi!\"),\n                        (\"ai\", \"How can I assist you today?\"),\n                        (\"human\", \"Can you make me an ice cream sundae?\"),\n                        (\"ai\", \"No.\")\n                    ]\n                }\n            )\n\n            # Output:\n            # ChatPromptValue(\n            #    messages=[\n            #        SystemMessage(content='You are a helpful AI bot.'),\n            #        HumanMessage(content='Hi!'),\n            #        AIMessage(content='How can I assist you today?'),\n            #        HumanMessage(content='Can you make me an ice cream sundae?'),\n            #        AIMessage(content='No.'),\n            #    ]\n            #)\n\n    Single-variable template:\n\n        If your prompt has only a single input variable (i.e., 1 instance of \"{variable_nams}\"),\n        and you invoke the template with a non-dict object, the prompt template will\n        inject the provided argument into that variable location.\n\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n\n            template = ChatPromptTemplate([\n                (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\n                (\"human\", \"{user_input}\"),\n            ])\n\n            prompt_value = template.invoke(\"Hello, there!\")\n            # Equivalent to\n            # prompt_value = template.invoke({\"user_input\": \"Hello, there!\"})\n\n            # Output:\n            #  ChatPromptValue(\n            #     messages=[\n            #         SystemMessage(content='You are a helpful AI bot. Your name is Carl.'),\n            #         HumanMessage(content='Hello, there!'),\n            #     ]\n            # )\n\n    \"\"\"  # noqa: E501\n\n    messages: Annotated[list[MessageLike], SkipValidation()]\n    \"\"\"List of messages consisting of either message prompt templates or messages.\"\"\"\n    validate_template: bool = False\n    \"\"\"Whether or not to try validating the template.\"\"\"\n\n    def __init__(\n        self,\n        messages: Sequence[MessageLikeRepresentation],\n        *,\n        template_format: PromptTemplateFormat = \"f-string\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a chat prompt template from a variety of message formats.\n\n        Args:\n            messages: sequence of message representations.\n                  A message can be represented using the following formats:\n                  (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\n                  (message type, template); e.g., (\"human\", \"{user_input}\"),\n                  (4) 2-tuple of (message class, template), (5) a string which is\n                  shorthand for (\"human\", template); e.g., \"{user_input}\".\n            template_format: format of the template. Defaults to \"f-string\".\n            input_variables: A list of the names of the variables whose values are\n                required as inputs to the prompt.\n            optional_variables: A list of the names of the variables for placeholder\n            or MessagePlaceholder that are optional. These variables are auto inferred\n            from the prompt and user need not provide them.\n            partial_variables: A dictionary of the partial variables the prompt\n                template carries. Partial variables populate the template so that you\n                don't need to pass them in every time you call the prompt.\n            validate_template: Whether to validate the template.\n            input_types: A dictionary of the types of the variables the prompt template\n                expects. If not provided, all variables are assumed to be strings.\n\n        Returns:\n            A chat prompt template.\n\n        Examples:\n            Instantiation from a list of message templates:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate([\n                    (\"human\", \"Hello, how are you?\"),\n                    (\"ai\", \"I'm doing well, thanks!\"),\n                    (\"human\", \"That's good to hear.\"),\n                ])\n\n            Instantiation from mixed message formats:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate([\n                    SystemMessage(content=\"hello\"),\n                    (\"human\", \"Hello, how are you?\"),\n                ])\n\n        \"\"\"\n        _messages = [\n            _convert_to_message(message, template_format) for message in messages\n        ]\n\n        # Automatically infer input variables from messages\n        input_vars: set[str] = set()\n        optional_variables: set[str] = set()\n        partial_vars: dict[str, Any] = {}\n        for _message in _messages:\n            if isinstance(_message, MessagesPlaceholder) and _message.optional:\n                partial_vars[_message.variable_name] = []\n                optional_variables.add(_message.variable_name)\n            elif isinstance(\n                _message, (BaseChatPromptTemplate, BaseMessagePromptTemplate)\n            ):\n                input_vars.update(_message.input_variables)\n\n        kwargs = {\n            \"input_variables\": sorted(input_vars),\n            \"optional_variables\": sorted(optional_variables),\n            \"partial_variables\": partial_vars,\n            **kwargs,\n        }\n        cast(type[ChatPromptTemplate], super()).__init__(messages=_messages, **kwargs)\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    def __add__(self, other: Any) -> ChatPromptTemplate:\n        \"\"\"Combine two prompt templates.\n\n        Args:\n            other: Another prompt template.\n\n        Returns:\n            Combined prompt template.\n        \"\"\"\n        # Allow for easy combining\n        if isinstance(other, ChatPromptTemplate):\n            return ChatPromptTemplate(messages=self.messages + other.messages)  # type: ignore[call-arg]\n        elif isinstance(\n            other, (BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate)\n        ):\n            return ChatPromptTemplate(messages=self.messages + [other])  # type: ignore[call-arg]\n        elif isinstance(other, (list, tuple)):\n            _other = ChatPromptTemplate.from_messages(other)\n            return ChatPromptTemplate(messages=self.messages + _other.messages)  # type: ignore[call-arg]\n        elif isinstance(other, str):\n            prompt = HumanMessagePromptTemplate.from_template(other)\n            return ChatPromptTemplate(messages=self.messages + [prompt])  # type: ignore[call-arg]\n        else:\n            msg = f\"Unsupported operand type for +: {type(other)}\"\n            raise NotImplementedError(msg)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_input_variables(cls, values: dict) -> Any:\n        \"\"\"Validate input variables.\n\n        If input_variables is not set, it will be set to the union of\n        all input variables in the messages.\n\n        Args:\n            values: values to validate.\n\n        Returns:\n            Validated values.\n\n        Raises:\n            ValueError: If input variables do not match.\n        \"\"\"\n        messages = values[\"messages\"]\n        input_vars = set()\n        optional_variables = set()\n        input_types: dict[str, Any] = values.get(\"input_types\", {})\n        for message in messages:\n            if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):\n                input_vars.update(message.input_variables)\n            if isinstance(message, MessagesPlaceholder):\n                if \"partial_variables\" not in values:\n                    values[\"partial_variables\"] = {}\n                if (\n                    message.optional\n                    and message.variable_name not in values[\"partial_variables\"]\n                ):\n                    values[\"partial_variables\"][message.variable_name] = []\n                    optional_variables.add(message.variable_name)\n                if message.variable_name not in input_types:\n                    input_types[message.variable_name] = list[AnyMessage]\n        if \"partial_variables\" in values:\n            input_vars = input_vars - set(values[\"partial_variables\"])\n        if optional_variables:\n            input_vars = input_vars - optional_variables\n        if \"input_variables\" in values and values.get(\"validate_template\"):\n            if input_vars != set(values[\"input_variables\"]):\n                msg = (\n                    \"Got mismatched input_variables. \"\n                    f\"Expected: {input_vars}. \"\n                    f\"Got: {values['input_variables']}\"\n                )\n                raise ValueError(msg)\n        else:\n            values[\"input_variables\"] = sorted(input_vars)\n        if optional_variables:\n            values[\"optional_variables\"] = sorted(optional_variables)\n        values[\"input_types\"] = input_types\n        return values\n\n    @classmethod\n    def from_template(cls, template: str, **kwargs: Any) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a template string.\n\n        Creates a chat template consisting of a single message assumed to be from\n        the human.\n\n        Args:\n            template: template string\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        prompt_template = PromptTemplate.from_template(template, **kwargs)\n        message = HumanMessagePromptTemplate(prompt=prompt_template)\n        return cls.from_messages([message])\n\n    @classmethod\n    @deprecated(\"0.0.1\", alternative=\"from_messages classmethod\", pending=True)\n    def from_role_strings(\n        cls, string_messages: list[tuple[str, str]]\n    ) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a list of (role, template) tuples.\n\n        Args:\n            string_messages: list of (role, template) tuples.\n\n        Returns:\n            a chat prompt template.\n        \"\"\"\n        return cls(  # type: ignore[call-arg]\n            messages=[\n                ChatMessagePromptTemplate.from_template(template, role=role)\n                for role, template in string_messages\n            ]\n        )\n\n    @classmethod\n    @deprecated(\"0.0.1\", alternative=\"from_messages classmethod\", pending=True)\n    def from_strings(\n        cls, string_messages: list[tuple[type[BaseMessagePromptTemplate], str]]\n    ) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a list of (role class, template) tuples.\n\n        Args:\n            string_messages: list of (role class, template) tuples.\n\n        Returns:\n            a chat prompt template.\n        \"\"\"\n        return cls.from_messages(string_messages)\n\n    @classmethod\n    def from_messages(\n        cls,\n        messages: Sequence[MessageLikeRepresentation],\n        template_format: PromptTemplateFormat = \"f-string\",\n    ) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a variety of message formats.\n\n        Examples:\n            Instantiation from a list of message templates:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate.from_messages([\n                    (\"human\", \"Hello, how are you?\"),\n                    (\"ai\", \"I'm doing well, thanks!\"),\n                    (\"human\", \"That's good to hear.\"),\n                ])\n\n            Instantiation from mixed message formats:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate.from_messages([\n                    SystemMessage(content=\"hello\"),\n                    (\"human\", \"Hello, how are you?\"),\n                ])\n\n        Args:\n            messages: sequence of message representations.\n                  A message can be represented using the following formats:\n                  (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\n                  (message type, template); e.g., (\"human\", \"{user_input}\"),\n                  (4) 2-tuple of (message class, template), (5) a string which is\n                  shorthand for (\"human\", template); e.g., \"{user_input}\".\n            template_format: format of the template. Defaults to \"f-string\".\n\n        Returns:\n            a chat prompt template.\n        \"\"\"\n        return cls(messages, template_format=template_format)\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format the chat template into a list of finalized messages.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            list of formatted messages.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        result = []\n        for message_template in self.messages:\n            if isinstance(message_template, BaseMessage):\n                result.extend([message_template])\n            elif isinstance(\n                message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n            ):\n                message = message_template.format_messages(**kwargs)\n                result.extend(message)\n            else:\n                msg = f\"Unexpected input: {message_template}\"\n                raise ValueError(msg)  # noqa: TRY004\n        return result\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format the chat template into a list of finalized messages.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            list of formatted messages.\n\n        Raises:\n            ValueError: If unexpected input.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        result = []\n        for message_template in self.messages:\n            if isinstance(message_template, BaseMessage):\n                result.extend([message_template])\n            elif isinstance(\n                message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n            ):\n                message = await message_template.aformat_messages(**kwargs)\n                result.extend(message)\n            else:\n                msg = f\"Unexpected input: {message_template}\"\n                raise ValueError(msg)  # noqa:TRY004\n        return result\n\n    def partial(self, **kwargs: Any) -> ChatPromptTemplate:\n        \"\"\"Get a new ChatPromptTemplate with some input variables already filled in.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables. Ought\n                        to be a subset of the input variables.\n\n        Returns:\n            A new ChatPromptTemplate.\n\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.prompts import ChatPromptTemplate\n\n                template = ChatPromptTemplate.from_messages(\n                    [\n                        (\"system\", \"You are an AI assistant named {name}.\"),\n                        (\"human\", \"Hi I'm {user}\"),\n                        (\"ai\", \"Hi there, {user}, I'm {name}.\"),\n                        (\"human\", \"{input}\"),\n                    ]\n                )\n                template2 = template.partial(user=\"Lucy\", name=\"R2D2\")\n\n                template2.format_messages(input=\"hello\")\n        \"\"\"\n        prompt_dict = self.__dict__.copy()\n        prompt_dict[\"input_variables\"] = list(\n            set(self.input_variables).difference(kwargs)\n        )\n        prompt_dict[\"partial_variables\"] = {**self.partial_variables, **kwargs}\n        return type(self)(**prompt_dict)\n\n    def append(self, message: MessageLikeRepresentation) -> None:\n        \"\"\"Append a message to the end of the chat template.\n\n        Args:\n            message: representation of a message to append.\n        \"\"\"\n        self.messages.append(_convert_to_message(message))\n\n    def extend(self, messages: Sequence[MessageLikeRepresentation]) -> None:\n        \"\"\"Extend the chat template with a sequence of messages.\n\n        Args:\n            messages: sequence of message representations to append.\n        \"\"\"\n        self.messages.extend([_convert_to_message(message) for message in messages])\n\n    @overload\n    def __getitem__(self, index: int) -> MessageLike: ...\n\n    @overload\n    def __getitem__(self, index: slice) -> ChatPromptTemplate: ...\n\n    def __getitem__(\n        self, index: Union[int, slice]\n    ) -> Union[MessageLike, ChatPromptTemplate]:\n        \"\"\"Use to index into the chat template.\"\"\"\n        if isinstance(index, slice):\n            start, stop, step = index.indices(len(self.messages))\n            messages = self.messages[start:stop:step]\n            return ChatPromptTemplate.from_messages(messages)\n        else:\n            return self.messages[index]\n\n    def __len__(self) -> int:\n        \"\"\"Get the length of the chat template.\"\"\"\n        return len(self.messages)\n\n    @property\n    def _prompt_type(self) -> str:\n        \"\"\"Name of prompt type. Used for serialization.\"\"\"\n        return \"chat\"\n\n    def save(self, file_path: Union[Path, str]) -> None:\n        \"\"\"Save prompt to file.\n\n        Args:\n            file_path: path to file.\n        \"\"\"\n        raise NotImplementedError\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        # TODO: handle partials\n        return \"\\n\\n\".join(msg.pretty_repr(html=html) for msg in self.messages)\n\n\ndef _create_template_from_message_type(\n    message_type: str,\n    template: Union[str, list],\n    template_format: PromptTemplateFormat = \"f-string\",\n) -> BaseMessagePromptTemplate:\n    \"\"\"Create a message prompt template from a message type and template string.\n\n    Args:\n        message_type: str the type of the message template (e.g., \"human\", \"ai\", etc.)\n        template: str the template string.\n        template_format: format of the template. Defaults to \"f-string\".\n\n    Returns:\n        a message prompt template of the appropriate type.\n\n    Raises:\n        ValueError: If unexpected message type.\n    \"\"\"\n    if message_type in (\"human\", \"user\"):\n        message: BaseMessagePromptTemplate = HumanMessagePromptTemplate.from_template(\n            template, template_format=template_format\n        )\n    elif message_type in (\"ai\", \"assistant\"):\n        message = AIMessagePromptTemplate.from_template(\n            cast(str, template), template_format=template_format\n        )\n    elif message_type == \"system\":\n        message = SystemMessagePromptTemplate.from_template(\n            cast(str, template), template_format=template_format\n        )\n    elif message_type == \"placeholder\":\n        if isinstance(template, str):\n            if template[0] != \"{\" or template[-1] != \"}\":\n                msg = (\n                    f\"Invalid placeholder template: {template}.\"\n                    \" Expected a variable name surrounded by curly braces.\"\n                )\n                raise ValueError(msg)\n            var_name = template[1:-1]\n            message = MessagesPlaceholder(variable_name=var_name, optional=True)\n        elif len(template) == 2 and isinstance(template[1], bool):\n            var_name_wrapped, is_optional = template\n            if not isinstance(var_name_wrapped, str):\n                msg = f\"Expected variable name to be a string. Got: {var_name_wrapped}\"\n                raise ValueError(msg)  # noqa:TRY004\n            if var_name_wrapped[0] != \"{\" or var_name_wrapped[-1] != \"}\":\n                msg = (\n                    f\"Invalid placeholder template: {var_name_wrapped}.\"\n                    \" Expected a variable name surrounded by curly braces.\"\n                )\n                raise ValueError(msg)\n            var_name = var_name_wrapped[1:-1]\n\n            message = MessagesPlaceholder(variable_name=var_name, optional=is_optional)\n        else:\n            msg = (\n                \"Unexpected arguments for placeholder message type.\"\n                \" Expected either a single string variable name\"\n                \" or a list of [variable_name: str, is_optional: bool].\"\n                f\" Got: {template}\"\n            )\n            raise ValueError(msg)\n    else:\n        msg = (\n            f\"Unexpected message type: {message_type}. Use one of 'human',\"\n            f\" 'user', 'ai', 'assistant', or 'system'.\"\n        )\n        raise ValueError(msg)\n    return message\n\n\ndef _convert_to_message(\n    message: MessageLikeRepresentation,\n    template_format: PromptTemplateFormat = \"f-string\",\n) -> Union[BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate]:\n    \"\"\"Instantiate a message from a variety of message formats.\n\n    The message format can be one of the following:\n\n    - BaseMessagePromptTemplate\n    - BaseMessage\n    - 2-tuple of (role string, template); e.g., (\"human\", \"{user_input}\")\n    - 2-tuple of (message class, template)\n    - string: shorthand for (\"human\", template); e.g., \"{user_input}\"\n\n    Args:\n        message: a representation of a message in one of the supported formats.\n        template_format: format of the template. Defaults to \"f-string\".\n\n    Returns:\n        an instance of a message or a message template.\n\n    Raises:\n        ValueError: If unexpected message type.\n        ValueError: If 2-tuple does not have 2 elements.\n    \"\"\"\n    if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):\n        _message: Union[\n            BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate\n        ] = message\n    elif isinstance(message, BaseMessage):\n        _message = message\n    elif isinstance(message, str):\n        _message = _create_template_from_message_type(\n            \"human\", message, template_format=template_format\n        )\n    elif isinstance(message, (tuple, dict)):\n        if isinstance(message, dict):\n            if set(message.keys()) != {\"content\", \"role\"}:\n                msg = (\n                    \"Expected dict to have exact keys 'role' and 'content'.\"\n                    f\" Got: {message}\"\n                )\n                raise ValueError(msg)\n            message = (message[\"role\"], message[\"content\"])\n        if len(message) != 2:\n            msg = f\"Expected 2-tuple of (role, template), got {message}\"\n            raise ValueError(msg)\n        message_type_str, template = message\n        if isinstance(message_type_str, str):\n            _message = _create_template_from_message_type(\n                message_type_str, template, template_format=template_format\n            )\n        else:\n            _message = message_type_str(\n                prompt=PromptTemplate.from_template(\n                    cast(str, template), template_format=template_format\n                )\n            )\n    else:\n        msg = f\"Unsupported message type: {type(message)}\"\n        raise NotImplementedError(msg)\n\n    return _message\n",
        "patch": "@@ -3,6 +3,7 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n+from pathlib import Path\n from typing import (\n     TYPE_CHECKING,\n     Annotated,\n@@ -48,7 +49,6 @@\n \n if TYPE_CHECKING:\n     from collections.abc import Sequence\n-    from pathlib import Path\n \n \n class BaseMessagePromptTemplate(Serializable, ABC):\n@@ -599,8 +599,7 @@ def from_template_file(\n         Returns:\n             A new instance of this class.\n         \"\"\"\n-        with open(str(template_file)) as f:\n-            template = f.read()\n+        template = Path(template_file).read_text()\n         return cls.from_template(template, input_variables=input_variables, **kwargs)\n \n     def format_messages(self, **kwargs: Any) -> list[BaseMessage]:"
      },
      {
        "filename": "libs/core/langchain_core/prompts/loading.py",
        "content_before": "\"\"\"Load prompts.\"\"\"\n\nimport json\nimport logging\nfrom pathlib import Path\nfrom typing import Callable, Optional, Union\n\nimport yaml\n\nfrom langchain_core.output_parsers.string import StrOutputParser\nfrom langchain_core.prompts.base import BasePromptTemplate\nfrom langchain_core.prompts.chat import ChatPromptTemplate\nfrom langchain_core.prompts.few_shot import FewShotPromptTemplate\nfrom langchain_core.prompts.prompt import PromptTemplate\n\nURL_BASE = \"https://raw.githubusercontent.com/hwchase17/langchain-hub/master/prompts/\"\nlogger = logging.getLogger(__name__)\n\n\ndef load_prompt_from_config(config: dict) -> BasePromptTemplate:\n    \"\"\"Load prompt from Config Dict.\n\n    Args:\n        config: Dict containing the prompt configuration.\n\n    Returns:\n        A PromptTemplate object.\n\n    Raises:\n        ValueError: If the prompt type is not supported.\n    \"\"\"\n    if \"_type\" not in config:\n        logger.warning(\"No `_type` key found, defaulting to `prompt`.\")\n    config_type = config.pop(\"_type\", \"prompt\")\n\n    if config_type not in type_to_loader_dict:\n        msg = f\"Loading {config_type} prompt not supported\"\n        raise ValueError(msg)\n\n    prompt_loader = type_to_loader_dict[config_type]\n    return prompt_loader(config)\n\n\ndef _load_template(var_name: str, config: dict) -> dict:\n    \"\"\"Load template from the path if applicable.\"\"\"\n    # Check if template_path exists in config.\n    if f\"{var_name}_path\" in config:\n        # If it does, make sure template variable doesn't also exist.\n        if var_name in config:\n            msg = f\"Both `{var_name}_path` and `{var_name}` cannot be provided.\"\n            raise ValueError(msg)\n        # Pop the template path from the config.\n        template_path = Path(config.pop(f\"{var_name}_path\"))\n        # Load the template.\n        if template_path.suffix == \".txt\":\n            with open(template_path) as f:\n                template = f.read()\n        else:\n            raise ValueError\n        # Set the template variable to the extracted variable.\n        config[var_name] = template\n    return config\n\n\ndef _load_examples(config: dict) -> dict:\n    \"\"\"Load examples if necessary.\"\"\"\n    if isinstance(config[\"examples\"], list):\n        pass\n    elif isinstance(config[\"examples\"], str):\n        with open(config[\"examples\"]) as f:\n            if config[\"examples\"].endswith(\".json\"):\n                examples = json.load(f)\n            elif config[\"examples\"].endswith((\".yaml\", \".yml\")):\n                examples = yaml.safe_load(f)\n            else:\n                msg = \"Invalid file format. Only json or yaml formats are supported.\"\n                raise ValueError(msg)\n        config[\"examples\"] = examples\n    else:\n        msg = \"Invalid examples format. Only list or string are supported.\"\n        raise ValueError(msg)  # noqa:TRY004\n    return config\n\n\ndef _load_output_parser(config: dict) -> dict:\n    \"\"\"Load output parser.\"\"\"\n    if \"output_parser\" in config and config[\"output_parser\"]:\n        _config = config.pop(\"output_parser\")\n        output_parser_type = _config.pop(\"_type\")\n        if output_parser_type == \"default\":\n            output_parser = StrOutputParser(**_config)\n        else:\n            msg = f\"Unsupported output parser {output_parser_type}\"\n            raise ValueError(msg)\n        config[\"output_parser\"] = output_parser\n    return config\n\n\ndef _load_few_shot_prompt(config: dict) -> FewShotPromptTemplate:\n    \"\"\"Load the \"few shot\" prompt from the config.\"\"\"\n    # Load the suffix and prefix templates.\n    config = _load_template(\"suffix\", config)\n    config = _load_template(\"prefix\", config)\n    # Load the example prompt.\n    if \"example_prompt_path\" in config:\n        if \"example_prompt\" in config:\n            msg = (\n                \"Only one of example_prompt and example_prompt_path should \"\n                \"be specified.\"\n            )\n            raise ValueError(msg)\n        config[\"example_prompt\"] = load_prompt(config.pop(\"example_prompt_path\"))\n    else:\n        config[\"example_prompt\"] = load_prompt_from_config(config[\"example_prompt\"])\n    # Load the examples.\n    config = _load_examples(config)\n    config = _load_output_parser(config)\n    return FewShotPromptTemplate(**config)\n\n\ndef _load_prompt(config: dict) -> PromptTemplate:\n    \"\"\"Load the prompt template from config.\"\"\"\n    # Load the template from disk if necessary.\n    config = _load_template(\"template\", config)\n    config = _load_output_parser(config)\n\n    template_format = config.get(\"template_format\", \"f-string\")\n    if template_format == \"jinja2\":\n        # Disabled due to:\n        # https://github.com/langchain-ai/langchain/issues/4394\n        msg = (\n            f\"Loading templates with '{template_format}' format is no longer supported \"\n            f\"since it can lead to arbitrary code execution. Please migrate to using \"\n            f\"the 'f-string' template format, which does not suffer from this issue.\"\n        )\n        raise ValueError(msg)\n\n    return PromptTemplate(**config)\n\n\ndef load_prompt(\n    path: Union[str, Path], encoding: Optional[str] = None\n) -> BasePromptTemplate:\n    \"\"\"Unified method for loading a prompt from LangChainHub or local fs.\n\n    Args:\n        path: Path to the prompt file.\n        encoding: Encoding of the file. Defaults to None.\n\n    Returns:\n        A PromptTemplate object.\n\n    Raises:\n        RuntimeError: If the path is a Lang Chain Hub path.\n    \"\"\"\n    if isinstance(path, str) and path.startswith(\"lc://\"):\n        msg = (\n            \"Loading from the deprecated github-based Hub is no longer supported. \"\n            \"Please use the new LangChain Hub at https://smith.langchain.com/hub \"\n            \"instead.\"\n        )\n        raise RuntimeError(msg)\n    return _load_prompt_from_file(path, encoding)\n\n\ndef _load_prompt_from_file(\n    file: Union[str, Path], encoding: Optional[str] = None\n) -> BasePromptTemplate:\n    \"\"\"Load prompt from file.\"\"\"\n    # Convert file to a Path object.\n    file_path = Path(file) if isinstance(file, str) else file\n    # Load from either json or yaml.\n    if file_path.suffix == \".json\":\n        with open(file_path, encoding=encoding) as f:\n            config = json.load(f)\n    elif file_path.suffix.endswith((\".yaml\", \".yml\")):\n        with open(file_path, encoding=encoding) as f:\n            config = yaml.safe_load(f)\n    else:\n        msg = f\"Got unsupported file type {file_path.suffix}\"\n        raise ValueError(msg)\n    # Load the prompt from the config now.\n    return load_prompt_from_config(config)\n\n\ndef _load_chat_prompt(config: dict) -> ChatPromptTemplate:\n    \"\"\"Load chat prompt from config.\"\"\"\n    messages = config.pop(\"messages\")\n    template = messages[0][\"prompt\"].pop(\"template\") if messages else None\n    config.pop(\"input_variables\")\n\n    if not template:\n        msg = \"Can't load chat prompt without template\"\n        raise ValueError(msg)\n\n    return ChatPromptTemplate.from_template(template=template, **config)\n\n\ntype_to_loader_dict: dict[str, Callable[[dict], BasePromptTemplate]] = {\n    \"prompt\": _load_prompt,\n    \"few_shot\": _load_few_shot_prompt,\n    \"chat\": _load_chat_prompt,\n}\n",
        "patch": "@@ -53,8 +53,7 @@ def _load_template(var_name: str, config: dict) -> dict:\n         template_path = Path(config.pop(f\"{var_name}_path\"))\n         # Load the template.\n         if template_path.suffix == \".txt\":\n-            with open(template_path) as f:\n-                template = f.read()\n+            template = template_path.read_text()\n         else:\n             raise ValueError\n         # Set the template variable to the extracted variable.\n@@ -67,10 +66,11 @@ def _load_examples(config: dict) -> dict:\n     if isinstance(config[\"examples\"], list):\n         pass\n     elif isinstance(config[\"examples\"], str):\n-        with open(config[\"examples\"]) as f:\n-            if config[\"examples\"].endswith(\".json\"):\n+        path = Path(config[\"examples\"])\n+        with path.open() as f:\n+            if path.suffix == \".json\":\n                 examples = json.load(f)\n-            elif config[\"examples\"].endswith((\".yaml\", \".yml\")):\n+            elif path.suffix in {\".yaml\", \".yml\"}:\n                 examples = yaml.safe_load(f)\n             else:\n                 msg = \"Invalid file format. Only json or yaml formats are supported.\"\n@@ -168,13 +168,13 @@ def _load_prompt_from_file(\n ) -> BasePromptTemplate:\n     \"\"\"Load prompt from file.\"\"\"\n     # Convert file to a Path object.\n-    file_path = Path(file) if isinstance(file, str) else file\n+    file_path = Path(file)\n     # Load from either json or yaml.\n     if file_path.suffix == \".json\":\n-        with open(file_path, encoding=encoding) as f:\n+        with file_path.open(encoding=encoding) as f:\n             config = json.load(f)\n     elif file_path.suffix.endswith((\".yaml\", \".yml\")):\n-        with open(file_path, encoding=encoding) as f:\n+        with file_path.open(encoding=encoding) as f:\n             config = yaml.safe_load(f)\n     else:\n         msg = f\"Got unsupported file type {file_path.suffix}\""
      },
      {
        "filename": "libs/core/langchain_core/prompts/prompt.py",
        "content_before": "\"\"\"Prompt schema definition.\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nfrom pydantic import BaseModel, model_validator\n\nfrom langchain_core.prompts.string import (\n    DEFAULT_FORMATTER_MAPPING,\n    PromptTemplateFormat,\n    StringPromptTemplate,\n    check_valid_template,\n    get_template_variables,\n    mustache_schema,\n)\n\nif TYPE_CHECKING:\n    from pathlib import Path\n\n    from langchain_core.runnables.config import RunnableConfig\n\n\nclass PromptTemplate(StringPromptTemplate):\n    \"\"\"Prompt template for a language model.\n\n    A prompt template consists of a string template. It accepts a set of parameters\n    from the user that can be used to generate a prompt for a language model.\n\n    The template can be formatted using either f-strings (default), jinja2,\n    or mustache syntax.\n\n    *Security warning*:\n        Prefer using `template_format=\"f-string\"` instead of\n        `template_format=\"jinja2\"`, or make sure to NEVER accept jinja2 templates\n        from untrusted sources as they may lead to arbitrary Python code execution.\n\n        As of LangChain 0.0.329, Jinja2 templates will be rendered using\n        Jinja2's SandboxedEnvironment by default. This sand-boxing should\n        be treated as a best-effort approach rather than a guarantee of security,\n        as it is an opt-out rather than opt-in approach.\n\n        Despite the sand-boxing, we recommend to never use jinja2 templates\n        from untrusted sources.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import PromptTemplate\n\n            # Instantiation using from_template (recommended)\n            prompt = PromptTemplate.from_template(\"Say {foo}\")\n            prompt.format(foo=\"bar\")\n\n            # Instantiation using initializer\n            prompt = PromptTemplate(template=\"Say {foo}\")\n    \"\"\"\n\n    @property\n    def lc_attributes(self) -> dict[str, Any]:\n        return {\n            \"template_format\": self.template_format,\n        }\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"prompt\"]\n\n    template: str\n    \"\"\"The prompt template.\"\"\"\n\n    template_format: PromptTemplateFormat = \"f-string\"\n    \"\"\"The format of the prompt template.\n    Options are: 'f-string', 'mustache', 'jinja2'.\"\"\"\n\n    validate_template: bool = False\n    \"\"\"Whether or not to try validating the template.\"\"\"\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_init_validation(cls, values: dict) -> Any:\n        \"\"\"Check that template and input variables are consistent.\"\"\"\n        if values.get(\"template\") is None:\n            # Will let pydantic fail with a ValidationError if template\n            # is not provided.\n            return values\n\n        # Set some default values based on the field defaults\n        values.setdefault(\"template_format\", \"f-string\")\n        values.setdefault(\"partial_variables\", {})\n\n        if values.get(\"validate_template\"):\n            if values[\"template_format\"] == \"mustache\":\n                msg = \"Mustache templates cannot be validated.\"\n                raise ValueError(msg)\n\n            if \"input_variables\" not in values:\n                msg = \"Input variables must be provided to validate the template.\"\n                raise ValueError(msg)\n\n            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\n            check_valid_template(\n                values[\"template\"], values[\"template_format\"], all_inputs\n            )\n\n        if values[\"template_format\"]:\n            values[\"input_variables\"] = [\n                var\n                for var in get_template_variables(\n                    values[\"template\"], values[\"template_format\"]\n                )\n                if var not in values[\"partial_variables\"]\n            ]\n\n        return values\n\n    def get_input_schema(self, config: RunnableConfig | None = None) -> type[BaseModel]:\n        \"\"\"Get the input schema for the prompt.\n\n        Args:\n            config: The runnable configuration.\n\n        Returns:\n            The input schema for the prompt.\n        \"\"\"\n        if self.template_format != \"mustache\":\n            return super().get_input_schema(config)\n\n        return mustache_schema(self.template)\n\n    def __add__(self, other: Any) -> PromptTemplate:\n        \"\"\"Override the + operator to allow for combining prompt templates.\"\"\"\n        # Allow for easy combining\n        if isinstance(other, PromptTemplate):\n            if self.template_format != \"f-string\":\n                msg = \"Adding prompt templates only supported for f-strings.\"\n                raise ValueError(msg)\n            if other.template_format != \"f-string\":\n                msg = \"Adding prompt templates only supported for f-strings.\"\n                raise ValueError(msg)\n            input_variables = list(\n                set(self.input_variables) | set(other.input_variables)\n            )\n            template = self.template + other.template\n            # If any do not want to validate, then don't\n            validate_template = self.validate_template and other.validate_template\n            partial_variables = dict(self.partial_variables.items())\n            for k, v in other.partial_variables.items():\n                if k in partial_variables:\n                    msg = \"Cannot have same variable partialed twice.\"\n                    raise ValueError(msg)\n                else:\n                    partial_variables[k] = v\n            return PromptTemplate(\n                template=template,\n                input_variables=input_variables,\n                partial_variables=partial_variables,\n                template_format=\"f-string\",\n                validate_template=validate_template,\n            )\n        elif isinstance(other, str):\n            prompt = PromptTemplate.from_template(other)\n            return self + prompt\n        else:\n            msg = f\"Unsupported operand type for +: {type(other)}\"\n            raise NotImplementedError(msg)\n\n    @property\n    def _prompt_type(self) -> str:\n        \"\"\"Return the prompt type key.\"\"\"\n        return \"prompt\"\n\n    def format(self, **kwargs: Any) -> str:\n        \"\"\"Format the prompt with the inputs.\n\n        Args:\n            kwargs: Any arguments to be passed to the prompt template.\n\n        Returns:\n            A formatted string.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\n\n    @classmethod\n    def from_examples(\n        cls,\n        examples: list[str],\n        suffix: str,\n        input_variables: list[str],\n        example_separator: str = \"\\n\\n\",\n        prefix: str = \"\",\n        **kwargs: Any,\n    ) -> PromptTemplate:\n        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\n\n        Intended to be used as a way to dynamically create a prompt from examples.\n\n        Args:\n            examples: List of examples to use in the prompt.\n            suffix: String to go after the list of examples. Should generally\n                set up the user's input.\n            input_variables: A list of variable names the final prompt template\n                will expect.\n            example_separator: The separator to use in between examples. Defaults\n                to two new line characters.\n            prefix: String that should go before any examples. Generally includes\n                examples. Default to an empty string.\n\n        Returns:\n            The final prompt generated.\n        \"\"\"\n        template = example_separator.join([prefix, *examples, suffix])\n        return cls(input_variables=input_variables, template=template, **kwargs)\n\n    @classmethod\n    def from_file(\n        cls,\n        template_file: Union[str, Path],\n        input_variables: Optional[list[str]] = None,\n        encoding: Optional[str] = None,\n        **kwargs: Any,\n    ) -> PromptTemplate:\n        \"\"\"Load a prompt from a file.\n\n        Args:\n            template_file: The path to the file containing the prompt template.\n            input_variables: [DEPRECATED] A list of variable names the final prompt\n                template will expect. Defaults to None.\n            encoding: The encoding system for opening the template file.\n                If not provided, will use the OS default.\n\n        input_variables is ignored as from_file now delegates to from_template().\n\n        Returns:\n            The prompt loaded from the file.\n        \"\"\"\n        with open(str(template_file), encoding=encoding) as f:\n            template = f.read()\n        if input_variables:\n            warnings.warn(\n                \"`input_variables' is deprecated and ignored.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        return cls.from_template(template=template, **kwargs)\n\n    @classmethod\n    def from_template(\n        cls,\n        template: str,\n        *,\n        template_format: PromptTemplateFormat = \"f-string\",\n        partial_variables: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> PromptTemplate:\n        \"\"\"Load a prompt template from a template.\n\n        *Security warning*:\n            Prefer using `template_format=\"f-string\"` instead of\n            `template_format=\"jinja2\"`, or make sure to NEVER accept jinja2 templates\n            from untrusted sources as they may lead to arbitrary Python code execution.\n\n            As of LangChain 0.0.329, Jinja2 templates will be rendered using\n            Jinja2's SandboxedEnvironment by default. This sand-boxing should\n            be treated as a best-effort approach rather than a guarantee of security,\n            as it is an opt-out rather than opt-in approach.\n\n            Despite the sand-boxing, we recommend never using jinja2 templates\n            from untrusted sources.\n\n        Args:\n            template: The template to load.\n            template_format: The format of the template. Use `jinja2` for jinja2,\n                             `mustache` for mustache, and `f-string` for f-strings.\n                             Defaults to `f-string`.\n            partial_variables: A dictionary of variables that can be used to partially\n                               fill in the template. For example, if the template is\n                              `\"{variable1} {variable2}\"`, and `partial_variables` is\n                              `{\"variable1\": \"foo\"}`, then the final prompt will be\n                              `\"foo {variable2}\"`. Defaults to None.\n            kwargs: Any other arguments to pass to the prompt template.\n\n        Returns:\n            The prompt template loaded from the template.\n        \"\"\"\n        input_variables = get_template_variables(template, template_format)\n        _partial_variables = partial_variables or {}\n\n        if _partial_variables:\n            input_variables = [\n                var for var in input_variables if var not in _partial_variables\n            ]\n\n        return cls(\n            input_variables=input_variables,\n            template=template,\n            template_format=template_format,  # type: ignore[arg-type]\n            partial_variables=_partial_variables,\n            **kwargs,\n        )\n",
        "patch": "@@ -3,6 +3,7 @@\n from __future__ import annotations\n \n import warnings\n+from pathlib import Path\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n from pydantic import BaseModel, model_validator\n@@ -17,8 +18,6 @@\n )\n \n if TYPE_CHECKING:\n-    from pathlib import Path\n-\n     from langchain_core.runnables.config import RunnableConfig\n \n \n@@ -238,8 +237,7 @@ def from_file(\n         Returns:\n             The prompt loaded from the file.\n         \"\"\"\n-        with open(str(template_file), encoding=encoding) as f:\n-            template = f.read()\n+        template = Path(template_file).read_text(encoding=encoding)\n         if input_variables:\n             warnings.warn(\n                 \"`input_variables' is deprecated and ignored.\","
      },
      {
        "filename": "libs/core/langchain_core/runnables/graph_mermaid.py",
        "content_before": "import asyncio\nimport base64\nimport re\nfrom dataclasses import asdict\nfrom typing import Literal, Optional\n\nfrom langchain_core.runnables.graph import (\n    CurveStyle,\n    Edge,\n    MermaidDrawMethod,\n    Node,\n    NodeStyles,\n)\n\nMARKDOWN_SPECIAL_CHARS = \"*_`\"\n\n\ndef draw_mermaid(\n    nodes: dict[str, Node],\n    edges: list[Edge],\n    *,\n    first_node: Optional[str] = None,\n    last_node: Optional[str] = None,\n    with_styles: bool = True,\n    curve_style: CurveStyle = CurveStyle.LINEAR,\n    node_styles: Optional[NodeStyles] = None,\n    wrap_label_n_words: int = 9,\n) -> str:\n    \"\"\"Draws a Mermaid graph using the provided graph data.\n\n    Args:\n        nodes (dict[str, str]): List of node ids.\n        edges (List[Edge]): List of edges, object with a source,\n            target and data.\n        first_node (str, optional): Id of the first node. Defaults to None.\n        last_node (str, optional): Id of the last node. Defaults to None.\n        with_styles (bool, optional): Whether to include styles in the graph.\n            Defaults to True.\n        curve_style (CurveStyle, optional): Curve style for the edges.\n            Defaults to CurveStyle.LINEAR.\n        node_styles (NodeStyles, optional): Node colors for different types.\n            Defaults to NodeStyles().\n        wrap_label_n_words (int, optional): Words to wrap the edge labels.\n            Defaults to 9.\n\n    Returns:\n        str: Mermaid graph syntax.\n    \"\"\"\n    # Initialize Mermaid graph configuration\n    mermaid_graph = (\n        (\n            f\"%%{{init: {{'flowchart': {{'curve': '{curve_style.value}'\"\n            f\"}}}}}}%%\\ngraph TD;\\n\"\n        )\n        if with_styles\n        else \"graph TD;\\n\"\n    )\n\n    if with_styles:\n        # Node formatting templates\n        default_class_label = \"default\"\n        format_dict = {default_class_label: \"{0}({1})\"}\n        if first_node is not None:\n            format_dict[first_node] = \"{0}([{1}]):::first\"\n        if last_node is not None:\n            format_dict[last_node] = \"{0}([{1}]):::last\"\n\n        # Add nodes to the graph\n        for key, node in nodes.items():\n            node_name = node.name.split(\":\")[-1]\n            label = (\n                f\"<p>{node_name}</p>\"\n                if node_name.startswith(tuple(MARKDOWN_SPECIAL_CHARS))\n                and node_name.endswith(tuple(MARKDOWN_SPECIAL_CHARS))\n                else node_name\n            )\n            if node.metadata:\n                label = (\n                    f\"{label}<hr/><small><em>\"\n                    + \"\\n\".join(\n                        f\"{key} = {value}\" for key, value in node.metadata.items()\n                    )\n                    + \"</em></small>\"\n                )\n            node_label = format_dict.get(key, format_dict[default_class_label]).format(\n                _escape_node_label(key), label\n            )\n            mermaid_graph += f\"\\t{node_label}\\n\"\n\n    # Group edges by their common prefixes\n    edge_groups: dict[str, list[Edge]] = {}\n    for edge in edges:\n        src_parts = edge.source.split(\":\")\n        tgt_parts = edge.target.split(\":\")\n        common_prefix = \":\".join(\n            src for src, tgt in zip(src_parts, tgt_parts) if src == tgt\n        )\n        edge_groups.setdefault(common_prefix, []).append(edge)\n\n    seen_subgraphs = set()\n\n    def add_subgraph(edges: list[Edge], prefix: str) -> None:\n        nonlocal mermaid_graph\n        self_loop = len(edges) == 1 and edges[0].source == edges[0].target\n        if prefix and not self_loop:\n            subgraph = prefix.split(\":\")[-1]\n            if subgraph in seen_subgraphs:\n                msg = (\n                    f\"Found duplicate subgraph '{subgraph}' -- this likely means that \"\n                    \"you're reusing a subgraph node with the same name. \"\n                    \"Please adjust your graph to have subgraph nodes with unique names.\"\n                )\n                raise ValueError(msg)\n\n            seen_subgraphs.add(subgraph)\n            mermaid_graph += f\"\\tsubgraph {subgraph}\\n\"\n\n        for edge in edges:\n            source, target = edge.source, edge.target\n\n            # Add BR every wrap_label_n_words words\n            if edge.data is not None:\n                edge_data = edge.data\n                words = str(edge_data).split()  # Split the string into words\n                # Group words into chunks of wrap_label_n_words size\n                if len(words) > wrap_label_n_words:\n                    edge_data = \"&nbsp<br>&nbsp\".join(\n                        \" \".join(words[i : i + wrap_label_n_words])\n                        for i in range(0, len(words), wrap_label_n_words)\n                    )\n                if edge.conditional:\n                    edge_label = f\" -. &nbsp;{edge_data}&nbsp; .-> \"\n                else:\n                    edge_label = f\" -- &nbsp;{edge_data}&nbsp; --> \"\n            else:\n                edge_label = \" -.-> \" if edge.conditional else \" --> \"\n\n            mermaid_graph += (\n                f\"\\t{_escape_node_label(source)}{edge_label}\"\n                f\"{_escape_node_label(target)};\\n\"\n            )\n\n        # Recursively add nested subgraphs\n        for nested_prefix in edge_groups:\n            if not nested_prefix.startswith(prefix + \":\") or nested_prefix == prefix:\n                continue\n            add_subgraph(edge_groups[nested_prefix], nested_prefix)\n\n        if prefix and not self_loop:\n            mermaid_graph += \"\\tend\\n\"\n\n    # Start with the top-level edges (no common prefix)\n    add_subgraph(edge_groups.get(\"\", []), \"\")\n\n    # Add remaining subgraphs\n    for prefix in edge_groups:\n        if \":\" in prefix or prefix == \"\":\n            continue\n        add_subgraph(edge_groups[prefix], prefix)\n\n    # Add custom styles for nodes\n    if with_styles:\n        mermaid_graph += _generate_mermaid_graph_styles(node_styles or NodeStyles())\n    return mermaid_graph\n\n\ndef _escape_node_label(node_label: str) -> str:\n    \"\"\"Escapes the node label for Mermaid syntax.\"\"\"\n    return re.sub(r\"[^a-zA-Z-_0-9]\", \"_\", node_label)\n\n\ndef _generate_mermaid_graph_styles(node_colors: NodeStyles) -> str:\n    \"\"\"Generates Mermaid graph styles for different node types.\"\"\"\n    styles = \"\"\n    for class_name, style in asdict(node_colors).items():\n        styles += f\"\\tclassDef {class_name} {style}\\n\"\n    return styles\n\n\ndef draw_mermaid_png(\n    mermaid_syntax: str,\n    output_file_path: Optional[str] = None,\n    draw_method: MermaidDrawMethod = MermaidDrawMethod.API,\n    background_color: Optional[str] = \"white\",\n    padding: int = 10,\n) -> bytes:\n    \"\"\"Draws a Mermaid graph as PNG using provided syntax.\n\n    Args:\n        mermaid_syntax (str): Mermaid graph syntax.\n        output_file_path (str, optional): Path to save the PNG image.\n            Defaults to None.\n        draw_method (MermaidDrawMethod, optional): Method to draw the graph.\n            Defaults to MermaidDrawMethod.API.\n        background_color (str, optional): Background color of the image.\n            Defaults to \"white\".\n        padding (int, optional): Padding around the image. Defaults to 10.\n\n    Returns:\n        bytes: PNG image bytes.\n\n    Raises:\n        ValueError: If an invalid draw method is provided.\n    \"\"\"\n    if draw_method == MermaidDrawMethod.PYPPETEER:\n        import asyncio\n\n        img_bytes = asyncio.run(\n            _render_mermaid_using_pyppeteer(\n                mermaid_syntax, output_file_path, background_color, padding\n            )\n        )\n    elif draw_method == MermaidDrawMethod.API:\n        img_bytes = _render_mermaid_using_api(\n            mermaid_syntax, output_file_path, background_color\n        )\n    else:\n        supported_methods = \", \".join([m.value for m in MermaidDrawMethod])\n        msg = (\n            f\"Invalid draw method: {draw_method}. \"\n            f\"Supported draw methods are: {supported_methods}\"\n        )\n        raise ValueError(msg)\n\n    return img_bytes\n\n\nasync def _render_mermaid_using_pyppeteer(\n    mermaid_syntax: str,\n    output_file_path: Optional[str] = None,\n    background_color: Optional[str] = \"white\",\n    padding: int = 10,\n    device_scale_factor: int = 3,\n) -> bytes:\n    \"\"\"Renders Mermaid graph using Pyppeteer.\"\"\"\n    try:\n        from pyppeteer import launch  # type: ignore[import]\n    except ImportError as e:\n        msg = \"Install Pyppeteer to use the Pyppeteer method: `pip install pyppeteer`.\"\n        raise ImportError(msg) from e\n\n    browser = await launch()\n    page = await browser.newPage()\n\n    # Setup Mermaid JS\n    await page.goto(\"about:blank\")\n    await page.addScriptTag(\n        {\"url\": \"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"}\n    )\n    await page.evaluate(\n        \"\"\"() => {\n                mermaid.initialize({startOnLoad:true});\n            }\"\"\"\n    )\n\n    # Render SVG\n    svg_code = await page.evaluate(\n        \"\"\"(mermaidGraph) => {\n                return mermaid.mermaidAPI.render('mermaid', mermaidGraph);\n            }\"\"\",\n        mermaid_syntax,\n    )\n\n    # Set the page background to white\n    await page.evaluate(\n        \"\"\"(svg, background_color) => {\n            document.body.innerHTML = svg;\n            document.body.style.background = background_color;\n        }\"\"\",\n        svg_code[\"svg\"],\n        background_color,\n    )\n\n    # Take a screenshot\n    dimensions = await page.evaluate(\n        \"\"\"() => {\n            const svgElement = document.querySelector('svg');\n            const rect = svgElement.getBoundingClientRect();\n            return { width: rect.width, height: rect.height };\n        }\"\"\"\n    )\n    await page.setViewport(\n        {\n            \"width\": int(dimensions[\"width\"] + padding),\n            \"height\": int(dimensions[\"height\"] + padding),\n            \"deviceScaleFactor\": device_scale_factor,\n        }\n    )\n\n    img_bytes = await page.screenshot({\"fullPage\": False})\n    await browser.close()\n\n    def write_to_file(path: str, bytes: bytes) -> None:\n        with open(path, \"wb\") as file:\n            file.write(bytes)\n\n    if output_file_path is not None:\n        await asyncio.get_event_loop().run_in_executor(\n            None, write_to_file, output_file_path, img_bytes\n        )\n\n    return img_bytes\n\n\ndef _render_mermaid_using_api(\n    mermaid_syntax: str,\n    output_file_path: Optional[str] = None,\n    background_color: Optional[str] = \"white\",\n    file_type: Optional[Literal[\"jpeg\", \"png\", \"webp\"]] = \"png\",\n) -> bytes:\n    \"\"\"Renders Mermaid graph using the Mermaid.INK API.\"\"\"\n    try:\n        import requests  # type: ignore[import]\n    except ImportError as e:\n        msg = (\n            \"Install the `requests` module to use the Mermaid.INK API: \"\n            \"`pip install requests`.\"\n        )\n        raise ImportError(msg) from e\n\n    # Use Mermaid API to render the image\n    mermaid_syntax_encoded = base64.b64encode(mermaid_syntax.encode(\"utf8\")).decode(\n        \"ascii\"\n    )\n\n    # Check if the background color is a hexadecimal color code using regex\n    if background_color is not None:\n        hex_color_pattern = re.compile(r\"^#(?:[0-9a-fA-F]{3}){1,2}$\")\n        if not hex_color_pattern.match(background_color):\n            background_color = f\"!{background_color}\"\n\n    image_url = (\n        f\"https://mermaid.ink/img/{mermaid_syntax_encoded}\"\n        f\"?type={file_type}&bgColor={background_color}\"\n    )\n    response = requests.get(image_url, timeout=10)\n    if response.status_code == 200:\n        img_bytes = response.content\n        if output_file_path is not None:\n            with open(output_file_path, \"wb\") as file:\n                file.write(response.content)\n\n        return img_bytes\n    else:\n        msg = (\n            f\"Failed to render the graph using the Mermaid.INK API. \"\n            f\"Status code: {response.status_code}.\"\n        )\n        raise ValueError(msg)\n",
        "patch": "@@ -2,6 +2,7 @@\n import base64\n import re\n from dataclasses import asdict\n+from pathlib import Path\n from typing import Literal, Optional\n \n from langchain_core.runnables.graph import (\n@@ -290,13 +291,9 @@ async def _render_mermaid_using_pyppeteer(\n     img_bytes = await page.screenshot({\"fullPage\": False})\n     await browser.close()\n \n-    def write_to_file(path: str, bytes: bytes) -> None:\n-        with open(path, \"wb\") as file:\n-            file.write(bytes)\n-\n     if output_file_path is not None:\n         await asyncio.get_event_loop().run_in_executor(\n-            None, write_to_file, output_file_path, img_bytes\n+            None, Path(output_file_path).write_bytes, img_bytes\n         )\n \n     return img_bytes\n@@ -337,8 +334,7 @@ def _render_mermaid_using_api(\n     if response.status_code == 200:\n         img_bytes = response.content\n         if output_file_path is not None:\n-            with open(output_file_path, \"wb\") as file:\n-                file.write(response.content)\n+            Path(output_file_path).write_bytes(response.content)\n \n         return img_bytes\n     else:"
      },
      {
        "filename": "libs/core/tests/unit_tests/prompts/test_prompt.py",
        "content_before": "\"\"\"Test functionality related to prompts.\"\"\"\n\nfrom typing import Any, Union\nfrom unittest import mock\n\nimport pydantic\nimport pytest\nfrom syrupy import SnapshotAssertion\n\nfrom langchain_core.prompts.prompt import PromptTemplate\nfrom langchain_core.prompts.string import PromptTemplateFormat\nfrom langchain_core.tracers.run_collector import RunCollectorCallbackHandler\nfrom tests.unit_tests.pydantic_utils import _normalize_schema\n\nPYDANTIC_VERSION = tuple(map(int, pydantic.__version__.split(\".\")))\n\n\ndef test_prompt_valid() -> None:\n    \"\"\"Test prompts can be constructed.\"\"\"\n    template = \"This is a {foo} test.\"\n    input_variables = [\"foo\"]\n    prompt = PromptTemplate(input_variables=input_variables, template=template)\n    assert prompt.template == template\n    assert prompt.input_variables == input_variables\n\n\ndef test_from_file_encoding() -> None:\n    \"\"\"Test that we can load a template from a file with a non utf-8 encoding.\"\"\"\n    template = \"This is a {foo} test with special character \u20ac.\"\n    input_variables = [\"foo\"]\n\n    # First write to a file using CP-1252 encoding.\n    from tempfile import NamedTemporaryFile\n\n    with NamedTemporaryFile(delete=True, mode=\"w\", encoding=\"cp1252\") as f:\n        f.write(template)\n        f.flush()\n        file_name = f.name\n\n        # Now read from the file using CP-1252 encoding and test\n        prompt = PromptTemplate.from_file(file_name, encoding=\"cp1252\")\n        assert prompt.template == template\n        assert prompt.input_variables == input_variables\n\n        # Now read from the file using UTF-8 encoding and test\n        with pytest.raises(UnicodeDecodeError):\n            PromptTemplate.from_file(file_name, encoding=\"utf-8\")\n\n\ndef test_prompt_from_template() -> None:\n    \"\"\"Test prompts can be constructed from a template.\"\"\"\n    # Single input variable.\n    template = \"This is a {foo} test.\"\n    prompt = PromptTemplate.from_template(template)\n    expected_prompt = PromptTemplate(template=template, input_variables=[\"foo\"])\n    assert prompt == expected_prompt\n\n    # Multiple input variables.\n    template = \"This {bar} is a {foo} test.\"\n    prompt = PromptTemplate.from_template(template)\n    expected_prompt = PromptTemplate(template=template, input_variables=[\"bar\", \"foo\"])\n    assert prompt == expected_prompt\n\n    # Multiple input variables with repeats.\n    template = \"This {bar} is a {foo} test {foo}.\"\n    prompt = PromptTemplate.from_template(template)\n    expected_prompt = PromptTemplate(template=template, input_variables=[\"bar\", \"foo\"])\n    assert prompt == expected_prompt\n\n\ndef test_mustache_prompt_from_template(snapshot: SnapshotAssertion) -> None:\n    \"\"\"Test prompts can be constructed from a template.\"\"\"\n    # Single input variable.\n    template = \"This is a {{foo}} test.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(foo=\"bar\") == \"This is a bar test.\"\n    assert prompt.input_variables == [\"foo\"]\n    assert prompt.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"foo\": {\"title\": \"Foo\", \"type\": \"string\", \"default\": None}},\n    }\n\n    # Multiple input variables.\n    template = \"This {{bar}} is a {{foo}} test.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(bar=\"baz\", foo=\"bar\") == \"This baz is a bar test.\"\n    assert prompt.input_variables == [\"bar\", \"foo\"]\n    assert prompt.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\", \"default\": None},\n            \"foo\": {\"title\": \"Foo\", \"type\": \"string\", \"default\": None},\n        },\n    }\n\n    # Multiple input variables with repeats.\n    template = \"This {{bar}} is a {{foo}} test {{&foo}}.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(bar=\"baz\", foo=\"bar\") == \"This baz is a bar test bar.\"\n    assert prompt.input_variables == [\"bar\", \"foo\"]\n    assert prompt.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\", \"default\": None},\n            \"foo\": {\"title\": \"Foo\", \"type\": \"string\", \"default\": None},\n        },\n    }\n\n    # Nested variables.\n    template = \"This {{obj.bar}} is a {{obj.foo}} test {{{foo}}}.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(obj={\"bar\": \"foo\", \"foo\": \"bar\"}, foo=\"baz\") == (\n        \"This foo is a bar test baz.\"\n    )\n    assert prompt.input_variables == [\"foo\", \"obj\"]\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(prompt.get_input_jsonschema()) == snapshot(\n            name=\"schema_0\"\n        )\n\n    # . variables\n    template = \"This {{.}} is a test.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(foo=\"baz\") == (\"This {'foo': 'baz'} is a test.\")\n    assert prompt.input_variables == []\n    assert prompt.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {},\n    }\n\n    # section/context variables\n    template = \"\"\"This{{#foo}}\n        {{bar}}\n    {{/foo}}is a test.\"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(foo={\"bar\": \"yo\"}) == (\n        \"\"\"This\n        yo\n    is a test.\"\"\"\n    )\n    assert prompt.input_variables == [\"foo\"]\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(prompt.get_input_jsonschema()) == snapshot(\n            name=\"schema_2\"\n        )\n\n    # more complex nested section/context variables\n    template = \"\"\"This{{#foo}}\n        {{bar}}\n        {{#baz}}\n            {{qux}}\n        {{/baz}}\n        {{quux}}\n    {{/foo}}is a test.\"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(\n        foo={\"bar\": \"yo\", \"baz\": [{\"qux\": \"wassup\"}], \"quux\": \"hello\"}\n    ) == (\n        \"\"\"This\n        yo\n            wassup\n        hello\n    is a test.\"\"\"\n    )\n    assert prompt.input_variables == [\"foo\"]\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(prompt.get_input_jsonschema()) == snapshot(\n            name=\"schema_3\"\n        )\n\n    # triply nested section/context variables\n    template = \"\"\"This{{#foo}}\n        {{bar}}\n        {{#baz.qux}}\n            {{#barfoo}}\n                {{foobar}}\n            {{/barfoo}}\n            {{foobar}}\n        {{/baz.qux}}\n        {{quux}}\n    {{/foo}}is a test.\"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(\n        foo={\n            \"bar\": \"yo\",\n            \"baz\": {\n                \"qux\": [\n                    {\"foobar\": \"wassup\"},\n                    {\"foobar\": \"yoyo\", \"barfoo\": {\"foobar\": \"hello there\"}},\n                ]\n            },\n            \"quux\": \"hello\",\n        }\n    ) == (\n        \"\"\"This\n        yo\n            wassup\n                hello there\n            yoyo\n        hello\n    is a test.\"\"\"\n    )\n    assert prompt.input_variables == [\"foo\"]\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(prompt.get_input_jsonschema()) == snapshot(\n            name=\"schema_4\"\n        )\n\n    # section/context variables with repeats\n    template = \"\"\"This{{#foo}}\n        {{bar}}\n    {{/foo}}is a test.\"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format(foo=[{\"bar\": \"yo\"}, {\"bar\": \"hello\"}]) == (\n        \"\"\"This\n        yo\n    \n        hello\n    is a test.\"\"\"  # noqa: W293\n    )\n    assert prompt.input_variables == [\"foo\"]\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(prompt.get_input_jsonschema()) == snapshot(\n            name=\"schema_5\"\n        )\n    template = \"\"\"This{{^foo}}\n        no foos\n    {{/foo}}is a test.\"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.format() == (\n        \"\"\"This\n        no foos\n    is a test.\"\"\"\n    )\n    assert prompt.input_variables == [\"foo\"]\n    assert prompt.get_input_jsonschema() == {\n        \"properties\": {\"foo\": {\"default\": None, \"title\": \"Foo\", \"type\": \"object\"}},\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n    }\n\n\ndef test_prompt_from_template_with_partial_variables() -> None:\n    \"\"\"Test prompts can be constructed from a template with partial variables.\"\"\"\n    # given\n    template = \"This is a {foo} test {bar}.\"\n    partial_variables = {\"bar\": \"baz\"}\n    # when\n    prompt = PromptTemplate.from_template(template, partial_variables=partial_variables)\n    # then\n    expected_prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"foo\"],\n        partial_variables=partial_variables,\n    )\n    assert prompt == expected_prompt\n\n\ndef test_prompt_missing_input_variables() -> None:\n    \"\"\"Test error is raised when input variables are not provided.\"\"\"\n    template = \"This is a {foo} test.\"\n    input_variables: list = []\n    with pytest.raises(ValueError):\n        PromptTemplate(\n            input_variables=input_variables, template=template, validate_template=True\n        )\n    assert PromptTemplate(\n        input_variables=input_variables, template=template\n    ).input_variables == [\"foo\"]\n\n\ndef test_prompt_empty_input_variable() -> None:\n    \"\"\"Test error is raised when empty string input variable.\"\"\"\n    with pytest.raises(ValueError):\n        PromptTemplate(input_variables=[\"\"], template=\"{}\", validate_template=True)\n\n\ndef test_prompt_wrong_input_variables() -> None:\n    \"\"\"Test error is raised when name of input variable is wrong.\"\"\"\n    template = \"This is a {foo} test.\"\n    input_variables = [\"bar\"]\n    with pytest.raises(ValueError):\n        PromptTemplate(\n            input_variables=input_variables, template=template, validate_template=True\n        )\n    assert PromptTemplate(\n        input_variables=input_variables, template=template\n    ).input_variables == [\"foo\"]\n\n\ndef test_prompt_from_examples_valid() -> None:\n    \"\"\"Test prompt can be successfully constructed from examples.\"\"\"\n    template = \"\"\"Test Prompt:\n\nQuestion: who are you?\nAnswer: foo\n\nQuestion: what are you?\nAnswer: bar\n\nQuestion: {question}\nAnswer:\"\"\"\n    input_variables = [\"question\"]\n    example_separator = \"\\n\\n\"\n    prefix = \"\"\"Test Prompt:\"\"\"\n    suffix = \"\"\"Question: {question}\\nAnswer:\"\"\"\n    examples = [\n        \"\"\"Question: who are you?\\nAnswer: foo\"\"\",\n        \"\"\"Question: what are you?\\nAnswer: bar\"\"\",\n    ]\n    prompt_from_examples = PromptTemplate.from_examples(\n        examples,\n        suffix,\n        input_variables,\n        example_separator=example_separator,\n        prefix=prefix,\n    )\n    prompt_from_template = PromptTemplate(\n        input_variables=input_variables, template=template\n    )\n    assert prompt_from_examples.template == prompt_from_template.template\n    assert prompt_from_examples.input_variables == prompt_from_template.input_variables\n\n\ndef test_prompt_invalid_template_format() -> None:\n    \"\"\"Test initializing a prompt with invalid template format.\"\"\"\n    template = \"This is a {foo} test.\"\n    input_variables = [\"foo\"]\n    with pytest.raises(ValueError):\n        PromptTemplate(\n            input_variables=input_variables,\n            template=template,\n            template_format=\"bar\",  # type: ignore[arg-type]\n        )\n\n\ndef test_prompt_from_file() -> None:\n    \"\"\"Test prompt can be successfully constructed from a file.\"\"\"\n    template_file = \"tests/unit_tests/data/prompt_file.txt\"\n    input_variables = [\"question\"]\n    prompt = PromptTemplate.from_file(template_file, input_variables)\n    assert prompt.template == \"Question: {question}\\nAnswer:\"\n\n\ndef test_prompt_from_file_with_partial_variables() -> None:\n    \"\"\"Test prompt can be successfully constructed from a file\n    with partial variables.\n    \"\"\"\n    # given\n    template = \"This is a {foo} test {bar}.\"\n    partial_variables = {\"bar\": \"baz\"}\n    # when\n    with mock.patch(\"builtins.open\", mock.mock_open(read_data=template)):\n        prompt = PromptTemplate.from_file(\n            \"mock_file_name\", partial_variables=partial_variables\n        )\n    # then\n    expected_prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"foo\"],\n        partial_variables=partial_variables,\n    )\n    assert prompt == expected_prompt\n\n\ndef test_partial_init_string() -> None:\n    \"\"\"Test prompt can be initialized with partial variables.\"\"\"\n    template = \"This is a {foo} test.\"\n    prompt = PromptTemplate(\n        input_variables=[], template=template, partial_variables={\"foo\": 1}\n    )\n    assert prompt.template == template\n    assert prompt.input_variables == []\n    result = prompt.format()\n    assert result == \"This is a 1 test.\"\n\n\ndef test_partial_init_func() -> None:\n    \"\"\"Test prompt can be initialized with partial variables.\"\"\"\n    template = \"This is a {foo} test.\"\n    prompt = PromptTemplate(\n        input_variables=[], template=template, partial_variables={\"foo\": lambda: 2}\n    )\n    assert prompt.template == template\n    assert prompt.input_variables == []\n    result = prompt.format()\n    assert result == \"This is a 2 test.\"\n\n\ndef test_partial() -> None:\n    \"\"\"Test prompt can be partialed.\"\"\"\n    template = \"This is a {foo} test.\"\n    prompt = PromptTemplate(input_variables=[\"foo\"], template=template)\n    assert prompt.template == template\n    assert prompt.input_variables == [\"foo\"]\n    new_prompt = prompt.partial(foo=\"3\")\n    new_result = new_prompt.format()\n    assert new_result == \"This is a 3 test.\"\n    result = prompt.format(foo=\"foo\")\n    assert result == \"This is a foo test.\"\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_prompt_from_jinja2_template() -> None:\n    \"\"\"Test prompts can be constructed from a jinja2 template.\"\"\"\n    # Empty input variable.\n    template = \"\"\"Hello there\nThere is no variable here {\nWill it get confused{ }?\n    \"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"jinja2\")\n    expected_prompt = PromptTemplate(\n        template=template, input_variables=[], template_format=\"jinja2\"\n    )\n    assert prompt == expected_prompt\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_basic_sandboxing_with_jinja2() -> None:\n    \"\"\"Test basic sandboxing with jinja2.\"\"\"\n    import jinja2\n\n    template = \" {{''.__class__.__bases__[0] }} \"  # malicious code\n    prompt = PromptTemplate.from_template(template, template_format=\"jinja2\")\n    with pytest.raises(jinja2.exceptions.SecurityError):\n        assert prompt.format() == []\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_prompt_from_jinja2_template_multiple_inputs() -> None:\n    \"\"\"Test with multiple input variables.\"\"\"\n    # Multiple input variables.\n    template = \"\"\"\\\nHello world\n\nYour variable: {{ foo }}\n\n{# This will not get rendered #}\n\n{% if bar %}\nYou just set bar boolean variable to true\n{% endif %}\n\n{% for i in foo_list %}\n{{ i }}\n{% endfor %}\n\"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"jinja2\")\n    expected_prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"bar\", \"foo\", \"foo_list\"],\n        template_format=\"jinja2\",\n    )\n\n    assert prompt == expected_prompt\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_prompt_from_jinja2_template_multiple_inputs_with_repeats() -> None:\n    \"\"\"Test with multiple input variables and repeats.\"\"\"\n    template = \"\"\"\\\nHello world\n\nYour variable: {{ foo }}\n\n{# This will not get rendered #}\n\n{% if bar %}\nYou just set bar boolean variable to true\n{% endif %}\n\n{% for i in foo_list %}\n{{ i }}\n{% endfor %}\n\n{% if bar %}\nYour variable again: {{ foo }}\n{% endif %}\n\"\"\"\n    prompt = PromptTemplate.from_template(template, template_format=\"jinja2\")\n    expected_prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"bar\", \"foo\", \"foo_list\"],\n        template_format=\"jinja2\",\n    )\n    assert prompt == expected_prompt\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_prompt_jinja2_missing_input_variables() -> None:\n    \"\"\"Test error is raised when input variables are not provided.\"\"\"\n    template = \"This is a {{ foo }} test.\"\n    input_variables: list = []\n    with pytest.warns(UserWarning):\n        PromptTemplate(\n            input_variables=input_variables,\n            template=template,\n            template_format=\"jinja2\",\n            validate_template=True,\n        )\n    assert PromptTemplate(\n        input_variables=input_variables, template=template, template_format=\"jinja2\"\n    ).input_variables == [\"foo\"]\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_prompt_jinja2_extra_input_variables() -> None:\n    \"\"\"Test error is raised when there are too many input variables.\"\"\"\n    template = \"This is a {{ foo }} test.\"\n    input_variables = [\"foo\", \"bar\"]\n    with pytest.warns(UserWarning):\n        PromptTemplate(\n            input_variables=input_variables,\n            template=template,\n            template_format=\"jinja2\",\n            validate_template=True,\n        )\n    assert PromptTemplate(\n        input_variables=input_variables, template=template, template_format=\"jinja2\"\n    ).input_variables == [\"foo\"]\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_prompt_jinja2_wrong_input_variables() -> None:\n    \"\"\"Test error is raised when name of input variable is wrong.\"\"\"\n    template = \"This is a {{ foo }} test.\"\n    input_variables = [\"bar\"]\n    with pytest.warns(UserWarning):\n        PromptTemplate(\n            input_variables=input_variables,\n            template=template,\n            template_format=\"jinja2\",\n            validate_template=True,\n        )\n    assert PromptTemplate(\n        input_variables=input_variables, template=template, template_format=\"jinja2\"\n    ).input_variables == [\"foo\"]\n\n\ndef test_prompt_invoke_with_metadata() -> None:\n    \"\"\"Test prompt can be invoked with metadata.\"\"\"\n    template = \"This is a {foo} test.\"\n    prompt = PromptTemplate(\n        input_variables=[\"foo\"],\n        template=template,\n        metadata={\"version\": \"1\"},\n        tags=[\"tag1\", \"tag2\"],\n    )\n    tracer = RunCollectorCallbackHandler()\n    result = prompt.invoke(\n        {\"foo\": \"bar\"}, {\"metadata\": {\"foo\": \"bar\"}, \"callbacks\": [tracer]}\n    )\n    assert result.to_string() == \"This is a bar test.\"\n    assert len(tracer.traced_runs) == 1\n    assert tracer.traced_runs[0].extra[\"metadata\"] == {\"version\": \"1\", \"foo\": \"bar\"}  # type: ignore\n    assert tracer.traced_runs[0].tags == [\"tag1\", \"tag2\"]  # type: ignore\n\n\nasync def test_prompt_ainvoke_with_metadata() -> None:\n    \"\"\"Test prompt can be invoked with metadata.\"\"\"\n    template = \"This is a {foo} test.\"\n    prompt = PromptTemplate(\n        input_variables=[\"foo\"],\n        template=template,\n        metadata={\"version\": \"1\"},\n        tags=[\"tag1\", \"tag2\"],\n    )\n    tracer = RunCollectorCallbackHandler()\n    result = await prompt.ainvoke(\n        {\"foo\": \"bar\"}, {\"metadata\": {\"foo\": \"bar\"}, \"callbacks\": [tracer]}\n    )\n    assert result.to_string() == \"This is a bar test.\"\n    assert len(tracer.traced_runs) == 1\n    assert tracer.traced_runs[0].extra[\"metadata\"] == {\"version\": \"1\", \"foo\": \"bar\"}  # type: ignore\n    assert tracer.traced_runs[0].tags == [\"tag1\", \"tag2\"]  # type: ignore\n\n\n@pytest.mark.parametrize(\n    \"value, expected\",\n    [\n        (\"0\", \"0\"),\n        (0, \"0\"),\n        (0.0, \"0.0\"),\n        (False, \"False\"),\n        (\"\", \"\"),\n        (\n            None,\n            {\n                \"mustache\": \"\",\n                \"f-string\": \"None\",\n            },\n        ),\n        (\n            [],\n            {\n                \"mustache\": \"\",\n                \"f-string\": \"[]\",\n            },\n        ),\n        (\n            {},\n            {\n                \"mustache\": \"\",\n                \"f-string\": \"{}\",\n            },\n        ),\n    ],\n)\n@pytest.mark.parametrize(\"template_format\", [\"f-string\", \"mustache\"])\ndef test_prompt_falsy_vars(\n    template_format: PromptTemplateFormat,\n    value: Any,\n    expected: Union[str, dict[str, str]],\n) -> None:\n    # each line is value, f-string, mustache\n    if template_format == \"f-string\":\n        template = \"{my_var}\"\n    elif template_format == \"mustache\":\n        template = \"{{my_var}}\"\n    else:\n        msg = f\"Invalid template format: {template_format}\"\n        raise ValueError(msg)\n\n    prompt = PromptTemplate.from_template(template, template_format=template_format)\n\n    result = prompt.invoke({\"my_var\": value})\n\n    expected_output = (\n        expected if not isinstance(expected, dict) else expected[template_format]\n    )\n    assert result.to_string() == expected_output\n\n\ndef test_prompt_missing_vars_error() -> None:\n    prompt = PromptTemplate.from_template(\"This is a {foo} {goingtobemissing} test.\")\n    with pytest.raises(KeyError) as e:\n        prompt.invoke({\"foo\": \"bar\"})\n\n    # Check that the error message contains the missing variable\n    assert \"{'goingtobemissing'}\" in str(e.value.args[0])\n\n    # Check helper text has right number of braces\n    assert \"'{{goingtobemissing}}'\" in str(e.value.args[0])\n\n\ndef test_prompt_with_template_variable_name_fstring() -> None:\n    template = \"This is a {template} test.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"f-string\")\n    assert prompt.invoke({\"template\": \"bar\"}).to_string() == \"This is a bar test.\"\n\n\ndef test_prompt_with_template_variable_name_mustache() -> None:\n    template = \"This is a {{template}} test.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"mustache\")\n    assert prompt.invoke({\"template\": \"bar\"}).to_string() == \"This is a bar test.\"\n\n\n@pytest.mark.requires(\"jinja2\")\ndef test_prompt_with_template_variable_name_jinja2() -> None:\n    template = \"This is a {{template}} test.\"\n    prompt = PromptTemplate.from_template(template, template_format=\"jinja2\")\n    assert prompt.invoke({\"template\": \"bar\"}).to_string() == \"This is a bar test.\"\n",
        "patch": "@@ -354,7 +354,7 @@ def test_prompt_from_file_with_partial_variables() -> None:\n     template = \"This is a {foo} test {bar}.\"\n     partial_variables = {\"bar\": \"baz\"}\n     # when\n-    with mock.patch(\"builtins.open\", mock.mock_open(read_data=template)):\n+    with mock.patch(\"pathlib.Path.open\", mock.mock_open(read_data=template)):\n         prompt = PromptTemplate.from_file(\n             \"mock_file_name\", partial_variables=partial_variables\n         )"
      },
      {
        "filename": "libs/core/tests/unit_tests/test_imports.py",
        "content_before": "import concurrent.futures\nimport glob\nimport importlib\nimport subprocess\nfrom pathlib import Path\n\n\ndef test_importable_all() -> None:\n    for path in glob.glob(\"../core/langchain_core/*\"):\n        relative_path = Path(path).parts[-1]\n        if relative_path.endswith(\".typed\"):\n            continue\n        module_name = relative_path.split(\".\")[0]\n        module = importlib.import_module(\"langchain_core.\" + module_name)\n        all_ = getattr(module, \"__all__\", [])\n        for cls_ in all_:\n            getattr(module, cls_)\n\n\ndef try_to_import(module_name: str) -> tuple[int, str]:\n    \"\"\"Try to import a module via subprocess.\"\"\"\n    module = importlib.import_module(\"langchain_core.\" + module_name)\n    all_ = getattr(module, \"__all__\", [])\n    for cls_ in all_:\n        getattr(module, cls_)\n\n    result = subprocess.run(\n        [\"python\", \"-c\", f\"import langchain_core.{module_name}\"],\n    )\n    return result.returncode, module_name\n\n\ndef test_importable_all_via_subprocess() -> None:\n    \"\"\"Test import in isolation.\n\n    Note: ImportErrors due to circular imports can be raised\n          for one sequence of imports but not another.\n    \"\"\"\n    module_names = []\n    for path in glob.glob(\"../core/langchain_core/*\"):\n        relative_path = Path(path).parts[-1]\n        if relative_path.endswith(\".typed\"):\n            continue\n        module_name = relative_path.split(\".\")[0]\n        module_names.append(module_name)\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        futures = [\n            executor.submit(try_to_import, module_name) for module_name in module_names\n        ]\n        for future in concurrent.futures.as_completed(futures):\n            result = future.result()  # Will raise an exception if the callable raised\n            code, module_name = result\n            if code != 0:\n                msg = f\"Failed to import {module_name}.\"\n                raise ValueError(msg)\n",
        "patch": "@@ -1,20 +1,17 @@\n import concurrent.futures\n-import glob\n import importlib\n import subprocess\n from pathlib import Path\n \n \n def test_importable_all() -> None:\n-    for path in glob.glob(\"../core/langchain_core/*\"):\n-        relative_path = Path(path).parts[-1]\n-        if relative_path.endswith(\".typed\"):\n-            continue\n-        module_name = relative_path.split(\".\")[0]\n-        module = importlib.import_module(\"langchain_core.\" + module_name)\n-        all_ = getattr(module, \"__all__\", [])\n-        for cls_ in all_:\n-            getattr(module, cls_)\n+    for path in Path(\"../core/langchain_core/\").glob(\"*\"):\n+        module_name = path.stem\n+        if not module_name.startswith(\".\") and path.suffix != \".typed\":\n+            module = importlib.import_module(\"langchain_core.\" + module_name)\n+            all_ = getattr(module, \"__all__\", [])\n+            for cls_ in all_:\n+                getattr(module, cls_)\n \n \n def try_to_import(module_name: str) -> tuple[int, str]:\n@@ -37,12 +34,10 @@ def test_importable_all_via_subprocess() -> None:\n           for one sequence of imports but not another.\n     \"\"\"\n     module_names = []\n-    for path in glob.glob(\"../core/langchain_core/*\"):\n-        relative_path = Path(path).parts[-1]\n-        if relative_path.endswith(\".typed\"):\n-            continue\n-        module_name = relative_path.split(\".\")[0]\n-        module_names.append(module_name)\n+    for path in Path(\"../core/langchain_core/\").glob(\"*\"):\n+        module_name = path.stem\n+        if not module_name.startswith(\".\") and path.suffix != \".typed\":\n+            module_names.append(module_name)\n \n     with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n         futures = ["
      }
    ]
  },
  {
    "number": 30048,
    "title": "anthropic, mistral: return `model_name` in response metadata",
    "body": "Took a \"census\" of models supported by init_chat_model-- of those that return model names in response metadata, these were the only two that had it keyed under `\"model\"` instead of `\"model_name\"`.",
    "issue_title": "anthropic, mistral: return `model_name` in response metadata",
    "issue_body": "Took a \"census\" of models supported by init_chat_model-- of those that return model names in response metadata, these were the only two that had it keyed under `\"model\"` instead of `\"model_name\"`.",
    "files": [
      {
        "filename": "libs/partners/anthropic/langchain_anthropic/chat_models.py",
        "content_before": "import copy\nimport re\nimport warnings\nfrom functools import cached_property\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport anthropic\nfrom langchain_core._api import beta, deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.ai import InputTokenDetails, UsageMetadata\nfrom langchain_core.messages.tool import tool_call_chunk as create_tool_call_chunk\nfrom langchain_core.output_parsers import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n)\nfrom langchain_core.output_parsers.base import OutputParserLike\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableMap,\n    RunnablePassthrough,\n)\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import (\n    from_env,\n    get_pydantic_field_names,\n    secret_from_env,\n)\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import is_basemodel_subclass\nfrom langchain_core.utils.utils import _build_model_kwargs\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SecretStr,\n    model_validator,\n)\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom langchain_anthropic.output_parsers import extract_tool_calls\n\n_message_type_lookups = {\n    \"human\": \"user\",\n    \"ai\": \"assistant\",\n    \"AIMessageChunk\": \"assistant\",\n    \"HumanMessageChunk\": \"user\",\n}\n\n\ndef _format_image(image_url: str) -> Dict:\n    \"\"\"\n    Formats an image of format data:image/jpeg;base64,{b64_string}\n    to a dict for anthropic api\n\n    {\n      \"type\": \"base64\",\n      \"media_type\": \"image/jpeg\",\n      \"data\": \"/9j/4AAQSkZJRg...\",\n    }\n\n    And throws an error if it's not a b64 image\n    \"\"\"\n    regex = r\"^data:(?P<media_type>image/.+);base64,(?P<data>.+)$\"\n    match = re.match(regex, image_url)\n    if match is None:\n        raise ValueError(\n            \"Anthropic only supports base64-encoded images currently.\"\n            \" Example: data:image/png;base64,'/9j/4AAQSk'...\"\n        )\n    return {\n        \"type\": \"base64\",\n        \"media_type\": match.group(\"media_type\"),\n        \"data\": match.group(\"data\"),\n    }\n\n\ndef _merge_messages(\n    messages: Sequence[BaseMessage],\n) -> List[Union[SystemMessage, AIMessage, HumanMessage]]:\n    \"\"\"Merge runs of human/tool messages into single human messages with content blocks.\"\"\"  # noqa: E501\n    merged: list = []\n    for curr in messages:\n        if isinstance(curr, ToolMessage):\n            if (\n                isinstance(curr.content, list)\n                and curr.content\n                and all(\n                    isinstance(block, dict) and block.get(\"type\") == \"tool_result\"\n                    for block in curr.content\n                )\n            ):\n                curr = HumanMessage(curr.content)  # type: ignore[misc]\n            else:\n                curr = HumanMessage(  # type: ignore[misc]\n                    [\n                        {\n                            \"type\": \"tool_result\",\n                            \"content\": curr.content,\n                            \"tool_use_id\": curr.tool_call_id,\n                            \"is_error\": curr.status == \"error\",\n                        }\n                    ]\n                )\n        last = merged[-1] if merged else None\n        if any(\n            all(isinstance(m, c) for m in (curr, last))\n            for c in (SystemMessage, HumanMessage)\n        ):\n            if isinstance(cast(BaseMessage, last).content, str):\n                new_content: List = [\n                    {\"type\": \"text\", \"text\": cast(BaseMessage, last).content}\n                ]\n            else:\n                new_content = copy.copy(cast(list, cast(BaseMessage, last).content))\n            if isinstance(curr.content, str):\n                new_content.append({\"type\": \"text\", \"text\": curr.content})\n            else:\n                new_content.extend(curr.content)\n            merged[-1] = curr.model_copy(update={\"content\": new_content})\n        else:\n            merged.append(curr)\n    return merged\n\n\ndef _format_messages(\n    messages: List[BaseMessage],\n) -> Tuple[Union[str, List[Dict], None], List[Dict]]:\n    \"\"\"Format messages for anthropic.\"\"\"\n\n    \"\"\"\n    [\n                {\n                    \"role\": _message_type_lookups[m.type],\n                    \"content\": [_AnthropicMessageContent(text=m.content).model_dump()],\n                }\n                for m in messages\n            ]\n    \"\"\"\n    system: Union[str, List[Dict], None] = None\n    formatted_messages: List[Dict] = []\n\n    merged_messages = _merge_messages(messages)\n    for i, message in enumerate(merged_messages):\n        if message.type == \"system\":\n            if system is not None:\n                raise ValueError(\"Received multiple non-consecutive system messages.\")\n            elif isinstance(message.content, list):\n                system = [\n                    (\n                        block\n                        if isinstance(block, dict)\n                        else {\"type\": \"text\", \"text\": block}\n                    )\n                    for block in message.content\n                ]\n            else:\n                system = message.content\n            continue\n\n        role = _message_type_lookups[message.type]\n        content: Union[str, List]\n\n        if not isinstance(message.content, str):\n            # parse as dict\n            assert isinstance(\n                message.content, list\n            ), \"Anthropic message content must be str or list of dicts\"\n\n            # populate content\n            content = []\n            for block in message.content:\n                if isinstance(block, str):\n                    content.append({\"type\": \"text\", \"text\": block})\n                elif isinstance(block, dict):\n                    if \"type\" not in block:\n                        raise ValueError(\"Dict content block must have a type key\")\n                    elif block[\"type\"] == \"image_url\":\n                        # convert format\n                        source = _format_image(block[\"image_url\"][\"url\"])\n                        content.append({\"type\": \"image\", \"source\": source})\n                    elif block[\"type\"] == \"tool_use\":\n                        # If a tool_call with the same id as a tool_use content block\n                        # exists, the tool_call is preferred.\n                        if isinstance(message, AIMessage) and block[\"id\"] in [\n                            tc[\"id\"] for tc in message.tool_calls\n                        ]:\n                            overlapping = [\n                                tc\n                                for tc in message.tool_calls\n                                if tc[\"id\"] == block[\"id\"]\n                            ]\n                            content.extend(\n                                _lc_tool_calls_to_anthropic_tool_use_blocks(overlapping)\n                            )\n                        else:\n                            block.pop(\"text\", None)\n                            content.append(block)\n                    elif block[\"type\"] == \"text\":\n                        text = block.get(\"text\", \"\")\n                        # Only add non-empty strings for now as empty ones are not\n                        # accepted.\n                        # https://github.com/anthropics/anthropic-sdk-python/issues/461\n                        if text.strip():\n                            content.append(\n                                {\n                                    k: v\n                                    for k, v in block.items()\n                                    if k in (\"type\", \"text\", \"cache_control\")\n                                }\n                            )\n                    elif block[\"type\"] == \"thinking\":\n                        content.append(\n                            {\n                                k: v\n                                for k, v in block.items()\n                                if k\n                                in (\"type\", \"thinking\", \"cache_control\", \"signature\")\n                            }\n                        )\n                    elif block[\"type\"] == \"redacted_thinking\":\n                        content.append(\n                            {\n                                k: v\n                                for k, v in block.items()\n                                if k in (\"type\", \"cache_control\", \"data\")\n                            }\n                        )\n                    elif block[\"type\"] == \"tool_result\":\n                        tool_content = _format_messages(\n                            [HumanMessage(block[\"content\"])]\n                        )[1][0][\"content\"]\n                        content.append({**block, **{\"content\": tool_content}})\n                    else:\n                        content.append(block)\n                else:\n                    raise ValueError(\n                        f\"Content blocks must be str or dict, instead was: \"\n                        f\"{type(block)}\"\n                    )\n        else:\n            content = message.content\n\n        # Ensure all tool_calls have a tool_use content block\n        if isinstance(message, AIMessage) and message.tool_calls:\n            content = content or []\n            content = (\n                [{\"type\": \"text\", \"text\": message.content}]\n                if isinstance(content, str) and content\n                else content\n            )\n            tool_use_ids = [\n                cast(dict, block)[\"id\"]\n                for block in content\n                if cast(dict, block)[\"type\"] == \"tool_use\"\n            ]\n            missing_tool_calls = [\n                tc for tc in message.tool_calls if tc[\"id\"] not in tool_use_ids\n            ]\n            cast(list, content).extend(\n                _lc_tool_calls_to_anthropic_tool_use_blocks(missing_tool_calls)\n            )\n\n        formatted_messages.append({\"role\": role, \"content\": content})\n    return system, formatted_messages\n\n\nclass ChatAnthropic(BaseChatModel):\n    \"\"\"Anthropic chat models.\n\n    See https://docs.anthropic.com/en/docs/models-overview for a list of the latest models.\n\n    Setup:\n        Install ``langchain-anthropic`` and set environment variable ``ANTHROPIC_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-anthropic\n            export ANTHROPIC_API_KEY=\"your-api-key\"\n\n    Key init args \u2014 completion params:\n        model: str\n            Name of Anthropic model to use. E.g. \"claude-3-sonnet-20240229\".\n        temperature: float\n            Sampling temperature. Ranges from 0.0 to 1.0.\n        max_tokens: int\n            Max number of tokens to generate.\n\n    Key init args \u2014 client params:\n        timeout: Optional[float]\n            Timeout for requests.\n        max_retries: int\n            Max number of retries if a request fails.\n        api_key: Optional[str]\n            Anthropic API key. If not passed in will be read from env var ANTHROPIC_API_KEY.\n        base_url: Optional[str]\n            Base URL for API requests. Only specify if using a proxy or service\n            emulator.\n\n    See full list of supported init args and their descriptions in the params section.\n\n    Instantiate:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(\n                model=\"claude-3-sonnet-20240229\",\n                temperature=0,\n                max_tokens=1024,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # base_url=\"...\",\n                # other params...\n            )\n\n    **NOTE**: Any param which is not explicitly supported will be passed directly to the\n    ``anthropic.Anthropic.messages.create(...)`` API every time to the model is\n    invoked. For example:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n            import anthropic\n\n            ChatAnthropic(..., extra_headers={}).invoke(...)\n\n            # results in underlying API call of:\n\n            anthropic.Anthropic(..).messages.create(..., extra_headers={})\n\n            # which is also equivalent to:\n\n            ChatAnthropic(...).invoke(..., extra_headers={})\n\n    Invoke:\n        .. code-block:: python\n\n            messages = [\n                (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Stream:\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            AIMessageChunk(content='J', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=\"'\", id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='a', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ime', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' la', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' programm', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ation', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='.', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n        .. code-block:: python\n\n            AIMessageChunk(content=\"J'aime la programmation.\", id='run-b34faef0-882f-4869-a19c-ed2b856e6361')\n\n    Async:\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Tool calling:\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n            ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [{'name': 'GetWeather',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01KzpPEAgzura7hpBqwHbWdo'},\n             {'name': 'GetWeather',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JtgbVGVJbiSwtZk3Uycezx'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01429aygngesudV9nTbCKGuw'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JPktyd44tVMeBcPPnFSEJG'}]\n\n        See ``ChatAnthropic.bind_tools()`` method for more.\n\n    Structured output:\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        .. code-block:: python\n\n            Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)\n\n        See ``ChatAnthropic.with_structured_output()`` for more.\n\n    Image input:\n        .. code-block:: python\n\n            import base64\n            import httpx\n            from langchain_core.messages import HumanMessage\n\n            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n            message = HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                    },\n                ],\n            )\n            ai_msg = llm.invoke([message])\n            ai_msg.content\n\n        .. code-block:: python\n\n            \"The image depicts a sunny day with a partly cloudy sky. The sky is a brilliant blue color with scattered white clouds drifting across. The lighting and cloud patterns suggest pleasant, mild weather conditions. The scene shows a grassy field or meadow with a wooden boardwalk trail leading through it, indicating an outdoor setting on a nice day well-suited for enjoying nature.\"\n\n    Extended thinking:\n        Claude 3.7 Sonnet supports an\n        `extended thinking <https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking>`_\n        feature, which will output the step-by-step reasoning process that led to its\n        final answer.\n\n        To use it, specify the `thinking` parameter when initializing `ChatAnthropic`.\n        It can also be passed in as a kwarg during invocation.\n\n        You will need to specify a token budget to use this feature. See usage example:\n\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(\n                model=\"claude-3-7-sonnet-latest\",\n                max_tokens=5000,\n                thinking={\"type\": \"enabled\", \"budget_tokens\": 2000},\n            )\n\n            response = llm.invoke(\"What is the cube root of 50.653?\")\n            response.content\n\n        .. code-block:: python\n\n            [{'signature': '...', 'thinking': \"To find the cube root of 50.653...\", 'type': 'thinking'}, {'text': 'The cube root of 50.653 is ...', 'type': 'text'}]\n\n    Citations:\n        Anthropic supports a\n        `citations <https://docs.anthropic.com/en/docs/build-with-claude/citations>`_\n        feature that lets Claude attach context to its answers based on source\n        documents supplied by the user. When\n        `document content blocks <https://docs.anthropic.com/en/docs/build-with-claude/citations#document-types>`_\n        with ``\"citations\": {\"enabled\": True}`` are included in a query, Claude may\n        generate citations in its response.\n\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"document\",\n                            \"source\": {\n                                \"type\": \"text\",\n                                \"media_type\": \"text/plain\",\n                                \"data\": \"The grass is green. The sky is blue.\",\n                            },\n                            \"title\": \"My Document\",\n                            \"context\": \"This is a trustworthy document.\",\n                            \"citations\": {\"enabled\": True},\n                        },\n                        {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n                    ],\n                }\n            ]\n            response = llm.invoke(messages)\n            response.content\n\n        .. code-block:: python\n\n            [{'text': 'Based on the document, ', 'type': 'text'},\n            {'text': 'the grass is green',\n            'type': 'text',\n            'citations': [{'type': 'char_location',\n                'cited_text': 'The grass is green. ',\n                'document_index': 0,\n                'document_title': 'My Document',\n                'start_char_index': 0,\n                'end_char_index': 20}]},\n            {'text': ', and ', 'type': 'text'},\n            {'text': 'the sky is blue',\n            'type': 'text',\n            'citations': [{'type': 'char_location',\n                'cited_text': 'The sky is blue.',\n                'document_index': 0,\n                'document_title': 'My Document',\n                'start_char_index': 20,\n                'end_char_index': 36}]},\n            {'text': '.', 'type': 'text'}]\n\n    Token usage:\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        Message chunks containing token usage will be included during streaming by\n        default:\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        These can be disabled by setting ``stream_usage=False`` in the stream method,\n        or by setting ``stream_usage=False`` when initializing ChatAnthropic.\n\n    Response metadata\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n        .. code-block:: python\n\n            {'id': 'msg_013xU6FHEGEq76aP4RgFerVT',\n             'model': 'claude-3-sonnet-20240229',\n             'stop_reason': 'end_turn',\n             'stop_sequence': None,\n             'usage': {'input_tokens': 25, 'output_tokens': 11}}\n\n    \"\"\"  # noqa: E501\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n    )\n\n    model: str = Field(alias=\"model_name\")\n    \"\"\"Model name to use.\"\"\"\n\n    max_tokens: int = Field(default=1024, alias=\"max_tokens_to_sample\")\n    \"\"\"Denotes the number of tokens to predict per generation.\"\"\"\n\n    temperature: Optional[float] = None\n    \"\"\"A non-negative float that tunes the degree of randomness in generation.\"\"\"\n\n    top_k: Optional[int] = None\n    \"\"\"Number of most likely tokens to consider at each step.\"\"\"\n\n    top_p: Optional[float] = None\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n\n    default_request_timeout: Optional[float] = Field(None, alias=\"timeout\")\n    \"\"\"Timeout for requests to Anthropic Completion API.\"\"\"\n\n    # sdk default = 2: https://github.com/anthropics/anthropic-sdk-python?tab=readme-ov-file#retries\n    max_retries: int = 2\n    \"\"\"Number of retries allowed for requests sent to the Anthropic Completion API.\"\"\"\n\n    stop_sequences: Optional[List[str]] = Field(None, alias=\"stop\")\n    \"\"\"Default stop sequences.\"\"\"\n\n    anthropic_api_url: Optional[str] = Field(\n        alias=\"base_url\",\n        default_factory=from_env(\n            [\"ANTHROPIC_API_URL\", \"ANTHROPIC_BASE_URL\"],\n            default=\"https://api.anthropic.com\",\n        ),\n    )\n    \"\"\"Base URL for API requests. Only specify if using a proxy or service emulator.\n\n    If a value isn't passed in, will attempt to read the value first from\n    ANTHROPIC_API_URL and if that is not set, ANTHROPIC_BASE_URL.\n    If neither are set, the default value of 'https://api.anthropic.com' will\n    be used.\n    \"\"\"\n\n    anthropic_api_key: SecretStr = Field(\n        alias=\"api_key\",\n        default_factory=secret_from_env(\"ANTHROPIC_API_KEY\", default=\"\"),\n    )\n\n    \"\"\"Automatically read from env var `ANTHROPIC_API_KEY` if not provided.\"\"\"\n\n    default_headers: Optional[Mapping[str, str]] = None\n    \"\"\"Headers to pass to the Anthropic clients, will be used for every API call.\"\"\"\n\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n\n    streaming: bool = False\n    \"\"\"Whether to use streaming or not.\"\"\"\n\n    stream_usage: bool = True\n    \"\"\"Whether to include usage metadata in streaming output. If True, additional\n    message chunks will be generated during the stream including usage metadata.\n    \"\"\"\n\n    thinking: Optional[Dict[str, Any]] = Field(default=None)\n    \"\"\"Parameters for Claude reasoning,\n    e.g., ``{\"type\": \"enabled\", \"budget_tokens\": 10_000}``\"\"\"\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"anthropic-chat\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"anthropic_api_key\": \"ANTHROPIC_API_KEY\"}\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"anthropic\"]\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"model_kwargs\": self.model_kwargs,\n            \"streaming\": self.streaming,\n            \"max_retries\": self.max_retries,\n            \"default_request_timeout\": self.default_request_timeout,\n            \"thinking\": self.thinking,\n        }\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"anthropic\",\n            ls_model_name=self.model,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict) -> Any:\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @cached_property\n    def _client_params(self) -> Dict[str, Any]:\n        client_params: Dict[str, Any] = {\n            \"api_key\": self.anthropic_api_key.get_secret_value(),\n            \"base_url\": self.anthropic_api_url,\n            \"max_retries\": self.max_retries,\n            \"default_headers\": (self.default_headers or None),\n        }\n        # value <= 0 indicates the param should be ignored. None is a meaningful value\n        # for Anthropic client and treated differently than not specifying the param at\n        # all.\n        if self.default_request_timeout is None or self.default_request_timeout > 0:\n            client_params[\"timeout\"] = self.default_request_timeout\n\n        return client_params\n\n    @cached_property\n    def _client(self) -> anthropic.Client:\n        return anthropic.Client(**self._client_params)\n\n    @cached_property\n    def _async_client(self) -> anthropic.AsyncClient:\n        return anthropic.AsyncClient(**self._client_params)\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Dict,\n    ) -> Dict:\n        messages = self._convert_input(input_).to_messages()\n        system, formatted_messages = _format_messages(messages)\n        payload = {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"messages\": formatted_messages,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"stop_sequences\": stop or self.stop_sequences,\n            \"system\": system,\n            **self.model_kwargs,\n            **kwargs,\n        }\n        if self.thinking is not None:\n            payload[\"thinking\"] = self.thinking\n        return {k: v for k, v in payload.items() if v is not None}\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = self._client.messages.create(**payload)\n        coerce_content_to_string = (\n            not _tools_in_params(payload)\n            and not _documents_in_params(payload)\n            and not _thinking_in_params(payload)\n        )\n        for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = await self._async_client.messages.create(**payload)\n        coerce_content_to_string = (\n            not _tools_in_params(payload)\n            and not _documents_in_params(payload)\n            and not _thinking_in_params(payload)\n        )\n        async for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    await run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    def _format_output(self, data: Any, **kwargs: Any) -> ChatResult:\n        data_dict = data.model_dump()\n        content = data_dict[\"content\"]\n\n        # Remove citations if they are None - introduced in anthropic sdk 0.45\n        for block in content:\n            if (\n                isinstance(block, dict)\n                and \"citations\" in block\n                and block[\"citations\"] is None\n            ):\n                block.pop(\"citations\")\n            if (\n                isinstance(block, dict)\n                and block.get(\"type\") == \"thinking\"\n                and \"text\" in block\n                and block[\"text\"] is None\n            ):\n                block.pop(\"text\")\n\n        llm_output = {\n            k: v for k, v in data_dict.items() if k not in (\"content\", \"role\", \"type\")\n        }\n        if (\n            len(content) == 1\n            and content[0][\"type\"] == \"text\"\n            and not content[0].get(\"citations\")\n        ):\n            msg = AIMessage(content=content[0][\"text\"])\n        elif any(block[\"type\"] == \"tool_use\" for block in content):\n            tool_calls = extract_tool_calls(content)\n            msg = AIMessage(\n                content=content,\n                tool_calls=tool_calls,\n            )\n        else:\n            msg = AIMessage(content=content)\n        msg.usage_metadata = _create_usage_metadata(data.usage)\n        return ChatResult(\n            generations=[ChatGeneration(message=msg)],\n            llm_output=llm_output,\n        )\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = self._client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = await self._async_client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[\n            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n        ] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        r\"\"\"Bind tool-like objects to this chat model.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports Anthropic format tool schemas and any tool definition handled\n                by :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call. Options are:\n\n                - name of the tool as a string or as dict ``{\"type\": \"tool\", \"name\": \"<<tool_name>>\"}``: calls corresponding tool;\n                - ``\"auto\"``, ``{\"type: \"auto\"}``, or None: automatically selects a tool (including no tool);\n                - ``\"any\"`` or ``{\"type: \"any\"}``: force at least one tool to be called;\n            parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n                Defaults to ``None`` (no specification, which allows parallel tool use).\n\n                .. versionadded:: 0.3.2\n            kwargs: Any additional parameters are passed directly to\n                :meth:`~langchain_anthropic.chat_models.ChatAnthropic.bind`.\n\n        Example:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n                # -> AIMessage(\n                #     content=[\n                #         {'text': '<thinking>\\nBased on the user\\'s question, the relevant function to call is GetWeather, which requires the \"location\" parameter.\\n\\nThe user has directly specified the location as \"San Francisco\". Since San Francisco is a well known city, I can reasonably infer they mean San Francisco, CA without needing the state specified.\\n\\nAll the required parameters are provided, so I can proceed with the API call.\\n</thinking>', 'type': 'text'},\n                #         {'text': None, 'type': 'tool_use', 'id': 'toolu_01SCgExKzQ7eqSkMHfygvYuu', 'name': 'GetWeather', 'input': {'location': 'San Francisco, CA'}}\n                #     ],\n                #     response_metadata={'id': 'msg_01GM3zQtoFv8jGQMW7abLnhi', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 487, 'output_tokens': 145}},\n                #     id='run-87b1331e-9251-4a68-acef-f0a018b639cc-0'\n                # )\n\n        Example \u2014 force tool call with tool_choice 'any':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"any\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n\n        Example \u2014 force specific tool call with tool_choice '<name_of_tool>':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"GetWeather\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n        Example \u2014 cache specific tools:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic, convert_to_anthropic_tool\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n                # We'll convert our pydantic class to the anthropic tool format\n                # before passing to bind_tools so that we can set the 'cache_control'\n                # field on our tool.\n                cached_price_tool = convert_to_anthropic_tool(GetPrice)\n                # Currently the only supported \"cache_control\" value is\n                # {\"type\": \"ephemeral\"}.\n                cached_price_tool[\"cache_control\"] = {\"type\": \"ephemeral\"}\n\n                # We need to pass in extra headers to enable use of the beta cache\n                # control API.\n                llm = ChatAnthropic(\n                    model=\"claude-3-5-sonnet-20240620\",\n                    temperature=0,\n                    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n                )\n                llm_with_tools = llm.bind_tools([GetWeather, cached_price_tool])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n            This outputs:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': \"Certainly! I can help you find out the current weather in San Francisco. To get this information, I'll use the GetWeather function. Let me fetch that data for you right away.\", 'type': 'text'}, {'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_01Xg7Wr5inFWgBxE5jH9rpRo', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 96, 'cache_creation_input_tokens': 1470, 'cache_read_input_tokens': 0}}, id='run-b36a5b54-5d69-470e-a1b0-b932d00b089e-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 96, 'total_tokens': 267})\n\n            If we invoke the tool again, we can see that the \"usage\" information in the AIMessage.response_metadata shows that we had a cache hit:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': 'To get the current weather in San Francisco, I can use the GetWeather function. Let me check that for you.', 'type': 'text'}, {'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_016RfWHrRvW6DAGCdwB6Ac64', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 82, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 1470}}, id='run-88b1f825-dcb7-4277-ac27-53df55d22001-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 82, 'total_tokens': 253})\n\n        \"\"\"  # noqa: E501\n        formatted_tools = [convert_to_anthropic_tool(tool) for tool in tools]\n        if not tool_choice:\n            pass\n        elif isinstance(tool_choice, dict):\n            kwargs[\"tool_choice\"] = tool_choice\n        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n        elif isinstance(tool_choice, str):\n            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n        else:\n            raise ValueError(\n                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n                f\"str, or None.\"\n            )\n\n        if parallel_tool_calls is not None:\n            disable_parallel_tool_use = not parallel_tool_calls\n            if \"tool_choice\" in kwargs:\n                kwargs[\"tool_choice\"][\"disable_parallel_tool_use\"] = (\n                    disable_parallel_tool_use\n                )\n            else:\n                kwargs[\"tool_choice\"] = {\n                    \"type\": \"auto\",\n                    \"disable_parallel_tool_use\": disable_parallel_tool_use,\n                }\n\n        return self.bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Union[Dict, type],\n        *,\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema: The output schema. Can be passed in as:\n\n                - an Anthropic tool schema,\n                - an OpenAI function/tool schema,\n                - a JSON Schema,\n                - a TypedDict class,\n                - or a Pydantic class.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            kwargs: Additional keyword arguments are ignored.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`~langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: Pydantic schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example:  Pydantic schema (include_raw=True):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: Dict schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n\n                schema = {\n                    \"name\": \"AnswerWithJustification\",\n                    \"description\": \"An answer to the user question along with justification for the answer.\",\n                    \"input_schema\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"answer\": {\"type\": \"string\"},\n                            \"justification\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"answer\", \"justification\"]\n                    }\n                }\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(schema)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. versionchanged:: 0.1.22\n\n                Added support for TypedDict class as `schema`.\n\n        \"\"\"  # noqa: E501\n        formatted_tool = convert_to_anthropic_tool(schema)\n        tool_name = formatted_tool[\"name\"]\n        llm = self.bind_tools(\n            [schema],\n            tool_choice=tool_name,\n            structured_output_format={\"kwargs\": {}, \"schema\": formatted_tool},\n        )\n        if isinstance(schema, type) and is_basemodel_subclass(schema):\n            output_parser: OutputParserLike = PydanticToolsParser(\n                tools=[schema], first_tool_only=True\n            )\n        else:\n            output_parser = JsonOutputKeyToolsParser(\n                key_name=tool_name, first_tool_only=True\n            )\n\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    @beta()\n    def get_num_tokens_from_messages(\n        self,\n        messages: List[BaseMessage],\n        tools: Optional[\n            Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]\n        ] = None,\n    ) -> int:\n        \"\"\"Count tokens in a sequence of input messages.\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n\n        Basic usage:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage, SystemMessage\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                messages = [\n                    SystemMessage(content=\"You are a scientist\"),\n                    HumanMessage(content=\"Hello, Claude\"),\n                ]\n                llm.get_num_tokens_from_messages(messages)\n\n            .. code-block:: none\n\n                14\n\n        Pass tool schemas:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage\n                from langchain_core.tools import tool\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                @tool(parse_docstring=True)\n                def get_weather(location: str) -> str:\n                    \\\"\\\"\\\"Get the current weather in a given location\n\n                    Args:\n                        location: The city and state, e.g. San Francisco, CA\n                    \\\"\\\"\\\"\n                    return \"Sunny\"\n\n                messages = [\n                    HumanMessage(content=\"What's the weather like in San Francisco?\"),\n                ]\n                llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n\n            .. code-block:: none\n\n                403\n\n        .. versionchanged:: 0.3.0\n\n                Uses Anthropic's token counting API to count tokens in messages. See:\n                https://docs.anthropic.com/en/docs/build-with-claude/token-counting\n        \"\"\"\n        formatted_system, formatted_messages = _format_messages(messages)\n        kwargs: Dict[str, Any] = {}\n        if isinstance(formatted_system, str):\n            kwargs[\"system\"] = formatted_system\n        if tools:\n            kwargs[\"tools\"] = [convert_to_anthropic_tool(tool) for tool in tools]\n\n        response = self._client.beta.messages.count_tokens(\n            betas=[\"token-counting-2024-11-01\"],\n            model=self.model,\n            messages=formatted_messages,  # type: ignore[arg-type]\n            **kwargs,\n        )\n        return response.input_tokens\n\n\nclass AnthropicTool(TypedDict):\n    \"\"\"Anthropic tool definition.\"\"\"\n\n    name: str\n    description: str\n    input_schema: Dict[str, Any]\n    cache_control: NotRequired[Dict[str, str]]\n\n\ndef convert_to_anthropic_tool(\n    tool: Union[Dict[str, Any], Type, Callable, BaseTool],\n) -> AnthropicTool:\n    \"\"\"Convert a tool-like object to an Anthropic tool definition.\"\"\"\n    # already in Anthropic tool format\n    if isinstance(tool, dict) and all(\n        k in tool for k in (\"name\", \"description\", \"input_schema\")\n    ):\n        anthropic_formatted = AnthropicTool(tool)  # type: ignore\n    else:\n        oai_formatted = convert_to_openai_tool(tool)[\"function\"]\n        anthropic_formatted = AnthropicTool(\n            name=oai_formatted[\"name\"],\n            description=oai_formatted[\"description\"],\n            input_schema=oai_formatted[\"parameters\"],\n        )\n    return anthropic_formatted\n\n\ndef _tools_in_params(params: dict) -> bool:\n    return \"tools\" in params or (\n        \"extra_body\" in params and params[\"extra_body\"].get(\"tools\")\n    )\n\n\ndef _thinking_in_params(params: dict) -> bool:\n    return params.get(\"thinking\", {}).get(\"type\") == \"enabled\"\n\n\ndef _documents_in_params(params: dict) -> bool:\n    for message in params.get(\"messages\", []):\n        if isinstance(message.get(\"content\"), list):\n            for block in message[\"content\"]:\n                if (\n                    isinstance(block, dict)\n                    and block.get(\"type\") == \"document\"\n                    and block.get(\"citations\", {}).get(\"enabled\")\n                ):\n                    return True\n    return False\n\n\nclass _AnthropicToolUse(TypedDict):\n    type: Literal[\"tool_use\"]\n    name: str\n    input: dict\n    id: str\n\n\ndef _lc_tool_calls_to_anthropic_tool_use_blocks(\n    tool_calls: List[ToolCall],\n) -> List[_AnthropicToolUse]:\n    blocks = []\n    for tool_call in tool_calls:\n        blocks.append(\n            _AnthropicToolUse(\n                type=\"tool_use\",\n                name=tool_call[\"name\"],\n                input=tool_call[\"args\"],\n                id=cast(str, tool_call[\"id\"]),\n            )\n        )\n    return blocks\n\n\ndef _make_message_chunk_from_anthropic_event(\n    event: anthropic.types.RawMessageStreamEvent,\n    *,\n    stream_usage: bool = True,\n    coerce_content_to_string: bool,\n) -> Optional[AIMessageChunk]:\n    \"\"\"Convert Anthropic event to AIMessageChunk.\n\n    Note that not all events will result in a message chunk. In these cases\n    we return None.\n    \"\"\"\n    message_chunk: Optional[AIMessageChunk] = None\n    # See https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/streaming/_messages.py  # noqa: E501\n    if event.type == \"message_start\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.message.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\" if coerce_content_to_string else [],\n            usage_metadata=usage_metadata,\n        )\n    elif (\n        event.type == \"content_block_start\"\n        and event.content_block is not None\n        and event.content_block.type in (\"tool_use\", \"document\", \"redacted_thinking\")\n    ):\n        if coerce_content_to_string:\n            warnings.warn(\"Received unexpected tool content block.\")\n        content_block = event.content_block.model_dump()\n        content_block[\"index\"] = event.index\n        if event.content_block.type == \"tool_use\":\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=event.content_block.id,\n                name=event.content_block.name,\n                args=\"\",\n            )\n            tool_call_chunks = [tool_call_chunk]\n        else:\n            tool_call_chunks = []\n        message_chunk = AIMessageChunk(\n            content=[content_block],\n            tool_call_chunks=tool_call_chunks,  # type: ignore\n        )\n    elif event.type == \"content_block_delta\":\n        if event.delta.type in (\"text_delta\", \"citations_delta\"):\n            if coerce_content_to_string and hasattr(event.delta, \"text\"):\n                text = event.delta.text\n                message_chunk = AIMessageChunk(content=text)\n            else:\n                content_block = event.delta.model_dump()\n                content_block[\"index\"] = event.index\n                content_block[\"type\"] = \"text\"\n                if \"citation\" in content_block:\n                    content_block[\"citations\"] = [content_block.pop(\"citation\")]\n                message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"thinking_delta\":\n            content_block = event.delta.model_dump()\n            if \"text\" in content_block and content_block[\"text\"] is None:\n                content_block.pop(\"text\")\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"thinking\"\n            message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"signature_delta\":\n            content_block = event.delta.model_dump()\n            if \"text\" in content_block and content_block[\"text\"] is None:\n                content_block.pop(\"text\")\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"thinking\"\n            message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"input_json_delta\":\n            content_block = event.delta.model_dump()\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"tool_use\"\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=None,\n                name=None,\n                args=event.delta.partial_json,\n            )\n            message_chunk = AIMessageChunk(\n                content=[content_block],\n                tool_call_chunks=[tool_call_chunk],  # type: ignore\n            )\n    elif event.type == \"message_delta\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\",\n            usage_metadata=usage_metadata,\n            response_metadata={\n                \"stop_reason\": event.delta.stop_reason,\n                \"stop_sequence\": event.delta.stop_sequence,\n            },\n        )\n    else:\n        pass\n\n    return message_chunk\n\n\n@deprecated(since=\"0.1.0\", removal=\"1.0.0\", alternative=\"ChatAnthropic\")\nclass ChatAnthropicMessages(ChatAnthropic):\n    pass\n\n\ndef _create_usage_metadata(anthropic_usage: BaseModel) -> UsageMetadata:\n    input_token_details: Dict = {\n        \"cache_read\": getattr(anthropic_usage, \"cache_read_input_tokens\", None),\n        \"cache_creation\": getattr(anthropic_usage, \"cache_creation_input_tokens\", None),\n    }\n\n    # Anthropic input_tokens exclude cached token counts.\n    input_tokens = (\n        getattr(anthropic_usage, \"input_tokens\", 0)\n        + (input_token_details[\"cache_read\"] or 0)\n        + (input_token_details[\"cache_creation\"] or 0)\n    )\n    output_tokens = getattr(anthropic_usage, \"output_tokens\", 0)\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=input_tokens + output_tokens,\n        input_token_details=InputTokenDetails(\n            **{k: v for k, v in input_token_details.items() if v is not None}\n        ),\n    )\n",
        "patch": "@@ -900,6 +900,8 @@ def _format_output(self, data: Any, **kwargs: Any) -> ChatResult:\n         llm_output = {\n             k: v for k, v in data_dict.items() if k not in (\"content\", \"role\", \"type\")\n         }\n+        if \"model\" in llm_output and \"model_name\" not in llm_output:\n+            llm_output[\"model_name\"] = llm_output[\"model\"]\n         if (\n             len(content) == 1\n             and content[0][\"type\"] == \"text\"\n@@ -1445,9 +1447,14 @@ def _make_message_chunk_from_anthropic_event(\n     # See https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/streaming/_messages.py  # noqa: E501\n     if event.type == \"message_start\" and stream_usage:\n         usage_metadata = _create_usage_metadata(event.message.usage)\n+        if hasattr(event.message, \"model\"):\n+            response_metadata = {\"model_name\": event.message.model}\n+        else:\n+            response_metadata = {}\n         message_chunk = AIMessageChunk(\n             content=\"\" if coerce_content_to_string else [],\n             usage_metadata=usage_metadata,\n+            response_metadata=response_metadata,\n         )\n     elif (\n         event.type == \"content_block_start\""
      },
      {
        "filename": "libs/partners/anthropic/tests/integration_tests/test_chat_models.py",
        "content_before": "\"\"\"Test ChatAnthropic chat model.\"\"\"\n\nimport json\nfrom base64 import b64encode\nfrom typing import List, Optional\n\nimport pytest\nimport requests\nfrom langchain_core.callbacks import CallbackManager\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    HumanMessage,\n    SystemMessage,\n    ToolMessage,\n)\nfrom langchain_core.outputs import ChatGeneration, LLMResult\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\nfrom langchain_anthropic import ChatAnthropic, ChatAnthropicMessages\nfrom tests.unit_tests._utils import FakeCallbackHandler\n\nMODEL_NAME = \"claude-3-5-haiku-latest\"\nIMAGE_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n\n\ndef test_stream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    for token in llm.stream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n\nasync def test_astream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    async for token in llm.astream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n    # Check expected raw API output\n    async_client = llm._async_client\n    params: dict = {\n        \"model\": MODEL_NAME,\n        \"max_tokens\": 1024,\n        \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}],\n        \"temperature\": 0.0,\n    }\n    stream = await async_client.messages.create(**params, stream=True)\n    async for event in stream:\n        if event.type == \"message_start\":\n            assert event.message.usage.input_tokens > 1\n            # Note: this single output token included in message start event\n            # does not appear to contribute to overall output token counts. It\n            # is excluded from the total token count.\n            assert event.message.usage.output_tokens == 1\n        elif event.type == \"message_delta\":\n            assert event.usage.output_tokens > 1\n        else:\n            pass\n\n\nasync def test_stream_usage() -> None:\n    \"\"\"Test usage metadata can be excluded.\"\"\"\n    model = ChatAnthropic(model_name=MODEL_NAME, stream_usage=False)  # type: ignore[call-arg]\n    async for token in model.astream(\"hi\"):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n    # check we override with kwarg\n    model = ChatAnthropic(model_name=MODEL_NAME)  # type: ignore[call-arg]\n    assert model.stream_usage\n    async for token in model.astream(\"hi\", stream_usage=False):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n\n\nasync def test_abatch() -> None:\n    \"\"\"Test streaming tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_abatch_tags() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch(\n        [\"I'm Pickle Rick\", \"I'm not Pickle Rick\"], config={\"tags\": [\"foo\"]}\n    )\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_async_tool_use() -> None:\n    llm = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = await llm_with_tools.ainvoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    first = True\n    chunks = []  # type: ignore\n    async for chunk in llm_with_tools.astream(\n        \"what's the weather in san francisco, ca\"\n    ):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_call_chunks, list)\n    assert len(gathered.tool_call_chunks) == 1\n    tool_call_chunk = gathered.tool_call_chunks[0]\n    assert tool_call_chunk[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call_chunk[\"args\"], str)\n    assert \"location\" in json.loads(tool_call_chunk[\"args\"])\n\n\ndef test_batch() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.batch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_ainvoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.ainvoke(\"I'm Pickle Rick\", config={\"tags\": [\"foo\"]})\n    assert isinstance(result.content, str)\n\n\ndef test_invoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.invoke(\"I'm Pickle Rick\", config=dict(tags=[\"foo\"]))\n    assert isinstance(result.content, str)\n\n\ndef test_system_invoke() -> None:\n    \"\"\"Test invoke tokens with a system message\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are an expert cartographer. If asked, you are a cartographer. \"\n                \"STAY IN CHARACTER\",\n            ),\n            (\"human\", \"Are you a mathematician?\"),\n        ]\n    )\n\n    chain = prompt | llm\n\n    result = chain.invoke({})\n    assert isinstance(result.content, str)\n\n\ndef test_anthropic_call() -> None:\n    \"\"\"Test valid call to anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.invoke([message])\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n\n\ndef test_anthropic_generate() -> None:\n    \"\"\"Test generate method of anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    chat_messages: List[List[BaseMessage]] = [\n        [HumanMessage(content=\"How many toes do dogs have?\")]\n    ]\n    messages_copy = [messages.copy() for messages in chat_messages]\n    result: LLMResult = chat.generate(chat_messages)\n    assert isinstance(result, LLMResult)\n    for response in result.generations[0]:\n        assert isinstance(response, ChatGeneration)\n        assert isinstance(response.text, str)\n        assert response.text == response.message.content\n    assert chat_messages == messages_copy\n\n\ndef test_anthropic_streaming() -> None:\n    \"\"\"Test streaming tokens from anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.stream([message])\n    for token in response:\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n\n\ndef test_anthropic_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    message = HumanMessage(content=\"Write me a sentence with 10 words.\")\n    for token in chat.stream([message]):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\nasync def test_anthropic_async_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    chat_messages: List[BaseMessage] = [\n        HumanMessage(content=\"How many toes do dogs have?\")\n    ]\n    async for token in chat.astream(chat_messages):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\ndef test_anthropic_multimodal() -> None:\n    \"\"\"Test that multimodal inputs are handled correctly.\"\"\"\n    chat = ChatAnthropic(model=IMAGE_MODEL_NAME)\n    messages: list[BaseMessage] = [\n        HumanMessage(\n            content=[\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        # langchain logo\n                        \"url\": \"data:image/jpeg;base64,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\",  # noqa: E501\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is this a logo for?\"},\n            ]\n        )\n    ]\n    response = chat.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n    num_tokens = chat.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n\ndef test_streaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = llm.generate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\nasync def test_astreaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = await llm.agenerate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\ndef test_tool_use() -> None:\n    llm = ChatAnthropic(model=MODEL_NAME)\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = llm_with_tools.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    input = \"how are you? what's the weather in san francisco, ca\"\n    first = True\n    chunks = []  # type: ignore\n    for chunk in llm_with_tools.stream(input):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered.content, list)\n    assert len(gathered.content) == 2\n    tool_use_block = None\n    for content_block in gathered.content:\n        assert isinstance(content_block, dict)\n        if content_block[\"type\"] == \"tool_use\":\n            tool_use_block = content_block\n            break\n    assert tool_use_block is not None\n    assert tool_use_block[\"name\"] == \"get_weather\"\n    assert \"location\" in json.loads(tool_use_block[\"partial_json\"])\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_calls, list)\n    assert len(gathered.tool_calls) == 1\n    tool_call = gathered.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n    assert tool_call[\"id\"] is not None\n\n    # Test passing response back to model\n    stream = llm_with_tools.stream(\n        [\n            input,\n            gathered,\n            ToolMessage(content=\"sunny and warm\", tool_call_id=tool_call[\"id\"]),\n        ]\n    )\n    chunks = []  # type: ignore\n    first = True\n    for chunk in stream:\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n\n\nclass GenerateUsername(BaseModel):\n    \"Get a username based on someone's name and hair color.\"\n\n    name: str\n    hair_color: str\n\n\ndef test_disable_parallel_tool_calling() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n    llm_with_tools = llm.bind_tools([GenerateUsername], parallel_tool_calls=False)\n    result = llm_with_tools.invoke(\n        \"Use the GenerateUsername tool to generate user names for:\\n\\n\"\n        \"Sally with green hair\\n\"\n        \"Bob with blue hair\"\n    )\n    assert isinstance(result, AIMessage)\n    assert len(result.tool_calls) == 1\n\n\ndef test_anthropic_with_empty_text_block() -> None:\n    \"\"\"Anthropic SDK can return an empty text block.\"\"\"\n\n    @tool\n    def type_letter(letter: str) -> str:\n        \"\"\"Type the given letter.\"\"\"\n        return \"OK\"\n\n    model = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0).bind_tools(\n        [type_letter]\n    )\n\n    messages = [\n        SystemMessage(\n            content=\"Repeat the given string using the provided tools. Do not write \"\n            \"anything else or provide any explanations. For example, \"\n            \"if the string is 'abc', you must print the \"\n            \"letters 'a', 'b', and 'c' one at a time and in that order. \"\n        ),\n        HumanMessage(content=\"dog\"),\n        AIMessage(\n            content=[\n                {\"text\": \"\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"letter\": \"d\"},\n                    \"name\": \"type_letter\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"type_letter\",\n                    \"args\": {\"letter\": \"d\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"OK\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n\n    model.invoke(messages)\n\n\ndef test_with_structured_output() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-opus-20240229\",\n    )\n\n    structured_llm = llm.with_structured_output(\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather report for a city\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\"location\": {\"type\": \"string\"}},\n            },\n        }\n    )\n    response = structured_llm.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, dict)\n    assert response[\"location\"]\n\n\ndef test_get_num_tokens_from_messages() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n    # Test simple case\n    messages = [\n        SystemMessage(content=\"You are a scientist\"),\n        HumanMessage(content=\"Hello, Claude\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n    # Test tool use\n    @tool(parse_docstring=True)\n    def get_weather(location: str) -> str:\n        \"\"\"Get the current weather in a given location\n\n        Args:\n            location: The city and state, e.g. San Francisco, CA\n        \"\"\"\n        return \"Sunny\"\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n        AIMessage(\n            content=[\n                {\"text\": \"Let's see.\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"location\": \"SF\"},\n                    \"name\": \"get_weather\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"get_weather\",\n                    \"args\": {\"location\": \"SF\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"Sunny\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n\nclass GetWeather(BaseModel):\n    \"\"\"Get the current weather in a given location\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n@pytest.mark.parametrize(\"tool_choice\", [\"GetWeather\", \"auto\", \"any\"])\ndef test_anthropic_bind_tools_tool_choice(tool_choice: str) -> None:\n    chat_model = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n    chat_model_with_tools = chat_model.bind_tools([GetWeather], tool_choice=tool_choice)\n    response = chat_model_with_tools.invoke(\"what's the weather in ny and la\")\n    assert isinstance(response, AIMessage)\n\n\ndef test_pdf_document_input() -> None:\n    url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n    data = b64encode(requests.get(url).content).decode()\n\n    result = ChatAnthropic(model=IMAGE_MODEL_NAME).invoke(\n        [\n            HumanMessage(\n                [\n                    \"summarize this document\",\n                    {\n                        \"type\": \"document\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"data\": data,\n                            \"media_type\": \"application/pdf\",\n                        },\n                    },\n                ]\n            )\n        ]\n    )\n    assert isinstance(result, AIMessage)\n    assert isinstance(result.content, str)\n    assert len(result.content) > 0\n\n\ndef test_citations() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"content\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": \"The grass is green\"},\n                            {\"type\": \"text\", \"text\": \"The sky is blue\"},\n                        ],\n                    },\n                    \"citations\": {\"enabled\": True},\n                },\n                {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n            ],\n        }\n    ]\n    response = llm.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert any(\"citations\" in block for block in response.content)\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(messages):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    assert any(\"citations\" in block for block in full.content)\n    assert not any(\"citation\" in block for block in full.content)\n\n\ndef test_thinking() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-7-sonnet-latest\",\n        max_tokens=5_000,\n        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n    )\n    response = llm.invoke(\"Hello\")\n    assert any(\"thinking\" in block for block in response.content)\n    for block in response.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"thinking\":\n            assert set(block.keys()) == {\"type\", \"thinking\", \"signature\"}\n            assert block[\"thinking\"] and isinstance(block[\"thinking\"], str)\n            assert block[\"signature\"] and isinstance(block[\"signature\"], str)\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(\"Hello\"):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    assert any(\"thinking\" in block for block in full.content)\n    for block in full.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"thinking\":\n            assert set(block.keys()) == {\"type\", \"thinking\", \"signature\", \"index\"}\n            assert block[\"thinking\"] and isinstance(block[\"thinking\"], str)\n            assert block[\"signature\"] and isinstance(block[\"signature\"], str)\n\n\ndef test_redacted_thinking() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-7-sonnet-latest\",\n        max_tokens=5_000,\n        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n    )\n    query = \"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"  # noqa: E501\n\n    response = llm.invoke(query)\n    has_reasoning = False\n    for block in response.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"redacted_thinking\":\n            has_reasoning = True\n            assert set(block.keys()) == {\"type\", \"data\"}\n            assert block[\"data\"] and isinstance(block[\"data\"], str)\n    assert has_reasoning\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(query):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    stream_has_reasoning = False\n    for block in full.content:\n        assert isinstance(block, dict)\n        if block[\"type\"] == \"redacted_thinking\":\n            stream_has_reasoning = True\n            assert set(block.keys()) == {\"type\", \"data\", \"index\"}\n            assert block[\"data\"] and isinstance(block[\"data\"], str)\n    assert stream_has_reasoning\n",
        "patch": "@@ -35,6 +35,7 @@ def test_stream() -> None:\n     full: Optional[BaseMessageChunk] = None\n     chunks_with_input_token_counts = 0\n     chunks_with_output_token_counts = 0\n+    chunks_with_model_name = 0\n     for token in llm.stream(\"I'm Pickle Rick\"):\n         assert isinstance(token.content, str)\n         full = token if full is None else full + token\n@@ -44,12 +45,14 @@ def test_stream() -> None:\n                 chunks_with_input_token_counts += 1\n             elif token.usage_metadata.get(\"output_tokens\"):\n                 chunks_with_output_token_counts += 1\n+        chunks_with_model_name += int(\"model_name\" in token.response_metadata)\n     if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n         raise AssertionError(\n             \"Expected exactly one chunk with input or output token counts. \"\n             \"AIMessageChunk aggregation adds counts. Check that \"\n             \"this is behaving properly.\"\n         )\n+    assert chunks_with_model_name == 1\n     # check token usage is populated\n     assert isinstance(full, AIMessageChunk)\n     assert full.usage_metadata is not None\n@@ -62,6 +65,7 @@ def test_stream() -> None:\n     )\n     assert \"stop_reason\" in full.response_metadata\n     assert \"stop_sequence\" in full.response_metadata\n+    assert \"model_name\" in full.response_metadata\n \n \n async def test_astream() -> None:\n@@ -219,6 +223,7 @@ async def test_ainvoke() -> None:\n \n     result = await llm.ainvoke(\"I'm Pickle Rick\", config={\"tags\": [\"foo\"]})\n     assert isinstance(result.content, str)\n+    assert \"model_name\" in result.response_metadata\n \n \n def test_invoke() -> None:"
      },
      {
        "filename": "libs/partners/mistralai/langchain_mistralai/chat_models.py",
        "content_before": "from __future__ import annotations\n\nimport hashlib\nimport json\nimport logging\nimport os\nimport re\nimport uuid\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncContextManager,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport httpx\nfrom httpx_sse import EventSource, aconnect_sse, connect_sse\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.language_models.llms import create_base_retry_decorator\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    ChatMessage,\n    ChatMessageChunk,\n    HumanMessage,\n    HumanMessageChunk,\n    InvalidToolCall,\n    SystemMessage,\n    SystemMessageChunk,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.tool import tool_call_chunk\nfrom langchain_core.output_parsers import (\n    JsonOutputParser,\n    PydanticOutputParser,\n)\nfrom langchain_core.output_parsers.base import OutputParserLike\nfrom langchain_core.output_parsers.openai_tools import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n    make_invalid_tool_call,\n    parse_tool_call,\n)\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import get_pydantic_field_names, secret_from_env\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import is_basemodel_subclass\nfrom langchain_core.utils.utils import _build_model_kwargs\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SecretStr,\n    model_validator,\n)\nfrom typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n# Mistral enforces a specific pattern for tool call IDs\nTOOL_CALL_ID_PATTERN = re.compile(r\"^[a-zA-Z0-9]{9}$\")\n\n\ndef _create_retry_decorator(\n    llm: ChatMistralAI,\n    run_manager: Optional[\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n    ] = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Returns a tenacity retry decorator, preconfigured to handle exceptions\"\"\"\n\n    errors = [httpx.RequestError, httpx.StreamError]\n    return create_base_retry_decorator(\n        error_types=errors, max_retries=llm.max_retries, run_manager=run_manager\n    )\n\n\ndef _is_valid_mistral_tool_call_id(tool_call_id: str) -> bool:\n    \"\"\"Check if tool call ID is nine character string consisting of a-z, A-Z, 0-9\"\"\"\n    return bool(TOOL_CALL_ID_PATTERN.match(tool_call_id))\n\n\ndef _base62_encode(num: int) -> str:\n    \"\"\"Encodes a number in base62 and ensures result is of a specified length.\"\"\"\n    base62 = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    if num == 0:\n        return base62[0]\n    arr = []\n    base = len(base62)\n    while num:\n        num, rem = divmod(num, base)\n        arr.append(base62[rem])\n    arr.reverse()\n    return \"\".join(arr)\n\n\ndef _convert_tool_call_id_to_mistral_compatible(tool_call_id: str) -> str:\n    \"\"\"Convert a tool call ID to a Mistral-compatible format\"\"\"\n    if _is_valid_mistral_tool_call_id(tool_call_id):\n        return tool_call_id\n    else:\n        hash_bytes = hashlib.sha256(tool_call_id.encode()).digest()\n        hash_int = int.from_bytes(hash_bytes, byteorder=\"big\")\n        base62_str = _base62_encode(hash_int)\n        if len(base62_str) >= 9:\n            return base62_str[:9]\n        else:\n            return base62_str.rjust(9, \"0\")\n\n\ndef _convert_mistral_chat_message_to_message(\n    _message: Dict,\n) -> BaseMessage:\n    role = _message[\"role\"]\n    assert role == \"assistant\", f\"Expected role to be 'assistant', got {role}\"\n    content = cast(str, _message[\"content\"])\n\n    additional_kwargs: Dict = {}\n    tool_calls = []\n    invalid_tool_calls = []\n    if raw_tool_calls := _message.get(\"tool_calls\"):\n        additional_kwargs[\"tool_calls\"] = raw_tool_calls\n        for raw_tool_call in raw_tool_calls:\n            try:\n                parsed: dict = cast(\n                    dict, parse_tool_call(raw_tool_call, return_id=True)\n                )\n                if not parsed[\"id\"]:\n                    parsed[\"id\"] = uuid.uuid4().hex[:]\n                tool_calls.append(parsed)\n            except Exception as e:\n                invalid_tool_calls.append(make_invalid_tool_call(raw_tool_call, str(e)))\n    return AIMessage(\n        content=content,\n        additional_kwargs=additional_kwargs,\n        tool_calls=tool_calls,\n        invalid_tool_calls=invalid_tool_calls,\n    )\n\n\ndef _raise_on_error(response: httpx.Response) -> None:\n    \"\"\"Raise an error if the response is an error.\"\"\"\n    if httpx.codes.is_error(response.status_code):\n        error_message = response.read().decode(\"utf-8\")\n        raise httpx.HTTPStatusError(\n            f\"Error response {response.status_code} \"\n            f\"while fetching {response.url}: {error_message}\",\n            request=response.request,\n            response=response,\n        )\n\n\nasync def _araise_on_error(response: httpx.Response) -> None:\n    \"\"\"Raise an error if the response is an error.\"\"\"\n    if httpx.codes.is_error(response.status_code):\n        error_message = (await response.aread()).decode(\"utf-8\")\n        raise httpx.HTTPStatusError(\n            f\"Error response {response.status_code} \"\n            f\"while fetching {response.url}: {error_message}\",\n            request=response.request,\n            response=response,\n        )\n\n\nasync def _aiter_sse(\n    event_source_mgr: AsyncContextManager[EventSource],\n) -> AsyncIterator[Dict]:\n    \"\"\"Iterate over the server-sent events.\"\"\"\n    async with event_source_mgr as event_source:\n        await _araise_on_error(event_source.response)\n        async for event in event_source.aiter_sse():\n            if event.data == \"[DONE]\":\n                return\n            yield event.json()\n\n\nasync def acompletion_with_retry(\n    llm: ChatMistralAI,\n    run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n    **kwargs: Any,\n) -> Any:\n    \"\"\"Use tenacity to retry the async completion call.\"\"\"\n    retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)\n\n    @retry_decorator\n    async def _completion_with_retry(**kwargs: Any) -> Any:\n        if \"stream\" not in kwargs:\n            kwargs[\"stream\"] = False\n        stream = kwargs[\"stream\"]\n        if stream:\n            event_source = aconnect_sse(\n                llm.async_client, \"POST\", \"/chat/completions\", json=kwargs\n            )\n            return _aiter_sse(event_source)\n        else:\n            response = await llm.async_client.post(url=\"/chat/completions\", json=kwargs)\n            await _araise_on_error(response)\n            return response.json()\n\n    return await _completion_with_retry(**kwargs)\n\n\ndef _convert_chunk_to_message_chunk(\n    chunk: Dict, default_class: Type[BaseMessageChunk]\n) -> BaseMessageChunk:\n    _delta = chunk[\"choices\"][0][\"delta\"]\n    role = _delta.get(\"role\")\n    content = _delta.get(\"content\") or \"\"\n    if role == \"user\" or default_class == HumanMessageChunk:\n        return HumanMessageChunk(content=content)\n    elif role == \"assistant\" or default_class == AIMessageChunk:\n        additional_kwargs: Dict = {}\n        if raw_tool_calls := _delta.get(\"tool_calls\"):\n            additional_kwargs[\"tool_calls\"] = raw_tool_calls\n            try:\n                tool_call_chunks = []\n                for raw_tool_call in raw_tool_calls:\n                    if not raw_tool_call.get(\"index\") and not raw_tool_call.get(\"id\"):\n                        tool_call_id = uuid.uuid4().hex[:]\n                    else:\n                        tool_call_id = raw_tool_call.get(\"id\")\n                    tool_call_chunks.append(\n                        tool_call_chunk(\n                            name=raw_tool_call[\"function\"].get(\"name\"),\n                            args=raw_tool_call[\"function\"].get(\"arguments\"),\n                            id=tool_call_id,\n                            index=raw_tool_call.get(\"index\"),\n                        )\n                    )\n            except KeyError:\n                pass\n        else:\n            tool_call_chunks = []\n        if token_usage := chunk.get(\"usage\"):\n            usage_metadata = {\n                \"input_tokens\": token_usage.get(\"prompt_tokens\", 0),\n                \"output_tokens\": token_usage.get(\"completion_tokens\", 0),\n                \"total_tokens\": token_usage.get(\"total_tokens\", 0),\n            }\n        else:\n            usage_metadata = None\n        return AIMessageChunk(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            tool_call_chunks=tool_call_chunks,  # type: ignore[arg-type]\n            usage_metadata=usage_metadata,  # type: ignore[arg-type]\n        )\n    elif role == \"system\" or default_class == SystemMessageChunk:\n        return SystemMessageChunk(content=content)\n    elif role or default_class == ChatMessageChunk:\n        return ChatMessageChunk(content=content, role=role)\n    else:\n        return default_class(content=content)  # type: ignore[call-arg]\n\n\ndef _format_tool_call_for_mistral(tool_call: ToolCall) -> dict:\n    \"\"\"Format Langchain ToolCall to dict expected by Mistral.\"\"\"\n    result: Dict[str, Any] = {\n        \"function\": {\n            \"name\": tool_call[\"name\"],\n            \"arguments\": json.dumps(tool_call[\"args\"]),\n        }\n    }\n    if _id := tool_call.get(\"id\"):\n        result[\"id\"] = _convert_tool_call_id_to_mistral_compatible(_id)\n\n    return result\n\n\ndef _format_invalid_tool_call_for_mistral(invalid_tool_call: InvalidToolCall) -> dict:\n    \"\"\"Format Langchain InvalidToolCall to dict expected by Mistral.\"\"\"\n    result: Dict[str, Any] = {\n        \"function\": {\n            \"name\": invalid_tool_call[\"name\"],\n            \"arguments\": invalid_tool_call[\"args\"],\n        }\n    }\n    if _id := invalid_tool_call.get(\"id\"):\n        result[\"id\"] = _convert_tool_call_id_to_mistral_compatible(_id)\n\n    return result\n\n\ndef _convert_message_to_mistral_chat_message(\n    message: BaseMessage,\n) -> Dict:\n    if isinstance(message, ChatMessage):\n        return dict(role=message.role, content=message.content)\n    elif isinstance(message, HumanMessage):\n        return dict(role=\"user\", content=message.content)\n    elif isinstance(message, AIMessage):\n        message_dict: Dict[str, Any] = {\"role\": \"assistant\"}\n        tool_calls = []\n        if message.tool_calls or message.invalid_tool_calls:\n            for tool_call in message.tool_calls:\n                tool_calls.append(_format_tool_call_for_mistral(tool_call))\n            for invalid_tool_call in message.invalid_tool_calls:\n                tool_calls.append(\n                    _format_invalid_tool_call_for_mistral(invalid_tool_call)\n                )\n        elif \"tool_calls\" in message.additional_kwargs:\n            for tc in message.additional_kwargs[\"tool_calls\"]:\n                chunk = {\n                    \"function\": {\n                        \"name\": tc[\"function\"][\"name\"],\n                        \"arguments\": tc[\"function\"][\"arguments\"],\n                    }\n                }\n                if _id := tc.get(\"id\"):\n                    chunk[\"id\"] = _id\n                tool_calls.append(chunk)\n        else:\n            pass\n        if tool_calls:  # do not populate empty list tool_calls\n            message_dict[\"tool_calls\"] = tool_calls\n        if tool_calls and message.content:\n            # Assistant message must have either content or tool_calls, but not both.\n            # Some providers may not support tool_calls in the same message as content.\n            # This is done to ensure compatibility with messages from other providers.\n            message_dict[\"content\"] = \"\"\n        else:\n            message_dict[\"content\"] = message.content\n        if \"prefix\" in message.additional_kwargs:\n            message_dict[\"prefix\"] = message.additional_kwargs[\"prefix\"]\n        return message_dict\n    elif isinstance(message, SystemMessage):\n        return dict(role=\"system\", content=message.content)\n    elif isinstance(message, ToolMessage):\n        return {\n            \"role\": \"tool\",\n            \"content\": message.content,\n            \"name\": message.name,\n            \"tool_call_id\": _convert_tool_call_id_to_mistral_compatible(\n                message.tool_call_id\n            ),\n        }\n    else:\n        raise ValueError(f\"Got unknown type {message}\")\n\n\nclass ChatMistralAI(BaseChatModel):\n    \"\"\"A chat model that uses the MistralAI API.\"\"\"\n\n    # The type for client and async_client is ignored because the type is not\n    # an Optional after the model is initialized and the model_validator\n    # is run.\n    client: httpx.Client = Field(  # type: ignore # : meta private:\n        default=None, exclude=True\n    )\n    async_client: httpx.AsyncClient = Field(  # type: ignore # : meta private:\n        default=None, exclude=True\n    )  #: :meta private:\n    mistral_api_key: Optional[SecretStr] = Field(\n        alias=\"api_key\",\n        default_factory=secret_from_env(\"MISTRAL_API_KEY\", default=None),\n    )\n    endpoint: Optional[str] = Field(default=None, alias=\"base_url\")\n    max_retries: int = 5\n    timeout: int = 120\n    max_concurrent_requests: int = 64\n    model: str = Field(default=\"mistral-small\", alias=\"model_name\")\n    temperature: float = 0.7\n    max_tokens: Optional[int] = None\n    top_p: float = 1\n    \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose\n       probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\"\n    random_seed: Optional[int] = None\n    safe_mode: Optional[bool] = None\n    streaming: bool = False\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Holds any invocation parameters not explicitly specified.\"\"\"\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict[str, Any]) -> Any:\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling the API.\"\"\"\n        defaults = {\n            \"model\": self.model,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"top_p\": self.top_p,\n            \"random_seed\": self.random_seed,\n            \"safe_prompt\": self.safe_mode,\n            **self.model_kwargs,\n        }\n        filtered = {k: v for k, v in defaults.items() if v is not None}\n        return filtered\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"mistral\",\n            ls_model_name=self.model,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @property\n    def _client_params(self) -> Dict[str, Any]:\n        \"\"\"Get the parameters used for the client.\"\"\"\n        return self._default_params\n\n    def completion_with_retry(\n        self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Use tenacity to retry the completion call.\"\"\"\n        # retry_decorator = _create_retry_decorator(self, run_manager=run_manager)\n\n        # @retry_decorator\n        def _completion_with_retry(**kwargs: Any) -> Any:\n            if \"stream\" not in kwargs:\n                kwargs[\"stream\"] = False\n            stream = kwargs[\"stream\"]\n            if stream:\n\n                def iter_sse() -> Iterator[Dict]:\n                    with connect_sse(\n                        self.client, \"POST\", \"/chat/completions\", json=kwargs\n                    ) as event_source:\n                        _raise_on_error(event_source.response)\n                        for event in event_source.iter_sse():\n                            if event.data == \"[DONE]\":\n                                return\n                            yield event.json()\n\n                return iter_sse()\n            else:\n                response = self.client.post(url=\"/chat/completions\", json=kwargs)\n                _raise_on_error(response)\n                return response.json()\n\n        rtn = _completion_with_retry(**kwargs)\n        return rtn\n\n    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:\n        overall_token_usage: dict = {}\n        for output in llm_outputs:\n            if output is None:\n                # Happens in streaming\n                continue\n            token_usage = output[\"token_usage\"]\n            if token_usage is not None:\n                for k, v in token_usage.items():\n                    if k in overall_token_usage:\n                        overall_token_usage[k] += v\n                    else:\n                        overall_token_usage[k] = v\n        combined = {\"token_usage\": overall_token_usage, \"model_name\": self.model}\n        return combined\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        \"\"\"Validate api key, python package exists, temperature, and top_p.\"\"\"\n        if isinstance(self.mistral_api_key, SecretStr):\n            api_key_str: Optional[str] = self.mistral_api_key.get_secret_value()\n        else:\n            api_key_str = self.mistral_api_key\n\n        # todo: handle retries\n        base_url_str = (\n            self.endpoint\n            or os.environ.get(\"MISTRAL_BASE_URL\")\n            or \"https://api.mistral.ai/v1\"\n        )\n        self.endpoint = base_url_str\n        if not self.client:\n            self.client = httpx.Client(\n                base_url=base_url_str,\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Accept\": \"application/json\",\n                    \"Authorization\": f\"Bearer {api_key_str}\",\n                },\n                timeout=self.timeout,\n            )\n        # todo: handle retries and max_concurrency\n        if not self.async_client:\n            self.async_client = httpx.AsyncClient(\n                base_url=base_url_str,\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Accept\": \"application/json\",\n                    \"Authorization\": f\"Bearer {api_key_str}\",\n                },\n                timeout=self.timeout,\n            )\n\n        if self.temperature is not None and not 0 <= self.temperature <= 1:\n            raise ValueError(\"temperature must be in the range [0.0, 1.0]\")\n\n        if self.top_p is not None and not 0 <= self.top_p <= 1:\n            raise ValueError(\"top_p must be in the range [0.0, 1.0]\")\n\n        return self\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs}\n        response = self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        )\n        return self._create_chat_result(response)\n\n    def _create_chat_result(self, response: Dict) -> ChatResult:\n        generations = []\n        token_usage = response.get(\"usage\", {})\n        for res in response[\"choices\"]:\n            finish_reason = res.get(\"finish_reason\")\n            message = _convert_mistral_chat_message_to_message(res[\"message\"])\n            if token_usage and isinstance(message, AIMessage):\n                message.usage_metadata = {\n                    \"input_tokens\": token_usage.get(\"prompt_tokens\", 0),\n                    \"output_tokens\": token_usage.get(\"completion_tokens\", 0),\n                    \"total_tokens\": token_usage.get(\"total_tokens\", 0),\n                }\n            gen = ChatGeneration(\n                message=message,\n                generation_info={\"finish_reason\": finish_reason},\n            )\n            generations.append(gen)\n\n        llm_output = {\"token_usage\": token_usage, \"model\": self.model}\n        return ChatResult(generations=generations, llm_output=llm_output)\n\n    def _create_message_dicts(\n        self, messages: List[BaseMessage], stop: Optional[List[str]]\n    ) -> Tuple[List[Dict], Dict[str, Any]]:\n        params = self._client_params\n        if stop is not None or \"stop\" in params:\n            if \"stop\" in params:\n                params.pop(\"stop\")\n            logger.warning(\n                \"Parameter `stop` not yet supported (https://docs.mistral.ai/api)\"\n            )\n        message_dicts = [_convert_message_to_mistral_chat_message(m) for m in messages]\n        return message_dicts, params\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs, \"stream\": True}\n\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        for chunk in self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        ):\n            if len(chunk.get(\"choices\", [])) == 0:\n                continue\n            new_chunk = _convert_chunk_to_message_chunk(chunk, default_chunk_class)\n            # make future chunks same type as first chunk\n            default_chunk_class = new_chunk.__class__\n            gen_chunk = ChatGenerationChunk(message=new_chunk)\n            if run_manager:\n                run_manager.on_llm_new_token(\n                    token=cast(str, new_chunk.content), chunk=gen_chunk\n                )\n            yield gen_chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs, \"stream\": True}\n\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        async for chunk in await acompletion_with_retry(\n            self, messages=message_dicts, run_manager=run_manager, **params\n        ):\n            if len(chunk.get(\"choices\", [])) == 0:\n                continue\n            new_chunk = _convert_chunk_to_message_chunk(chunk, default_chunk_class)\n            # make future chunks same type as first chunk\n            default_chunk_class = new_chunk.__class__\n            gen_chunk = ChatGenerationChunk(message=new_chunk)\n            if run_manager:\n                await run_manager.on_llm_new_token(\n                    token=cast(str, new_chunk.content), chunk=gen_chunk\n                )\n            yield gen_chunk\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._astream(\n                messages=messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs}\n        response = await acompletion_with_retry(\n            self, messages=message_dicts, run_manager=run_manager, **params\n        )\n        return self._create_chat_result(response)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        Assumes model is compatible with OpenAI tool-calling API.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports any tool definition handled by\n                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call.\n                Must be the name of the single provided function or\n                \"auto\" to automatically determine which function to call\n                (if any), or a dict of the form:\n                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n            kwargs: Any additional parameters are passed directly to\n                ``self.bind(**kwargs)``.\n        \"\"\"\n\n        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n        return super().bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Optional[Union[Dict, Type]] = None,\n        *,\n        method: Literal[\n            \"function_calling\", \"json_mode\", \"json_schema\"\n        ] = \"function_calling\",\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n                    - an OpenAI function/tool schema,\n                    - a JSON Schema,\n                    - a TypedDict class (support added in 0.1.12),\n                    - or a Pydantic class.\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n                .. versionchanged:: 0.1.12\n\n                        Added support for TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"function_calling\":\n                    Uses Mistral's\n                    `function-calling feature <https://docs.mistral.ai/capabilities/function_calling/>`_.\n                - \"json_schema\":\n                    Uses Mistral's\n                    `structured output feature <https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/>`_.\n                - \"json_mode\":\n                    Uses Mistral's\n                    `JSON mode <https://docs.mistral.ai/capabilities/structured-output/json_mode/>`_.\n                    Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call.\n\n                .. versionchanged:: 0.2.5\n\n                        Added method=\"json_schema\"\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: schema=Pydantic class, method=\"function_calling\", include_raw=False:\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_mistralai import ChatMistralAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    # If we provide default values and/or descriptions for fields, these will be passed\n                    # to the model. This is an important part of improving a model's ability to\n                    # correctly return structured outputs.\n                    justification: Optional[str] = Field(\n                        default=None, description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example: schema=Pydantic class, method=\"function_calling\", include_raw=True:\n            .. code-block:: python\n\n                from langchain_mistralai import ChatMistralAI\n                from pydantic import BaseModel\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: str\n\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: schema=TypedDict class, method=\"function_calling\", include_raw=False:\n            .. code-block:: python\n\n                # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n                # from typing_extensions, not from typing.\n                from typing_extensions import Annotated, TypedDict\n\n                from langchain_mistralai import ChatMistralAI\n\n\n                class AnswerWithJustification(TypedDict):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Annotated[\n                        Optional[str], None, \"A justification for the answer.\"\n                    ]\n\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        Example: schema=OpenAI function schema, method=\"function_calling\", include_raw=False:\n            .. code-block:: python\n\n                from langchain_mistralai import ChatMistralAI\n\n                oai_schema = {\n                    'name': 'AnswerWithJustification',\n                    'description': 'An answer to the user question along with justification for the answer.',\n                    'parameters': {\n                        'type': 'object',\n                        'properties': {\n                            'answer': {'type': 'string'},\n                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n                        },\n                       'required': ['answer']\n                   }\n               }\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(oai_schema)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        Example: schema=Pydantic class, method=\"json_mode\", include_raw=True:\n            .. code-block::\n\n                from langchain_mistralai import ChatMistralAI\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    answer: str\n                    justification: str\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification,\n                    method=\"json_mode\",\n                    include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n                #     'parsing_error': None\n                # }\n\n        Example: schema=None, method=\"json_mode\", include_raw=True:\n            .. code-block::\n\n                structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': {\n                #         'answer': 'They are both the same weight.',\n                #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n                #     },\n                #     'parsing_error': None\n                # }\n\n        \"\"\"  # noqa: E501\n        if kwargs:\n            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n        is_pydantic_schema = isinstance(schema, type) and is_basemodel_subclass(schema)\n        if method == \"function_calling\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is 'function_calling'. \"\n                    \"Received None.\"\n                )\n            # TODO: Update to pass in tool name as tool_choice if/when Mistral supports\n            # specifying a tool.\n            llm = self.bind_tools(\n                [schema],\n                tool_choice=\"any\",\n                structured_output_format={\n                    \"kwargs\": {\"method\": \"function_calling\"},\n                    \"schema\": schema,\n                },\n            )\n            if is_pydantic_schema:\n                output_parser: OutputParserLike = PydanticToolsParser(\n                    tools=[schema],  # type: ignore[list-item]\n                    first_tool_only=True,  # type: ignore[list-item]\n                )\n            else:\n                key_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n                output_parser = JsonOutputKeyToolsParser(\n                    key_name=key_name, first_tool_only=True\n                )\n        elif method == \"json_mode\":\n            llm = self.bind(\n                response_format={\"type\": \"json_object\"},\n                structured_output_format={\n                    \"kwargs\": {\n                        # this is correct - name difference with mistral api\n                        \"method\": \"json_mode\"\n                    },\n                    \"schema\": schema,\n                },\n            )\n            output_parser = (\n                PydanticOutputParser(pydantic_object=schema)  # type: ignore[type-var, arg-type]\n                if is_pydantic_schema\n                else JsonOutputParser()\n            )\n        elif method == \"json_schema\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is 'json_schema'. \"\n                    \"Received None.\"\n                )\n            response_format = _convert_to_openai_response_format(schema, strict=True)\n            llm = self.bind(\n                response_format=response_format,\n                structured_output_format={\n                    \"kwargs\": {\"method\": \"json_schema\"},\n                    \"schema\": schema,\n                },\n            )\n\n            output_parser = (\n                PydanticOutputParser(pydantic_object=schema)  # type: ignore[arg-type]\n                if is_pydantic_schema\n                else JsonOutputParser()\n            )\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return self._default_params\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"mistralai-chat\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"mistral_api_key\": \"MISTRAL_API_KEY\"}\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this model can be serialized by Langchain.\"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"mistralai\"]\n\n\ndef _convert_to_openai_response_format(\n    schema: Union[Dict[str, Any], Type], *, strict: Optional[bool] = None\n) -> Dict:\n    \"\"\"Same as in ChatOpenAI, but don't pass through Pydantic BaseModels.\"\"\"\n    if (\n        isinstance(schema, dict)\n        and \"json_schema\" in schema\n        and schema.get(\"type\") == \"json_schema\"\n    ):\n        response_format = schema\n    elif isinstance(schema, dict) and \"name\" in schema and \"schema\" in schema:\n        response_format = {\"type\": \"json_schema\", \"json_schema\": schema}\n    else:\n        if strict is None:\n            if isinstance(schema, dict) and isinstance(schema.get(\"strict\"), bool):\n                strict = schema[\"strict\"]\n            else:\n                strict = False\n        function = convert_to_openai_tool(schema, strict=strict)[\"function\"]\n        function[\"schema\"] = function.pop(\"parameters\")\n        response_format = {\"type\": \"json_schema\", \"json_schema\": function}\n\n    if strict is not None and strict is not response_format[\"json_schema\"].get(\n        \"strict\"\n    ):\n        msg = (\n            f\"Output schema already has 'strict' value set to \"\n            f\"{schema['json_schema']['strict']} but 'strict' also passed in to \"\n            f\"with_structured_output as {strict}. Please make sure that \"\n            f\"'strict' is only specified in one place.\"\n        )\n        raise ValueError(msg)\n    return response_format\n",
        "patch": "@@ -579,7 +579,11 @@ def _create_chat_result(self, response: Dict) -> ChatResult:\n             )\n             generations.append(gen)\n \n-        llm_output = {\"token_usage\": token_usage, \"model\": self.model}\n+        llm_output = {\n+            \"token_usage\": token_usage,\n+            \"model_name\": self.model,\n+            \"model\": self.model,  # Backwards compatability\n+        }\n         return ChatResult(generations=generations, llm_output=llm_output)\n \n     def _create_message_dicts("
      },
      {
        "filename": "libs/partners/mistralai/tests/integration_tests/test_chat_models.py",
        "content_before": "\"\"\"Test ChatMistral chat model.\"\"\"\n\nimport json\nfrom typing import Any, Optional\n\nimport pytest\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessageChunk,\n    HumanMessage,\n)\nfrom pydantic import BaseModel\nfrom typing_extensions import TypedDict\n\nfrom langchain_mistralai.chat_models import ChatMistralAI\n\n\ndef test_stream() -> None:\n    \"\"\"Test streaming tokens from ChatMistralAI.\"\"\"\n    llm = ChatMistralAI()\n\n    for token in llm.stream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n\n\nasync def test_astream() -> None:\n    \"\"\"Test streaming tokens from ChatMistralAI.\"\"\"\n    llm = ChatMistralAI()\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_token_counts = 0\n    async for token in llm.astream(\"I'm Pickle Rick\"):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        if token.usage_metadata is not None:\n            chunks_with_token_counts += 1\n    if chunks_with_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n\n\nasync def test_abatch() -> None:\n    \"\"\"Test streaming tokens from ChatMistralAI\"\"\"\n    llm = ChatMistralAI()\n\n    result = await llm.abatch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_abatch_tags() -> None:\n    \"\"\"Test batch tokens from ChatMistralAI\"\"\"\n    llm = ChatMistralAI()\n\n    result = await llm.abatch(\n        [\"I'm Pickle Rick\", \"I'm not Pickle Rick\"], config={\"tags\": [\"foo\"]}\n    )\n    for token in result:\n        assert isinstance(token.content, str)\n\n\ndef test_batch() -> None:\n    \"\"\"Test batch tokens from ChatMistralAI\"\"\"\n    llm = ChatMistralAI()\n\n    result = llm.batch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_ainvoke() -> None:\n    \"\"\"Test invoke tokens from ChatMistralAI\"\"\"\n    llm = ChatMistralAI()\n\n    result = await llm.ainvoke(\"I'm Pickle Rick\", config={\"tags\": [\"foo\"]})\n    assert isinstance(result.content, str)\n\n\ndef test_invoke() -> None:\n    \"\"\"Test invoke tokens from ChatMistralAI\"\"\"\n    llm = ChatMistralAI()\n\n    result = llm.invoke(\"I'm Pickle Rick\", config=dict(tags=[\"foo\"]))\n    assert isinstance(result.content, str)\n\n\ndef test_chat_mistralai_llm_output_contains_model_name() -> None:\n    \"\"\"Test llm_output contains model_name.\"\"\"\n    chat = ChatMistralAI(max_tokens=10)\n    message = HumanMessage(content=\"Hello\")\n    llm_result = chat.generate([[message]])\n    assert llm_result.llm_output is not None\n    assert llm_result.llm_output[\"model_name\"] == chat.model\n\n\ndef test_chat_mistralai_streaming_llm_output_contains_model_name() -> None:\n    \"\"\"Test llm_output contains model_name.\"\"\"\n    chat = ChatMistralAI(max_tokens=10, streaming=True)\n    message = HumanMessage(content=\"Hello\")\n    llm_result = chat.generate([[message]])\n    assert llm_result.llm_output is not None\n    assert llm_result.llm_output[\"model_name\"] == chat.model\n\n\ndef test_chat_mistralai_llm_output_contains_token_usage() -> None:\n    \"\"\"Test llm_output contains model_name.\"\"\"\n    chat = ChatMistralAI(max_tokens=10)\n    message = HumanMessage(content=\"Hello\")\n    llm_result = chat.generate([[message]])\n    assert llm_result.llm_output is not None\n    assert \"token_usage\" in llm_result.llm_output\n    token_usage = llm_result.llm_output[\"token_usage\"]\n    assert \"prompt_tokens\" in token_usage\n    assert \"completion_tokens\" in token_usage\n    assert \"total_tokens\" in token_usage\n\n\ndef test_chat_mistralai_streaming_llm_output_not_contain_token_usage() -> None:\n    \"\"\"Mistral currently doesn't return token usage when streaming.\"\"\"\n    chat = ChatMistralAI(max_tokens=10, streaming=True)\n    message = HumanMessage(content=\"Hello\")\n    llm_result = chat.generate([[message]])\n    assert llm_result.llm_output is not None\n    assert \"token_usage\" in llm_result.llm_output\n    token_usage = llm_result.llm_output[\"token_usage\"]\n    assert not token_usage\n\n\ndef test_structured_output() -> None:\n    llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)  # type: ignore[call-arg]\n    schema = {\n        \"title\": \"AnswerWithJustification\",\n        \"description\": (\n            \"An answer to the user question along with justification for the answer.\"\n        ),\n        \"type\": \"object\",\n        \"properties\": {\n            \"answer\": {\"title\": \"Answer\", \"type\": \"string\"},\n            \"justification\": {\"title\": \"Justification\", \"type\": \"string\"},\n        },\n        \"required\": [\"answer\", \"justification\"],\n    }\n    structured_llm = llm.with_structured_output(schema)\n    result = structured_llm.invoke(\n        \"What weighs more a pound of bricks or a pound of feathers\"\n    )\n    assert isinstance(result, dict)\n\n\ndef test_streaming_structured_output() -> None:\n    llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)  # type: ignore[call-arg]\n\n    class Person(BaseModel):\n        name: str\n        age: int\n\n    structured_llm = llm.with_structured_output(Person)\n    strm = structured_llm.stream(\"Erick, 27 years old\")\n    chunk_num = 0\n    for chunk in strm:\n        assert chunk_num == 0, \"should only have one chunk with model\"\n        assert isinstance(chunk, Person)\n        assert chunk.name == \"Erick\"\n        assert chunk.age == 27\n        chunk_num += 1\n\n\nclass Book(BaseModel):\n    name: str\n    authors: list[str]\n\n\nclass BookDict(TypedDict):\n    name: str\n    authors: list[str]\n\n\ndef _check_parsed_result(result: Any, schema: Any) -> None:\n    if schema == Book:\n        assert isinstance(result, Book)\n    else:\n        assert all(key in [\"name\", \"authors\"] for key in result.keys())\n\n\n@pytest.mark.parametrize(\"schema\", [Book, BookDict, Book.model_json_schema()])\ndef test_structured_output_json_schema(schema: Any) -> None:\n    llm = ChatMistralAI(model=\"ministral-8b-latest\")  # type: ignore[call-arg]\n    structured_llm = llm.with_structured_output(schema, method=\"json_schema\")\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"Extract the book's information.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"I recently read 'To Kill a Mockingbird' by Harper Lee.\",\n        },\n    ]\n    # Test invoke\n    result = structured_llm.invoke(messages)\n    _check_parsed_result(result, schema)\n\n    # Test stream\n    for chunk in structured_llm.stream(messages):\n        _check_parsed_result(chunk, schema)\n\n\n@pytest.mark.parametrize(\"schema\", [Book, BookDict, Book.model_json_schema()])\nasync def test_structured_output_json_schema_async(schema: Any) -> None:\n    llm = ChatMistralAI(model=\"ministral-8b-latest\")  # type: ignore[call-arg]\n    structured_llm = llm.with_structured_output(schema, method=\"json_schema\")\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"Extract the book's information.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"I recently read 'To Kill a Mockingbird' by Harper Lee.\",\n        },\n    ]\n    # Test invoke\n    result = await structured_llm.ainvoke(messages)\n    _check_parsed_result(result, schema)\n\n    # Test stream\n    async for chunk in structured_llm.astream(messages):\n        _check_parsed_result(chunk, schema)\n\n\ndef test_tool_call() -> None:\n    llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)  # type: ignore[call-arg]\n\n    class Person(BaseModel):\n        name: str\n        age: int\n\n    tool_llm = llm.bind_tools([Person])\n\n    result = tool_llm.invoke(\"Erick, 27 years old\")\n    assert isinstance(result, AIMessage)\n    assert len(result.tool_calls) == 1\n    tool_call = result.tool_calls[0]\n    assert tool_call[\"name\"] == \"Person\"\n    assert tool_call[\"args\"] == {\"name\": \"Erick\", \"age\": 27}\n\n\ndef test_streaming_tool_call() -> None:\n    llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)  # type: ignore[call-arg]\n\n    class Person(BaseModel):\n        name: str\n        age: int\n\n    tool_llm = llm.bind_tools([Person])\n\n    # where it calls the tool\n    strm = tool_llm.stream(\"Erick, 27 years old\")\n\n    additional_kwargs = None\n    for chunk in strm:\n        assert isinstance(chunk, AIMessageChunk)\n        assert chunk.content == \"\"\n        additional_kwargs = chunk.additional_kwargs\n\n    assert additional_kwargs is not None\n    assert \"tool_calls\" in additional_kwargs\n    assert len(additional_kwargs[\"tool_calls\"]) == 1\n    assert additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"] == \"Person\"\n    assert json.loads(additional_kwargs[\"tool_calls\"][0][\"function\"][\"arguments\"]) == {\n        \"name\": \"Erick\",\n        \"age\": 27,\n    }\n\n    assert isinstance(chunk, AIMessageChunk)\n    assert len(chunk.tool_call_chunks) == 1\n    tool_call_chunk = chunk.tool_call_chunks[0]\n    assert tool_call_chunk[\"name\"] == \"Person\"\n    assert tool_call_chunk[\"args\"] == '{\"name\": \"Erick\", \"age\": 27}'\n\n    # where it doesn't call the tool\n    strm = tool_llm.stream(\"What is 2+2?\")\n    acc: Any = None\n    for chunk in strm:\n        assert isinstance(chunk, AIMessageChunk)\n        acc = chunk if acc is None else acc + chunk\n    assert acc.content != \"\"\n    assert \"tool_calls\" not in acc.additional_kwargs\n",
        "patch": "@@ -87,6 +87,7 @@ async def test_ainvoke() -> None:\n \n     result = await llm.ainvoke(\"I'm Pickle Rick\", config={\"tags\": [\"foo\"]})\n     assert isinstance(result.content, str)\n+    assert \"model_name\" in result.response_metadata\n \n \n def test_invoke() -> None:"
      }
    ]
  },
  {
    "number": 30001,
    "title": "Add asynchronous generate interface",
    "body": "- [ ] **PR title**: [langchain_community.llms.xinference]: Add asynchronous generate interface\r\n\r\n- [ ] **PR message**: The asynchronous generate interface  support  stream data and non-stream data.\r\n          \r\n        chain = prompt | llm\r\n        async for chunk in chain.astream(input=user_input):\r\n            yield chunk\r\n\r\n\r\n- [ ] **Add tests and docs**:\r\n\r\n       from langchain_community.llms import Xinference\r\n       from langchain.prompts import PromptTemplate\r\n\r\n       llm = Xinference(\r\n       server_url=\"http://0.0.0.0:9997\",  # replace your xinference server url\r\n       model_uid={model_uid}  # replace model_uid with the model UID return from launching the model\r\n           stream = True\r\n            )\r\n       prompt = PromptTemplate(input=['country'], template=\"Q: where can we visit in the capital of {country}? A:\")\r\n       chain = prompt | llm\r\n       async for chunk in chain.astream(input=user_input):\r\n           yield chunk",
    "issue_title": "Add asynchronous generate interface",
    "issue_body": "- [ ] **PR title**: [langchain_community.llms.xinference]: Add asynchronous generate interface\r\n\r\n- [ ] **PR message**: The asynchronous generate interface  support  stream data and non-stream data.\r\n          \r\n        chain = prompt | llm\r\n        async for chunk in chain.astream(input=user_input):\r\n            yield chunk\r\n\r\n\r\n- [ ] **Add tests and docs**:\r\n\r\n       from langchain_community.llms import Xinference\r\n       from langchain.prompts import PromptTemplate\r\n\r\n       llm = Xinference(\r\n       server_url=\"http://0.0.0.0:9997\",  # replace your xinference server url\r\n       model_uid={model_uid}  # replace model_uid with the model UID return from launching the model\r\n           stream = True\r\n            )\r\n       prompt = PromptTemplate(input=['country'], template=\"Q: where can we visit in the capital of {country}? A:\")\r\n       chain = prompt | llm\r\n       async for chunk in chain.astream(input=user_input):\r\n           yield chunk",
    "files": [
      {
        "filename": "libs/community/langchain_community/llms/xinference.py",
        "content_before": "from __future__ import annotations\n\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Dict,\n    Generator,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Union,\n)\n\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.outputs import GenerationChunk\n\nif TYPE_CHECKING:\n    from xinference.client import RESTfulChatModelHandle, RESTfulGenerateModelHandle\n    from xinference.model.llm.core import LlamaCppGenerateConfig\n\n\nclass Xinference(LLM):\n    \"\"\"`Xinference` large-scale model inference service.\n\n    To use, you should have the xinference library installed:\n\n    .. code-block:: bash\n\n       pip install \"xinference[all]\"\n\n    If you're simply using the services provided by Xinference, you can utilize the xinference_client package:\n\n    .. code-block:: bash\n\n        pip install xinference_client\n\n    Check out: https://github.com/xorbitsai/inference\n    To run, you need to start a Xinference supervisor on one server and Xinference workers on the other servers\n\n    Example:\n        To start a local instance of Xinference, run\n\n        .. code-block:: bash\n\n           $ xinference\n\n        You can also deploy Xinference in a distributed cluster. Here are the steps:\n\n        Starting the supervisor:\n\n        .. code-block:: bash\n\n           $ xinference-supervisor\n\n        Starting the worker:\n\n        .. code-block:: bash\n\n           $ xinference-worker\n\n    Then, launch a model using command line interface (CLI).\n\n    Example:\n\n    .. code-block:: bash\n\n       $ xinference launch -n orca -s 3 -q q4_0\n\n    It will return a model UID. Then, you can use Xinference with LangChain.\n\n    Example:\n\n    .. code-block:: python\n\n        from langchain_community.llms import Xinference\n\n        llm = Xinference(\n            server_url=\"http://0.0.0.0:9997\",\n            model_uid = {model_uid} # replace model_uid with the model UID return from launching the model\n        )\n\n        llm.invoke(\n            prompt=\"Q: where can we visit in the capital of France? A:\",\n            generate_config={\"max_tokens\": 1024, \"stream\": True},\n        )\n\n    Example:\n\n    .. code-block:: python\n\n        from langchain_community.llms import Xinference\n        from langchain.prompts import PromptTemplate\n\n        llm = Xinference(\n            server_url=\"http://0.0.0.0:9997\",\n            model_uid={model_uid}, # replace model_uid with the model UID return from launching the model\n            stream=True\n        )\n        prompt = PromptTemplate(\n            input=['country'],\n            template=\"Q: where can we visit in the capital of {country}? A:\"\n        )\n        chain = prompt | llm\n        chain.stream(input={'country': 'France'})\n\n\n    To view all the supported builtin models, run:\n\n    .. code-block:: bash\n\n        $ xinference list --all\n\n    \"\"\"  # noqa: E501\n\n    client: Optional[Any] = None\n    server_url: Optional[str]\n    \"\"\"URL of the xinference server\"\"\"\n    model_uid: Optional[str]\n    \"\"\"UID of the launched model\"\"\"\n    model_kwargs: Dict[str, Any]\n    \"\"\"Keyword arguments to be passed to xinference.LLM\"\"\"\n\n    def __init__(\n        self,\n        server_url: Optional[str] = None,\n        model_uid: Optional[str] = None,\n        **model_kwargs: Any,\n    ):\n        try:\n            from xinference.client import RESTfulClient\n        except ImportError:\n            try:\n                from xinference_client import RESTfulClient\n            except ImportError as e:\n                raise ImportError(\n                    \"Could not import RESTfulClient from xinference. Please install it\"\n                    \" with `pip install xinference` or `pip install xinference_client`.\"\n                ) from e\n\n        model_kwargs = model_kwargs or {}\n\n        super().__init__(\n            **{  # type: ignore[arg-type]\n                \"server_url\": server_url,\n                \"model_uid\": model_uid,\n                \"model_kwargs\": model_kwargs,\n            }\n        )\n\n        if self.server_url is None:\n            raise ValueError(\"Please provide server URL\")\n\n        if self.model_uid is None:\n            raise ValueError(\"Please provide the model UID\")\n\n        self.client = RESTfulClient(server_url)\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n        return \"xinference\"\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            **{\"server_url\": self.server_url},\n            **{\"model_uid\": self.model_uid},\n            **{\"model_kwargs\": self.model_kwargs},\n        }\n\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Call the xinference model and return the output.\n\n        Args:\n            prompt: The prompt to use for generation.\n            stop: Optional list of stop words to use when generating.\n            generate_config: Optional dictionary for the configuration used for\n                generation.\n\n        Returns:\n            The generated string by the model.\n        \"\"\"\n        if self.client is None:\n            raise ValueError(\"Client is not initialized!\")\n        model = self.client.get_model(self.model_uid)\n\n        generate_config: \"LlamaCppGenerateConfig\" = kwargs.get(\"generate_config\", {})\n\n        generate_config = {**self.model_kwargs, **generate_config}\n\n        if stop:\n            generate_config[\"stop\"] = stop\n\n        if generate_config and generate_config.get(\"stream\"):\n            combined_text_output = \"\"\n            for token in self._stream_generate(\n                model=model,\n                prompt=prompt,\n                run_manager=run_manager,\n                generate_config=generate_config,\n            ):\n                combined_text_output += token\n            return combined_text_output\n\n        else:\n            completion = model.generate(prompt=prompt, generate_config=generate_config)\n            return completion[\"choices\"][0][\"text\"]\n\n    def _stream_generate(\n        self,\n        model: Union[\"RESTfulGenerateModelHandle\", \"RESTfulChatModelHandle\"],\n        prompt: str,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        generate_config: Optional[\"LlamaCppGenerateConfig\"] = None,\n    ) -> Generator[str, None, None]:\n        \"\"\"\n        Args:\n            prompt: The prompt to use for generation.\n            model: The model used for generation.\n            stop: Optional list of stop words to use when generating.\n            generate_config: Optional dictionary for the configuration used for\n                generation.\n\n        Yields:\n            A string token.\n        \"\"\"\n        streaming_response = model.generate(\n            prompt=prompt, generate_config=generate_config\n        )\n        for chunk in streaming_response:\n            if isinstance(chunk, dict):\n                choices = chunk.get(\"choices\", [])\n                if choices:\n                    choice = choices[0]\n                    if isinstance(choice, dict):\n                        token = choice.get(\"text\", \"\")\n                        log_probs = choice.get(\"logprobs\")\n                        if run_manager:\n                            run_manager.on_llm_new_token(\n                                token=token, verbose=self.verbose, log_probs=log_probs\n                            )\n                        yield token\n\n    def _stream(\n        self,\n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[GenerationChunk]:\n        generate_config = kwargs.get(\"generate_config\", {})\n        generate_config = {**self.model_kwargs, **generate_config}\n        if stop:\n            generate_config[\"stop\"] = stop\n        for stream_resp in self._create_generate_stream(prompt, generate_config):\n            if stream_resp:\n                chunk = self._stream_response_to_generation_chunk(stream_resp)\n                if run_manager:\n                    run_manager.on_llm_new_token(\n                        chunk.text,\n                        verbose=self.verbose,\n                    )\n                yield chunk\n\n    def _create_generate_stream(\n        self, prompt: str, generate_config: Optional[Dict[str, List[str]]] = None\n    ) -> Iterator[str]:\n        if self.client is None:\n            raise ValueError(\"Client is not initialized!\")\n        model = self.client.get_model(self.model_uid)\n        yield from model.generate(prompt=prompt, generate_config=generate_config)\n\n    @staticmethod\n    def _stream_response_to_generation_chunk(\n        stream_response: str,\n    ) -> GenerationChunk:\n        \"\"\"Convert a stream response to a generation chunk.\"\"\"\n        token = \"\"\n        if isinstance(stream_response, dict):\n            choices = stream_response.get(\"choices\", [])\n            if choices:\n                choice = choices[0]\n                if isinstance(choice, dict):\n                    token = choice.get(\"text\", \"\")\n\n                    return GenerationChunk(\n                        text=token,\n                        generation_info=dict(\n                            finish_reason=choice.get(\"finish_reason\", None),\n                            logprobs=choice.get(\"logprobs\", None),\n                        ),\n                    )\n                else:\n                    raise TypeError(\"choice type error!\")\n            else:\n                return GenerationChunk(text=token)\n        else:\n            raise TypeError(\"stream_response type error!\")\n",
        "patch": "@@ -1,8 +1,10 @@\n from __future__ import annotations\n \n+import json\n from typing import (\n     TYPE_CHECKING,\n     Any,\n+    AsyncIterator,\n     Dict,\n     Generator,\n     Iterator,\n@@ -12,7 +14,12 @@\n     Union,\n )\n \n-from langchain_core.callbacks import CallbackManagerForLLMRun\n+import aiohttp\n+import requests\n+from langchain_core.callbacks import (\n+    AsyncCallbackManagerForLLMRun,\n+    CallbackManagerForLLMRun,\n+)\n from langchain_core.language_models.llms import LLM\n from langchain_core.outputs import GenerationChunk\n \n@@ -126,6 +133,7 @@ def __init__(\n         self,\n         server_url: Optional[str] = None,\n         model_uid: Optional[str] = None,\n+        api_key: Optional[str] = None,\n         **model_kwargs: Any,\n     ):\n         try:\n@@ -155,7 +163,13 @@ def __init__(\n         if self.model_uid is None:\n             raise ValueError(\"Please provide the model UID\")\n \n-        self.client = RESTfulClient(server_url)\n+        self._headers: Dict[str, str] = {}\n+        self._cluster_authed = False\n+        self._check_cluster_authenticated()\n+        if api_key is not None and self._cluster_authed:\n+            self._headers[\"Authorization\"] = f\"Bearer {api_key}\"\n+\n+        self.client = RESTfulClient(server_url, api_key)\n \n     @property\n     def _llm_type(self) -> str:\n@@ -171,6 +185,20 @@ def _identifying_params(self) -> Mapping[str, Any]:\n             **{\"model_kwargs\": self.model_kwargs},\n         }\n \n+    def _check_cluster_authenticated(self) -> None:\n+        url = f\"{self.server_url}/v1/cluster/auth\"\n+        response = requests.get(url)\n+        if response.status_code == 404:\n+            self._cluster_authed = False\n+        else:\n+            if response.status_code != 200:\n+                raise RuntimeError(\n+                    f\"Failed to get cluster information, \"\n+                    f\"detail: {response.json()['detail']}\"\n+                )\n+            response_data = response.json()\n+            self._cluster_authed = bool(response_data[\"auth\"])\n+\n     def _call(\n         self,\n         prompt: str,\n@@ -305,3 +333,61 @@ def _stream_response_to_generation_chunk(\n                 return GenerationChunk(text=token)\n         else:\n             raise TypeError(\"stream_response type error!\")\n+\n+    async def _astream(\n+        self,\n+        prompt: str,\n+        stop: Optional[List[str]] = None,\n+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n+        **kwargs: Any,\n+    ) -> AsyncIterator[GenerationChunk]:\n+        generate_config = kwargs.get(\"generate_config\", {})\n+        generate_config = {**self.model_kwargs, **generate_config}\n+        if stop:\n+            generate_config[\"stop\"] = stop\n+        async for stream_resp in self._acreate_generate_stream(prompt, generate_config):\n+            if stream_resp:\n+                chunk = self._stream_response_to_generation_chunk(stream_resp)\n+                if run_manager:\n+                    await run_manager.on_llm_new_token(\n+                        chunk.text,\n+                        verbose=self.verbose,\n+                    )\n+                yield chunk\n+\n+    async def _acreate_generate_stream(\n+        self, prompt: str, generate_config: Optional[Dict[str, List[str]]] = None\n+    ) -> AsyncIterator[str]:\n+        request_body: Dict[str, Any] = {\"model\": self.model_uid, \"prompt\": prompt}\n+        if generate_config is not None:\n+            for key, value in generate_config.items():\n+                request_body[key] = value\n+\n+        stream = bool(generate_config and generate_config.get(\"stream\"))\n+        async with aiohttp.ClientSession() as session:\n+            async with session.post(\n+                url=f\"{self.server_url}/v1/completions\",\n+                json=request_body,\n+            ) as response:\n+                if response.status != 200:\n+                    if response.status == 404:\n+                        raise FileNotFoundError(\n+                            \"astream call failed with status code 404.\"\n+                        )\n+                    else:\n+                        optional_detail = response.text\n+                        raise ValueError(\n+                            f\"astream call failed with status code {response.status}.\"\n+                            f\" Details: {optional_detail}\"\n+                        )\n+\n+                async for line in response.content:\n+                    if not stream:\n+                        yield json.loads(line)\n+                    else:\n+                        json_str = line.decode(\"utf-8\")\n+                        if line.startswith(b\"data:\"):\n+                            json_str = json_str[len(b\"data:\") :].strip()\n+                            if not json_str:\n+                                continue\n+                            yield json.loads(json_str)"
      }
    ]
  },
  {
    "number": 30033,
    "title": "Implementing the MMR algorithm for OLAP vector storage",
    "body": "\r\nThank you for contributing to LangChain!\r\n\r\n-  **Implementing the MMR algorithm for OLAP vector storage**: \r\n  - Support Apache Doris and StarRocks OLAP database.\r\n  - Example: \"vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\"\r\n\r\n\r\n- **Implementing the MMR algorithm for OLAP vector storage**: \r\n    - **Apache Doris\r\n    - **StarRocks\r\n    - **Dependencies:** any dependencies required for this change\r\n    - **Twitter handle:** if your PR gets announced, and you'd like a mention, we'll gladly shout you out!\r\n\r\n\r\n- **Add tests and docs**: \r\n  - Example: \"vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\"\r\n\r\n\r\n- [ ] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n\r\nAdditional guidelines:\r\n- Make sure optional dependencies are imported within a function.\r\n- Please do not add dependencies to pyproject.toml files (even optional ones) unless they are required for unit tests.\r\n- Most PRs should not touch more than one package.\r\n- Changes should be backwards compatible.\r\n- If you are adding something to community, do not re-import it in langchain.\r\n\r\nIf no one reviews your PR within a few days, please @-mention one of baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\r\n",
    "issue_title": "Implementing the MMR algorithm for OLAP vector storage",
    "issue_body": "\r\nThank you for contributing to LangChain!\r\n\r\n-  **Implementing the MMR algorithm for OLAP vector storage**: \r\n  - Support Apache Doris and StarRocks OLAP database.\r\n  - Example: \"vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\"\r\n\r\n\r\n- **Implementing the MMR algorithm for OLAP vector storage**: \r\n    - **Apache Doris\r\n    - **StarRocks\r\n    - **Dependencies:** any dependencies required for this change\r\n    - **Twitter handle:** if your PR gets announced, and you'd like a mention, we'll gladly shout you out!\r\n\r\n\r\n- **Add tests and docs**: \r\n  - Example: \"vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10})\"\r\n\r\n\r\n- [ ] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n\r\nAdditional guidelines:\r\n- Make sure optional dependencies are imported within a function.\r\n- Please do not add dependencies to pyproject.toml files (even optional ones) unless they are required for unit tests.\r\n- Most PRs should not touch more than one package.\r\n- Changes should be backwards compatible.\r\n- If you are adding something to community, do not re-import it in langchain.\r\n\r\nIf no one reviews your PR within a few days, please @-mention one of baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/vectorstores/apache_doris.py",
        "content_before": "from __future__ import annotations\n\nimport json\nimport logging\nfrom hashlib import sha1\nfrom threading import Thread\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple\n\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.vectorstores import VectorStore\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nlogger = logging.getLogger()\nDEBUG = False\n\n\nclass ApacheDorisSettings(BaseSettings):\n    \"\"\"Apache Doris client configuration.\n\n    Attributes:\n        apache_doris_host (str) : An URL to connect to frontend.\n                             Defaults to 'localhost'.\n        apache_doris_port (int) : URL port to connect with HTTP. Defaults to 9030.\n        username (str) : Username to login. Defaults to 'root'.\n        password (str) : Password to login. Defaults to None.\n        database (str) : Database name to find the table. Defaults to 'default'.\n        table (str) : Table name to operate on.\n                      Defaults to 'langchain'.\n\n        column_map (Dict) : Column type map to project column name onto langchain\n                            semantics. Must have keys: `text`, `id`, `vector`,\n                            must be same size to number of columns. For example:\n                            .. code-block:: python\n\n                                {\n                                    'id': 'text_id',\n                                    'embedding': 'text_embedding',\n                                    'document': 'text_plain',\n                                    'metadata': 'metadata_dictionary_in_json',\n                                }\n\n                            Defaults to identity map.\n    \"\"\"\n\n    host: str = \"localhost\"\n    port: int = 9030\n    username: str = \"root\"\n    password: str = \"\"\n\n    column_map: Dict[str, str] = {\n        \"id\": \"id\",\n        \"document\": \"document\",\n        \"embedding\": \"embedding\",\n        \"metadata\": \"metadata\",\n    }\n\n    database: str = \"default\"\n    table: str = \"langchain\"\n\n    def __getitem__(self, item: str) -> Any:\n        return getattr(self, item)\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        env_prefix=\"apache_doris_\",\n        extra=\"ignore\",\n    )\n\n\nclass ApacheDoris(VectorStore):\n    \"\"\"`Apache Doris` vector store.\n\n    You need a `pymysql` python package, and a valid account\n    to connect to Apache Doris.\n\n    For more information, please visit\n        [Apache Doris official site](https://doris.apache.org/)\n        [Apache Doris github](https://github.com/apache/doris)\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding: Embeddings,\n        *,\n        config: Optional[ApacheDorisSettings] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Constructor for Apache Doris.\n\n        Args:\n            embedding (Embeddings): Text embedding model.\n            config (ApacheDorisSettings): Apache Doris client configuration information.\n        \"\"\"\n        try:\n            import pymysql  # type: ignore[import]\n        except ImportError:\n            raise ImportError(\n                \"Could not import pymysql python package. \"\n                \"Please install it with `pip install pymysql`.\"\n            )\n        try:\n            from tqdm import tqdm\n\n            self.pgbar = tqdm\n        except ImportError:\n            # Just in case if tqdm is not installed\n            self.pgbar = lambda x, **kwargs: x\n        super().__init__()\n        if config is not None:\n            self.config = config\n        else:\n            self.config = ApacheDorisSettings()\n        assert self.config\n        assert self.config.host and self.config.port\n        assert self.config.column_map and self.config.database and self.config.table\n        for k in [\"id\", \"embedding\", \"document\", \"metadata\"]:\n            assert k in self.config.column_map\n\n        # initialize the schema\n        dim = len(embedding.embed_query(\"test\"))\n\n        self.schema = f\"\"\"\\\nCREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.table}(    \n    {self.config.column_map[\"id\"]} varchar(50),\n    {self.config.column_map[\"document\"]} string,\n    {self.config.column_map[\"embedding\"]} array<float>,\n    {self.config.column_map[\"metadata\"]} string\n) ENGINE = OLAP UNIQUE KEY(id) DISTRIBUTED BY HASH(id) \\\n  PROPERTIES (\"replication_allocation\" = \"tag.location.default: 1\")\\\n\"\"\"\n        self.dim = dim\n        self.BS = \"\\\\\"\n        self.must_escape = (\"\\\\\", \"'\")\n        self._embedding = embedding\n        self.dist_order = \"DESC\"\n        _debug_output(self.config)\n\n        # Create a connection to Apache Doris\n        self.connection = pymysql.connect(\n            host=self.config.host,\n            port=self.config.port,\n            user=self.config.username,\n            password=self.config.password,\n            database=self.config.database,\n            **kwargs,\n        )\n\n        _debug_output(self.schema)\n        _get_named_result(self.connection, self.schema)\n\n    def escape_str(self, value: str) -> str:\n        return \"\".join(f\"{self.BS}{c}\" if c in self.must_escape else c for c in value)\n\n    @property\n    def embeddings(self) -> Embeddings:\n        return self._embedding\n\n    def _build_insert_sql(self, transac: Iterable, column_names: Iterable[str]) -> str:\n        ks = \",\".join(column_names)\n        embed_tuple_index = tuple(column_names).index(\n            self.config.column_map[\"embedding\"]\n        )\n        _data = []\n        for n in transac:\n            n = \",\".join(\n                [\n                    (\n                        f\"'{self.escape_str(str(_n))}'\"\n                        if idx != embed_tuple_index\n                        else f\"{str(_n)}\"\n                    )\n                    for (idx, _n) in enumerate(n)\n                ]\n            )\n            _data.append(f\"({n})\")\n        i_str = f\"\"\"\n                INSERT INTO\n                    {self.config.database}.{self.config.table}({ks})\n                VALUES\n                {\",\".join(_data)}\n                \"\"\"\n        return i_str\n\n    def _insert(self, transac: Iterable, column_names: Iterable[str]) -> None:\n        _insert_query = self._build_insert_sql(transac, column_names)\n        _debug_output(_insert_query)\n        _get_named_result(self.connection, _insert_query)\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        batch_size: int = 32,\n        ids: Optional[Iterable[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Insert more texts through the embeddings and add to the VectorStore.\n\n        Args:\n            texts: Iterable of strings to add to the VectorStore.\n            ids: Optional list of ids to associate with the texts.\n            batch_size: Batch size of insertion\n            metadata: Optional column data to be inserted\n\n        Returns:\n            List of ids from adding the texts into the VectorStore.\n\n        \"\"\"\n        # Embed and create the documents\n        ids = ids or [sha1(t.encode(\"utf-8\")).hexdigest() for t in texts]\n        colmap_ = self.config.column_map\n        transac = []\n        column_names = {\n            colmap_[\"id\"]: ids,\n            colmap_[\"document\"]: texts,\n            colmap_[\"embedding\"]: self._embedding.embed_documents(list(texts)),\n        }\n        metadatas = metadatas or [{} for _ in texts]\n        column_names[colmap_[\"metadata\"]] = map(json.dumps, metadatas)\n        assert len(set(colmap_) - set(column_names)) >= 0\n        keys, values = zip(*column_names.items())\n        try:\n            t = None\n            for v in self.pgbar(\n                zip(*values), desc=\"Inserting data...\", total=len(metadatas)\n            ):\n                assert (\n                    len(v[keys.index(self.config.column_map[\"embedding\"])]) == self.dim\n                )\n                transac.append(v)\n                if len(transac) == batch_size:\n                    if t:\n                        t.join()\n                    t = Thread(target=self._insert, args=[transac, keys])\n                    t.start()\n                    transac = []\n            if len(transac) > 0:\n                if t:\n                    t.join()\n                self._insert(transac, keys)\n            return [i for i in ids]\n        except Exception as e:\n            logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n            return []\n\n    @classmethod\n    def from_texts(\n        cls,\n        texts: List[str],\n        embedding: Embeddings,\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\n        config: Optional[ApacheDorisSettings] = None,\n        text_ids: Optional[Iterable[str]] = None,\n        batch_size: int = 32,\n        **kwargs: Any,\n    ) -> ApacheDoris:\n        \"\"\"Create Apache Doris wrapper with existing texts\n\n        Args:\n            embedding_function (Embeddings): Function to extract text embedding\n            texts (Iterable[str]): List or tuple of strings to be added\n            config (ApacheDorisSettings, Optional): Apache Doris configuration\n            text_ids (Optional[Iterable], optional): IDs for the texts.\n                                                     Defaults to None.\n            batch_size (int, optional): BatchSize when transmitting data to Apache\n                                        Doris. Defaults to 32.\n            metadata (List[dict], optional): metadata to texts. Defaults to None.\n        Returns:\n            Apache Doris Index\n        \"\"\"\n        ctx = cls(embedding, config=config, **kwargs)\n        ctx.add_texts(texts, ids=text_ids, batch_size=batch_size, metadatas=metadatas)\n        return ctx\n\n    def __repr__(self) -> str:\n        \"\"\"Text representation for Apache Doris Vector Store, prints frontends, username\n            and schemas. Easy to use with `str(ApacheDoris())`\n\n        Returns:\n            repr: string to show connection info and data schema\n        \"\"\"\n        _repr = f\"\\033[92m\\033[1m{self.config.database}.{self.config.table} @ \"\n        _repr += f\"{self.config.host}:{self.config.port}\\033[0m\\n\\n\"\n        _repr += f\"\\033[1musername: {self.config.username}\\033[0m\\n\\nTable Schema:\\n\"\n        width = 25\n        fields = 3\n        _repr += \"-\" * (width * fields + 1) + \"\\n\"\n        columns = [\"name\", \"type\", \"key\"]\n        _repr += f\"|\\033[94m{columns[0]:24s}\\033[0m|\\033[96m{columns[1]:24s}\"\n        _repr += f\"\\033[0m|\\033[96m{columns[2]:24s}\\033[0m|\\n\"\n        _repr += \"-\" * (width * fields + 1) + \"\\n\"\n        q_str = f\"DESC {self.config.database}.{self.config.table}\"\n        _debug_output(q_str)\n        rs = _get_named_result(self.connection, q_str)\n        for r in rs:\n            _repr += f\"|\\033[94m{r['Field']:24s}\\033[0m|\\033[96m{r['Type']:24s}\"\n            _repr += f\"\\033[0m|\\033[96m{r['Key']:24s}\\033[0m|\\n\"\n        _repr += \"-\" * (width * fields + 1) + \"\\n\"\n        return _repr\n\n    def _build_query_sql(\n        self, q_emb: List[float], topk: int, where_str: Optional[str] = None\n    ) -> str:\n        q_emb_str = \",\".join(map(str, q_emb))\n        if where_str:\n            where_str = f\"WHERE {where_str}\"\n        else:\n            where_str = \"\"\n\n        q_str = f\"\"\"\n            SELECT {self.config.column_map[\"document\"]}, \n                {self.config.column_map[\"metadata\"]}, \n                cosine_distance(array<float>[{q_emb_str}],\n                  {self.config.column_map[\"embedding\"]}) as dist\n            FROM {self.config.database}.{self.config.table}\n            {where_str}\n            ORDER BY dist {self.dist_order}\n            LIMIT {topk}\n            \"\"\"\n\n        _debug_output(q_str)\n        return q_str\n\n    def similarity_search(\n        self, query: str, k: int = 4, where_str: Optional[str] = None, **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Perform a similarity search with Apache Doris\n\n        Args:\n            query (str): query string\n            k (int, optional): Top K neighbors to retrieve. Defaults to 4.\n            where_str (Optional[str], optional): where condition string.\n                                                 Defaults to None.\n\n            NOTE: Please do not let end-user to fill this and always be aware\n                  of SQL injection. When dealing with metadatas, remember to\n                  use `{self.metadata_column}.attribute` instead of `attribute`\n                  alone. The default name for it is `metadata`.\n\n        Returns:\n            List[Document]: List of Documents\n        \"\"\"\n        return self.similarity_search_by_vector(\n            self._embedding.embed_query(query), k, where_str, **kwargs\n        )\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = 4,\n        where_str: Optional[str] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Perform a similarity search with Apache Doris by vectors\n\n        Args:\n            query (str): query string\n            k (int, optional): Top K neighbors to retrieve. Defaults to 4.\n            where_str (Optional[str], optional): where condition string.\n                                                 Defaults to None.\n\n            NOTE: Please do not let end-user to fill this and always be aware\n                  of SQL injection. When dealing with metadatas, remember to\n                  use `{self.metadata_column}.attribute` instead of `attribute`\n                  alone. The default name for it is `metadata`.\n\n        Returns:\n            List[Document]: List of (Document, similarity)\n        \"\"\"\n        q_str = self._build_query_sql(embedding, k, where_str)\n        try:\n            return [\n                Document(\n                    page_content=r[self.config.column_map[\"document\"]],\n                    metadata=json.loads(r[self.config.column_map[\"metadata\"]]),\n                )\n                for r in _get_named_result(self.connection, q_str)\n            ]\n        except Exception as e:\n            logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n            return []\n\n    def similarity_search_with_relevance_scores(\n        self, query: str, k: int = 4, where_str: Optional[str] = None, **kwargs: Any\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Perform a similarity search with Apache Doris\n\n        Args:\n            query (str): query string\n            k (int, optional): Top K neighbors to retrieve. Defaults to 4.\n            where_str (Optional[str], optional): where condition string.\n                                                 Defaults to None.\n\n            NOTE: Please do not let end-user to fill this and always be aware\n                  of SQL injection. When dealing with metadatas, remember to\n                  use `{self.metadata_column}.attribute` instead of `attribute`\n                  alone. The default name for it is `metadata`.\n\n        Returns:\n            List[Document]: List of documents\n        \"\"\"\n        q_str = self._build_query_sql(self._embedding.embed_query(query), k, where_str)\n        try:\n            return [\n                (\n                    Document(\n                        page_content=r[self.config.column_map[\"document\"]],\n                        metadata=json.loads(r[self.config.column_map[\"metadata\"]]),\n                    ),\n                    r[\"dist\"],\n                )\n                for r in _get_named_result(self.connection, q_str)\n            ]\n        except Exception as e:\n            logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n            return []\n\n    def drop(self) -> None:\n        \"\"\"\n        Helper function: Drop data\n        \"\"\"\n        _get_named_result(\n            self.connection,\n            f\"DROP TABLE IF EXISTS {self.config.database}.{self.config.table}\",\n        )\n\n    @property\n    def metadata_column(self) -> str:\n        return self.config.column_map[\"metadata\"]\n\n\ndef _has_mul_sub_str(s: str, *args: Any) -> bool:\n    \"\"\"Check if a string has multiple substrings.\n\n    Args:\n        s: The string to check\n        *args: The substrings to check for in the string\n\n    Returns:\n        bool: True if all substrings are present in the string, False otherwise\n    \"\"\"\n    for a in args:\n        if a not in s:\n            return False\n    return True\n\n\ndef _debug_output(s: Any) -> None:\n    \"\"\"Print a debug message if DEBUG is True.\n\n    Args:\n        s: The message to print\n    \"\"\"\n    if DEBUG:\n        print(s)  # noqa: T201\n\n\ndef _get_named_result(connection: Any, query: str) -> List[dict[str, Any]]:\n    \"\"\"Get a named result from a query.\n\n    Args:\n        connection: The connection to the database\n        query: The query to execute\n\n    Returns:\n        List[dict[str, Any]]: The result of the query\n    \"\"\"\n    cursor = connection.cursor()\n    cursor.execute(query)\n    columns = cursor.description\n    result = []\n    for value in cursor.fetchall():\n        r = {}\n        for idx, datum in enumerate(value):\n            k = columns[idx][0]\n            r[k] = datum\n        result.append(r)\n    _debug_output(result)\n    cursor.close()\n    return result\n",
        "patch": "@@ -4,16 +4,30 @@\n import logging\n from hashlib import sha1\n from threading import Thread\n-from typing import Any, Dict, Iterable, List, Optional, Tuple\n+from typing import Any, Dict, Iterable, List, Mapping, Optional, Tuple, Union\n \n+import numpy as np\n from langchain_core.documents import Document\n from langchain_core.embeddings import Embeddings\n from langchain_core.vectorstores import VectorStore\n from pydantic_settings import BaseSettings, SettingsConfigDict\n+from typing_extensions import TypedDict\n+\n+from langchain_community.vectorstores.utils import maximal_marginal_relevance\n \n logger = logging.getLogger()\n DEBUG = False\n \n+Metadata = Mapping[str, Union[str, int, float, bool]]\n+\n+\n+class QueryResult(TypedDict):\n+    ids: List[List[str]]\n+    embeddings: List[Any]\n+    documents: List[Document]\n+    metadatas: Optional[List[Metadata]]\n+    distances: Optional[List[float]]\n+\n \n class ApacheDorisSettings(BaseSettings):\n     \"\"\"Apache Doris client configuration.\n@@ -310,10 +324,13 @@ def _build_query_sql(\n             where_str = \"\"\n \n         q_str = f\"\"\"\n-            SELECT {self.config.column_map[\"document\"]}, \n-                {self.config.column_map[\"metadata\"]}, \n+            SELECT \n+                id as id,\n+                {self.config.column_map[\"document\"]} as document, \n+                {self.config.column_map[\"metadata\"]} as metadata, \n                 cosine_distance(array<float>[{q_emb_str}],\n-                  {self.config.column_map[\"embedding\"]}) as dist\n+                {self.config.column_map[\"embedding\"]}) as dist,\n+                {self.config.column_map[\"embedding\"]} as embedding\n             FROM {self.config.database}.{self.config.table}\n             {where_str}\n             ORDER BY dist {self.dist_order}\n@@ -371,12 +388,13 @@ def similarity_search_by_vector(\n         \"\"\"\n         q_str = self._build_query_sql(embedding, k, where_str)\n         try:\n+            q_r = _get_named_result(self.connection, q_str)\n             return [\n                 Document(\n                     page_content=r[self.config.column_map[\"document\"]],\n                     metadata=json.loads(r[self.config.column_map[\"metadata\"]]),\n                 )\n-                for r in _get_named_result(self.connection, q_str)\n+                for r in q_r\n             ]\n         except Exception as e:\n             logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n@@ -430,6 +448,63 @@ def drop(self) -> None:\n     def metadata_column(self) -> str:\n         return self.config.column_map[\"metadata\"]\n \n+    def max_marginal_relevance_search_by_vector(\n+        self,\n+        embedding: list[float],\n+        k: int = 4,\n+        fetch_k: int = 20,\n+        lambda_mult: float = 0.5,\n+        **kwargs: Any,\n+    ) -> list[Document]:\n+        q_str = self._build_query_sql(embedding, fetch_k, None)\n+        q_r = _get_named_result(self.connection, q_str)\n+        results = QueryResult(\n+            ids=[r[\"id\"] for r in q_r],\n+            embeddings=[\n+                json.loads(r[self.config.column_map[\"embedding\"]]) for r in q_r\n+            ],\n+            documents=[r[self.config.column_map[\"document\"]] for r in q_r],\n+            metadatas=[json.loads(r[self.config.column_map[\"metadata\"]]) for r in q_r],\n+            distances=[r[\"dist\"] for r in q_r],\n+        )\n+\n+        mmr_selected = maximal_marginal_relevance(\n+            np.array(embedding, dtype=np.float32),\n+            results[\"embeddings\"],\n+            k=k,\n+            lambda_mult=lambda_mult,\n+        )\n+\n+        candidates = _results_to_docs(results)\n+\n+        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]\n+        return selected_results\n+\n+    def max_marginal_relevance_search(\n+        self,\n+        query: str,\n+        k: int = 5,\n+        fetch_k: int = 20,\n+        lambda_mult: float = 0.5,\n+        filter: Optional[Dict[str, str]] = None,\n+        where_document: Optional[Dict[str, str]] = None,\n+        **kwargs: Any,\n+    ) -> List[Document]:\n+        if self.embeddings is None:\n+            raise ValueError(\n+                \"For MMR search, you must specify an embedding function oncreation.\"\n+            )\n+\n+        embedding = self.embeddings.embed_query(query)\n+        return self.max_marginal_relevance_search_by_vector(\n+            embedding,\n+            k,\n+            fetch_k,\n+            lambda_mult=lambda_mult,\n+            filter=filter,\n+            where_document=where_document,\n+        )\n+\n \n def _has_mul_sub_str(s: str, *args: Any) -> bool:\n     \"\"\"Check if a string has multiple substrings.\n@@ -480,3 +555,18 @@ def _get_named_result(connection: Any, query: str) -> List[dict[str, Any]]:\n     _debug_output(result)\n     cursor.close()\n     return result\n+\n+\n+def _results_to_docs(results: Any) -> List[Document]:\n+    return [doc for doc, _ in _results_to_docs_and_scores(results)]\n+\n+\n+def _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n+    return [\n+        (Document(page_content=result[0], metadata=result[1] or {}), result[2])\n+        for result in zip(\n+            results[\"documents\"],\n+            results[\"metadatas\"],\n+            results[\"distances\"],\n+        )\n+    ]"
      },
      {
        "filename": "libs/community/langchain_community/vectorstores/starrocks.py",
        "content_before": "from __future__ import annotations\n\nimport json\nimport logging\nfrom hashlib import sha1\nfrom threading import Thread\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple\n\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.vectorstores import VectorStore\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nlogger = logging.getLogger()\nDEBUG = False\n\n\ndef has_mul_sub_str(s: str, *args: Any) -> bool:\n    \"\"\"\n    Check if a string has multiple substrings.\n    Args:\n        s: The string to check\n        *args: The substrings to check for in the string\n\n    Returns:\n        bool: True if all substrings are present in the string, False otherwise\n    \"\"\"\n    for a in args:\n        if a not in s:\n            return False\n    return True\n\n\ndef debug_output(s: Any) -> None:\n    \"\"\"\n    Print a debug message if DEBUG is True.\n    Args:\n        s: The message to print\n    \"\"\"\n    if DEBUG:\n        print(s)  # noqa: T201\n\n\ndef get_named_result(connection: Any, query: str) -> List[dict[str, Any]]:\n    \"\"\"\n    Get a named result from a query.\n    Args:\n        connection: The connection to the database\n        query: The query to execute\n\n    Returns:\n        List[dict[str, Any]]: The result of the query\n    \"\"\"\n    cursor = connection.cursor()\n    cursor.execute(query)\n    columns = cursor.description\n    result = []\n    for value in cursor.fetchall():\n        r = {}\n        for idx, datum in enumerate(value):\n            k = columns[idx][0]\n            r[k] = datum\n        result.append(r)\n    debug_output(result)\n    cursor.close()\n    return result\n\n\nclass StarRocksSettings(BaseSettings):\n    \"\"\"StarRocks client configuration.\n\n    Attribute:\n        StarRocks_host (str) : An URL to connect to MyScale backend.\n                             Defaults to 'localhost'.\n        StarRocks_port (int) : URL port to connect with HTTP. Defaults to 8443.\n        username (str) : Username to login. Defaults to None.\n        password (str) : Password to login. Defaults to None.\n        database (str) : Database name to find the table. Defaults to 'default'.\n        table (str) : Table name to operate on.\n                      Defaults to 'vector_table'.\n\n        column_map (Dict) : Column type map to project column name onto langchain\n                            semantics. Must have keys: `text`, `id`, `vector`,\n                            must be same size to number of columns. For example:\n                            .. code-block:: python\n\n                                {\n                                    'id': 'text_id',\n                                    'embedding': 'text_embedding',\n                                    'document': 'text_plain',\n                                    'metadata': 'metadata_dictionary_in_json',\n                                }\n\n                            Defaults to identity map.\n    \"\"\"\n\n    host: str = \"localhost\"\n    port: int = 9030\n    username: str = \"root\"\n    password: str = \"\"\n\n    column_map: Dict[str, str] = {\n        \"id\": \"id\",\n        \"document\": \"document\",\n        \"embedding\": \"embedding\",\n        \"metadata\": \"metadata\",\n    }\n\n    database: str = \"default\"\n    table: str = \"langchain\"\n\n    def __getitem__(self, item: str) -> Any:\n        return getattr(self, item)\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        env_prefix=\"starrocks_\",\n        extra=\"ignore\",\n    )\n\n\nclass StarRocks(VectorStore):\n    \"\"\"`StarRocks` vector store.\n\n    You need a `pymysql` python package, and a valid account\n    to connect to StarRocks.\n\n    Right now StarRocks has only implemented `cosine_similarity` function to\n    compute distance between two vectors. And there is no vector inside right now,\n    so we have to iterate all vectors and compute spatial distance.\n\n    For more information, please visit\n        [StarRocks official site](https://www.starrocks.io/)\n        [StarRocks github](https://github.com/StarRocks/starrocks)\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding: Embeddings,\n        config: Optional[StarRocksSettings] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"StarRocks Wrapper to LangChain\n\n        embedding_function (Embeddings):\n        config (StarRocksSettings): Configuration to StarRocks Client\n        \"\"\"\n        try:\n            import pymysql  # type: ignore[import]\n        except ImportError:\n            raise ImportError(\n                \"Could not import pymysql python package. \"\n                \"Please install it with `pip install pymysql`.\"\n            )\n        try:\n            from tqdm import tqdm\n\n            self.pgbar = tqdm\n        except ImportError:\n            # Just in case if tqdm is not installed\n            self.pgbar = lambda x, **kwargs: x\n        super().__init__()\n        if config is not None:\n            self.config = config\n        else:\n            self.config = StarRocksSettings()\n        assert self.config\n        assert self.config.host and self.config.port\n        assert self.config.column_map and self.config.database and self.config.table\n        for k in [\"id\", \"embedding\", \"document\", \"metadata\"]:\n            assert k in self.config.column_map\n\n        # initialize the schema\n        dim = len(embedding.embed_query(\"test\"))\n\n        self.schema = f\"\"\"\\\nCREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.table}(    \n    {self.config.column_map[\"id\"]} string,\n    {self.config.column_map[\"document\"]} string,\n    {self.config.column_map[\"embedding\"]} array<float>,\n    {self.config.column_map[\"metadata\"]} string\n) ENGINE = OLAP PRIMARY KEY(id) DISTRIBUTED BY HASH(id) \\\n  PROPERTIES (\"replication_num\" = \"1\")\\\n\"\"\"\n        self.dim = dim\n        self.BS = \"\\\\\"\n        self.must_escape = (\"\\\\\", \"'\")\n        self.embedding_function = embedding\n        self.dist_order = \"DESC\"\n        debug_output(self.config)\n\n        # Create a connection to StarRocks\n        self.connection = pymysql.connect(\n            host=self.config.host,\n            port=self.config.port,\n            user=self.config.username,\n            password=self.config.password,\n            database=self.config.database,\n            **kwargs,\n        )\n\n        debug_output(self.schema)\n        get_named_result(self.connection, self.schema)\n\n    def escape_str(self, value: str) -> str:\n        return \"\".join(f\"{self.BS}{c}\" if c in self.must_escape else c for c in value)\n\n    @property\n    def embeddings(self) -> Embeddings:\n        return self.embedding_function\n\n    def _build_insert_sql(self, transac: Iterable, column_names: Iterable[str]) -> str:\n        ks = \",\".join(column_names)\n        embed_tuple_index = tuple(column_names).index(\n            self.config.column_map[\"embedding\"]\n        )\n        _data = []\n        for n in transac:\n            n = \",\".join(\n                [\n                    (\n                        f\"'{self.escape_str(str(_n))}'\"\n                        if idx != embed_tuple_index\n                        else f\"array<float>{str(_n)}\"\n                    )\n                    for (idx, _n) in enumerate(n)\n                ]\n            )\n            _data.append(f\"({n})\")\n        i_str = f\"\"\"\n                INSERT INTO\n                    {self.config.database}.{self.config.table}({ks})\n                VALUES\n                {\",\".join(_data)}\n                \"\"\"\n        return i_str\n\n    def _insert(self, transac: Iterable, column_names: Iterable[str]) -> None:\n        _insert_query = self._build_insert_sql(transac, column_names)\n        debug_output(_insert_query)\n        get_named_result(self.connection, _insert_query)\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        batch_size: int = 32,\n        ids: Optional[Iterable[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Insert more texts through the embeddings and add to the VectorStore.\n\n        Args:\n            texts: Iterable of strings to add to the VectorStore.\n            ids: Optional list of ids to associate with the texts.\n            batch_size: Batch size of insertion\n            metadata: Optional column data to be inserted\n\n        Returns:\n            List of ids from adding the texts into the VectorStore.\n\n        \"\"\"\n        # Embed and create the documents\n        ids = ids or [sha1(t.encode(\"utf-8\")).hexdigest() for t in texts]\n        colmap_ = self.config.column_map\n        transac = []\n        column_names = {\n            colmap_[\"id\"]: ids,\n            colmap_[\"document\"]: texts,\n            colmap_[\"embedding\"]: self.embedding_function.embed_documents(list(texts)),\n        }\n        metadatas = metadatas or [{} for _ in texts]\n        column_names[colmap_[\"metadata\"]] = map(json.dumps, metadatas)\n        assert len(set(colmap_) - set(column_names)) >= 0\n        keys, values = zip(*column_names.items())\n        try:\n            t = None\n            for v in self.pgbar(\n                zip(*values), desc=\"Inserting data...\", total=len(metadatas)\n            ):\n                assert (\n                    len(v[keys.index(self.config.column_map[\"embedding\"])]) == self.dim\n                )\n                transac.append(v)\n                if len(transac) == batch_size:\n                    if t:\n                        t.join()\n                    t = Thread(target=self._insert, args=[transac, keys])\n                    t.start()\n                    transac = []\n            if len(transac) > 0:\n                if t:\n                    t.join()\n                self._insert(transac, keys)\n            return [i for i in ids]\n        except Exception as e:\n            logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n            return []\n\n    @classmethod\n    def from_texts(\n        cls,\n        texts: List[str],\n        embedding: Embeddings,\n        metadatas: Optional[List[Dict[Any, Any]]] = None,\n        config: Optional[StarRocksSettings] = None,\n        text_ids: Optional[Iterable[str]] = None,\n        batch_size: int = 32,\n        **kwargs: Any,\n    ) -> StarRocks:\n        \"\"\"Create StarRocks wrapper with existing texts\n\n        Args:\n            embedding_function (Embeddings): Function to extract text embedding\n            texts (Iterable[str]): List or tuple of strings to be added\n            config (StarRocksSettings, Optional): StarRocks configuration\n            text_ids (Optional[Iterable], optional): IDs for the texts.\n                                                     Defaults to None.\n            batch_size (int, optional): Batchsize when transmitting data to StarRocks.\n                                        Defaults to 32.\n            metadata (List[dict], optional): metadata to texts. Defaults to None.\n        Returns:\n            StarRocks Index\n        \"\"\"\n        ctx = cls(embedding, config, **kwargs)\n        ctx.add_texts(texts, ids=text_ids, batch_size=batch_size, metadatas=metadatas)\n        return ctx\n\n    def __repr__(self) -> str:\n        \"\"\"Text representation for StarRocks Vector Store, prints backends, username\n            and schemas. Easy to use with `str(StarRocks())`\n\n        Returns:\n            repr: string to show connection info and data schema\n        \"\"\"\n        _repr = f\"\\033[92m\\033[1m{self.config.database}.{self.config.table} @ \"\n        _repr += f\"{self.config.host}:{self.config.port}\\033[0m\\n\\n\"\n        _repr += f\"\\033[1musername: {self.config.username}\\033[0m\\n\\nTable Schema:\\n\"\n        width = 25\n        fields = 3\n        _repr += \"-\" * (width * fields + 1) + \"\\n\"\n        columns = [\"name\", \"type\", \"key\"]\n        _repr += f\"|\\033[94m{columns[0]:24s}\\033[0m|\\033[96m{columns[1]:24s}\"\n        _repr += f\"\\033[0m|\\033[96m{columns[2]:24s}\\033[0m|\\n\"\n        _repr += \"-\" * (width * fields + 1) + \"\\n\"\n        q_str = f\"DESC {self.config.database}.{self.config.table}\"\n        debug_output(q_str)\n        rs = get_named_result(self.connection, q_str)\n        for r in rs:\n            _repr += f\"|\\033[94m{r['Field']:24s}\\033[0m|\\033[96m{r['Type']:24s}\"\n            _repr += f\"\\033[0m|\\033[96m{r['Key']:24s}\\033[0m|\\n\"\n        _repr += \"-\" * (width * fields + 1) + \"\\n\"\n        return _repr\n\n    def _build_query_sql(\n        self, q_emb: List[float], topk: int, where_str: Optional[str] = None\n    ) -> str:\n        q_emb_str = \",\".join(map(str, q_emb))\n        if where_str:\n            where_str = f\"WHERE {where_str}\"\n        else:\n            where_str = \"\"\n\n        q_str = f\"\"\"\n            SELECT {self.config.column_map[\"document\"]}, \n                {self.config.column_map[\"metadata\"]}, \n                cosine_similarity_norm(array<float>[{q_emb_str}],\n                  {self.config.column_map[\"embedding\"]}) as dist\n            FROM {self.config.database}.{self.config.table}\n            {where_str}\n            ORDER BY dist {self.dist_order}\n            LIMIT {topk}\n            \"\"\"\n\n        debug_output(q_str)\n        return q_str\n\n    def similarity_search(\n        self, query: str, k: int = 4, where_str: Optional[str] = None, **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Perform a similarity search with StarRocks\n\n        Args:\n            query (str): query string\n            k (int, optional): Top K neighbors to retrieve. Defaults to 4.\n            where_str (Optional[str], optional): where condition string.\n                                                 Defaults to None.\n\n            NOTE: Please do not let end-user to fill this and always be aware\n                  of SQL injection. When dealing with metadatas, remember to\n                  use `{self.metadata_column}.attribute` instead of `attribute`\n                  alone. The default name for it is `metadata`.\n\n        Returns:\n            List[Document]: List of Documents\n        \"\"\"\n        return self.similarity_search_by_vector(\n            self.embedding_function.embed_query(query), k, where_str, **kwargs\n        )\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = 4,\n        where_str: Optional[str] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Perform a similarity search with StarRocks by vectors\n\n        Args:\n            query (str): query string\n            k (int, optional): Top K neighbors to retrieve. Defaults to 4.\n            where_str (Optional[str], optional): where condition string.\n                                                 Defaults to None.\n\n            NOTE: Please do not let end-user to fill this and always be aware\n                  of SQL injection. When dealing with metadatas, remember to\n                  use `{self.metadata_column}.attribute` instead of `attribute`\n                  alone. The default name for it is `metadata`.\n\n        Returns:\n            List[Document]: List of (Document, similarity)\n        \"\"\"\n        q_str = self._build_query_sql(embedding, k, where_str)\n        try:\n            return [\n                Document(\n                    page_content=r[self.config.column_map[\"document\"]],\n                    metadata=json.loads(r[self.config.column_map[\"metadata\"]]),\n                )\n                for r in get_named_result(self.connection, q_str)\n            ]\n        except Exception as e:\n            logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n            return []\n\n    def similarity_search_with_relevance_scores(\n        self, query: str, k: int = 4, where_str: Optional[str] = None, **kwargs: Any\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Perform a similarity search with StarRocks\n\n        Args:\n            query (str): query string\n            k (int, optional): Top K neighbors to retrieve. Defaults to 4.\n            where_str (Optional[str], optional): where condition string.\n                                                 Defaults to None.\n\n            NOTE: Please do not let end-user to fill this and always be aware\n                  of SQL injection. When dealing with metadatas, remember to\n                  use `{self.metadata_column}.attribute` instead of `attribute`\n                  alone. The default name for it is `metadata`.\n\n        Returns:\n            List[Document]: List of documents\n        \"\"\"\n        q_str = self._build_query_sql(\n            self.embedding_function.embed_query(query), k, where_str\n        )\n        try:\n            return [\n                (\n                    Document(\n                        page_content=r[self.config.column_map[\"document\"]],\n                        metadata=json.loads(r[self.config.column_map[\"metadata\"]]),\n                    ),\n                    r[\"dist\"],\n                )\n                for r in get_named_result(self.connection, q_str)\n            ]\n        except Exception as e:\n            logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n            return []\n\n    def drop(self) -> None:\n        \"\"\"\n        Helper function: Drop data\n        \"\"\"\n        get_named_result(\n            self.connection,\n            f\"DROP TABLE IF EXISTS {self.config.database}.{self.config.table}\",\n        )\n\n    @property\n    def metadata_column(self) -> str:\n        return self.config.column_map[\"metadata\"]\n",
        "patch": "@@ -4,12 +4,16 @@\n import logging\n from hashlib import sha1\n from threading import Thread\n-from typing import Any, Dict, Iterable, List, Optional, Tuple\n+from typing import Any, Dict, Iterable, List, Mapping, Optional, Tuple, Union\n \n+import numpy as np\n from langchain_core.documents import Document\n from langchain_core.embeddings import Embeddings\n from langchain_core.vectorstores import VectorStore\n from pydantic_settings import BaseSettings, SettingsConfigDict\n+from typing_extensions import TypedDict\n+\n+from langchain_community.vectorstores.utils import maximal_marginal_relevance\n \n logger = logging.getLogger()\n DEBUG = False\n@@ -66,6 +70,17 @@ def get_named_result(connection: Any, query: str) -> List[dict[str, Any]]:\n     return result\n \n \n+Metadata = Mapping[str, Union[str, int, float, bool]]\n+\n+\n+class QueryResult(TypedDict):\n+    ids: List[List[str]]\n+    embeddings: List[Any]\n+    documents: List[Document]\n+    metadatas: Optional[List[Metadata]]\n+    distances: Optional[List[float]]\n+\n+\n class StarRocksSettings(BaseSettings):\n     \"\"\"StarRocks client configuration.\n \n@@ -363,10 +378,13 @@ def _build_query_sql(\n             where_str = \"\"\n \n         q_str = f\"\"\"\n-            SELECT {self.config.column_map[\"document\"]}, \n-                {self.config.column_map[\"metadata\"]}, \n+            SELECT \n+                id as id,\n+                {self.config.column_map[\"document\"]} as document, \n+                {self.config.column_map[\"metadata\"]} as metadata, \n                 cosine_similarity_norm(array<float>[{q_emb_str}],\n-                  {self.config.column_map[\"embedding\"]}) as dist\n+                {self.config.column_map[\"embedding\"]}) as dist,\n+                {self.config.column_map[\"embedding\"]} as embedding\n             FROM {self.config.database}.{self.config.table}\n             {where_str}\n             ORDER BY dist {self.dist_order}\n@@ -424,12 +442,13 @@ def similarity_search_by_vector(\n         \"\"\"\n         q_str = self._build_query_sql(embedding, k, where_str)\n         try:\n+            q_r = get_named_result(self.connection, q_str)\n             return [\n                 Document(\n                     page_content=r[self.config.column_map[\"document\"]],\n                     metadata=json.loads(r[self.config.column_map[\"metadata\"]]),\n                 )\n-                for r in get_named_result(self.connection, q_str)\n+                for r in q_r\n             ]\n         except Exception as e:\n             logger.error(f\"\\033[91m\\033[1m{type(e)}\\033[0m \\033[95m{str(e)}\\033[0m\")\n@@ -484,3 +503,75 @@ def drop(self) -> None:\n     @property\n     def metadata_column(self) -> str:\n         return self.config.column_map[\"metadata\"]\n+\n+    def max_marginal_relevance_search_by_vector(\n+        self,\n+        embedding: list[float],\n+        k: int = 4,\n+        fetch_k: int = 20,\n+        lambda_mult: float = 0.5,\n+        **kwargs: Any,\n+    ) -> list[Document]:\n+        q_str = self._build_query_sql(embedding, fetch_k, None)\n+        q_r = get_named_result(self.connection, q_str)\n+        results = QueryResult(\n+            ids=[r[\"id\"] for r in q_r],\n+            embeddings=[\n+                json.loads(r[self.config.column_map[\"embedding\"]]) for r in q_r\n+            ],\n+            documents=[r[self.config.column_map[\"document\"]] for r in q_r],\n+            metadatas=[json.loads(r[self.config.column_map[\"metadata\"]]) for r in q_r],\n+            distances=[r[\"dist\"] for r in q_r],\n+        )\n+\n+        mmr_selected = maximal_marginal_relevance(\n+            np.array(embedding, dtype=np.float32),\n+            results[\"embeddings\"],\n+            k=k,\n+            lambda_mult=lambda_mult,\n+        )\n+\n+        candidates = _results_to_docs(results)\n+\n+        selected_results = [r for i, r in enumerate(candidates) if i in mmr_selected]\n+        return selected_results\n+\n+    def max_marginal_relevance_search(\n+        self,\n+        query: str,\n+        k: int = 5,\n+        fetch_k: int = 20,\n+        lambda_mult: float = 0.5,\n+        filter: Optional[Dict[str, str]] = None,\n+        where_document: Optional[Dict[str, str]] = None,\n+        **kwargs: Any,\n+    ) -> List[Document]:\n+        if self.embeddings is None:\n+            raise ValueError(\n+                \"For MMR search, you must specify an embedding function oncreation.\"\n+            )\n+\n+        embedding = self.embeddings.embed_query(query)\n+        return self.max_marginal_relevance_search_by_vector(\n+            embedding,\n+            k,\n+            fetch_k,\n+            lambda_mult=lambda_mult,\n+            filter=filter,\n+            where_document=where_document,\n+        )\n+\n+\n+def _results_to_docs(results: Any) -> List[Document]:\n+    return [doc for doc, _ in _results_to_docs_and_scores(results)]\n+\n+\n+def _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n+    return [\n+        (Document(page_content=result[0], metadata=result[1] or {}), result[2])\n+        for result in zip(\n+            results[\"documents\"],\n+            results[\"metadatas\"],\n+            results[\"distances\"],\n+        )\n+    ]"
      }
    ]
  },
  {
    "number": 30043,
    "title": "community: PlaywrightURLLoader should wait for page load event before attempting to extract data",
    "body": "## Description\r\n\r\nThe PlaywrightURLLoader should wait for a page to be loaded before attempting to extract data.\r\n",
    "issue_title": "community: PlaywrightURLLoader should wait for page load event before attempting to extract data",
    "issue_body": "## Description\r\n\r\nThe PlaywrightURLLoader should wait for a page to be loaded before attempting to extract data.\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/document_loaders/url_playwright.py",
        "content_before": "\"\"\"Loader that uses Playwright to load a page, then uses unstructured to parse html.\"\"\"\n\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, AsyncIterator, Dict, Iterator, List, Optional\n\nfrom langchain_core.documents import Document\n\nfrom langchain_community.document_loaders.base import BaseLoader\n\nif TYPE_CHECKING:\n    from playwright.async_api import Browser as AsyncBrowser\n    from playwright.async_api import Page as AsyncPage\n    from playwright.async_api import Response as AsyncResponse\n    from playwright.sync_api import Browser, Page, Response\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass PlaywrightEvaluator(ABC):\n    \"\"\"Abstract base class for all evaluators.\n\n    Each evaluator should take a page, a browser instance, and a response\n    object, process the page as necessary, and return the resulting text.\n    \"\"\"\n\n    @abstractmethod\n    def evaluate(self, page: \"Page\", browser: \"Browser\", response: \"Response\") -> str:\n        \"\"\"Synchronously process the page and return the resulting text.\n\n        Args:\n            page: The page to process.\n            browser: The browser instance.\n            response: The response from page.goto().\n\n        Returns:\n            text: The text content of the page.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def evaluate_async(\n        self, page: \"AsyncPage\", browser: \"AsyncBrowser\", response: \"AsyncResponse\"\n    ) -> str:\n        \"\"\"Asynchronously process the page and return the resulting text.\n\n        Args:\n            page: The page to process.\n            browser: The browser instance.\n            response: The response from page.goto().\n\n        Returns:\n            text: The text content of the page.\n        \"\"\"\n        pass\n\n\nclass UnstructuredHtmlEvaluator(PlaywrightEvaluator):\n    \"\"\"Evaluate the page HTML content using the `unstructured` library.\"\"\"\n\n    def __init__(self, remove_selectors: Optional[List[str]] = None):\n        \"\"\"Initialize UnstructuredHtmlEvaluator.\"\"\"\n        try:\n            import unstructured  # noqa:F401\n        except ImportError:\n            raise ImportError(\n                \"unstructured package not found, please install it with \"\n                \"`pip install unstructured`\"\n            )\n\n        self.remove_selectors = remove_selectors\n\n    def evaluate(self, page: \"Page\", browser: \"Browser\", response: \"Response\") -> str:\n        \"\"\"Synchronously process the HTML content of the page.\"\"\"\n        from unstructured.partition.html import partition_html\n\n        for selector in self.remove_selectors or []:\n            elements = page.locator(selector).all()\n            for element in elements:\n                if element.is_visible():\n                    element.evaluate(\"element => element.remove()\")\n\n        page_source = page.content()\n        elements = partition_html(text=page_source)\n        return \"\\n\\n\".join([str(el) for el in elements])\n\n    async def evaluate_async(\n        self, page: \"AsyncPage\", browser: \"AsyncBrowser\", response: \"AsyncResponse\"\n    ) -> str:\n        \"\"\"Asynchronously process the HTML content of the page.\"\"\"\n        from unstructured.partition.html import partition_html\n\n        for selector in self.remove_selectors or []:\n            elements = await page.locator(selector).all()\n            for element in elements:\n                if await element.is_visible():\n                    await element.evaluate(\"element => element.remove()\")\n\n        page_source = await page.content()\n        elements = partition_html(text=page_source)\n        return \"\\n\\n\".join([str(el) for el in elements])\n\n\nclass PlaywrightURLLoader(BaseLoader):\n    \"\"\"Load `HTML` pages with `Playwright` and parse with `Unstructured`.\n\n    This is useful for loading pages that require javascript to render.\n\n    Attributes:\n        urls (List[str]): List of URLs to load.\n        continue_on_failure (bool): If True, continue loading other URLs on failure.\n        headless (bool): If True, the browser will run in headless mode.\n        proxy (Optional[Dict[str, str]]): If set, the browser will access URLs\n            through the specified proxy.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_community.document_loaders import PlaywrightURLLoader\n\n            urls = [\"https://api.ipify.org/?format=json\",]\n            proxy={\n                \"server\": \"https://xx.xx.xx:15818\", # https://<host>:<port>\n                \"username\": \"username\",\n                \"password\": \"password\"\n            }\n            loader = PlaywrightURLLoader(urls, proxy=proxy)\n            data = loader.load()\n    \"\"\"\n\n    def __init__(\n        self,\n        urls: List[str],\n        continue_on_failure: bool = True,\n        headless: bool = True,\n        remove_selectors: Optional[List[str]] = None,\n        evaluator: Optional[PlaywrightEvaluator] = None,\n        proxy: Optional[Dict[str, str]] = None,\n    ):\n        \"\"\"Load a list of URLs using Playwright.\"\"\"\n        try:\n            import playwright  # noqa:F401\n        except ImportError:\n            raise ImportError(\n                \"playwright package not found, please install it with \"\n                \"`pip install playwright`\"\n            )\n\n        self.urls = urls\n        self.continue_on_failure = continue_on_failure\n        self.headless = headless\n        self.proxy = proxy\n\n        if remove_selectors and evaluator:\n            raise ValueError(\n                \"`remove_selectors` and `evaluator` cannot be both not None\"\n            )\n\n        # Use the provided evaluator, if any, otherwise, use the default.\n        self.evaluator = evaluator or UnstructuredHtmlEvaluator(remove_selectors)\n\n    def lazy_load(self) -> Iterator[Document]:\n        \"\"\"Load the specified URLs using Playwright and create Document instances.\n\n        Returns:\n            A list of Document instances with loaded content.\n        \"\"\"\n        from playwright.sync_api import sync_playwright\n\n        with sync_playwright() as p:\n            browser = p.chromium.launch(headless=self.headless, proxy=self.proxy)\n            for url in self.urls:\n                try:\n                    page = browser.new_page()\n                    response = page.goto(url)\n                    if response is None:\n                        raise ValueError(f\"page.goto() returned None for url {url}\")\n\n                    text = self.evaluator.evaluate(page, browser, response)\n                    metadata = {\"source\": url}\n                    yield Document(page_content=text, metadata=metadata)\n                except Exception as e:\n                    if self.continue_on_failure:\n                        logger.error(\n                            f\"Error fetching or processing {url}, exception: {e}\"\n                        )\n                    else:\n                        raise e\n            browser.close()\n\n    async def aload(self) -> List[Document]:\n        \"\"\"Load the specified URLs with Playwright and create Documents asynchronously.\n        Use this function when in a jupyter notebook environment.\n\n        Returns:\n            A list of Document instances with loaded content.\n        \"\"\"\n        return [doc async for doc in self.alazy_load()]\n\n    async def alazy_load(self) -> AsyncIterator[Document]:\n        \"\"\"Load the specified URLs with Playwright and create Documents asynchronously.\n        Use this function when in a jupyter notebook environment.\n\n        Returns:\n            A list of Document instances with loaded content.\n        \"\"\"\n        from playwright.async_api import async_playwright\n\n        async with async_playwright() as p:\n            browser = await p.chromium.launch(headless=self.headless, proxy=self.proxy)\n            for url in self.urls:\n                try:\n                    page = await browser.new_page()\n                    response = await page.goto(url)\n                    if response is None:\n                        raise ValueError(f\"page.goto() returned None for url {url}\")\n\n                    text = await self.evaluator.evaluate_async(page, browser, response)\n                    metadata = {\"source\": url}\n                    yield Document(page_content=text, metadata=metadata)\n                except Exception as e:\n                    if self.continue_on_failure:\n                        logger.error(\n                            f\"Error fetching or processing {url}, exception: {e}\"\n                        )\n                    else:\n                        raise e\n            await browser.close()\n",
        "patch": "@@ -177,6 +177,8 @@ def lazy_load(self) -> Iterator[Document]:\n                     if response is None:\n                         raise ValueError(f\"page.goto() returned None for url {url}\")\n \n+                    page.wait_for_load_state(\"load\")\n+\n                     text = self.evaluator.evaluate(page, browser, response)\n                     metadata = {\"source\": url}\n                     yield Document(page_content=text, metadata=metadata)\n@@ -216,6 +218,8 @@ async def alazy_load(self) -> AsyncIterator[Document]:\n                     if response is None:\n                         raise ValueError(f\"page.goto() returned None for url {url}\")\n \n+                    await page.wait_for_load_state(\"load\")\n+\n                     text = await self.evaluator.evaluate_async(page, browser, response)\n                     metadata = {\"source\": url}\n                     yield Document(page_content=text, metadata=metadata)"
      }
    ]
  },
  {
    "number": 14427,
    "title": "Simplify ensemble retriever",
    "body": "  - **Description:** code simplification to improve readability and remove unnecessary memory allocations.\r\n  - **Tag maintainer**: @baskaryan, @eyurtsev, @hwchase17.",
    "issue_title": "Simplify ensemble retriever",
    "issue_body": "  - **Description:** code simplification to improve readability and remove unnecessary memory allocations.\r\n  - **Tag maintainer**: @baskaryan, @eyurtsev, @hwchase17.",
    "files": [
      {
        "filename": "libs/langchain/langchain/retrievers/ensemble.py",
        "content_before": "\"\"\"\nEnsemble retriever that ensemble the results of \nmultiple retrievers by using weighted  Reciprocal Rank Fusion\n\"\"\"\nimport asyncio\nfrom typing import Any, Dict, List, Optional, cast\n\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForRetrieverRun,\n    CallbackManagerForRetrieverRun,\n)\nfrom langchain_core.documents import Document\nfrom langchain_core.load.dump import dumpd\nfrom langchain_core.pydantic_v1 import root_validator\nfrom langchain_core.retrievers import BaseRetriever, RetrieverLike\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.runnables.config import ensure_config, patch_config\nfrom langchain_core.runnables.utils import (\n    ConfigurableFieldSpec,\n    get_unique_config_specs,\n)\n\n\nclass EnsembleRetriever(BaseRetriever):\n    \"\"\"Retriever that ensembles the multiple retrievers.\n\n    It uses a rank fusion.\n\n    Args:\n        retrievers: A list of retrievers to ensemble.\n        weights: A list of weights corresponding to the retrievers. Defaults to equal\n            weighting for all retrievers.\n        c: A constant added to the rank, controlling the balance between the importance\n            of high-ranked items and the consideration given to lower-ranked items.\n            Default is 60.\n    \"\"\"\n\n    retrievers: List[RetrieverLike]\n    weights: List[float]\n    c: int = 60\n\n    @property\n    def config_specs(self) -> List[ConfigurableFieldSpec]:\n        \"\"\"List configurable fields for this runnable.\"\"\"\n        return get_unique_config_specs(\n            spec for retriever in self.retrievers for spec in retriever.config_specs\n        )\n\n    @root_validator(pre=True)\n    def set_weights(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        if not values.get(\"weights\"):\n            n_retrievers = len(values[\"retrievers\"])\n            values[\"weights\"] = [1 / n_retrievers] * n_retrievers\n        return values\n\n    def invoke(\n        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> List[Document]:\n        from langchain_core.callbacks import CallbackManager\n\n        config = ensure_config(config)\n        callback_manager = CallbackManager.configure(\n            config.get(\"callbacks\"),\n            None,\n            verbose=kwargs.get(\"verbose\", False),\n            inheritable_tags=config.get(\"tags\", []),\n            local_tags=self.tags,\n            inheritable_metadata=config.get(\"metadata\", {}),\n            local_metadata=self.metadata,\n        )\n        run_manager = callback_manager.on_retriever_start(\n            dumpd(self),\n            input,\n            name=config.get(\"run_name\"),\n            **kwargs,\n        )\n        try:\n            result = self.rank_fusion(input, run_manager=run_manager, config=config)\n        except Exception as e:\n            run_manager.on_retriever_error(e)\n            raise e\n        else:\n            run_manager.on_retriever_end(\n                result,\n                **kwargs,\n            )\n            return result\n\n    async def ainvoke(\n        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> List[Document]:\n        from langchain_core.callbacks import AsyncCallbackManager\n\n        config = ensure_config(config)\n        callback_manager = AsyncCallbackManager.configure(\n            config.get(\"callbacks\"),\n            None,\n            verbose=kwargs.get(\"verbose\", False),\n            inheritable_tags=config.get(\"tags\", []),\n            local_tags=self.tags,\n            inheritable_metadata=config.get(\"metadata\", {}),\n            local_metadata=self.metadata,\n        )\n        run_manager = await callback_manager.on_retriever_start(\n            dumpd(self),\n            input,\n            name=config.get(\"run_name\"),\n            **kwargs,\n        )\n        try:\n            result = await self.arank_fusion(\n                input, run_manager=run_manager, config=config\n            )\n        except Exception as e:\n            await run_manager.on_retriever_error(e)\n            raise e\n        else:\n            await run_manager.on_retriever_end(\n                result,\n                **kwargs,\n            )\n            return result\n\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: CallbackManagerForRetrieverRun,\n    ) -> List[Document]:\n        \"\"\"\n        Get the relevant documents for a given query.\n\n        Args:\n            query: The query to search for.\n\n        Returns:\n            A list of reranked documents.\n        \"\"\"\n\n        # Get fused result of the retrievers.\n        fused_documents = self.rank_fusion(query, run_manager)\n\n        return fused_documents\n\n    async def _aget_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: AsyncCallbackManagerForRetrieverRun,\n    ) -> List[Document]:\n        \"\"\"\n        Asynchronously get the relevant documents for a given query.\n\n        Args:\n            query: The query to search for.\n\n        Returns:\n            A list of reranked documents.\n        \"\"\"\n\n        # Get fused result of the retrievers.\n        fused_documents = await self.arank_fusion(query, run_manager)\n\n        return fused_documents\n\n    def rank_fusion(\n        self,\n        query: str,\n        run_manager: CallbackManagerForRetrieverRun,\n        *,\n        config: Optional[RunnableConfig] = None,\n    ) -> List[Document]:\n        \"\"\"\n        Retrieve the results of the retrievers and use rank_fusion_func to get\n        the final result.\n\n        Args:\n            query: The query to search for.\n\n        Returns:\n            A list of reranked documents.\n        \"\"\"\n\n        # Get the results of all retrievers.\n        retriever_docs = [\n            retriever.invoke(\n                query,\n                patch_config(\n                    config, callbacks=run_manager.get_child(tag=f\"retriever_{i+1}\")\n                ),\n            )\n            for i, retriever in enumerate(self.retrievers)\n        ]\n\n        # Enforce that retrieved docs are Documents for each list in retriever_docs\n        for i in range(len(retriever_docs)):\n            retriever_docs[i] = [\n                Document(page_content=cast(str, doc)) if isinstance(doc, str) else doc\n                for doc in retriever_docs[i]\n            ]\n\n        # apply rank fusion\n        fused_documents = self.weighted_reciprocal_rank(retriever_docs)\n\n        return fused_documents\n\n    async def arank_fusion(\n        self,\n        query: str,\n        run_manager: AsyncCallbackManagerForRetrieverRun,\n        *,\n        config: Optional[RunnableConfig] = None,\n    ) -> List[Document]:\n        \"\"\"\n        Asynchronously retrieve the results of the retrievers\n        and use rank_fusion_func to get the final result.\n\n        Args:\n            query: The query to search for.\n\n        Returns:\n            A list of reranked documents.\n        \"\"\"\n\n        # Get the results of all retrievers.\n        retriever_docs = await asyncio.gather(\n            *[\n                retriever.ainvoke(\n                    query,\n                    patch_config(\n                        config, callbacks=run_manager.get_child(tag=f\"retriever_{i+1}\")\n                    ),\n                )\n                for i, retriever in enumerate(self.retrievers)\n            ]\n        )\n\n        # Enforce that retrieved docs are Documents for each list in retriever_docs\n        for i in range(len(retriever_docs)):\n            retriever_docs[i] = [\n                Document(page_content=doc) if not isinstance(doc, Document) else doc  # type: ignore[arg-type]\n                for doc in retriever_docs[i]\n            ]\n\n        # apply rank fusion\n        fused_documents = self.weighted_reciprocal_rank(retriever_docs)\n\n        return fused_documents\n\n    def weighted_reciprocal_rank(\n        self, doc_lists: List[List[Document]]\n    ) -> List[Document]:\n        \"\"\"\n        Perform weighted Reciprocal Rank Fusion on multiple rank lists.\n        You can find more details about RRF here:\n        https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf\n\n        Args:\n            doc_lists: A list of rank lists, where each rank list contains unique items.\n\n        Returns:\n            list: The final aggregated list of items sorted by their weighted RRF\n                    scores in descending order.\n        \"\"\"\n        if len(doc_lists) != len(self.weights):\n            raise ValueError(\n                \"Number of rank lists must be equal to the number of weights.\"\n            )\n\n        # Create a union of all unique documents in the input doc_lists\n        all_documents = set()\n        for doc_list in doc_lists:\n            for doc in doc_list:\n                all_documents.add(doc.page_content)\n\n        # Initialize the RRF score dictionary for each document\n        rrf_score_dic = {doc: 0.0 for doc in all_documents}\n\n        # Calculate RRF scores for each document\n        for doc_list, weight in zip(doc_lists, self.weights):\n            for rank, doc in enumerate(doc_list, start=1):\n                rrf_score = weight * (1 / (rank + self.c))\n                rrf_score_dic[doc.page_content] += rrf_score\n\n        # Sort documents by their RRF scores in descending order\n        sorted_documents = sorted(\n            rrf_score_dic.keys(), key=lambda x: rrf_score_dic[x], reverse=True\n        )\n\n        # Map the sorted page_content back to the original document objects\n        page_content_to_doc_map = {\n            doc.page_content: doc for doc_list in doc_lists for doc in doc_list\n        }\n        sorted_docs = [\n            page_content_to_doc_map[page_content] for page_content in sorted_documents\n        ]\n\n        return sorted_docs\n",
        "patch": "@@ -1,9 +1,22 @@\n \"\"\"\n-Ensemble retriever that ensemble the results of \n+Ensemble retriever that ensemble the results of\n multiple retrievers by using weighted  Reciprocal Rank Fusion\n \"\"\"\n import asyncio\n-from typing import Any, Dict, List, Optional, cast\n+from collections import defaultdict\n+from collections.abc import Hashable\n+from itertools import chain\n+from typing import (\n+    Any,\n+    Callable,\n+    Dict,\n+    Iterable,\n+    Iterator,\n+    List,\n+    Optional,\n+    TypeVar,\n+    cast,\n+)\n \n from langchain_core.callbacks import (\n     AsyncCallbackManagerForRetrieverRun,\n@@ -20,6 +33,17 @@\n     get_unique_config_specs,\n )\n \n+T = TypeVar(\"T\")\n+H = TypeVar(\"H\", bound=Hashable)\n+\n+\n+def unique_by_key(iterable: Iterable[T], key: Callable[[T], H]) -> Iterator[T]:\n+    seen = set()\n+    for e in iterable:\n+        if (k := key(e)) not in seen:\n+            seen.add(k)\n+            yield e\n+\n \n class EnsembleRetriever(BaseRetriever):\n     \"\"\"Retriever that ensembles the multiple retrievers.\n@@ -267,32 +291,18 @@ def weighted_reciprocal_rank(\n                 \"Number of rank lists must be equal to the number of weights.\"\n             )\n \n-        # Create a union of all unique documents in the input doc_lists\n-        all_documents = set()\n-        for doc_list in doc_lists:\n-            for doc in doc_list:\n-                all_documents.add(doc.page_content)\n-\n-        # Initialize the RRF score dictionary for each document\n-        rrf_score_dic = {doc: 0.0 for doc in all_documents}\n-\n-        # Calculate RRF scores for each document\n+        # Associate each doc's content with its RRF score for later sorting by it\n+        # Duplicated contents across retrievers are collapsed & scored cumulatively\n+        rrf_score: Dict[str, float] = defaultdict(float)\n         for doc_list, weight in zip(doc_lists, self.weights):\n             for rank, doc in enumerate(doc_list, start=1):\n-                rrf_score = weight * (1 / (rank + self.c))\n-                rrf_score_dic[doc.page_content] += rrf_score\n-\n-        # Sort documents by their RRF scores in descending order\n-        sorted_documents = sorted(\n-            rrf_score_dic.keys(), key=lambda x: rrf_score_dic[x], reverse=True\n+                rrf_score[doc.page_content] += weight / (rank + self.c)\n+\n+        # Docs are deduplicated by their contents then sorted by their scores\n+        all_docs = chain.from_iterable(doc_lists)\n+        sorted_docs = sorted(\n+            unique_by_key(all_docs, lambda doc: doc.page_content),\n+            reverse=True,\n+            key=lambda doc: rrf_score[doc.page_content],\n         )\n-\n-        # Map the sorted page_content back to the original document objects\n-        page_content_to_doc_map = {\n-            doc.page_content: doc for doc_list in doc_lists for doc in doc_list\n-        }\n-        sorted_docs = [\n-            page_content_to_doc_map[page_content] for page_content in sorted_documents\n-        ]\n-\n         return sorted_docs"
      }
    ]
  },
  {
    "number": 30012,
    "title": "docs: fix tavily_search code-block format.",
    "body": "This pull request includes a change to the `TavilySearchResults` class in the `tool.py` file, which updates the code block format in the documentation.\r\n\r\nDocumentation update:\r\n\r\n* [`libs/community/langchain_community/tools/tavily_search/tool.py`](diffhunk://#diff-e3b6a980979268b639c6a86e9b182756b0f7c7e9e5605e613bc0a72ea6aa5301L54-R59): Changed the code block format from Python to JSON in the example provided in the docstring.Thank you for contributing to LangChain!\r\n\r\n",
    "issue_title": "docs: fix tavily_search code-block format.",
    "issue_body": "This pull request includes a change to the `TavilySearchResults` class in the `tool.py` file, which updates the code block format in the documentation.\r\n\r\nDocumentation update:\r\n\r\n* [`libs/community/langchain_community/tools/tavily_search/tool.py`](diffhunk://#diff-e3b6a980979268b639c6a86e9b182756b0f7c7e9e5605e613bc0a72ea6aa5301L54-R59): Changed the code block format from Python to JSON in the example provided in the docstring.Thank you for contributing to LangChain!\r\n\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/tools/tavily_search/tool.py",
        "content_before": "\"\"\"Tool for the Tavily search API.\"\"\"\n\nfrom typing import Dict, List, Literal, Optional, Tuple, Type, Union\n\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.tools import BaseTool\nfrom pydantic import BaseModel, Field\n\nfrom langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n\n\nclass TavilyInput(BaseModel):\n    \"\"\"Input for the Tavily tool.\"\"\"\n\n    query: str = Field(description=\"search query to look up\")\n\n\nclass TavilySearchResults(BaseTool):  # type: ignore[override, override]\n    \"\"\"Tool that queries the Tavily Search API and gets back json.\n\n    Setup:\n        Install ``langchain-openai`` and ``tavily-python``, and set environment variable ``TAVILY_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-community tavily-python\n            export TAVILY_API_KEY=\"your-api-key\"\n\n    Instantiate:\n\n        .. code-block:: python\n\n            from langchain_community.tools import TavilySearchResults\n\n            tool = TavilySearchResults(\n                max_results=5,\n                include_answer=True,\n                include_raw_content=True,\n                include_images=True,\n                # search_depth=\"advanced\",\n                # include_domains = []\n                # exclude_domains = []\n            )\n\n    Invoke directly with args:\n\n        .. code-block:: python\n\n            tool.invoke({'query': 'who won the last french open'})\n\n        .. code-block:: python\n\n            '{\\n  \"url\": \"https://www.nytimes.com...\", \"content\": \"Novak Djokovic won the last French Open by beating Casper Ruud ...'\n\n    Invoke with tool call:\n\n        .. code-block:: python\n\n            tool.invoke({\"args\": {'query': 'who won the last french open'}, \"type\": \"tool_call\", \"id\": \"foo\", \"name\": \"tavily\"})\n\n        .. code-block:: python\n\n            ToolMessage(\n                content='{\\n  \"url\": \"https://www.nytimes.com...\", \"content\": \"Novak Djokovic won the last French Open by beating Casper Ruud ...',\n                artifact={\n                    'query': 'who won the last french open',\n                    'follow_up_questions': None,\n                    'answer': 'Novak ...',\n                    'images': [\n                        'https://www.amny.com/wp-content/uploads/2023/06/AP23162622181176-1200x800.jpg',\n                        ...\n                        ],\n                    'results': [\n                        {\n                            'title': 'Djokovic ...',\n                            'url': 'https://www.nytimes.com...',\n                            'content': \"Novak...\",\n                            'score': 0.99505633,\n                            'raw_content': 'Tennis\\nNovak ...'\n                        },\n                        ...\n                    ],\n                    'response_time': 2.92\n                },\n                tool_call_id='1',\n                name='tavily_search_results_json',\n            )\n\n    \"\"\"  # noqa: E501\n\n    name: str = \"tavily_search_results_json\"\n    description: str = (\n        \"A search engine optimized for comprehensive, accurate, and trusted results. \"\n        \"Useful for when you need to answer questions about current events. \"\n        \"Input should be a search query.\"\n    )\n    args_schema: Type[BaseModel] = TavilyInput\n    \"\"\"The tool response format.\"\"\"\n\n    max_results: int = 5\n    \"\"\"Max search results to return, default is 5\"\"\"\n    search_depth: str = \"advanced\"\n    \"\"\"The depth of the search. It can be \"basic\" or \"advanced\"\n    \n    .. versionadded:: 0.2.5\n    \"\"\"\n    include_domains: List[str] = []\n    \"\"\"A list of domains to specifically include in the search results. \n    \n    Default is None, which includes all domains.\n    \n    .. versionadded:: 0.2.5\n    \"\"\"\n    exclude_domains: List[str] = []\n    \"\"\"A list of domains to specifically exclude from the search results. \n    \n    Default is None, which doesn't exclude any domains.\n    \n    .. versionadded:: 0.2.5\n    \"\"\"\n    include_answer: bool = False\n    \"\"\"Include a short answer to original query in the search results. \n    \n    Default is False.\n    \n    .. versionadded:: 0.2.5\n    \"\"\"\n    include_raw_content: bool = False\n    \"\"\"Include cleaned and parsed HTML of each site search results. \n    \n    Default is False.\n    \n    .. versionadded:: 0.2.5\n    \"\"\"\n    include_images: bool = False\n    \"\"\"Include a list of query related images in the response. \n    \n    Default is False.\n    \n    .. versionadded:: 0.2.5\n    \"\"\"\n\n    api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)  # type: ignore[arg-type]\n    response_format: Literal[\"content_and_artifact\"] = \"content_and_artifact\"\n\n    def _run(\n        self,\n        query: str,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n    ) -> Tuple[Union[List[Dict[str, str]], str], Dict]:\n        \"\"\"Use the tool.\"\"\"\n        # TODO: remove try/except, should be handled by BaseTool\n        try:\n            raw_results = self.api_wrapper.raw_results(\n                query,\n                self.max_results,\n                self.search_depth,\n                self.include_domains,\n                self.exclude_domains,\n                self.include_answer,\n                self.include_raw_content,\n                self.include_images,\n            )\n        except Exception as e:\n            return repr(e), {}\n        return self.api_wrapper.clean_results(raw_results[\"results\"]), raw_results\n\n    async def _arun(\n        self,\n        query: str,\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n    ) -> Tuple[Union[List[Dict[str, str]], str], Dict]:\n        \"\"\"Use the tool asynchronously.\"\"\"\n        try:\n            raw_results = await self.api_wrapper.raw_results_async(\n                query,\n                self.max_results,\n                self.search_depth,\n                self.include_domains,\n                self.exclude_domains,\n                self.include_answer,\n                self.include_raw_content,\n                self.include_images,\n            )\n        except Exception as e:\n            return repr(e), {}\n        return self.api_wrapper.clean_results(raw_results[\"results\"]), raw_results\n\n\nclass TavilyAnswer(BaseTool):  # type: ignore[override, override]\n    \"\"\"Tool that queries the Tavily Search API and gets back an answer.\"\"\"\n\n    name: str = \"tavily_answer\"\n    description: str = (\n        \"A search engine optimized for comprehensive, accurate, and trusted results. \"\n        \"Useful for when you need to answer questions about current events. \"\n        \"Input should be a search query. \"\n        \"This returns only the answer - not the original source data.\"\n    )\n    api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)  # type: ignore[arg-type]\n    args_schema: Type[BaseModel] = TavilyInput\n\n    def _run(\n        self,\n        query: str,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n    ) -> Union[List[Dict], str]:\n        \"\"\"Use the tool.\"\"\"\n        try:\n            return self.api_wrapper.raw_results(\n                query,\n                max_results=5,\n                include_answer=True,\n                search_depth=\"basic\",\n            )[\"answer\"]\n        except Exception as e:\n            return repr(e)\n\n    async def _arun(\n        self,\n        query: str,\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n    ) -> Union[List[Dict], str]:\n        \"\"\"Use the tool asynchronously.\"\"\"\n        try:\n            result = await self.api_wrapper.raw_results_async(\n                query,\n                max_results=5,\n                include_answer=True,\n                search_depth=\"basic\",\n            )\n            return result[\"answer\"]\n        except Exception as e:\n            return repr(e)\n",
        "patch": "@@ -51,9 +51,12 @@ class TavilySearchResults(BaseTool):  # type: ignore[override, override]\n \n             tool.invoke({'query': 'who won the last french open'})\n \n-        .. code-block:: python\n+        .. code-block:: json\n \n-            '{\\n  \"url\": \"https://www.nytimes.com...\", \"content\": \"Novak Djokovic won the last French Open by beating Casper Ruud ...'\n+            {\n+                \"url\": \"https://www.nytimes.com...\",\n+                \"content\": \"Novak Djokovic won the last French Open by beating Casper Ruud ...\"\n+            }\n \n     Invoke with tool call:\n \n@@ -64,7 +67,7 @@ class TavilySearchResults(BaseTool):  # type: ignore[override, override]\n         .. code-block:: python\n \n             ToolMessage(\n-                content='{\\n  \"url\": \"https://www.nytimes.com...\", \"content\": \"Novak Djokovic won the last French Open by beating Casper Ruud ...',\n+                content='{ \"url\": \"https://www.nytimes.com...\", \"content\": \"Novak Djokovic won the last French Open by beating Casper Ruud ...\" }',\n                 artifact={\n                     'query': 'who won the last french open',\n                     'follow_up_questions': None,"
      }
    ]
  },
  {
    "number": 30030,
    "title": "docs: document anthropic features",
    "body": "Update integrations page with extended thinking feature.\r\n\r\nUpdate API reference with extended thinking and citations.\r\n",
    "issue_title": "docs: document anthropic features",
    "issue_body": "Update integrations page with extended thinking feature.\r\n\r\nUpdate API reference with extended thinking and citations.\r\n",
    "files": [
      {
        "filename": "libs/partners/anthropic/langchain_anthropic/chat_models.py",
        "content_before": "import copy\nimport re\nimport warnings\nfrom functools import cached_property\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport anthropic\nfrom langchain_core._api import beta, deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.ai import InputTokenDetails, UsageMetadata\nfrom langchain_core.messages.tool import tool_call_chunk as create_tool_call_chunk\nfrom langchain_core.output_parsers import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n)\nfrom langchain_core.output_parsers.base import OutputParserLike\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableMap,\n    RunnablePassthrough,\n)\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import (\n    from_env,\n    get_pydantic_field_names,\n    secret_from_env,\n)\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import is_basemodel_subclass\nfrom langchain_core.utils.utils import _build_model_kwargs\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SecretStr,\n    model_validator,\n)\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom langchain_anthropic.output_parsers import extract_tool_calls\n\n_message_type_lookups = {\n    \"human\": \"user\",\n    \"ai\": \"assistant\",\n    \"AIMessageChunk\": \"assistant\",\n    \"HumanMessageChunk\": \"user\",\n}\n\n\ndef _format_image(image_url: str) -> Dict:\n    \"\"\"\n    Formats an image of format data:image/jpeg;base64,{b64_string}\n    to a dict for anthropic api\n\n    {\n      \"type\": \"base64\",\n      \"media_type\": \"image/jpeg\",\n      \"data\": \"/9j/4AAQSkZJRg...\",\n    }\n\n    And throws an error if it's not a b64 image\n    \"\"\"\n    regex = r\"^data:(?P<media_type>image/.+);base64,(?P<data>.+)$\"\n    match = re.match(regex, image_url)\n    if match is None:\n        raise ValueError(\n            \"Anthropic only supports base64-encoded images currently.\"\n            \" Example: data:image/png;base64,'/9j/4AAQSk'...\"\n        )\n    return {\n        \"type\": \"base64\",\n        \"media_type\": match.group(\"media_type\"),\n        \"data\": match.group(\"data\"),\n    }\n\n\ndef _merge_messages(\n    messages: Sequence[BaseMessage],\n) -> List[Union[SystemMessage, AIMessage, HumanMessage]]:\n    \"\"\"Merge runs of human/tool messages into single human messages with content blocks.\"\"\"  # noqa: E501\n    merged: list = []\n    for curr in messages:\n        if isinstance(curr, ToolMessage):\n            if (\n                isinstance(curr.content, list)\n                and curr.content\n                and all(\n                    isinstance(block, dict) and block.get(\"type\") == \"tool_result\"\n                    for block in curr.content\n                )\n            ):\n                curr = HumanMessage(curr.content)  # type: ignore[misc]\n            else:\n                curr = HumanMessage(  # type: ignore[misc]\n                    [\n                        {\n                            \"type\": \"tool_result\",\n                            \"content\": curr.content,\n                            \"tool_use_id\": curr.tool_call_id,\n                            \"is_error\": curr.status == \"error\",\n                        }\n                    ]\n                )\n        last = merged[-1] if merged else None\n        if any(\n            all(isinstance(m, c) for m in (curr, last))\n            for c in (SystemMessage, HumanMessage)\n        ):\n            if isinstance(cast(BaseMessage, last).content, str):\n                new_content: List = [\n                    {\"type\": \"text\", \"text\": cast(BaseMessage, last).content}\n                ]\n            else:\n                new_content = copy.copy(cast(list, cast(BaseMessage, last).content))\n            if isinstance(curr.content, str):\n                new_content.append({\"type\": \"text\", \"text\": curr.content})\n            else:\n                new_content.extend(curr.content)\n            merged[-1] = curr.model_copy(update={\"content\": new_content})\n        else:\n            merged.append(curr)\n    return merged\n\n\ndef _format_messages(\n    messages: List[BaseMessage],\n) -> Tuple[Union[str, List[Dict], None], List[Dict]]:\n    \"\"\"Format messages for anthropic.\"\"\"\n\n    \"\"\"\n    [\n                {\n                    \"role\": _message_type_lookups[m.type],\n                    \"content\": [_AnthropicMessageContent(text=m.content).model_dump()],\n                }\n                for m in messages\n            ]\n    \"\"\"\n    system: Union[str, List[Dict], None] = None\n    formatted_messages: List[Dict] = []\n\n    merged_messages = _merge_messages(messages)\n    for i, message in enumerate(merged_messages):\n        if message.type == \"system\":\n            if system is not None:\n                raise ValueError(\"Received multiple non-consecutive system messages.\")\n            elif isinstance(message.content, list):\n                system = [\n                    (\n                        block\n                        if isinstance(block, dict)\n                        else {\"type\": \"text\", \"text\": block}\n                    )\n                    for block in message.content\n                ]\n            else:\n                system = message.content\n            continue\n\n        role = _message_type_lookups[message.type]\n        content: Union[str, List]\n\n        if not isinstance(message.content, str):\n            # parse as dict\n            assert isinstance(\n                message.content, list\n            ), \"Anthropic message content must be str or list of dicts\"\n\n            # populate content\n            content = []\n            for block in message.content:\n                if isinstance(block, str):\n                    content.append({\"type\": \"text\", \"text\": block})\n                elif isinstance(block, dict):\n                    if \"type\" not in block:\n                        raise ValueError(\"Dict content block must have a type key\")\n                    elif block[\"type\"] == \"image_url\":\n                        # convert format\n                        source = _format_image(block[\"image_url\"][\"url\"])\n                        content.append({\"type\": \"image\", \"source\": source})\n                    elif block[\"type\"] == \"tool_use\":\n                        # If a tool_call with the same id as a tool_use content block\n                        # exists, the tool_call is preferred.\n                        if isinstance(message, AIMessage) and block[\"id\"] in [\n                            tc[\"id\"] for tc in message.tool_calls\n                        ]:\n                            overlapping = [\n                                tc\n                                for tc in message.tool_calls\n                                if tc[\"id\"] == block[\"id\"]\n                            ]\n                            content.extend(\n                                _lc_tool_calls_to_anthropic_tool_use_blocks(overlapping)\n                            )\n                        else:\n                            block.pop(\"text\", None)\n                            content.append(block)\n                    elif block[\"type\"] == \"text\":\n                        text = block.get(\"text\", \"\")\n                        # Only add non-empty strings for now as empty ones are not\n                        # accepted.\n                        # https://github.com/anthropics/anthropic-sdk-python/issues/461\n                        if text.strip():\n                            content.append(\n                                {\n                                    k: v\n                                    for k, v in block.items()\n                                    if k in (\"type\", \"text\", \"cache_control\")\n                                }\n                            )\n                    elif block[\"type\"] == \"thinking\":\n                        content.append(\n                            {\n                                k: v\n                                for k, v in block.items()\n                                if k\n                                in (\"type\", \"thinking\", \"cache_control\", \"signature\")\n                            }\n                        )\n                    elif block[\"type\"] == \"redacted_thinking\":\n                        content.append(\n                            {\n                                k: v\n                                for k, v in block.items()\n                                if k in (\"type\", \"cache_control\", \"data\")\n                            }\n                        )\n                    elif block[\"type\"] == \"tool_result\":\n                        tool_content = _format_messages(\n                            [HumanMessage(block[\"content\"])]\n                        )[1][0][\"content\"]\n                        content.append({**block, **{\"content\": tool_content}})\n                    else:\n                        content.append(block)\n                else:\n                    raise ValueError(\n                        f\"Content blocks must be str or dict, instead was: \"\n                        f\"{type(block)}\"\n                    )\n        else:\n            content = message.content\n\n        # Ensure all tool_calls have a tool_use content block\n        if isinstance(message, AIMessage) and message.tool_calls:\n            content = content or []\n            content = (\n                [{\"type\": \"text\", \"text\": message.content}]\n                if isinstance(content, str) and content\n                else content\n            )\n            tool_use_ids = [\n                cast(dict, block)[\"id\"]\n                for block in content\n                if cast(dict, block)[\"type\"] == \"tool_use\"\n            ]\n            missing_tool_calls = [\n                tc for tc in message.tool_calls if tc[\"id\"] not in tool_use_ids\n            ]\n            cast(list, content).extend(\n                _lc_tool_calls_to_anthropic_tool_use_blocks(missing_tool_calls)\n            )\n\n        formatted_messages.append({\"role\": role, \"content\": content})\n    return system, formatted_messages\n\n\nclass ChatAnthropic(BaseChatModel):\n    \"\"\"Anthropic chat models.\n\n    See https://docs.anthropic.com/en/docs/models-overview for a list of the latest models.\n\n    Setup:\n        Install ``langchain-anthropic`` and set environment variable ``ANTHROPIC_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-anthropic\n            export ANTHROPIC_API_KEY=\"your-api-key\"\n\n    Key init args \u2014 completion params:\n        model: str\n            Name of Anthropic model to use. E.g. \"claude-3-sonnet-20240229\".\n        temperature: float\n            Sampling temperature. Ranges from 0.0 to 1.0.\n        max_tokens: int\n            Max number of tokens to generate.\n\n    Key init args \u2014 client params:\n        timeout: Optional[float]\n            Timeout for requests.\n        max_retries: int\n            Max number of retries if a request fails.\n        api_key: Optional[str]\n            Anthropic API key. If not passed in will be read from env var ANTHROPIC_API_KEY.\n        base_url: Optional[str]\n            Base URL for API requests. Only specify if using a proxy or service\n            emulator.\n\n    See full list of supported init args and their descriptions in the params section.\n\n    Instantiate:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(\n                model=\"claude-3-sonnet-20240229\",\n                temperature=0,\n                max_tokens=1024,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # base_url=\"...\",\n                # other params...\n            )\n\n    **NOTE**: Any param which is not explicitly supported will be passed directly to the\n    ``anthropic.Anthropic.messages.create(...)`` API every time to the model is\n    invoked. For example:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n            import anthropic\n\n            ChatAnthropic(..., extra_headers={}).invoke(...)\n\n            # results in underlying API call of:\n\n            anthropic.Anthropic(..).messages.create(..., extra_headers={})\n\n            # which is also equivalent to:\n\n            ChatAnthropic(...).invoke(..., extra_headers={})\n\n    Invoke:\n        .. code-block:: python\n\n            messages = [\n                (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Stream:\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            AIMessageChunk(content='J', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=\"'\", id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='a', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ime', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' la', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' programm', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ation', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='.', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n        .. code-block:: python\n\n            AIMessageChunk(content=\"J'aime la programmation.\", id='run-b34faef0-882f-4869-a19c-ed2b856e6361')\n\n    Async:\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Tool calling:\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n            ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [{'name': 'GetWeather',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01KzpPEAgzura7hpBqwHbWdo'},\n             {'name': 'GetWeather',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JtgbVGVJbiSwtZk3Uycezx'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01429aygngesudV9nTbCKGuw'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JPktyd44tVMeBcPPnFSEJG'}]\n\n        See ``ChatAnthropic.bind_tools()`` method for more.\n\n    Structured output:\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        .. code-block:: python\n\n            Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)\n\n        See ``ChatAnthropic.with_structured_output()`` for more.\n\n    Image input:\n        .. code-block:: python\n\n            import base64\n            import httpx\n            from langchain_core.messages import HumanMessage\n\n            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n            message = HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                    },\n                ],\n            )\n            ai_msg = llm.invoke([message])\n            ai_msg.content\n\n        .. code-block:: python\n\n            \"The image depicts a sunny day with a partly cloudy sky. The sky is a brilliant blue color with scattered white clouds drifting across. The lighting and cloud patterns suggest pleasant, mild weather conditions. The scene shows a grassy field or meadow with a wooden boardwalk trail leading through it, indicating an outdoor setting on a nice day well-suited for enjoying nature.\"\n\n    Token usage:\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        Message chunks containing token usage will be included during streaming by\n        default:\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        These can be disabled by setting ``stream_usage=False`` in the stream method,\n        or by setting ``stream_usage=False`` when initializing ChatAnthropic.\n\n    Response metadata\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n        .. code-block:: python\n\n            {'id': 'msg_013xU6FHEGEq76aP4RgFerVT',\n             'model': 'claude-3-sonnet-20240229',\n             'stop_reason': 'end_turn',\n             'stop_sequence': None,\n             'usage': {'input_tokens': 25, 'output_tokens': 11}}\n\n    \"\"\"  # noqa: E501\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n    )\n\n    model: str = Field(alias=\"model_name\")\n    \"\"\"Model name to use.\"\"\"\n\n    max_tokens: int = Field(default=1024, alias=\"max_tokens_to_sample\")\n    \"\"\"Denotes the number of tokens to predict per generation.\"\"\"\n\n    temperature: Optional[float] = None\n    \"\"\"A non-negative float that tunes the degree of randomness in generation.\"\"\"\n\n    top_k: Optional[int] = None\n    \"\"\"Number of most likely tokens to consider at each step.\"\"\"\n\n    top_p: Optional[float] = None\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n\n    default_request_timeout: Optional[float] = Field(None, alias=\"timeout\")\n    \"\"\"Timeout for requests to Anthropic Completion API.\"\"\"\n\n    # sdk default = 2: https://github.com/anthropics/anthropic-sdk-python?tab=readme-ov-file#retries\n    max_retries: int = 2\n    \"\"\"Number of retries allowed for requests sent to the Anthropic Completion API.\"\"\"\n\n    stop_sequences: Optional[List[str]] = Field(None, alias=\"stop\")\n    \"\"\"Default stop sequences.\"\"\"\n\n    anthropic_api_url: Optional[str] = Field(\n        alias=\"base_url\",\n        default_factory=from_env(\n            [\"ANTHROPIC_API_URL\", \"ANTHROPIC_BASE_URL\"],\n            default=\"https://api.anthropic.com\",\n        ),\n    )\n    \"\"\"Base URL for API requests. Only specify if using a proxy or service emulator.\n\n    If a value isn't passed in, will attempt to read the value first from\n    ANTHROPIC_API_URL and if that is not set, ANTHROPIC_BASE_URL.\n    If neither are set, the default value of 'https://api.anthropic.com' will\n    be used.\n    \"\"\"\n\n    anthropic_api_key: SecretStr = Field(\n        alias=\"api_key\",\n        default_factory=secret_from_env(\"ANTHROPIC_API_KEY\", default=\"\"),\n    )\n\n    \"\"\"Automatically read from env var `ANTHROPIC_API_KEY` if not provided.\"\"\"\n\n    default_headers: Optional[Mapping[str, str]] = None\n    \"\"\"Headers to pass to the Anthropic clients, will be used for every API call.\"\"\"\n\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n\n    streaming: bool = False\n    \"\"\"Whether to use streaming or not.\"\"\"\n\n    stream_usage: bool = True\n    \"\"\"Whether to include usage metadata in streaming output. If True, additional\n    message chunks will be generated during the stream including usage metadata.\n    \"\"\"\n\n    thinking: Optional[Dict[str, Any]] = Field(default=None)\n    \"\"\"Parameters for Claude reasoning,\n    e.g., ``{\"type\": \"enabled\", \"budget_tokens\": 10_000}``\"\"\"\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"anthropic-chat\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"anthropic_api_key\": \"ANTHROPIC_API_KEY\"}\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"anthropic\"]\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"model_kwargs\": self.model_kwargs,\n            \"streaming\": self.streaming,\n            \"max_retries\": self.max_retries,\n            \"default_request_timeout\": self.default_request_timeout,\n            \"thinking\": self.thinking,\n        }\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"anthropic\",\n            ls_model_name=self.model,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict) -> Any:\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @cached_property\n    def _client_params(self) -> Dict[str, Any]:\n        client_params: Dict[str, Any] = {\n            \"api_key\": self.anthropic_api_key.get_secret_value(),\n            \"base_url\": self.anthropic_api_url,\n            \"max_retries\": self.max_retries,\n            \"default_headers\": (self.default_headers or None),\n        }\n        # value <= 0 indicates the param should be ignored. None is a meaningful value\n        # for Anthropic client and treated differently than not specifying the param at\n        # all.\n        if self.default_request_timeout is None or self.default_request_timeout > 0:\n            client_params[\"timeout\"] = self.default_request_timeout\n\n        return client_params\n\n    @cached_property\n    def _client(self) -> anthropic.Client:\n        return anthropic.Client(**self._client_params)\n\n    @cached_property\n    def _async_client(self) -> anthropic.AsyncClient:\n        return anthropic.AsyncClient(**self._client_params)\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Dict,\n    ) -> Dict:\n        messages = self._convert_input(input_).to_messages()\n        system, formatted_messages = _format_messages(messages)\n        payload = {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"messages\": formatted_messages,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"stop_sequences\": stop or self.stop_sequences,\n            \"system\": system,\n            **self.model_kwargs,\n            **kwargs,\n        }\n        if self.thinking is not None:\n            payload[\"thinking\"] = self.thinking\n        return {k: v for k, v in payload.items() if v is not None}\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = self._client.messages.create(**payload)\n        coerce_content_to_string = (\n            not _tools_in_params(payload)\n            and not _documents_in_params(payload)\n            and not _thinking_in_params(payload)\n        )\n        for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = await self._async_client.messages.create(**payload)\n        coerce_content_to_string = (\n            not _tools_in_params(payload)\n            and not _documents_in_params(payload)\n            and not _thinking_in_params(payload)\n        )\n        async for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    await run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    def _format_output(self, data: Any, **kwargs: Any) -> ChatResult:\n        data_dict = data.model_dump()\n        content = data_dict[\"content\"]\n\n        # Remove citations if they are None - introduced in anthropic sdk 0.45\n        for block in content:\n            if (\n                isinstance(block, dict)\n                and \"citations\" in block\n                and block[\"citations\"] is None\n            ):\n                block.pop(\"citations\")\n            if (\n                isinstance(block, dict)\n                and block.get(\"type\") == \"thinking\"\n                and \"text\" in block\n                and block[\"text\"] is None\n            ):\n                block.pop(\"text\")\n\n        llm_output = {\n            k: v for k, v in data_dict.items() if k not in (\"content\", \"role\", \"type\")\n        }\n        if (\n            len(content) == 1\n            and content[0][\"type\"] == \"text\"\n            and not content[0].get(\"citations\")\n        ):\n            msg = AIMessage(content=content[0][\"text\"])\n        elif any(block[\"type\"] == \"tool_use\" for block in content):\n            tool_calls = extract_tool_calls(content)\n            msg = AIMessage(\n                content=content,\n                tool_calls=tool_calls,\n            )\n        else:\n            msg = AIMessage(content=content)\n        msg.usage_metadata = _create_usage_metadata(data.usage)\n        return ChatResult(\n            generations=[ChatGeneration(message=msg)],\n            llm_output=llm_output,\n        )\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = self._client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = await self._async_client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[\n            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n        ] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        r\"\"\"Bind tool-like objects to this chat model.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports Anthropic format tool schemas and any tool definition handled\n                by :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call. Options are:\n\n                - name of the tool as a string or as dict ``{\"type\": \"tool\", \"name\": \"<<tool_name>>\"}``: calls corresponding tool;\n                - ``\"auto\"``, ``{\"type: \"auto\"}``, or None: automatically selects a tool (including no tool);\n                - ``\"any\"`` or ``{\"type: \"any\"}``: force at least one tool to be called;\n            parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n                Defaults to ``None`` (no specification, which allows parallel tool use).\n\n                .. versionadded:: 0.3.2\n            kwargs: Any additional parameters are passed directly to\n                :meth:`~langchain_anthropic.chat_models.ChatAnthropic.bind`.\n\n        Example:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n                # -> AIMessage(\n                #     content=[\n                #         {'text': '<thinking>\\nBased on the user\\'s question, the relevant function to call is GetWeather, which requires the \"location\" parameter.\\n\\nThe user has directly specified the location as \"San Francisco\". Since San Francisco is a well known city, I can reasonably infer they mean San Francisco, CA without needing the state specified.\\n\\nAll the required parameters are provided, so I can proceed with the API call.\\n</thinking>', 'type': 'text'},\n                #         {'text': None, 'type': 'tool_use', 'id': 'toolu_01SCgExKzQ7eqSkMHfygvYuu', 'name': 'GetWeather', 'input': {'location': 'San Francisco, CA'}}\n                #     ],\n                #     response_metadata={'id': 'msg_01GM3zQtoFv8jGQMW7abLnhi', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 487, 'output_tokens': 145}},\n                #     id='run-87b1331e-9251-4a68-acef-f0a018b639cc-0'\n                # )\n\n        Example \u2014 force tool call with tool_choice 'any':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"any\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n\n        Example \u2014 force specific tool call with tool_choice '<name_of_tool>':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"GetWeather\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n        Example \u2014 cache specific tools:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic, convert_to_anthropic_tool\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n                # We'll convert our pydantic class to the anthropic tool format\n                # before passing to bind_tools so that we can set the 'cache_control'\n                # field on our tool.\n                cached_price_tool = convert_to_anthropic_tool(GetPrice)\n                # Currently the only supported \"cache_control\" value is\n                # {\"type\": \"ephemeral\"}.\n                cached_price_tool[\"cache_control\"] = {\"type\": \"ephemeral\"}\n\n                # We need to pass in extra headers to enable use of the beta cache\n                # control API.\n                llm = ChatAnthropic(\n                    model=\"claude-3-5-sonnet-20240620\",\n                    temperature=0,\n                    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n                )\n                llm_with_tools = llm.bind_tools([GetWeather, cached_price_tool])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n            This outputs:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': \"Certainly! I can help you find out the current weather in San Francisco. To get this information, I'll use the GetWeather function. Let me fetch that data for you right away.\", 'type': 'text'}, {'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_01Xg7Wr5inFWgBxE5jH9rpRo', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 96, 'cache_creation_input_tokens': 1470, 'cache_read_input_tokens': 0}}, id='run-b36a5b54-5d69-470e-a1b0-b932d00b089e-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 96, 'total_tokens': 267})\n\n            If we invoke the tool again, we can see that the \"usage\" information in the AIMessage.response_metadata shows that we had a cache hit:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': 'To get the current weather in San Francisco, I can use the GetWeather function. Let me check that for you.', 'type': 'text'}, {'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_016RfWHrRvW6DAGCdwB6Ac64', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 82, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 1470}}, id='run-88b1f825-dcb7-4277-ac27-53df55d22001-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 82, 'total_tokens': 253})\n\n        \"\"\"  # noqa: E501\n        formatted_tools = [convert_to_anthropic_tool(tool) for tool in tools]\n        if not tool_choice:\n            pass\n        elif isinstance(tool_choice, dict):\n            kwargs[\"tool_choice\"] = tool_choice\n        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n        elif isinstance(tool_choice, str):\n            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n        else:\n            raise ValueError(\n                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n                f\"str, or None.\"\n            )\n\n        if parallel_tool_calls is not None:\n            disable_parallel_tool_use = not parallel_tool_calls\n            if \"tool_choice\" in kwargs:\n                kwargs[\"tool_choice\"][\"disable_parallel_tool_use\"] = (\n                    disable_parallel_tool_use\n                )\n            else:\n                kwargs[\"tool_choice\"] = {\n                    \"type\": \"auto\",\n                    \"disable_parallel_tool_use\": disable_parallel_tool_use,\n                }\n\n        return self.bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Union[Dict, type],\n        *,\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema: The output schema. Can be passed in as:\n\n                - an Anthropic tool schema,\n                - an OpenAI function/tool schema,\n                - a JSON Schema,\n                - a TypedDict class,\n                - or a Pydantic class.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            kwargs: Additional keyword arguments are ignored.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`~langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: Pydantic schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example:  Pydantic schema (include_raw=True):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: Dict schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n\n                schema = {\n                    \"name\": \"AnswerWithJustification\",\n                    \"description\": \"An answer to the user question along with justification for the answer.\",\n                    \"input_schema\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"answer\": {\"type\": \"string\"},\n                            \"justification\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"answer\", \"justification\"]\n                    }\n                }\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(schema)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. versionchanged:: 0.1.22\n\n                Added support for TypedDict class as `schema`.\n\n        \"\"\"  # noqa: E501\n        formatted_tool = convert_to_anthropic_tool(schema)\n        tool_name = formatted_tool[\"name\"]\n        llm = self.bind_tools(\n            [schema],\n            tool_choice=tool_name,\n            structured_output_format={\"kwargs\": {}, \"schema\": formatted_tool},\n        )\n        if isinstance(schema, type) and is_basemodel_subclass(schema):\n            output_parser: OutputParserLike = PydanticToolsParser(\n                tools=[schema], first_tool_only=True\n            )\n        else:\n            output_parser = JsonOutputKeyToolsParser(\n                key_name=tool_name, first_tool_only=True\n            )\n\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    @beta()\n    def get_num_tokens_from_messages(\n        self,\n        messages: List[BaseMessage],\n        tools: Optional[\n            Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]\n        ] = None,\n    ) -> int:\n        \"\"\"Count tokens in a sequence of input messages.\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n\n        Basic usage:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage, SystemMessage\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                messages = [\n                    SystemMessage(content=\"You are a scientist\"),\n                    HumanMessage(content=\"Hello, Claude\"),\n                ]\n                llm.get_num_tokens_from_messages(messages)\n\n            .. code-block:: none\n\n                14\n\n        Pass tool schemas:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage\n                from langchain_core.tools import tool\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                @tool(parse_docstring=True)\n                def get_weather(location: str) -> str:\n                    \\\"\\\"\\\"Get the current weather in a given location\n\n                    Args:\n                        location: The city and state, e.g. San Francisco, CA\n                    \\\"\\\"\\\"\n                    return \"Sunny\"\n\n                messages = [\n                    HumanMessage(content=\"What's the weather like in San Francisco?\"),\n                ]\n                llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n\n            .. code-block:: none\n\n                403\n\n        .. versionchanged:: 0.3.0\n\n                Uses Anthropic's token counting API to count tokens in messages. See:\n                https://docs.anthropic.com/en/docs/build-with-claude/token-counting\n        \"\"\"\n        formatted_system, formatted_messages = _format_messages(messages)\n        kwargs: Dict[str, Any] = {}\n        if isinstance(formatted_system, str):\n            kwargs[\"system\"] = formatted_system\n        if tools:\n            kwargs[\"tools\"] = [convert_to_anthropic_tool(tool) for tool in tools]\n\n        response = self._client.beta.messages.count_tokens(\n            betas=[\"token-counting-2024-11-01\"],\n            model=self.model,\n            messages=formatted_messages,  # type: ignore[arg-type]\n            **kwargs,\n        )\n        return response.input_tokens\n\n\nclass AnthropicTool(TypedDict):\n    \"\"\"Anthropic tool definition.\"\"\"\n\n    name: str\n    description: str\n    input_schema: Dict[str, Any]\n    cache_control: NotRequired[Dict[str, str]]\n\n\ndef convert_to_anthropic_tool(\n    tool: Union[Dict[str, Any], Type, Callable, BaseTool],\n) -> AnthropicTool:\n    \"\"\"Convert a tool-like object to an Anthropic tool definition.\"\"\"\n    # already in Anthropic tool format\n    if isinstance(tool, dict) and all(\n        k in tool for k in (\"name\", \"description\", \"input_schema\")\n    ):\n        anthropic_formatted = AnthropicTool(tool)  # type: ignore\n    else:\n        oai_formatted = convert_to_openai_tool(tool)[\"function\"]\n        anthropic_formatted = AnthropicTool(\n            name=oai_formatted[\"name\"],\n            description=oai_formatted[\"description\"],\n            input_schema=oai_formatted[\"parameters\"],\n        )\n    return anthropic_formatted\n\n\ndef _tools_in_params(params: dict) -> bool:\n    return \"tools\" in params or (\n        \"extra_body\" in params and params[\"extra_body\"].get(\"tools\")\n    )\n\n\ndef _thinking_in_params(params: dict) -> bool:\n    return params.get(\"thinking\", {}).get(\"type\") == \"enabled\"\n\n\ndef _documents_in_params(params: dict) -> bool:\n    for message in params.get(\"messages\", []):\n        if isinstance(message.get(\"content\"), list):\n            for block in message[\"content\"]:\n                if (\n                    isinstance(block, dict)\n                    and block.get(\"type\") == \"document\"\n                    and block.get(\"citations\", {}).get(\"enabled\")\n                ):\n                    return True\n    return False\n\n\nclass _AnthropicToolUse(TypedDict):\n    type: Literal[\"tool_use\"]\n    name: str\n    input: dict\n    id: str\n\n\ndef _lc_tool_calls_to_anthropic_tool_use_blocks(\n    tool_calls: List[ToolCall],\n) -> List[_AnthropicToolUse]:\n    blocks = []\n    for tool_call in tool_calls:\n        blocks.append(\n            _AnthropicToolUse(\n                type=\"tool_use\",\n                name=tool_call[\"name\"],\n                input=tool_call[\"args\"],\n                id=cast(str, tool_call[\"id\"]),\n            )\n        )\n    return blocks\n\n\ndef _make_message_chunk_from_anthropic_event(\n    event: anthropic.types.RawMessageStreamEvent,\n    *,\n    stream_usage: bool = True,\n    coerce_content_to_string: bool,\n) -> Optional[AIMessageChunk]:\n    \"\"\"Convert Anthropic event to AIMessageChunk.\n\n    Note that not all events will result in a message chunk. In these cases\n    we return None.\n    \"\"\"\n    message_chunk: Optional[AIMessageChunk] = None\n    # See https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/streaming/_messages.py  # noqa: E501\n    if event.type == \"message_start\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.message.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\" if coerce_content_to_string else [],\n            usage_metadata=usage_metadata,\n        )\n    elif (\n        event.type == \"content_block_start\"\n        and event.content_block is not None\n        and event.content_block.type in (\"tool_use\", \"document\", \"redacted_thinking\")\n    ):\n        if coerce_content_to_string:\n            warnings.warn(\"Received unexpected tool content block.\")\n        content_block = event.content_block.model_dump()\n        content_block[\"index\"] = event.index\n        if event.content_block.type == \"tool_use\":\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=event.content_block.id,\n                name=event.content_block.name,\n                args=\"\",\n            )\n            tool_call_chunks = [tool_call_chunk]\n        else:\n            tool_call_chunks = []\n        message_chunk = AIMessageChunk(\n            content=[content_block],\n            tool_call_chunks=tool_call_chunks,  # type: ignore\n        )\n    elif event.type == \"content_block_delta\":\n        if event.delta.type in (\"text_delta\", \"citations_delta\"):\n            if coerce_content_to_string and hasattr(event.delta, \"text\"):\n                text = event.delta.text\n                message_chunk = AIMessageChunk(content=text)\n            else:\n                content_block = event.delta.model_dump()\n                content_block[\"index\"] = event.index\n                content_block[\"type\"] = \"text\"\n                if \"citation\" in content_block:\n                    content_block[\"citations\"] = [content_block.pop(\"citation\")]\n                message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"thinking_delta\":\n            content_block = event.delta.model_dump()\n            if \"text\" in content_block and content_block[\"text\"] is None:\n                content_block.pop(\"text\")\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"thinking\"\n            message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"signature_delta\":\n            content_block = event.delta.model_dump()\n            if \"text\" in content_block and content_block[\"text\"] is None:\n                content_block.pop(\"text\")\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"thinking\"\n            message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"input_json_delta\":\n            content_block = event.delta.model_dump()\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"tool_use\"\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=None,\n                name=None,\n                args=event.delta.partial_json,\n            )\n            message_chunk = AIMessageChunk(\n                content=[content_block],\n                tool_call_chunks=[tool_call_chunk],  # type: ignore\n            )\n    elif event.type == \"message_delta\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\",\n            usage_metadata=usage_metadata,\n            response_metadata={\n                \"stop_reason\": event.delta.stop_reason,\n                \"stop_sequence\": event.delta.stop_sequence,\n            },\n        )\n    else:\n        pass\n\n    return message_chunk\n\n\n@deprecated(since=\"0.1.0\", removal=\"1.0.0\", alternative=\"ChatAnthropic\")\nclass ChatAnthropicMessages(ChatAnthropic):\n    pass\n\n\ndef _create_usage_metadata(anthropic_usage: BaseModel) -> UsageMetadata:\n    input_token_details: Dict = {\n        \"cache_read\": getattr(anthropic_usage, \"cache_read_input_tokens\", None),\n        \"cache_creation\": getattr(anthropic_usage, \"cache_creation_input_tokens\", None),\n    }\n\n    # Anthropic input_tokens exclude cached token counts.\n    input_tokens = (\n        getattr(anthropic_usage, \"input_tokens\", 0)\n        + (input_token_details[\"cache_read\"] or 0)\n        + (input_token_details[\"cache_creation\"] or 0)\n    )\n    output_tokens = getattr(anthropic_usage, \"output_tokens\", 0)\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=input_tokens + output_tokens,\n        input_token_details=InputTokenDetails(\n            **{k: v for k, v in input_token_details.items() if v is not None}\n        ),\n    )\n",
        "patch": "@@ -509,6 +509,93 @@ class Joke(BaseModel):\n \n             \"The image depicts a sunny day with a partly cloudy sky. The sky is a brilliant blue color with scattered white clouds drifting across. The lighting and cloud patterns suggest pleasant, mild weather conditions. The scene shows a grassy field or meadow with a wooden boardwalk trail leading through it, indicating an outdoor setting on a nice day well-suited for enjoying nature.\"\n \n+    Extended thinking:\n+        Claude 3.7 Sonnet supports an\n+        `extended thinking <https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking>`_\n+        feature, which will output the step-by-step reasoning process that led to its\n+        final answer.\n+\n+        To use it, specify the `thinking` parameter when initializing `ChatAnthropic`.\n+        It can also be passed in as a kwarg during invocation.\n+\n+        You will need to specify a token budget to use this feature. See usage example:\n+\n+        .. code-block:: python\n+\n+            from langchain_anthropic import ChatAnthropic\n+\n+            llm = ChatAnthropic(\n+                model=\"claude-3-7-sonnet-latest\",\n+                max_tokens=5000,\n+                thinking={\"type\": \"enabled\", \"budget_tokens\": 2000},\n+            )\n+\n+            response = llm.invoke(\"What is the cube root of 50.653?\")\n+            response.content\n+\n+        .. code-block:: python\n+\n+            [{'signature': '...', 'thinking': \"To find the cube root of 50.653...\", 'type': 'thinking'}, {'text': 'The cube root of 50.653 is ...', 'type': 'text'}]\n+\n+    Citations:\n+        Anthropic supports a\n+        `citations <https://docs.anthropic.com/en/docs/build-with-claude/citations>`_\n+        feature that lets Claude attach context to its answers based on source\n+        documents supplied by the user. When\n+        `document content blocks <https://docs.anthropic.com/en/docs/build-with-claude/citations#document-types>`_\n+        with ``\"citations\": {\"enabled\": True}`` are included in a query, Claude may\n+        generate citations in its response.\n+\n+        .. code-block:: python\n+\n+            from langchain_anthropic import ChatAnthropic\n+\n+            llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n+\n+            messages = [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"document\",\n+                            \"source\": {\n+                                \"type\": \"text\",\n+                                \"media_type\": \"text/plain\",\n+                                \"data\": \"The grass is green. The sky is blue.\",\n+                            },\n+                            \"title\": \"My Document\",\n+                            \"context\": \"This is a trustworthy document.\",\n+                            \"citations\": {\"enabled\": True},\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n+                    ],\n+                }\n+            ]\n+            response = llm.invoke(messages)\n+            response.content\n+\n+        .. code-block:: python\n+\n+            [{'text': 'Based on the document, ', 'type': 'text'},\n+            {'text': 'the grass is green',\n+            'type': 'text',\n+            'citations': [{'type': 'char_location',\n+                'cited_text': 'The grass is green. ',\n+                'document_index': 0,\n+                'document_title': 'My Document',\n+                'start_char_index': 0,\n+                'end_char_index': 20}]},\n+            {'text': ', and ', 'type': 'text'},\n+            {'text': 'the sky is blue',\n+            'type': 'text',\n+            'citations': [{'type': 'char_location',\n+                'cited_text': 'The sky is blue.',\n+                'document_index': 0,\n+                'document_title': 'My Document',\n+                'start_char_index': 20,\n+                'end_char_index': 36}]},\n+            {'text': '.', 'type': 'text'}]\n+\n     Token usage:\n         .. code-block:: python\n "
      }
    ]
  },
  {
    "number": 29857,
    "title": "partners: add langchain-vdms",
    "body": "**Description:** Deprecate vdms in community, add integration langchain-vdms, and update any related files\r\n**Issue:** n/a\r\n**Dependencies:** langchain-vdms\r\n**Twitter handle:** n/a\r\n",
    "issue_title": "partners: add langchain-vdms",
    "issue_body": "**Description:** Deprecate vdms in community, add integration langchain-vdms, and update any related files\r\n**Issue:** n/a\r\n**Dependencies:** langchain-vdms\r\n**Twitter handle:** n/a\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/vectorstores/vdms.py",
        "content_before": "from __future__ import annotations\n\nimport base64\nimport logging\nimport os\nimport uuid\nfrom copy import deepcopy\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Sized,\n    Tuple,\n    Type,\n    Union,\n    get_args,\n)\n\nimport numpy as np\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.vectorstores import VectorStore\n\nfrom langchain_community.vectorstores.utils import maximal_marginal_relevance\n\nif TYPE_CHECKING:\n    import vdms\n\n\nDISTANCE_METRICS = Literal[\n    \"L2\",  # Euclidean Distance\n    \"IP\",  # Inner Product\n]\nAVAILABLE_DISTANCE_METRICS: List[DISTANCE_METRICS] = list(get_args(DISTANCE_METRICS))\nENGINES = Literal[\n    \"TileDBDense\",  # TileDB Dense\n    \"TileDBSparse\",  # TileDB Sparse\n    \"FaissFlat\",  # FAISS IndexFlat\n    \"FaissIVFFlat\",  # FAISS IndexIVFFlat\n    \"Flinng\",  # FLINNG\n]\nAVAILABLE_ENGINES: List[ENGINES] = list(get_args(ENGINES))\nDEFAULT_COLLECTION_NAME = \"langchain\"\nDEFAULT_INSERT_BATCH_SIZE = 32\n# Number of Documents to return.\nDEFAULT_K = 3\n# Number of Documents to fetch to pass to knn when filters applied.\nDEFAULT_FETCH_K = DEFAULT_K * 5\nDEFAULT_PROPERTIES = [\"_distance\", \"id\", \"content\"]\nINVALID_DOC_METADATA_KEYS = [\"_distance\", \"content\", \"blob\"]\nINVALID_METADATA_VALUE = [\"Missing property\", None, {}]  # type: List\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _len_check_if_sized(x: Any, y: Any, x_name: str, y_name: str) -> None:\n    \"\"\"\n    Check that sizes of two variables are the same\n\n    Args:\n        x: Variable to compare\n        y: Variable to compare\n        x_name: Name for variable x\n        y_name: Name for variable y\n    \"\"\"\n    if isinstance(x, Sized) and isinstance(y, Sized) and len(x) != len(y):\n        raise ValueError(\n            f\"{x_name} and {y_name} expected to be equal length but \"\n            f\"len({x_name})={len(x)} and len({y_name})={len(y)}\"\n        )\n    return\n\n\ndef _results_to_docs(results: Any) -> List[Document]:\n    return [doc for doc, _ in _results_to_docs_and_scores(results)]\n\n\ndef _results_to_docs_and_scores(results: Any) -> List[Tuple[Document, float]]:\n    final_res: List[Any] = []\n    try:\n        responses, blobs = results[0]\n        if (\n            len(responses) > 0\n            and \"FindDescriptor\" in responses[0]\n            and \"entities\" in responses[0][\"FindDescriptor\"]\n        ):\n            result_entities = responses[0][\"FindDescriptor\"][\"entities\"]\n            # result_blobs = blobs\n            for ent in result_entities:\n                distance = round(ent[\"_distance\"], 10)\n                txt_contents = ent[\"content\"]\n                for p in INVALID_DOC_METADATA_KEYS:\n                    if p in ent:\n                        del ent[p]\n                props = {\n                    mkey: mval\n                    for mkey, mval in ent.items()\n                    if mval not in INVALID_METADATA_VALUE\n                }\n\n                final_res.append(\n                    (\n                        Document(page_content=txt_contents, metadata=props),\n                        distance,\n                    )\n                )\n    except Exception as e:\n        logger.warning(f\"No results returned. Error while parsing results: {e}\")\n    return final_res\n\n\ndef VDMS_Client(host: str = \"localhost\", port: int = 55555) -> vdms.vdms:\n    \"\"\"VDMS client for the VDMS server.\n\n    Args:\n        host: IP or hostname of VDMS server\n        port: Port to connect to VDMS server\n    \"\"\"\n    try:\n        import vdms\n    except ImportError:\n        raise ImportError(\n            \"Could not import vdms python package. \"\n            \"Please install it with `pip install vdms.\"\n        )\n\n    client = vdms.vdms()\n    client.connect(host, port)\n    return client\n\n\nclass VDMS(VectorStore):\n    \"\"\"Intel Lab's VDMS for vector-store workloads.\n\n    To use, you should have both:\n    - the ``vdms`` python package installed\n    - a host (str) and port (int) associated with a deployed VDMS Server\n\n    Visit https://github.com/IntelLabs/vdms/wiki more information.\n\n    IT IS HIGHLY SUGGESTED TO NORMALIZE YOUR DATA.\n\n    Args:\n        client: VDMS Client used to connect to VDMS server\n        collection_name: Name of data collection [Default: langchain]\n        distance_strategy: Method used to calculate distances. VDMS supports\n            \"L2\" (euclidean distance) or \"IP\" (inner product) [Default: L2]\n        engine: Underlying implementation for indexing and computing distances.\n            VDMS supports TileDBDense, TileDBSparse, FaissFlat, FaissIVFFlat,\n            and Flinng [Default: FaissFlat]\n        embedding: Any embedding function implementing\n            `langchain_core.embeddings.Embeddings` interface.\n        relevance_score_fn: Function for obtaining relevance score\n\n    Example:\n        .. code-block:: python\n\n            from langchain_huggingface import HuggingFaceEmbeddings\n            from langchain_community.vectorstores.vdms import VDMS, VDMS_Client\n\n            model_name = \"sentence-transformers/all-mpnet-base-v2\"\n            vectorstore = VDMS(\n                client=VDMS_Client(\"localhost\", 55555),\n                embedding=HuggingFaceEmbeddings(model_name=model_name),\n                collection_name=\"langchain-demo\",\n                distance_strategy=\"L2\",\n                engine=\"FaissFlat\",\n            )\n    \"\"\"\n\n    def __init__(\n        self,\n        client: vdms.vdms,\n        *,\n        embedding: Optional[Embeddings] = None,\n        collection_name: str = DEFAULT_COLLECTION_NAME,  # DescriptorSet name\n        distance_strategy: DISTANCE_METRICS = \"L2\",\n        engine: ENGINES = \"FaissFlat\",\n        relevance_score_fn: Optional[Callable[[float], float]] = None,\n        embedding_dimensions: Optional[int] = None,\n    ) -> None:\n        # Check required parameters\n        self._client = client\n        self.similarity_search_engine = engine\n        self.distance_strategy = distance_strategy\n        self.embedding = embedding\n        self._check_required_inputs(collection_name, embedding_dimensions)\n\n        # Update other parameters\n        self.override_relevance_score_fn = relevance_score_fn\n\n        # Initialize collection\n        self._collection_name = self.add_set(\n            collection_name,\n            engine=self.similarity_search_engine,\n            metric=self.distance_strategy,\n        )\n\n    @property\n    def embeddings(self) -> Optional[Embeddings]:\n        return self.embedding\n\n    def _embed_documents(self, texts: List[str]) -> List[List[float]]:\n        if isinstance(self.embedding, Embeddings):\n            return self.embedding.embed_documents(texts)\n        else:\n            p_str = \"Must provide `embedding` which is expected\"\n            p_str += \" to be an Embeddings object\"\n            raise ValueError(p_str)\n\n    def _embed_video(self, paths: List[str], **kwargs: Any) -> List[List[float]]:\n        if self.embedding is not None and hasattr(self.embedding, \"embed_video\"):\n            return self.embedding.embed_video(paths=paths, **kwargs)\n        else:\n            raise ValueError(\n                \"Must provide `embedding` which has attribute `embed_video`\"\n            )\n\n    def _embed_image(self, uris: List[str]) -> List[List[float]]:\n        if self.embedding is not None and hasattr(self.embedding, \"embed_image\"):\n            return self.embedding.embed_image(uris=uris)\n        else:\n            raise ValueError(\n                \"Must provide `embedding` which has attribute `embed_image`\"\n            )\n\n    def _embed_query(self, text: str) -> List[float]:\n        if isinstance(self.embedding, Embeddings):\n            return self.embedding.embed_query(text)\n        else:\n            raise ValueError(\n                \"Must provide `embedding` which is expected to be an Embeddings object\"\n            )\n\n    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n        \"\"\"\n        The 'correct' relevance function\n        may differ depending on a few things, including:\n        - the distance / similarity metric used by the VectorStore\n        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)\n        - embedding dimensionality\n        - etc.\n        \"\"\"\n        if self.override_relevance_score_fn is not None:\n            return self.override_relevance_score_fn\n\n        # Default strategy is to rely on distance strategy provided\n        # in vectorstore constructor\n        if self.distance_strategy.lower() in [\"ip\", \"l2\"]:\n            return lambda x: x\n        else:\n            raise ValueError(\n                \"No supported normalization function\"\n                f\" for distance_strategy of {self.distance_strategy}.\"\n                \"Consider providing relevance_score_fn to VDMS constructor.\"\n            )\n\n    def _similarity_search_with_relevance_scores(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        filter: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Return docs and their similarity scores on a scale from 0 to 1.\"\"\"\n        if self.override_relevance_score_fn is None:\n            kwargs[\"normalize_distance\"] = True\n        docs_and_scores = self.similarity_search_with_score(\n            query=query,\n            k=k,\n            fetch_k=fetch_k,\n            filter=filter,\n            **kwargs,\n        )\n\n        docs_and_rel_scores: List[Any] = []\n        for doc, score in docs_and_scores:\n            if self.override_relevance_score_fn is None:\n                docs_and_rel_scores.append((doc, score))\n            else:\n                docs_and_rel_scores.append(\n                    (\n                        doc,\n                        self.override_relevance_score_fn(score),\n                    )\n                )\n        return docs_and_rel_scores\n\n    def add(\n        self,\n        collection_name: str,\n        texts: List[str],\n        embeddings: List[List[float]],\n        metadatas: Optional[Union[List[None], List[Dict[str, Any]]]] = None,\n        ids: Optional[List[str]] = None,\n    ) -> List:\n        _len_check_if_sized(texts, embeddings, \"texts\", \"embeddings\")\n\n        metadatas = metadatas if metadatas is not None else [None for _ in texts]\n        _len_check_if_sized(texts, metadatas, \"texts\", \"metadatas\")\n\n        ids = ids if ids is not None else [str(uuid.uuid4()) for _ in texts]\n        _len_check_if_sized(texts, ids, \"texts\", \"ids\")\n\n        all_queries: List[Any] = []\n        all_blobs: List[Any] = []\n        inserted_ids: List[Any] = []\n        for meta, emb, doc, id in zip(metadatas, embeddings, texts, ids):\n            query, blob = self.__get_add_query(\n                collection_name, metadata=meta, embedding=emb, document=doc, id=id\n            )\n\n            if blob is not None:\n                all_queries.append(query)\n                all_blobs.append(blob)\n                inserted_ids.append(id)\n\n        response, response_array = self.__run_vdms_query(all_queries, all_blobs)\n\n        return inserted_ids\n\n    def add_set(\n        self,\n        collection_name: str,\n        engine: ENGINES = \"FaissFlat\",\n        metric: DISTANCE_METRICS = \"L2\",\n    ) -> str:\n        query = _add_descriptorset(\n            \"AddDescriptorSet\",\n            collection_name,\n            self.embedding_dimension,\n            engine=getattr(engine, \"value\", engine),\n            metric=getattr(metric, \"value\", metric),\n        )\n\n        response, _ = self.__run_vdms_query([query])\n\n        if \"FailedCommand\" in response[0]:\n            raise ValueError(f\"Failed to add collection {collection_name}\")\n\n        return collection_name\n\n    def __delete(\n        self,\n        collection_name: str,\n        ids: Union[None, List[str]] = None,\n        constraints: Union[None, Dict[str, Any]] = None,\n    ) -> bool:\n        \"\"\"\n        Deletes entire collection if id is not provided\n        \"\"\"\n        all_queries: List[Any] = []\n        all_blobs: List[Any] = []\n\n        collection_properties = self.__get_properties(collection_name)\n        results = {\"list\": collection_properties}\n\n        if constraints is None:\n            constraints = {\"_deletion\": [\"==\", 1]}\n        else:\n            constraints[\"_deletion\"] = [\"==\", 1]\n\n        if ids is not None:\n            constraints[\"id\"] = [\"==\", ids[0]]  # if len(ids) > 1 else ids[0]]\n\n        query = _add_descriptor(\n            \"FindDescriptor\",\n            collection_name,\n            label=None,\n            ref=None,\n            props=None,\n            link=None,\n            k_neighbors=None,\n            constraints=constraints,\n            results=results,\n        )\n\n        all_queries.append(query)\n        response, response_array = self.__run_vdms_query(all_queries, all_blobs)\n\n        # Update/store indices after deletion\n        query = _add_descriptorset(\n            \"FindDescriptorSet\", collection_name, storeIndex=True\n        )\n        responseSet, _ = self.__run_vdms_query([query], all_blobs)\n        return \"FindDescriptor\" in response[0]\n\n    def __get_add_query(\n        self,\n        collection_name: str,\n        metadata: Optional[Any] = None,\n        embedding: Union[List[float], None] = None,\n        document: Optional[Any] = None,\n        id: Optional[str] = None,\n    ) -> Tuple[Dict[str, Dict[str, Any]], Union[bytes, None]]:\n        if id is None:\n            props: Dict[str, Any] = {}\n        else:\n            props = {\"id\": id}\n            id_exists, query = _check_descriptor_exists_by_id(\n                self._client, collection_name, id\n            )\n            if id_exists:\n                skipped_value = {\n                    prop_key: prop_val[-1]\n                    for prop_key, prop_val in query[\"FindDescriptor\"][\n                        \"constraints\"\n                    ].items()\n                }\n                pstr = f\"[!] Embedding with id ({id}) exists in DB;\"\n                pstr += \"Therefore, skipped and not inserted\"\n                print(pstr)  # noqa: T201\n                print(f\"\\tSkipped values are: {skipped_value}\")  # noqa: T201\n                return query, None\n\n        if metadata:\n            props.update(metadata)\n        if document not in [None, \"\"]:\n            props[\"content\"] = document\n\n        for k in props.keys():\n            if k not in self.collection_properties:\n                self.collection_properties.append(k)\n\n        query = _add_descriptor(\n            \"AddDescriptor\",\n            collection_name,\n            label=None,\n            ref=None,\n            props=props,\n            link=None,\n            k_neighbors=None,\n            constraints=None,\n            results=None,\n        )\n\n        blob = embedding2bytes(embedding)\n\n        return (\n            query,\n            blob,\n        )\n\n    def __get_properties(\n        self,\n        collection_name: str,\n        unique_entity: Optional[bool] = False,\n        deletion: Optional[bool] = False,\n    ) -> List[str]:\n        find_query = _find_property_entity(\n            collection_name, unique_entity=unique_entity, deletion=deletion\n        )\n        response, response_blob = self.__run_vdms_query([find_query])\n        if len(response_blob) > 0:\n            collection_properties = _bytes2str(response_blob[0]).split(\",\")\n        else:\n            collection_properties = deepcopy(DEFAULT_PROPERTIES)\n        return collection_properties\n\n    def __run_vdms_query(\n        self,\n        all_queries: List[Dict],\n        all_blobs: Optional[List] = [],\n        print_last_response: Optional[bool] = False,\n    ) -> Tuple[Any, Any]:\n        response, response_array = self._client.query(all_queries, all_blobs)\n\n        _ = _check_valid_response(all_queries, response)\n        if print_last_response:\n            self._client.print_last_response()\n        return response, response_array\n\n    def __update(\n        self,\n        collection_name: str,\n        ids: List[str],\n        documents: List[str],\n        embeddings: List[List[float]],\n        metadatas: Optional[Union[List[None], List[Dict[str, Any]]]] = None,\n    ) -> None:\n        \"\"\"\n        Updates (find, delete, add) a collection based on id.\n        If more than one collection returned with id, error occuers\n        \"\"\"\n        _len_check_if_sized(ids, documents, \"ids\", \"documents\")\n\n        _len_check_if_sized(ids, embeddings, \"ids\", \"embeddings\")\n\n        metadatas = metadatas if metadatas is not None else [None for _ in ids]\n        _len_check_if_sized(ids, metadatas, \"ids\", \"metadatas\")\n\n        orig_props = self.__get_properties(collection_name)\n\n        updated_ids: List[Any] = []\n        for meta, emb, doc, id in zip(metadatas, embeddings, documents, ids):\n            results = {\"list\": self.collection_properties}\n\n            constraints = {\"_deletion\": [\"==\", 1]}\n\n            if id is not None:\n                constraints[\"id\"] = [\"==\", id]\n\n            query = _add_descriptor(\n                \"FindDescriptor\",\n                collection_name,\n                label=None,\n                ref=None,\n                props=None,\n                link=None,\n                k_neighbors=None,\n                constraints=constraints,\n                results=results,\n            )\n\n            response, response_array = self.__run_vdms_query([query])\n\n            query, blob = self.__get_add_query(\n                collection_name,\n                metadata=meta,\n                embedding=emb,\n                document=doc,\n                id=id,\n            )\n            if blob is not None:\n                response, response_array = self.__run_vdms_query([query], [blob])\n                updated_ids.append(id)\n\n        self.__update_properties(\n            collection_name, orig_props, self.collection_properties\n        )\n\n    def __update_properties(\n        self,\n        collection_name: str,\n        current_collection_properties: List,\n        new_collection_properties: Optional[List],\n    ) -> None:\n        if new_collection_properties is not None:\n            old_collection_properties = deepcopy(current_collection_properties)\n            for prop in new_collection_properties:\n                if prop not in current_collection_properties:\n                    current_collection_properties.append(prop)\n\n            if current_collection_properties != old_collection_properties:\n                all_queries, blob_arr = _build_property_query(\n                    collection_name,\n                    command_type=\"update\",\n                    all_properties=current_collection_properties,\n                )\n                response, _ = self.__run_vdms_query(all_queries, [blob_arr])\n\n    def add_images(\n        self,\n        uris: List[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        batch_size: int = DEFAULT_INSERT_BATCH_SIZE,\n        add_path: Optional[bool] = True,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more images through the embeddings and add to the vectorstore.\n\n        Images are added as embeddings (AddDescriptor) instead of separate\n        entity (AddImage) within VDMS to leverage similarity search capability\n\n        Args:\n            uris: List of paths to the images to add to the vectorstore.\n            metadatas: Optional list of metadatas associated with the images.\n            ids: Optional list of unique IDs.\n            batch_size (int): Number of concurrent requests to send to the server.\n            add_path: Bool to add image path as metadata\n\n        Returns:\n            List of ids from adding images into the vectorstore.\n        \"\"\"\n        # Map from uris to blobs to base64\n        b64_texts = [self.encode_image(image_path=uri) for uri in uris]\n\n        if add_path and metadatas:\n            for midx, uri in enumerate(uris):\n                metadatas[midx][\"image_path\"] = uri\n        elif add_path:\n            metadatas = []\n            for uri in uris:\n                metadatas.append({\"image_path\": uri})\n\n        # Populate IDs\n        ids = ids if ids is not None else [str(uuid.uuid4()) for _ in uris]\n\n        # Set embeddings\n        embeddings = self._embed_image(uris=uris)\n\n        if metadatas is None:\n            metadatas = [{} for _ in uris]\n        else:\n            metadatas = [_validate_vdms_properties(m) for m in metadatas]\n\n        self.add_from(\n            texts=b64_texts,\n            embeddings=embeddings,\n            ids=ids,\n            metadatas=metadatas,\n            batch_size=batch_size,\n            **kwargs,\n        )\n        return ids\n\n    def add_videos(\n        self,\n        paths: List[str],\n        texts: Optional[List[str]] = None,\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        batch_size: int = 1,\n        add_path: Optional[bool] = True,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run videos through the embeddings and add to the vectorstore.\n\n        Videos are added as embeddings (AddDescriptor) instead of separate\n        entity (AddVideo) within VDMS to leverage similarity search capability\n\n        Args:\n            paths: List of paths to the videos to add to the vectorstore.\n            metadatas: Optional list of text associated with the videos.\n            metadatas: Optional list of metadatas associated with the videos.\n            ids: Optional list of unique IDs.\n            batch_size (int): Number of concurrent requests to send to the server.\n            add_path: Bool to add video path as metadata\n\n        Returns:\n            List of ids from adding videos into the vectorstore.\n        \"\"\"\n        if texts is None:\n            texts = [\"\" for _ in paths]\n\n        if add_path and metadatas:\n            for midx, path in enumerate(paths):\n                metadatas[midx][\"video_path\"] = path\n        elif add_path:\n            metadatas = []\n            for path in paths:\n                metadatas.append({\"video_path\": path})\n\n        # Populate IDs\n        ids = ids if ids is not None else [str(uuid.uuid4()) for _ in paths]\n\n        # Set embeddings\n        embeddings = self._embed_video(paths=paths, **kwargs)\n\n        if metadatas is None:\n            metadatas = [{} for _ in paths]\n\n        self.add_from(\n            texts=texts,\n            embeddings=embeddings,\n            ids=ids,\n            metadatas=metadatas,\n            batch_size=batch_size,\n            **kwargs,\n        )\n        return ids\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        batch_size: int = DEFAULT_INSERT_BATCH_SIZE,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n\n        Args:\n            texts: List of strings to add to the vectorstore.\n            metadatas: Optional list of metadatas associated with the texts.\n            ids: Optional list of unique IDs.\n            batch_size (int): Number of concurrent requests to send to the server.\n\n        Returns:\n            List of ids from adding the texts into the vectorstore.\n        \"\"\"\n\n        texts = list(texts)\n        if ids is None:\n            ids = [str(uuid.uuid4()) for _ in texts]\n\n        embeddings = self._embed_documents(texts)\n\n        if metadatas is None:\n            metadatas = [{} for _ in texts]\n        else:\n            metadatas = [_validate_vdms_properties(m) for m in metadatas]\n\n        inserted_ids = self.add_from(\n            texts=texts,\n            embeddings=embeddings,\n            ids=ids,\n            metadatas=metadatas,\n            batch_size=batch_size,\n            **kwargs,\n        )\n        return inserted_ids\n\n    def add_from(\n        self,\n        texts: List[str],\n        embeddings: List[List[float]],\n        ids: List[str],\n        metadatas: Optional[List[dict]] = None,\n        batch_size: int = DEFAULT_INSERT_BATCH_SIZE,\n        **kwargs: Any,\n    ) -> List[str]:\n        # Get initial properties\n        orig_props = self.__get_properties(self._collection_name)\n        inserted_ids: List[str] = []\n        for start_idx in range(0, len(texts), batch_size):\n            end_idx = min(start_idx + batch_size, len(texts))\n\n            batch_texts = texts[start_idx:end_idx]\n            batch_embedding_vectors = embeddings[start_idx:end_idx]\n            batch_ids = ids[start_idx:end_idx]\n            if metadatas:\n                batch_metadatas = metadatas[start_idx:end_idx]\n\n            result = self.add(\n                self._collection_name,\n                embeddings=batch_embedding_vectors,\n                texts=batch_texts,\n                metadatas=batch_metadatas,\n                ids=batch_ids,\n            )\n\n            inserted_ids.extend(result)\n\n        # Update Properties\n        self.__update_properties(\n            self._collection_name, orig_props, self.collection_properties\n        )\n        return inserted_ids\n\n    def _check_required_inputs(\n        self, collection_name: str, embedding_dimensions: Union[int, None]\n    ) -> None:\n        # Check connection to client\n        if not self._client.is_connected():\n            raise ValueError(\n                \"VDMS client must be connected to a VDMS server.\"\n                + \"Please use VDMS_Client to establish a connection\"\n            )\n\n        # Check Distance Metric\n        if self.distance_strategy not in AVAILABLE_DISTANCE_METRICS:\n            raise ValueError(\"distance_strategy must be either 'L2' or 'IP'\")\n\n        # Check Engines\n        if self.similarity_search_engine not in AVAILABLE_ENGINES:\n            raise ValueError(\n                \"engine must be either 'TileDBDense', 'TileDBSparse', \"\n                + \"'FaissFlat', 'FaissIVFFlat', or 'Flinng'\"\n            )\n\n        # Check Embedding Func is provided and store dimension size\n        if self.embedding is None:\n            raise ValueError(\"Must provide embedding function\")\n\n        if embedding_dimensions is not None:\n            self.embedding_dimension = embedding_dimensions\n        elif self.embedding is not None and hasattr(self.embedding, \"embed_query\"):\n            self.embedding_dimension = len(\n                self._embed_query(\"This is a sample sentence.\")\n            )\n        elif self.embedding is not None and (\n            hasattr(self.embedding, \"embed_image\")\n            or hasattr(self.embedding, \"embed_video\")\n        ):\n            if hasattr(self.embedding, \"model\"):\n                try:\n                    self.embedding_dimension = (\n                        self.embedding.model.token_embedding.embedding_dim\n                    )\n                except ValueError:\n                    raise ValueError(\n                        \"Embedding dimension needed. Please define embedding_dimensions\"\n                    )\n            else:\n                raise ValueError(\n                    \"Embedding dimension needed. Please define embedding_dimensions\"\n                )\n\n        # Check for properties\n        current_props = self.__get_properties(collection_name)\n        if hasattr(self, \"collection_properties\"):\n            self.collection_properties.extend(current_props)\n        else:\n            self.collection_properties: List[str] = current_props\n\n    def count(self, collection_name: str) -> int:\n        all_queries: List[Any] = []\n        all_blobs: List[Any] = []\n\n        results = {\"count\": \"\", \"list\": [\"id\"]}  # collection_properties}\n        query = _add_descriptor(\n            \"FindDescriptor\",\n            collection_name,\n            label=None,\n            ref=None,\n            props=None,\n            link=None,\n            k_neighbors=None,\n            constraints=None,\n            results=results,\n        )\n\n        all_queries.append(query)\n\n        response, response_array = self.__run_vdms_query(all_queries, all_blobs)\n        return response[0][\"FindDescriptor\"][\"returned\"]\n\n    def decode_image(self, base64_image: str) -> bytes:\n        return base64.b64decode(base64_image)\n\n    def delete(\n        self,\n        ids: Optional[List[str]] = None,\n        collection_name: Optional[str] = None,\n        constraints: Optional[Dict] = None,\n        **kwargs: Any,\n    ) -> bool:\n        \"\"\"Delete by ID. These are the IDs in the vectorstore.\n\n        Args:\n            ids: List of ids to delete.\n\n        Returns:\n            Optional[bool]: True if deletion is successful,\n            False otherwise, None if not implemented.\n        \"\"\"\n        name = collection_name if collection_name is not None else self._collection_name\n        return self.__delete(name, ids=ids, constraints=constraints)\n\n    def get_k_candidates(\n        self,\n        setname: str,\n        fetch_k: Optional[int],\n        results: Optional[Dict[str, Any]] = None,\n        all_blobs: Optional[List] = None,\n        normalize: Optional[bool] = False,\n    ) -> Tuple[List[Dict[str, Any]], List, float]:\n        max_dist = 1\n        command_str = \"FindDescriptor\"\n        query = _add_descriptor(\n            command_str,\n            setname,\n            k_neighbors=fetch_k,\n            results=results,\n        )\n        response, response_array = self.__run_vdms_query([query], all_blobs)\n\n        if normalize and command_str in response[0]:\n            max_dist = response[0][command_str][\"entities\"][-1][\"_distance\"]\n\n        return response, response_array, max_dist\n\n    def get_descriptor_response(\n        self,\n        command_str: str,\n        setname: str,\n        k_neighbors: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        constraints: Optional[dict] = None,\n        results: Optional[Dict[str, Any]] = None,\n        query_embedding: Optional[List[float]] = None,\n        normalize_distance: bool = False,\n    ) -> Tuple[List[Dict[str, Any]], List]:\n        all_blobs: List[Any] = []\n        blob = embedding2bytes(query_embedding)\n        if blob is not None:\n            all_blobs.append(blob)\n\n        if constraints is None:\n            # K results returned\n            response, response_array, max_dist = self.get_k_candidates(\n                setname, k_neighbors, results, all_blobs, normalize=normalize_distance\n            )\n        else:\n            if results is None:\n                results = {\"list\": [\"id\"]}\n            elif \"list\" not in results:\n                results[\"list\"] = [\"id\"]\n            elif \"id\" not in results[\"list\"]:\n                results[\"list\"].append(\"id\")\n\n            # (1) Find docs satisfy constraints\n            query = _add_descriptor(\n                command_str,\n                setname,\n                constraints=constraints,\n                results=results,\n            )\n            response, response_array = self.__run_vdms_query([query])\n            if command_str in response[0] and response[0][command_str][\"returned\"] > 0:\n                ids_of_interest = [\n                    ent[\"id\"] for ent in response[0][command_str][\"entities\"]\n                ]\n            else:\n                return [], []\n\n            # (2) Find top fetch_k results\n            response, response_array, max_dist = self.get_k_candidates(\n                setname, fetch_k, results, all_blobs, normalize=normalize_distance\n            )\n            if command_str not in response[0] or (\n                command_str in response[0] and response[0][command_str][\"returned\"] == 0\n            ):\n                return [], []\n\n            # (3) Intersection of (1) & (2) using ids\n            new_entities: List[Dict] = []\n            for ent in response[0][command_str][\"entities\"]:\n                if ent[\"id\"] in ids_of_interest:\n                    new_entities.append(ent)\n                if len(new_entities) == k_neighbors:\n                    break\n            response[0][command_str][\"entities\"] = new_entities\n            response[0][command_str][\"returned\"] = len(new_entities)\n            if len(new_entities) < k_neighbors:\n                p_str = \"Returned items < k_neighbors; Try increasing fetch_k\"\n                print(p_str)  # noqa: T201\n\n        if normalize_distance:\n            max_dist = 1.0 if max_dist in [0, np.inf] else max_dist\n            for ent_idx, ent in enumerate(response[0][command_str][\"entities\"]):\n                ent[\"_distance\"] = ent[\"_distance\"] / max_dist\n                response[0][command_str][\"entities\"][ent_idx][\"_distance\"] = ent[\n                    \"_distance\"\n                ]\n\n        return response, response_array\n\n    def encode_image(self, image_path: str) -> str:\n        with open(image_path, \"rb\") as f:\n            blob = f.read()\n            return base64.b64encode(blob).decode(\"utf-8\")\n\n    @classmethod\n    def from_documents(\n        cls: Type[VDMS],\n        documents: List[Document],\n        embedding: Optional[Embeddings] = None,\n        ids: Optional[List[str]] = None,\n        batch_size: int = DEFAULT_INSERT_BATCH_SIZE,\n        collection_name: str = DEFAULT_COLLECTION_NAME,  # Add this line\n        **kwargs: Any,\n    ) -> VDMS:\n        \"\"\"Create a VDMS vectorstore from a list of documents.\n\n        Args:\n            collection_name (str): Name of the collection to create.\n            documents (List[Document]): List of documents to add to vectorstore.\n            embedding (Embeddings): Embedding function. Defaults to None.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            batch_size (int): Number of concurrent requests to send to the server.\n\n        Returns:\n            VDMS: VDMS vectorstore.\n        \"\"\"\n        client: vdms.vdms = kwargs[\"client\"]\n\n        return cls.from_texts(\n            client=client,\n            texts=[doc.page_content for doc in documents],\n            metadatas=[doc.metadata for doc in documents],\n            embedding=embedding,\n            ids=ids,\n            batch_size=batch_size,\n            collection_name=collection_name,\n            # **kwargs,\n        )\n\n    @classmethod\n    def from_texts(\n        cls: Type[VDMS],\n        texts: List[str],\n        embedding: Optional[Embeddings] = None,\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        batch_size: int = DEFAULT_INSERT_BATCH_SIZE,\n        collection_name: str = DEFAULT_COLLECTION_NAME,\n        **kwargs: Any,\n    ) -> VDMS:\n        \"\"\"Create a VDMS vectorstore from a raw documents.\n\n        Args:\n            texts (List[str]): List of texts to add to the collection.\n            embedding (Embeddings): Embedding function. Defaults to None.\n            metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n            ids (Optional[List[str]]): List of document IDs. Defaults to None.\n            batch_size (int): Number of concurrent requests to send to the server.\n            collection_name (str): Name of the collection to create.\n\n        Returns:\n            VDMS: VDMS vectorstore.\n        \"\"\"\n        client: vdms.vdms = kwargs[\"client\"]\n        vdms_collection = cls(\n            collection_name=collection_name,\n            embedding=embedding,\n            client=client,\n            # **kwargs,\n        )\n        if ids is None:\n            ids = [str(uuid.uuid4()) for _ in texts]\n        vdms_collection.add_texts(\n            texts=texts,\n            metadatas=metadatas,\n            ids=ids,\n            batch_size=batch_size,  # **kwargs\n        )\n        return vdms_collection\n\n    def get(\n        self,\n        collection_name: str,\n        constraints: Optional[Dict] = None,\n        limit: Optional[int] = None,\n        include: List[str] = [\"metadata\"],\n    ) -> Tuple[Any, Any]:\n        \"\"\"Gets the collection.\n        Get embeddings and their associated data from the data store.\n        If no constraints provided returns all embeddings up to limit.\n\n        Args:\n            constraints: A dict used to filter results by.\n                   E.g. `{\"color\" : [\"==\", \"red\"], \"price\": [\">\", 4.00]}`. Optional.\n            limit: The number of documents to return. Optional.\n            include: A list of what to include in the results.\n                     Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n                     Ids are always included.\n                     Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n        \"\"\"\n        all_queries: List[Any] = []\n        all_blobs: List[Any] = []\n\n        results: Dict[str, Any] = {\"count\": \"\"}\n\n        if limit is not None:\n            results[\"limit\"] = limit\n\n        # Include metadata\n        if \"metadata\" in include:\n            collection_properties = self.__get_properties(collection_name)\n            results[\"list\"] = collection_properties\n\n        # Include embedding\n        if \"embeddings\" in include:\n            results[\"blob\"] = True\n\n        query = _add_descriptor(\n            \"FindDescriptor\",\n            collection_name,\n            k_neighbors=None,\n            constraints=constraints,\n            results=results,\n        )\n\n        all_queries.append(query)\n\n        response, response_array = self.__run_vdms_query(all_queries, all_blobs)\n        return response, response_array\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query (str): Query to look up. Text or path for image or video.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\n                \"For MMR search, you must specify an embedding function oncreation.\"\n            )\n\n        # embedding_vector: List[float] = self._embed_query(query)\n        embedding_vector: List[float]\n        if not os.path.isfile(query) and hasattr(self.embedding, \"embed_query\"):\n            embedding_vector = self._embed_query(query)\n        elif os.path.isfile(query) and hasattr(self.embedding, \"embed_image\"):\n            embedding_vector = self._embed_image(uris=[query])[0]\n        elif os.path.isfile(query) and hasattr(self.embedding, \"embed_video\"):\n            embedding_vector = self._embed_video(paths=[query])[0]\n        else:\n            error_msg = f\"Could not generate embedding for query '{query}'.\"\n            error_msg += \"If using path for image or video, verify embedding model \"\n            error_msg += \"has callable functions 'embed_image' or 'embed_video'.\"\n            raise ValueError(error_msg)\n\n        docs = self.max_marginal_relevance_search_by_vector(\n            embedding_vector,\n            k,\n            fetch_k,\n            lambda_mult=lambda_mult,\n            filter=filter,\n        )\n        return docs\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        results = self.query_collection_embeddings(\n            query_embeddings=[embedding],\n            n_results=fetch_k,\n            filter=filter,\n            include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"],\n        )\n\n        if len(results[0][1]) == 0:\n            # No results returned\n            return []\n        else:\n            embedding_list = [\n                list(_bytes2embedding(result)) for result in results[0][1]\n            ]\n\n            mmr_selected = maximal_marginal_relevance(\n                np.array(embedding, dtype=np.float32),\n                embedding_list,\n                k=k,\n                lambda_mult=lambda_mult,\n            )\n\n            candidates = _results_to_docs(results)\n\n            selected_results = [\n                r for i, r in enumerate(candidates) if i in mmr_selected\n            ]\n            return selected_results\n\n    def max_marginal_relevance_search_with_score(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query (str): Query to look up. Text or path for image or video.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\n                \"For MMR search, you must specify an embedding function oncreation.\"\n            )\n\n        if not os.path.isfile(query) and hasattr(self.embedding, \"embed_query\"):\n            embedding = self._embed_query(query)\n        elif os.path.isfile(query) and hasattr(self.embedding, \"embed_image\"):\n            embedding = self._embed_image(uris=[query])[0]\n        elif os.path.isfile(query) and hasattr(self.embedding, \"embed_video\"):\n            embedding = self._embed_video(paths=[query])[0]\n        else:\n            error_msg = f\"Could not generate embedding for query '{query}'.\"\n            error_msg += \"If using path for image or video, verify embedding model \"\n            error_msg += \"has callable functions 'embed_image' or 'embed_video'.\"\n            raise ValueError(error_msg)\n\n        docs = self.max_marginal_relevance_search_with_score_by_vector(\n            embedding,\n            k,\n            fetch_k,\n            lambda_mult=lambda_mult,\n            filter=filter,\n        )\n        return docs\n\n    def max_marginal_relevance_search_with_score_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        lambda_mult: float = 0.5,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                        of diversity among the results with 0 corresponding\n                        to maximum diversity and 1 to minimum diversity.\n                        Defaults to 0.5.\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        results = self.query_collection_embeddings(\n            query_embeddings=[embedding],\n            n_results=fetch_k,\n            filter=filter,\n            include=[\"metadatas\", \"documents\", \"distances\", \"embeddings\"],\n        )\n\n        if len(results[0][1]) == 0:\n            # No results returned\n            return []\n        else:\n            embedding_list = [\n                list(_bytes2embedding(result)) for result in results[0][1]\n            ]\n\n            mmr_selected = maximal_marginal_relevance(\n                np.array(embedding, dtype=np.float32),\n                embedding_list,\n                k=k,\n                lambda_mult=lambda_mult,\n            )\n\n            candidates = _results_to_docs_and_scores(results)\n\n            selected_results = [\n                (r, s) for i, (r, s) in enumerate(candidates) if i in mmr_selected\n            ]\n            return selected_results\n\n    def query_collection_embeddings(\n        self,\n        query_embeddings: Optional[List[List[float]]] = None,\n        collection_name: Optional[str] = None,\n        n_results: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        filter: Union[None, Dict[str, Any]] = None,\n        results: Union[None, Dict[str, Any]] = None,\n        normalize_distance: bool = False,\n        **kwargs: Any,\n    ) -> List[Tuple[Dict[str, Any], List]]:\n        all_responses: List[Any] = []\n\n        if collection_name is None:\n            collection_name = self._collection_name\n\n        if query_embeddings is None:\n            return all_responses\n\n        include = kwargs.get(\"include\", [\"metadatas\"])\n        if results is None and \"metadatas\" in include:\n            results = {\n                \"list\": self.collection_properties,\n                \"blob\": \"embeddings\" in include,\n            }\n\n        for qemb in query_embeddings:\n            response, response_array = self.get_descriptor_response(\n                \"FindDescriptor\",\n                collection_name,\n                k_neighbors=n_results,\n                fetch_k=fetch_k,\n                constraints=filter,\n                results=results,\n                normalize_distance=normalize_distance,\n                query_embedding=qemb,\n            )\n            all_responses.append([response, response_array])\n\n        return all_responses\n\n    def similarity_search(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Run similarity search with VDMS.\n\n        Args:\n            query (str): Query to look up. Text or path for image or video.\n            k (int): Number of results to return. Defaults to 3.\n            fetch_k (int): Number of candidates to fetch for knn (>= k).\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Document]: List of documents most similar to the query text.\n        \"\"\"\n        docs_and_scores = self.similarity_search_with_score(\n            query, k=k, fetch_k=fetch_k, filter=filter, **kwargs\n        )\n        return [doc for doc, _ in docs_and_scores]\n\n    def similarity_search_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to embedding vector.\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 3.\n            fetch_k (int): Number of candidates to fetch for knn (>= k).\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n        Returns:\n            List of Documents most similar to the query vector.\n        \"\"\"\n        results = self.query_collection_embeddings(\n            query_embeddings=[embedding],\n            n_results=k,\n            fetch_k=fetch_k,\n            filter=filter,\n            **kwargs,\n        )\n\n        return _results_to_docs(results)\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Run similarity search with VDMS with distance.\n\n        Args:\n            query (str): Query to look up. Text or path for image or video.\n            k (int): Number of results to return. Defaults to 3.\n            fetch_k (int): Number of candidates to fetch for knn (>= k).\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text and cosine distance in float for each.\n            Lower score represents more similarity.\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"Must provide embedding function\")\n        else:\n            if not os.path.isfile(query) and hasattr(self.embedding, \"embed_query\"):\n                query_embedding: List[float] = self._embed_query(query)\n            elif os.path.isfile(query) and hasattr(self.embedding, \"embed_image\"):\n                query_embedding = self._embed_image(uris=[query])[0]\n            elif os.path.isfile(query) and hasattr(self.embedding, \"embed_video\"):\n                query_embedding = self._embed_video(paths=[query])[0]\n            else:\n                error_msg = f\"Could not generate embedding for query '{query}'.\"\n                error_msg += \"If using path for image or video, verify embedding model \"\n                error_msg += \"has callable functions 'embed_image' or 'embed_video'.\"\n                raise ValueError(error_msg)\n\n            results = self.query_collection_embeddings(\n                query_embeddings=[query_embedding],\n                n_results=k,\n                fetch_k=fetch_k,\n                filter=filter,\n                **kwargs,\n            )\n\n        return _results_to_docs_and_scores(results)\n\n    def similarity_search_with_score_by_vector(\n        self,\n        embedding: List[float],\n        k: int = DEFAULT_K,\n        fetch_k: int = DEFAULT_FETCH_K,\n        filter: Optional[Dict[str, List]] = None,\n        **kwargs: Any,\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"\n        Return docs most similar to embedding vector and similarity score.\n\n        Args:\n            embedding (List[float]): Embedding to look up documents similar to.\n            k (int): Number of Documents to return. Defaults to 3.\n            fetch_k (int): Number of candidates to fetch for knn (>= k).\n            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n\n        Returns:\n            List[Tuple[Document, float]]: List of documents most similar to\n            the query text. Lower score represents more similarity.\n        \"\"\"\n\n        # kwargs[\"normalize_distance\"] = True\n\n        results = self.query_collection_embeddings(\n            query_embeddings=[embedding],\n            n_results=k,\n            fetch_k=fetch_k,\n            filter=filter,\n            **kwargs,\n        )\n        return _results_to_docs_and_scores(results)\n\n    def update_document(\n        self, collection_name: str, document_id: str, document: Document\n    ) -> None:\n        \"\"\"Update a document in the collection.\n\n        Args:\n            document_id (str): ID of the document to update.\n            document (Document): Document to update.\n        \"\"\"\n        return self.update_documents(collection_name, [document_id], [document])\n\n    def update_documents(\n        self, collection_name: str, ids: List[str], documents: List[Document]\n    ) -> None:\n        \"\"\"Update a document in the collection.\n\n        Args:\n            ids (List[str]): List of ids of the document to update.\n            documents (List[Document]): List of documents to update.\n        \"\"\"\n        text = [document.page_content for document in documents]\n        metadata = [\n            _validate_vdms_properties(document.metadata) for document in documents\n        ]\n        embeddings = self._embed_documents(text)\n\n        self.__update(\n            collection_name,\n            ids,\n            metadatas=metadata,\n            embeddings=embeddings,\n            documents=text,\n        )\n\n\n# VDMS UTILITY\n\n\ndef _add_descriptor(\n    command_str: str,\n    setname: str,\n    label: Optional[str] = None,\n    ref: Optional[int] = None,\n    props: Optional[dict] = None,\n    link: Optional[dict] = None,\n    k_neighbors: Optional[int] = None,\n    constraints: Optional[dict] = None,\n    results: Optional[dict] = None,\n) -> Dict[str, Dict[str, Any]]:\n    entity: Dict[str, Any] = {\"set\": setname}\n\n    if \"Add\" in command_str and label:\n        entity[\"label\"] = label\n\n    if ref is not None:\n        entity[\"_ref\"] = ref\n\n    if props not in INVALID_METADATA_VALUE:\n        entity[\"properties\"] = props\n\n    if \"Add\" in command_str and link is not None:\n        entity[\"link\"] = link\n\n    if \"Find\" in command_str and k_neighbors is not None:\n        entity[\"k_neighbors\"] = int(k_neighbors)\n\n    if \"Find\" in command_str and constraints not in INVALID_METADATA_VALUE:\n        entity[\"constraints\"] = constraints\n\n    if \"Find\" in command_str and results not in INVALID_METADATA_VALUE:\n        entity[\"results\"] = results\n\n    query = {command_str: entity}\n    return query\n\n\ndef _add_descriptorset(\n    command_str: str,\n    name: str,\n    num_dims: Optional[int] = None,\n    engine: Optional[str] = None,\n    metric: Optional[str] = None,\n    ref: Optional[int] = None,\n    props: Optional[Dict] = None,\n    link: Optional[Dict] = None,\n    storeIndex: bool = False,\n    constraints: Optional[Dict] = None,\n    results: Optional[Dict] = None,\n) -> Dict[str, Any]:\n    if command_str == \"AddDescriptorSet\" and all(\n        var is not None for var in [name, num_dims]\n    ):\n        entity: Dict[str, Any] = {\n            \"name\": name,\n            \"dimensions\": num_dims,\n        }\n\n        if engine is not None:\n            entity[\"engine\"] = engine\n\n        if metric is not None:\n            entity[\"metric\"] = metric\n\n        if ref is not None:\n            entity[\"_ref\"] = ref\n\n        if props not in [None, {}]:\n            entity[\"properties\"] = props\n\n        if link is not None:\n            entity[\"link\"] = link\n\n    elif command_str == \"FindDescriptorSet\":\n        entity = {\"set\": name}\n\n        if storeIndex:\n            entity[\"storeIndex\"] = storeIndex\n\n        if constraints not in [None, {}]:\n            entity[\"constraints\"] = constraints\n\n        if results is not None:\n            entity[\"results\"] = results\n\n    else:\n        raise ValueError(f\"Unknown command: {command_str}\")\n\n    query = {command_str: entity}\n    return query\n\n\ndef _add_entity_with_blob(\n    collection_name: str, all_properties: List\n) -> Tuple[Dict[str, Any], bytes]:\n    all_properties_str = \",\".join(all_properties) if len(all_properties) > 0 else \"\"\n\n    querytype = \"AddEntity\"\n    entity: Dict[str, Any] = {}\n    entity[\"class\"] = \"properties\"\n    entity[\"blob\"] = True  # New\n\n    props: Dict[str, Any] = {\"name\": collection_name}\n    props[\"type\"] = \"queryable properties\"\n    props[\"content\"] = all_properties_str\n    entity[\"properties\"] = props\n\n    byte_data = _str2bytes(all_properties_str)\n\n    query: Dict[str, Any] = {}\n    query[querytype] = entity\n    return query, byte_data\n\n\ndef _build_property_query(\n    collection_name: str,\n    command_type: str = \"find\",\n    all_properties: List = [],\n    ref: Optional[int] = None,\n) -> Tuple[Any, Any]:\n    all_queries: List[Any] = []\n    blob_arr: List[Any] = []\n\n    choices = [\"find\", \"add\", \"update\"]\n    if command_type.lower() not in choices:\n        raise ValueError(\"[!] Invalid type. Choices are : {}\".format(\",\".join(choices)))\n\n    if command_type.lower() == \"find\":\n        query = _find_property_entity(collection_name, unique_entity=True)\n        all_queries.append(query)\n\n    elif command_type.lower() == \"add\":\n        query, byte_data = _add_entity_with_blob(collection_name, all_properties)\n        all_queries.append(query)\n        blob_arr.append(byte_data)\n\n    elif command_type.lower() == \"update\":\n        # Find & Delete\n        query = _find_property_entity(collection_name, deletion=True)\n        all_queries.append(query)\n\n        # Add\n        query, byte_data = _add_entity_with_blob(collection_name, all_properties)\n        all_queries.append(query)\n        blob_arr.append(byte_data)\n\n    return all_queries, blob_arr\n\n\ndef _bytes2embedding(blob: bytes) -> Any:\n    emb = np.frombuffer(blob, dtype=\"float32\")\n    return emb\n\n\ndef _bytes2str(in_bytes: bytes) -> str:\n    return in_bytes.decode()\n\n\ndef _get_cmds_from_query(all_queries: list) -> List[str]:\n    return list(set([k for q in all_queries for k in q.keys()]))\n\n\ndef _check_valid_response(all_queries: List[dict], response: Any) -> bool:\n    cmd_list = _get_cmds_from_query(all_queries)\n    valid_res = isinstance(response, list) and any(\n        cmd in response[0]\n        and \"returned\" in response[0][cmd]\n        and response[0][cmd][\"returned\"] > 0\n        for cmd in cmd_list\n    )\n    return valid_res\n\n\ndef _check_descriptor_exists_by_id(\n    client: vdms.vdms,\n    setname: str,\n    id: str,\n) -> Tuple[bool, Any]:\n    constraints = {\"id\": [\"==\", id]}\n    findDescriptor = _add_descriptor(\n        \"FindDescriptor\",\n        setname,\n        constraints=constraints,\n        results={\"list\": [\"id\"], \"count\": \"\"},\n    )\n    all_queries = [findDescriptor]\n    res, _ = client.query(all_queries)\n\n    valid_res = _check_valid_response(all_queries, res)\n    return valid_res, findDescriptor\n\n\ndef embedding2bytes(embedding: Union[List[float], None]) -> Union[bytes, None]:\n    \"\"\"Convert embedding to bytes.\"\"\"\n\n    blob = None\n    if embedding is not None:\n        emb = np.array(embedding, dtype=\"float32\")\n        blob = emb.tobytes()\n    return blob\n\n\ndef _find_property_entity(\n    collection_name: str,\n    unique_entity: Optional[bool] = False,\n    deletion: Optional[bool] = False,\n) -> Dict[str, Dict[str, Any]]:\n    querytype = \"FindEntity\"\n    entity: Dict[str, Any] = {}\n    entity[\"class\"] = \"properties\"\n    if unique_entity:\n        entity[\"unique\"] = unique_entity\n\n    results: Dict[str, Any] = {}\n    results[\"blob\"] = True\n    results[\"count\"] = \"\"\n    results[\"list\"] = [\"content\"]\n    entity[\"results\"] = results\n\n    constraints: Dict[str, Any] = {}\n    if deletion:\n        constraints[\"_deletion\"] = [\"==\", 1]\n    constraints[\"name\"] = [\"==\", collection_name]\n    entity[\"constraints\"] = constraints\n\n    query: Dict[str, Any] = {}\n    query[querytype] = entity\n    return query\n\n\ndef _str2bytes(in_str: str) -> bytes:\n    return str.encode(in_str)\n\n\ndef _validate_vdms_properties(metadata: Dict[str, Any]) -> Dict:\n    new_metadata: Dict[str, Any] = {}\n    for key, value in metadata.items():\n        if not isinstance(value, list):\n            new_metadata[str(key)] = value\n    return new_metadata\n",
        "patch": "@@ -22,6 +22,7 @@\n )\n \n import numpy as np\n+from langchain_core._api.deprecation import deprecated\n from langchain_core.documents import Document\n from langchain_core.embeddings import Embeddings\n from langchain_core.vectorstores import VectorStore\n@@ -135,6 +136,7 @@ def VDMS_Client(host: str = \"localhost\", port: int = 55555) -> vdms.vdms:\n     return client\n \n \n+@deprecated(since=\"0.3.18\", removal=\"1.0.0\", alternative_import=\"langchain_vdms.VDMS\")\n class VDMS(VectorStore):\n     \"\"\"Intel Lab's VDMS for vector-store workloads.\n "
      }
    ]
  },
  {
    "number": 30022,
    "title": "openai[patch]: add unit test",
    "body": "Test `max_completion_tokens` is propagated to payload for AzureChatOpenAI.",
    "issue_title": "openai[patch]: add unit test",
    "issue_body": "Test `max_completion_tokens` is propagated to payload for AzureChatOpenAI.",
    "files": [
      {
        "filename": "libs/partners/openai/tests/unit_tests/chat_models/test_azure.py",
        "content_before": "\"\"\"Test Azure OpenAI Chat API wrapper.\"\"\"\n\nimport os\nfrom unittest import mock\n\nimport pytest\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import AzureChatOpenAI\n\n\ndef test_initialize_azure_openai() -> None:\n    llm = AzureChatOpenAI(  # type: ignore[call-arg]\n        azure_deployment=\"35-turbo-dev\",\n        openai_api_version=\"2023-05-15\",\n        azure_endpoint=\"my-base-url\",\n    )\n    assert llm.deployment_name == \"35-turbo-dev\"\n    assert llm.openai_api_version == \"2023-05-15\"\n    assert llm.azure_endpoint == \"my-base-url\"\n\n\ndef test_initialize_more() -> None:\n    llm = AzureChatOpenAI(  # type: ignore[call-arg]\n        api_key=\"xyz\",  # type: ignore[arg-type]\n        azure_endpoint=\"my-base-url\",\n        azure_deployment=\"35-turbo-dev\",\n        openai_api_version=\"2023-05-15\",\n        temperature=0,\n        model=\"gpt-35-turbo\",\n        model_version=\"0125\",\n    )\n    assert llm.openai_api_key is not None\n    assert llm.openai_api_key.get_secret_value() == \"xyz\"\n    assert llm.azure_endpoint == \"my-base-url\"\n    assert llm.deployment_name == \"35-turbo-dev\"\n    assert llm.openai_api_version == \"2023-05-15\"\n    assert llm.temperature == 0\n\n    ls_params = llm._get_ls_params()\n    assert ls_params[\"ls_provider\"] == \"azure\"\n    assert ls_params[\"ls_model_name\"] == \"gpt-35-turbo-0125\"\n\n\ndef test_initialize_azure_openai_with_openai_api_base_set() -> None:\n    with mock.patch.dict(os.environ, {\"OPENAI_API_BASE\": \"https://api.openai.com\"}):\n        llm = AzureChatOpenAI(  # type: ignore[call-arg, call-arg]\n            api_key=\"xyz\",  # type: ignore[arg-type]\n            azure_endpoint=\"my-base-url\",\n            azure_deployment=\"35-turbo-dev\",\n            openai_api_version=\"2023-05-15\",\n            temperature=0,\n            openai_api_base=None,\n        )\n        assert llm.openai_api_key is not None\n        assert llm.openai_api_key.get_secret_value() == \"xyz\"\n        assert llm.azure_endpoint == \"my-base-url\"\n        assert llm.deployment_name == \"35-turbo-dev\"\n        assert llm.openai_api_version == \"2023-05-15\"\n        assert llm.temperature == 0\n\n        ls_params = llm._get_ls_params()\n        assert ls_params[\"ls_provider\"] == \"azure\"\n        assert ls_params[\"ls_model_name\"] == \"35-turbo-dev\"\n\n\ndef test_structured_output_old_model() -> None:\n    class Output(TypedDict):\n        \"\"\"output.\"\"\"\n\n        foo: str\n\n    with pytest.warns(match=\"Cannot use method='json_schema'\"):\n        llm = AzureChatOpenAI(  # type: ignore[call-arg]\n            model=\"gpt-35-turbo\",\n            azure_deployment=\"35-turbo-dev\",\n            openai_api_version=\"2023-05-15\",\n            azure_endpoint=\"my-base-url\",\n        ).with_structured_output(Output)\n\n    # assert tool calling was used instead of json_schema\n    assert \"tools\" in llm.steps[0].kwargs  # type: ignore\n    assert \"response_format\" not in llm.steps[0].kwargs  # type: ignore\n",
        "patch": "@@ -4,6 +4,7 @@\n from unittest import mock\n \n import pytest\n+from langchain_core.messages import HumanMessage\n from typing_extensions import TypedDict\n \n from langchain_openai import AzureChatOpenAI\n@@ -81,3 +82,20 @@ class Output(TypedDict):\n     # assert tool calling was used instead of json_schema\n     assert \"tools\" in llm.steps[0].kwargs  # type: ignore\n     assert \"response_format\" not in llm.steps[0].kwargs  # type: ignore\n+\n+\n+def test_max_completion_tokens_in_payload() -> None:\n+    llm = AzureChatOpenAI(\n+        azure_deployment=\"o1-mini\",\n+        api_version=\"2024-12-01-preview\",\n+        azure_endpoint=\"my-base-url\",\n+        model_kwargs={\"max_completion_tokens\": 300},\n+    )\n+    messages = [HumanMessage(\"Hello\")]\n+    payload = llm._get_request_payload(messages)\n+    assert payload == {\n+        \"messages\": [{\"content\": \"Hello\", \"role\": \"user\"}],\n+        \"model\": None,\n+        \"stream\": False,\n+        \"max_completion_tokens\": 300,\n+    }"
      }
    ]
  },
  {
    "number": 30021,
    "title": "community[fix]: Handle None value in raw_content from Tavily API response",
    "body": "## **Description:**\r\n\r\nWhen using the Tavily retriever with include_raw_content=True, the retriever occasionally fails with a Pydantic ValidationError because raw_content can be None.\r\n\r\nThe Document model in langchain_core/documents/base.py requires page_content to be a non-None value, but the Tavily API sometimes returns None for raw_content.\r\n\r\nThis PR fixes the issue by ensuring that even when raw_content is None, an empty string is used instead:\r\n\r\n```python\r\npage_content=result.get(\"content\", \"\")\r\n            if not self.include_raw_content\r\n            else (result.get(\"raw_content\") or \"\"),",
    "issue_title": "community[fix]: Handle None value in raw_content from Tavily API response",
    "issue_body": "## **Description:**\r\n\r\nWhen using the Tavily retriever with include_raw_content=True, the retriever occasionally fails with a Pydantic ValidationError because raw_content can be None.\r\n\r\nThe Document model in langchain_core/documents/base.py requires page_content to be a non-None value, but the Tavily API sometimes returns None for raw_content.\r\n\r\nThis PR fixes the issue by ensuring that even when raw_content is None, an empty string is used instead:\r\n\r\n```python\r\npage_content=result.get(\"content\", \"\")\r\n            if not self.include_raw_content\r\n            else (result.get(\"raw_content\") or \"\"),",
    "files": [
      {
        "filename": "libs/community/langchain_community/retrievers/tavily_search_api.py",
        "content_before": "import os\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\nfrom langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\n\n\nclass SearchDepth(Enum):\n    \"\"\"Search depth as enumerator.\"\"\"\n\n    BASIC = \"basic\"\n    ADVANCED = \"advanced\"\n\n\nclass TavilySearchAPIRetriever(BaseRetriever):\n    \"\"\"Tavily Search API retriever.\n\n    Setup:\n        Install ``langchain-community`` and set environment variable ``TAVILY_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-community\n            export TAVILY_API_KEY=\"your-api-key\"\n\n    Key init args:\n        k: int\n            Number of results to include.\n        include_generated_answer: bool\n            Include a generated answer with results\n        include_raw_content: bool\n            Include raw content with results.\n        include_images: bool\n            Return images in addition to text.\n\n    Instantiate:\n        .. code-block:: python\n\n            from langchain_community.retrievers import TavilySearchAPIRetriever\n\n            retriever = TavilySearchAPIRetriever(k=3)\n\n    Usage:\n        .. code-block:: python\n\n            query = \"what year was breath of the wild released?\"\n\n            retriever.invoke(query)\n\n    Use within a chain:\n        .. code-block:: python\n\n            from langchain_core.output_parsers import StrOutputParser\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.runnables import RunnablePassthrough\n            from langchain_openai import ChatOpenAI\n\n            prompt = ChatPromptTemplate.from_template(\n                \\\"\\\"\\\"Answer the question based only on the context provided.\n\n            Context: {context}\n\n            Question: {question}\\\"\\\"\\\"\n            )\n\n            llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n\n            def format_docs(docs):\n                return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n            chain = (\n                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n                | prompt\n                | llm\n                | StrOutputParser()\n            )\n\n            chain.invoke(\"how many units did bretch of the wild sell in 2020\")\n\n    \"\"\"  # noqa: E501\n\n    k: int = 10\n    include_generated_answer: bool = False\n    include_raw_content: bool = False\n    include_images: bool = False\n    search_depth: SearchDepth = SearchDepth.BASIC\n    include_domains: Optional[List[str]] = None\n    exclude_domains: Optional[List[str]] = None\n    kwargs: Optional[Dict[str, Any]] = {}\n    api_key: Optional[str] = None\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> List[Document]:\n        try:\n            try:\n                from tavily import TavilyClient\n            except ImportError:\n                # Older of tavily used Client\n                from tavily import Client as TavilyClient\n        except ImportError:\n            raise ImportError(\n                \"Tavily python package not found. \"\n                \"Please install it with `pip install tavily-python`.\"\n            )\n\n        tavily = TavilyClient(api_key=self.api_key or os.environ[\"TAVILY_API_KEY\"])\n        max_results = self.k if not self.include_generated_answer else self.k - 1\n        response = tavily.search(\n            query=query,\n            max_results=max_results,\n            search_depth=self.search_depth.value,\n            include_answer=self.include_generated_answer,\n            include_domains=self.include_domains,\n            exclude_domains=self.exclude_domains,\n            include_raw_content=self.include_raw_content,\n            include_images=self.include_images,\n            **self.kwargs,\n        )\n        docs = [\n            Document(\n                page_content=result.get(\"content\", \"\")\n                if not self.include_raw_content\n                else result.get(\"raw_content\", \"\"),\n                metadata={\n                    \"title\": result.get(\"title\", \"\"),\n                    \"source\": result.get(\"url\", \"\"),\n                    **{\n                        k: v\n                        for k, v in result.items()\n                        if k not in (\"content\", \"title\", \"url\", \"raw_content\")\n                    },\n                    \"images\": response.get(\"images\"),\n                },\n            )\n            for result in response.get(\"results\")\n        ]\n        if self.include_generated_answer:\n            docs = [\n                Document(\n                    page_content=response.get(\"answer\", \"\"),\n                    metadata={\n                        \"title\": \"Suggested Answer\",\n                        \"source\": \"https://tavily.com/\",\n                    },\n                ),\n                *docs,\n            ]\n\n        return docs\n",
        "patch": "@@ -123,7 +123,7 @@ def _get_relevant_documents(\n             Document(\n                 page_content=result.get(\"content\", \"\")\n                 if not self.include_raw_content\n-                else result.get(\"raw_content\", \"\"),\n+                else (result.get(\"raw_content\") or \"\"),\n                 metadata={\n                     \"title\": result.get(\"title\", \"\"),\n                     \"source\": result.get(\"url\", \"\"),"
      }
    ]
  },
  {
    "number": 30016,
    "title": "community: Add cost data for aws bedrock anthropic.claude-3-7 model",
    "body": "This pull request includes updates to the `libs/community/langchain_community/callbacks/bedrock_anthropic_callback.py` file to add a new model version to the list of supported models.\r\n\r\nUpdates to supported models:\r\n\r\n* Added support for the `anthropic.claude-3-7-sonnet-20250219-v1:0` model with a rate of `0.003` for 1000 input tokens.\r\n* Added support for the `anthropic.claude-3-7-sonnet-20250219-v1:0` model with a rate of `0.015` for 1000 output tokens.\r\n\r\nAWS Bedrock pricing reference : https://aws.amazon.com/bedrock/pricing",
    "issue_title": "community: Add cost data for aws bedrock anthropic.claude-3-7 model",
    "issue_body": "This pull request includes updates to the `libs/community/langchain_community/callbacks/bedrock_anthropic_callback.py` file to add a new model version to the list of supported models.\r\n\r\nUpdates to supported models:\r\n\r\n* Added support for the `anthropic.claude-3-7-sonnet-20250219-v1:0` model with a rate of `0.003` for 1000 input tokens.\r\n* Added support for the `anthropic.claude-3-7-sonnet-20250219-v1:0` model with a rate of `0.015` for 1000 output tokens.\r\n\r\nAWS Bedrock pricing reference : https://aws.amazon.com/bedrock/pricing",
    "files": [
      {
        "filename": "libs/community/langchain_community/callbacks/bedrock_anthropic_callback.py",
        "content_before": "import threading\nfrom typing import Any, Dict, List, Union\n\nfrom langchain_core.callbacks import BaseCallbackHandler\nfrom langchain_core.outputs import LLMResult\n\nMODEL_COST_PER_1K_INPUT_TOKENS = {\n    \"anthropic.claude-instant-v1\": 0.0008,\n    \"anthropic.claude-v2\": 0.008,\n    \"anthropic.claude-v2:1\": 0.008,\n    \"anthropic.claude-3-sonnet-20240229-v1:0\": 0.003,\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\": 0.003,\n    \"anthropic.claude-3-5-sonnet-20241022-v2:0\": 0.003,\n    \"anthropic.claude-3-haiku-20240307-v1:0\": 0.00025,\n    \"anthropic.claude-3-opus-20240229-v1:0\": 0.015,\n    \"anthropic.claude-3-5-haiku-20241022-v1:0\": 0.0008,\n}\n\nMODEL_COST_PER_1K_OUTPUT_TOKENS = {\n    \"anthropic.claude-instant-v1\": 0.0024,\n    \"anthropic.claude-v2\": 0.024,\n    \"anthropic.claude-v2:1\": 0.024,\n    \"anthropic.claude-3-sonnet-20240229-v1:0\": 0.015,\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\": 0.015,\n    \"anthropic.claude-3-5-sonnet-20241022-v2:0\": 0.015,\n    \"anthropic.claude-3-haiku-20240307-v1:0\": 0.00125,\n    \"anthropic.claude-3-opus-20240229-v1:0\": 0.075,\n    \"anthropic.claude-3-5-haiku-20241022-v1:0\": 0.004,\n}\n\n\ndef _get_anthropic_claude_token_cost(\n    prompt_tokens: int, completion_tokens: int, model_id: Union[str, None]\n) -> float:\n    if model_id:\n        # The model ID can be a cross-region (system-defined) inference profile ID,\n        # which has a prefix indicating the region (e.g., 'us', 'eu') but\n        # shares the same token costs as the \"base model\".\n        # By extracting the \"base model ID\", by taking the last two segments\n        # of the model ID, we can map cross-region inference profile IDs to\n        # their corresponding cost entries.\n        base_model_id = model_id.split(\".\")[-2] + \".\" + model_id.split(\".\")[-1]\n    else:\n        base_model_id = None\n    \"\"\"Get the cost of tokens for the Claude model.\"\"\"\n    if base_model_id not in MODEL_COST_PER_1K_INPUT_TOKENS:\n        raise ValueError(\n            f\"Unknown model: {model_id}. Please provide a valid Anthropic model name.\"\n            \"Known models are: \" + \", \".join(MODEL_COST_PER_1K_INPUT_TOKENS.keys())\n        )\n    return (prompt_tokens / 1000) * MODEL_COST_PER_1K_INPUT_TOKENS[base_model_id] + (\n        completion_tokens / 1000\n    ) * MODEL_COST_PER_1K_OUTPUT_TOKENS[base_model_id]\n\n\nclass BedrockAnthropicTokenUsageCallbackHandler(BaseCallbackHandler):\n    \"\"\"Callback Handler that tracks bedrock anthropic info.\"\"\"\n\n    total_tokens: int = 0\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n    successful_requests: int = 0\n    total_cost: float = 0.0\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._lock = threading.Lock()\n\n    def __repr__(self) -> str:\n        return (\n            f\"Tokens Used: {self.total_tokens}\\n\"\n            f\"\\tPrompt Tokens: {self.prompt_tokens}\\n\"\n            f\"\\tCompletion Tokens: {self.completion_tokens}\\n\"\n            f\"Successful Requests: {self.successful_requests}\\n\"\n            f\"Total Cost (USD): ${self.total_cost}\"\n        )\n\n    @property\n    def always_verbose(self) -> bool:\n        \"\"\"Whether to call verbose callbacks even if verbose is False.\"\"\"\n        return True\n\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out the prompts.\"\"\"\n        pass\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n        \"\"\"Print out the token.\"\"\"\n        pass\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        \"\"\"Collect token usage.\"\"\"\n        if response.llm_output is None:\n            return None\n\n        if \"usage\" not in response.llm_output:\n            with self._lock:\n                self.successful_requests += 1\n            return None\n\n        # compute tokens and cost for this request\n        token_usage = response.llm_output[\"usage\"]\n        completion_tokens = token_usage.get(\"completion_tokens\", 0)\n        prompt_tokens = token_usage.get(\"prompt_tokens\", 0)\n        total_tokens = token_usage.get(\"total_tokens\", 0)\n        model_id = response.llm_output.get(\"model_id\", None)\n        total_cost = _get_anthropic_claude_token_cost(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            model_id=model_id,\n        )\n\n        # update shared state behind lock\n        with self._lock:\n            self.total_cost += total_cost\n            self.total_tokens += total_tokens\n            self.prompt_tokens += prompt_tokens\n            self.completion_tokens += completion_tokens\n            self.successful_requests += 1\n\n    def __copy__(self) -> \"BedrockAnthropicTokenUsageCallbackHandler\":\n        \"\"\"Return a copy of the callback handler.\"\"\"\n        return self\n\n    def __deepcopy__(self, memo: Any) -> \"BedrockAnthropicTokenUsageCallbackHandler\":\n        \"\"\"Return a deep copy of the callback handler.\"\"\"\n        return self\n",
        "patch": "@@ -11,6 +11,7 @@\n     \"anthropic.claude-3-sonnet-20240229-v1:0\": 0.003,\n     \"anthropic.claude-3-5-sonnet-20240620-v1:0\": 0.003,\n     \"anthropic.claude-3-5-sonnet-20241022-v2:0\": 0.003,\n+    \"anthropic.claude-3-7-sonnet-20250219-v1:0\": 0.003,\n     \"anthropic.claude-3-haiku-20240307-v1:0\": 0.00025,\n     \"anthropic.claude-3-opus-20240229-v1:0\": 0.015,\n     \"anthropic.claude-3-5-haiku-20241022-v1:0\": 0.0008,\n@@ -23,6 +24,7 @@\n     \"anthropic.claude-3-sonnet-20240229-v1:0\": 0.015,\n     \"anthropic.claude-3-5-sonnet-20240620-v1:0\": 0.015,\n     \"anthropic.claude-3-5-sonnet-20241022-v2:0\": 0.015,\n+    \"anthropic.claude-3-7-sonnet-20250219-v1:0\": 0.015,\n     \"anthropic.claude-3-haiku-20240307-v1:0\": 0.00125,\n     \"anthropic.claude-3-opus-20240229-v1:0\": 0.075,\n     \"anthropic.claude-3-5-haiku-20241022-v1:0\": 0.004,"
      }
    ]
  },
  {
    "number": 29268,
    "title": "core: Add ruff rules TC",
    "body": "See https://docs.astral.sh/ruff/rules/#flake8-type-checking-tc\r\nSome fixes done for TC001,TC002 and TC003 but these rules are excluded since they don't play well with Pydantic.",
    "issue_title": "core: Add ruff rules TC",
    "issue_body": "See https://docs.astral.sh/ruff/rules/#flake8-type-checking-tc\r\nSome fixes done for TC001,TC002 and TC003 but these rules are excluded since they don't play well with Pydantic.",
    "files": [
      {
        "filename": "libs/core/langchain_core/callbacks/base.py",
        "content_before": "\"\"\"Base callback handler for LangChain.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\nfrom uuid import UUID\n\nfrom tenacity import RetryCallState\n\nif TYPE_CHECKING:\n    from langchain_core.agents import AgentAction, AgentFinish\n    from langchain_core.documents import Document\n    from langchain_core.messages import BaseMessage\n    from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass RetrieverManagerMixin:\n    \"\"\"Mixin for Retriever callbacks.\"\"\"\n\n    def on_retriever_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when Retriever errors.\n\n        Args:\n            error (BaseException): The error that occurred.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_retriever_end(\n        self,\n        documents: Sequence[Document],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when Retriever ends running.\n\n        Args:\n            documents (Sequence[Document]): The documents retrieved.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n\nclass LLMManagerMixin:\n    \"\"\"Mixin for LLM callbacks.\"\"\"\n\n    def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\n\n        Args:\n            token (str): The new token.\n            chunk (GenerationChunk | ChatGenerationChunk): The new generated chunk,\n              containing content and other information.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_llm_end(\n        self,\n        response: LLMResult,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when LLM ends running.\n\n        Args:\n            response (LLMResult): The response which was generated.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_llm_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when LLM errors.\n\n        Args:\n            error (BaseException): The error that occurred.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n\nclass ChainManagerMixin:\n    \"\"\"Mixin for chain callbacks.\"\"\"\n\n    def on_chain_end(\n        self,\n        outputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when chain ends running.\n\n        Args:\n            outputs (Dict[str, Any]): The outputs of the chain.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_chain_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when chain errors.\n\n        Args:\n            error (BaseException): The error that occurred.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_agent_action(\n        self,\n        action: AgentAction,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run on agent action.\n\n        Args:\n            action (AgentAction): The agent action.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_agent_finish(\n        self,\n        finish: AgentFinish,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run on the agent end.\n\n        Args:\n            finish (AgentFinish): The agent finish.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n\nclass ToolManagerMixin:\n    \"\"\"Mixin for tool callbacks.\"\"\"\n\n    def on_tool_end(\n        self,\n        output: Any,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when the tool ends running.\n\n        Args:\n            output (Any): The output of the tool.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_tool_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when tool errors.\n\n        Args:\n            error (BaseException): The error that occurred.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n\nclass CallbackManagerMixin:\n    \"\"\"Mixin for callback manager.\"\"\"\n\n    def on_llm_start(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when LLM starts running.\n\n        **ATTENTION**: This method is called for non-chat models (regular LLMs). If\n            you're implementing a handler for a chat model,\n            you should use on_chat_model_start instead.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized LLM.\n            prompts (List[str]): The prompts.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when a chat model starts running.\n\n        **ATTENTION**: This method is called for chat models. If you're implementing\n            a handler for a non-chat model, you should use on_llm_start instead.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized chat model.\n            messages (List[List[BaseMessage]]): The messages.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        # NotImplementedError is thrown intentionally\n        # Callback handler will fall back to on_llm_start if this is exception is thrown\n        msg = f\"{self.__class__.__name__} does not implement `on_chat_model_start`\"\n        raise NotImplementedError(msg)\n\n    def on_retriever_start(\n        self,\n        serialized: dict[str, Any],\n        query: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when the Retriever starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized Retriever.\n            query (str): The query.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_chain_start(\n        self,\n        serialized: dict[str, Any],\n        inputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when a chain starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized chain.\n            inputs (Dict[str, Any]): The inputs.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_tool_start(\n        self,\n        serialized: dict[str, Any],\n        input_str: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when the tool starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized tool.\n            input_str (str): The input string.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            inputs (Optional[Dict[str, Any]]): The inputs.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n\nclass RunManagerMixin:\n    \"\"\"Mixin for run manager.\"\"\"\n\n    def on_text(\n        self,\n        text: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run on an arbitrary text.\n\n        Args:\n            text (str): The text.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_retry(\n        self,\n        retry_state: RetryCallState,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run on a retry event.\n\n        Args:\n            retry_state (RetryCallState): The retry state.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    def on_custom_event(\n        self,\n        name: str,\n        data: Any,\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Override to define a handler for a custom event.\n\n        Args:\n            name: The name of the custom event.\n            data: The data for the custom event. Format will match\n                  the format specified by the user.\n            run_id: The ID of the run.\n            tags: The tags associated with the custom event\n                (includes inherited tags).\n            metadata: The metadata associated with the custom event\n                (includes inherited metadata).\n\n        .. versionadded:: 0.2.15\n        \"\"\"\n\n\nclass BaseCallbackHandler(\n    LLMManagerMixin,\n    ChainManagerMixin,\n    ToolManagerMixin,\n    RetrieverManagerMixin,\n    CallbackManagerMixin,\n    RunManagerMixin,\n):\n    \"\"\"Base callback handler for LangChain.\"\"\"\n\n    raise_error: bool = False\n    \"\"\"Whether to raise an error if an exception occurs.\"\"\"\n\n    run_inline: bool = False\n    \"\"\"Whether to run the callback inline.\"\"\"\n\n    @property\n    def ignore_llm(self) -> bool:\n        \"\"\"Whether to ignore LLM callbacks.\"\"\"\n        return False\n\n    @property\n    def ignore_retry(self) -> bool:\n        \"\"\"Whether to ignore retry callbacks.\"\"\"\n        return False\n\n    @property\n    def ignore_chain(self) -> bool:\n        \"\"\"Whether to ignore chain callbacks.\"\"\"\n        return False\n\n    @property\n    def ignore_agent(self) -> bool:\n        \"\"\"Whether to ignore agent callbacks.\"\"\"\n        return False\n\n    @property\n    def ignore_retriever(self) -> bool:\n        \"\"\"Whether to ignore retriever callbacks.\"\"\"\n        return False\n\n    @property\n    def ignore_chat_model(self) -> bool:\n        \"\"\"Whether to ignore chat model callbacks.\"\"\"\n        return False\n\n    @property\n    def ignore_custom_event(self) -> bool:\n        \"\"\"Ignore custom event.\"\"\"\n        return False\n\n\nclass AsyncCallbackHandler(BaseCallbackHandler):\n    \"\"\"Async callback handler for LangChain.\"\"\"\n\n    async def on_llm_start(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when LLM starts running.\n\n        **ATTENTION**: This method is called for non-chat models (regular LLMs). If\n            you're implementing a handler for a chat model,\n            you should use on_chat_model_start instead.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized LLM.\n            prompts (List[str]): The prompts.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when a chat model starts running.\n\n        **ATTENTION**: This method is called for chat models. If you're implementing\n            a handler for a non-chat model, you should use on_llm_start instead.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized chat model.\n            messages (List[List[BaseMessage]]): The messages.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        # NotImplementedError is thrown intentionally\n        # Callback handler will fall back to on_llm_start if this is exception is thrown\n        msg = f\"{self.__class__.__name__} does not implement `on_chat_model_start`\"\n        raise NotImplementedError(msg)\n\n    async def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\n\n        Args:\n            token (str): The new token.\n            chunk (GenerationChunk | ChatGenerationChunk): The new generated chunk,\n              containing content and other information.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_llm_end(\n        self,\n        response: LLMResult,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when LLM ends running.\n\n        Args:\n            response (LLMResult): The response which was generated.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_llm_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when LLM errors.\n\n        Args:\n            error: The error that occurred.\n            run_id: The run ID. This is the ID of the current run.\n            parent_run_id: The parent run ID. This is the ID of the parent run.\n            tags: The tags.\n            kwargs (Any): Additional keyword arguments.\n                - response (LLMResult): The response which was generated before\n                    the error occurred.\n        \"\"\"\n\n    async def on_chain_start(\n        self,\n        serialized: dict[str, Any],\n        inputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when a chain starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized chain.\n            inputs (Dict[str, Any]): The inputs.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_chain_end(\n        self,\n        outputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when a chain ends running.\n\n        Args:\n            outputs (Dict[str, Any]): The outputs of the chain.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_chain_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when chain errors.\n\n        Args:\n            error (BaseException): The error that occurred.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_tool_start(\n        self,\n        serialized: dict[str, Any],\n        input_str: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when the tool starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized tool.\n            input_str (str): The input string.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            inputs (Optional[Dict[str, Any]]): The inputs.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_tool_end(\n        self,\n        output: Any,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when the tool ends running.\n\n        Args:\n            output (Any): The output of the tool.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_tool_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when tool errors.\n\n        Args:\n            error (BaseException): The error that occurred.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_text(\n        self,\n        text: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on an arbitrary text.\n\n        Args:\n            text (str): The text.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_retry(\n        self,\n        retry_state: RetryCallState,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run on a retry event.\n\n        Args:\n            retry_state (RetryCallState): The retry state.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_agent_action(\n        self,\n        action: AgentAction,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on agent action.\n\n        Args:\n            action (AgentAction): The agent action.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_agent_finish(\n        self,\n        finish: AgentFinish,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on the agent end.\n\n        Args:\n            finish (AgentFinish): The agent finish.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_retriever_start(\n        self,\n        serialized: dict[str, Any],\n        query: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on the retriever start.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized retriever.\n            query (str): The query.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_retriever_end(\n        self,\n        documents: Sequence[Document],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on the retriever end.\n\n        Args:\n            documents (Sequence[Document]): The documents retrieved.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_retriever_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on retriever error.\n\n        Args:\n            error (BaseException): The error that occurred.\n            run_id (UUID): The run ID. This is the ID of the current run.\n            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.\n            tags (Optional[List[str]]): The tags.\n            kwargs (Any): Additional keyword arguments.\n        \"\"\"\n\n    async def on_custom_event(\n        self,\n        name: str,\n        data: Any,\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Override to define a handler for a custom event.\n\n        Args:\n            name: The name of the custom event.\n            data: The data for the custom event. Format will match\n                  the format specified by the user.\n            run_id: The ID of the run.\n            tags: The tags associated with the custom event\n                (includes inherited tags).\n            metadata: The metadata associated with the custom event\n                (includes inherited metadata).\n\n        .. versionadded:: 0.2.15\n        \"\"\"\n\n\nT = TypeVar(\"T\", bound=\"BaseCallbackManager\")\n\n\nclass BaseCallbackManager(CallbackManagerMixin):\n    \"\"\"Base callback manager for LangChain.\"\"\"\n\n    def __init__(\n        self,\n        handlers: list[BaseCallbackHandler],\n        inheritable_handlers: Optional[list[BaseCallbackHandler]] = None,\n        parent_run_id: Optional[UUID] = None,\n        *,\n        tags: Optional[list[str]] = None,\n        inheritable_tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        inheritable_metadata: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Initialize callback manager.\n\n        Args:\n            handlers (List[BaseCallbackHandler]): The handlers.\n            inheritable_handlers (Optional[List[BaseCallbackHandler]]):\n              The inheritable handlers. Default is None.\n            parent_run_id (Optional[UUID]): The parent run ID. Default is None.\n            tags (Optional[List[str]]): The tags. Default is None.\n            inheritable_tags (Optional[List[str]]): The inheritable tags.\n                Default is None.\n            metadata (Optional[Dict[str, Any]]): The metadata. Default is None.\n        \"\"\"\n        self.handlers: list[BaseCallbackHandler] = handlers\n        self.inheritable_handlers: list[BaseCallbackHandler] = (\n            inheritable_handlers or []\n        )\n        self.parent_run_id: Optional[UUID] = parent_run_id\n        self.tags = tags or []\n        self.inheritable_tags = inheritable_tags or []\n        self.metadata = metadata or {}\n        self.inheritable_metadata = inheritable_metadata or {}\n\n    def copy(self: T) -> T:\n        \"\"\"Copy the callback manager.\"\"\"\n        return self.__class__(\n            handlers=self.handlers.copy(),\n            inheritable_handlers=self.inheritable_handlers.copy(),\n            parent_run_id=self.parent_run_id,\n            tags=self.tags.copy(),\n            inheritable_tags=self.inheritable_tags.copy(),\n            metadata=self.metadata.copy(),\n            inheritable_metadata=self.inheritable_metadata.copy(),\n        )\n\n    def merge(self: T, other: BaseCallbackManager) -> T:\n        \"\"\"Merge the callback manager with another callback manager.\n\n        May be overwritten in subclasses. Primarily used internally\n        within merge_configs.\n\n        Returns:\n            BaseCallbackManager: The merged callback manager of the same type\n                as the current object.\n\n        Example: Merging two callback managers.\n\n            .. code-block:: python\n\n                from langchain_core.callbacks.manager import CallbackManager, trace_as_chain_group\n                from langchain_core.callbacks.stdout import StdOutCallbackHandler\n\n                manager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=[\"tag2\"])\n                with trace_as_chain_group(\"My Group Name\", tags=[\"tag1\"]) as group_manager:\n                    merged_manager = group_manager.merge(manager)\n                    print(merged_manager.handlers)\n                    # [\n                    #    <langchain_core.callbacks.stdout.StdOutCallbackHandler object at ...>,\n                    #    <langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler object at ...>,\n                    # ]\n\n                    print(merged_manager.tags)\n                    #    ['tag2', 'tag1']\n\n        \"\"\"  # noqa: E501\n        manager = self.__class__(\n            parent_run_id=self.parent_run_id or other.parent_run_id,\n            handlers=[],\n            inheritable_handlers=[],\n            tags=list(set(self.tags + other.tags)),\n            inheritable_tags=list(set(self.inheritable_tags + other.inheritable_tags)),\n            metadata={\n                **self.metadata,\n                **other.metadata,\n            },\n        )\n\n        handlers = self.handlers + other.handlers\n        inheritable_handlers = self.inheritable_handlers + other.inheritable_handlers\n\n        for handler in handlers:\n            manager.add_handler(handler)\n\n        for handler in inheritable_handlers:\n            manager.add_handler(handler, inherit=True)\n        return manager\n\n    @property\n    def is_async(self) -> bool:\n        \"\"\"Whether the callback manager is async.\"\"\"\n        return False\n\n    def add_handler(self, handler: BaseCallbackHandler, inherit: bool = True) -> None:\n        \"\"\"Add a handler to the callback manager.\n\n        Args:\n            handler (BaseCallbackHandler): The handler to add.\n            inherit (bool): Whether to inherit the handler. Default is True.\n        \"\"\"\n        if handler not in self.handlers:\n            self.handlers.append(handler)\n        if inherit and handler not in self.inheritable_handlers:\n            self.inheritable_handlers.append(handler)\n\n    def remove_handler(self, handler: BaseCallbackHandler) -> None:\n        \"\"\"Remove a handler from the callback manager.\n\n        Args:\n            handler (BaseCallbackHandler): The handler to remove.\n        \"\"\"\n        self.handlers.remove(handler)\n        self.inheritable_handlers.remove(handler)\n\n    def set_handlers(\n        self, handlers: list[BaseCallbackHandler], inherit: bool = True\n    ) -> None:\n        \"\"\"Set handlers as the only handlers on the callback manager.\n\n        Args:\n            handlers (List[BaseCallbackHandler]): The handlers to set.\n            inherit (bool): Whether to inherit the handlers. Default is True.\n        \"\"\"\n        self.handlers = []\n        self.inheritable_handlers = []\n        for handler in handlers:\n            self.add_handler(handler, inherit=inherit)\n\n    def set_handler(self, handler: BaseCallbackHandler, inherit: bool = True) -> None:\n        \"\"\"Set handler as the only handler on the callback manager.\n\n        Args:\n            handler (BaseCallbackHandler): The handler to set.\n            inherit (bool): Whether to inherit the handler. Default is True.\n        \"\"\"\n        self.set_handlers([handler], inherit=inherit)\n\n    def add_tags(self, tags: list[str], inherit: bool = True) -> None:\n        \"\"\"Add tags to the callback manager.\n\n        Args:\n            tags (List[str]): The tags to add.\n            inherit (bool): Whether to inherit the tags. Default is True.\n        \"\"\"\n        for tag in tags:\n            if tag in self.tags:\n                self.remove_tags([tag])\n        self.tags.extend(tags)\n        if inherit:\n            self.inheritable_tags.extend(tags)\n\n    def remove_tags(self, tags: list[str]) -> None:\n        \"\"\"Remove tags from the callback manager.\n\n        Args:\n            tags (List[str]): The tags to remove.\n        \"\"\"\n        for tag in tags:\n            self.tags.remove(tag)\n            self.inheritable_tags.remove(tag)\n\n    def add_metadata(self, metadata: dict[str, Any], inherit: bool = True) -> None:\n        \"\"\"Add metadata to the callback manager.\n\n        Args:\n            metadata (Dict[str, Any]): The metadata to add.\n            inherit (bool): Whether to inherit the metadata. Default is True.\n        \"\"\"\n        self.metadata.update(metadata)\n        if inherit:\n            self.inheritable_metadata.update(metadata)\n\n    def remove_metadata(self, keys: list[str]) -> None:\n        \"\"\"Remove metadata from the callback manager.\n\n        Args:\n            keys (List[str]): The keys to remove.\n        \"\"\"\n        for key in keys:\n            self.metadata.pop(key)\n            self.inheritable_metadata.pop(key)\n\n\nCallbacks = Optional[Union[list[BaseCallbackHandler], BaseCallbackManager]]\n",
        "patch": "@@ -3,13 +3,14 @@\n from __future__ import annotations\n \n import logging\n-from collections.abc import Sequence\n from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n-from uuid import UUID\n-\n-from tenacity import RetryCallState\n \n if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+    from uuid import UUID\n+\n+    from tenacity import RetryCallState\n+\n     from langchain_core.agents import AgentAction, AgentFinish\n     from langchain_core.documents import Document\n     from langchain_core.messages import BaseMessage"
      },
      {
        "filename": "libs/core/langchain_core/callbacks/file.py",
        "content_before": "\"\"\"Callback Handler that writes to a file.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Optional, TextIO, cast\n\nfrom langchain_core.agents import AgentAction, AgentFinish\nfrom langchain_core.callbacks import BaseCallbackHandler\nfrom langchain_core.utils.input import print_text\n\n\nclass FileCallbackHandler(BaseCallbackHandler):\n    \"\"\"Callback Handler that writes to a file.\n\n    Parameters:\n        filename: The file to write to.\n        mode: The mode to open the file in. Defaults to \"a\".\n        color: The color to use for the text.\n    \"\"\"\n\n    def __init__(\n        self, filename: str, mode: str = \"a\", color: Optional[str] = None\n    ) -> None:\n        \"\"\"Initialize callback handler.\n\n        Args:\n            filename: The filename to write to.\n            mode: The mode to open the file in. Defaults to \"a\".\n            color: The color to use for the text. Defaults to None.\n        \"\"\"\n        self.file = cast(TextIO, open(filename, mode, encoding=\"utf-8\"))  # noqa: SIM115\n        self.color = color\n\n    def __del__(self) -> None:\n        \"\"\"Destructor to cleanup when done.\"\"\"\n        self.file.close()\n\n    def on_chain_start(\n        self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out that we are entering a chain.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized chain.\n            inputs (Dict[str, Any]): The inputs to the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n        print_text(\n            f\"\\n\\n\\033[1m> Entering new {class_name} chain...\\033[0m\",\n            end=\"\\n\",\n            file=self.file,\n        )\n\n    def on_chain_end(self, outputs: dict[str, Any], **kwargs: Any) -> None:\n        \"\"\"Print out that we finished a chain.\n\n        Args:\n            outputs (Dict[str, Any]): The outputs of the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(\"\\n\\033[1m> Finished chain.\\033[0m\", end=\"\\n\", file=self.file)\n\n    def on_agent_action(\n        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Run on agent action.\n\n        Args:\n            action (AgentAction): The agent action.\n            color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(action.log, color=color or self.color, file=self.file)\n\n    def on_tool_end(\n        self,\n        output: str,\n        color: Optional[str] = None,\n        observation_prefix: Optional[str] = None,\n        llm_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"If not the final action, print out observation.\n\n        Args:\n           output (str): The output to print.\n           color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n           observation_prefix (Optional[str], optional): The observation prefix.\n            Defaults to None.\n           llm_prefix (Optional[str], optional): The LLM prefix.\n                Defaults to None.\n           **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if observation_prefix is not None:\n            print_text(f\"\\n{observation_prefix}\", file=self.file)\n        print_text(output, color=color or self.color, file=self.file)\n        if llm_prefix is not None:\n            print_text(f\"\\n{llm_prefix}\", file=self.file)\n\n    def on_text(\n        self, text: str, color: Optional[str] = None, end: str = \"\", **kwargs: Any\n    ) -> None:\n        \"\"\"Run when the agent ends.\n\n        Args:\n           text (str): The text to print.\n           color (Optional[str], optional): The color to use for the text.\n            Defaults to None.\n           end (str, optional): The end character. Defaults to \"\".\n           **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(text, color=color or self.color, end=end, file=self.file)\n\n    def on_agent_finish(\n        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n    ) -> None:\n        \"\"\"Run on the agent end.\n\n        Args:\n            finish (AgentFinish): The agent finish.\n            color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(finish.log, color=color or self.color, end=\"\\n\", file=self.file)\n",
        "patch": "@@ -2,12 +2,14 @@\n \n from __future__ import annotations\n \n-from typing import Any, Optional, TextIO, cast\n+from typing import TYPE_CHECKING, Any, Optional, TextIO, cast\n \n-from langchain_core.agents import AgentAction, AgentFinish\n from langchain_core.callbacks import BaseCallbackHandler\n from langchain_core.utils.input import print_text\n \n+if TYPE_CHECKING:\n+    from langchain_core.agents import AgentAction, AgentFinish\n+\n \n class FileCallbackHandler(BaseCallbackHandler):\n     \"\"\"Callback Handler that writes to a file."
      },
      {
        "filename": "libs/core/langchain_core/callbacks/manager.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport functools\nimport logging\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom collections.abc import AsyncGenerator, Coroutine, Generator, Sequence\nfrom concurrent.futures import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager, contextmanager\nfrom contextvars import copy_context\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom uuid import UUID\n\nfrom langsmith.run_helpers import get_tracing_context\nfrom tenacity import RetryCallState\n\nfrom langchain_core.callbacks.base import (\n    BaseCallbackHandler,\n    BaseCallbackManager,\n    Callbacks,\n    ChainManagerMixin,\n    LLMManagerMixin,\n    RetrieverManagerMixin,\n    RunManagerMixin,\n    ToolManagerMixin,\n)\nfrom langchain_core.callbacks.stdout import StdOutCallbackHandler\nfrom langchain_core.messages import BaseMessage, get_buffer_string\nfrom langchain_core.tracers.schemas import Run\nfrom langchain_core.utils.env import env_var_is_set\n\nif TYPE_CHECKING:\n    from langchain_core.agents import AgentAction, AgentFinish\n    from langchain_core.documents import Document\n    from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n    from langchain_core.runnables.config import RunnableConfig\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_debug() -> bool:\n    from langchain_core.globals import get_debug\n\n    return get_debug()\n\n\n@contextmanager\ndef trace_as_chain_group(\n    group_name: str,\n    callback_manager: Optional[CallbackManager] = None,\n    *,\n    inputs: Optional[dict[str, Any]] = None,\n    project_name: Optional[str] = None,\n    example_id: Optional[Union[str, UUID]] = None,\n    run_id: Optional[UUID] = None,\n    tags: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n) -> Generator[CallbackManagerForChainGroup, None, None]:\n    \"\"\"Get a callback manager for a chain group in a context manager.\n    Useful for grouping different calls together as a single run even if\n    they aren't composed in a single chain.\n\n    Args:\n        group_name (str): The name of the chain group.\n        callback_manager (CallbackManager, optional): The callback manager to use.\n            Defaults to None.\n        inputs (Dict[str, Any], optional): The inputs to the chain group.\n            Defaults to None.\n        project_name (str, optional): The name of the project.\n            Defaults to None.\n        example_id (str or UUID, optional): The ID of the example.\n            Defaults to None.\n        run_id (UUID, optional): The ID of the run.\n        tags (List[str], optional): The inheritable tags to apply to all runs.\n            Defaults to None.\n        metadata (Dict[str, Any], optional): The metadata to apply to all runs.\n            Defaults to None.\n\n    Note: must have LANGCHAIN_TRACING_V2 env var set to true to see the trace in LangSmith.\n\n    Returns:\n        CallbackManagerForChainGroup: The callback manager for the chain group.\n\n    Example:\n        .. code-block:: python\n\n            llm_input = \"Foo\"\n            with trace_as_chain_group(\"group_name\", inputs={\"input\": llm_input}) as manager:\n                # Use the callback manager for the chain group\n                res = llm.invoke(llm_input, {\"callbacks\": manager})\n                manager.on_chain_end({\"output\": res})\n    \"\"\"  # noqa: E501\n    from langchain_core.tracers.context import _get_trace_callbacks\n\n    cb = _get_trace_callbacks(\n        project_name, example_id, callback_manager=callback_manager\n    )\n    cm = CallbackManager.configure(\n        inheritable_callbacks=cb,\n        inheritable_tags=tags,\n        inheritable_metadata=metadata,\n    )\n\n    run_manager = cm.on_chain_start({\"name\": group_name}, inputs or {}, run_id=run_id)\n    child_cm = run_manager.get_child()\n    group_cm = CallbackManagerForChainGroup(\n        child_cm.handlers,\n        child_cm.inheritable_handlers,\n        child_cm.parent_run_id,\n        parent_run_manager=run_manager,\n        tags=child_cm.tags,\n        inheritable_tags=child_cm.inheritable_tags,\n        metadata=child_cm.metadata,\n        inheritable_metadata=child_cm.inheritable_metadata,\n    )\n    try:\n        yield group_cm\n    except Exception as e:\n        if not group_cm.ended:\n            run_manager.on_chain_error(e)\n        raise\n    else:\n        if not group_cm.ended:\n            run_manager.on_chain_end({})\n\n\n@asynccontextmanager\nasync def atrace_as_chain_group(\n    group_name: str,\n    callback_manager: Optional[AsyncCallbackManager] = None,\n    *,\n    inputs: Optional[dict[str, Any]] = None,\n    project_name: Optional[str] = None,\n    example_id: Optional[Union[str, UUID]] = None,\n    run_id: Optional[UUID] = None,\n    tags: Optional[list[str]] = None,\n    metadata: Optional[dict[str, Any]] = None,\n) -> AsyncGenerator[AsyncCallbackManagerForChainGroup, None]:\n    \"\"\"Get an async callback manager for a chain group in a context manager.\n    Useful for grouping different async calls together as a single run even if\n    they aren't composed in a single chain.\n\n    Args:\n        group_name (str): The name of the chain group.\n        callback_manager (AsyncCallbackManager, optional): The async callback manager to use,\n            which manages tracing and other callback behavior. Defaults to None.\n        inputs (Dict[str, Any], optional): The inputs to the chain group.\n            Defaults to None.\n        project_name (str, optional): The name of the project.\n            Defaults to None.\n        example_id (str or UUID, optional): The ID of the example.\n            Defaults to None.\n        run_id (UUID, optional): The ID of the run.\n        tags (List[str], optional): The inheritable tags to apply to all runs.\n            Defaults to None.\n        metadata (Dict[str, Any], optional): The metadata to apply to all runs.\n            Defaults to None.\n\n    Returns:\n        AsyncCallbackManager: The async callback manager for the chain group.\n\n    Note: must have LANGCHAIN_TRACING_V2 env var set to true to see the trace in LangSmith.\n\n    Example:\n        .. code-block:: python\n\n            llm_input = \"Foo\"\n            async with atrace_as_chain_group(\"group_name\", inputs={\"input\": llm_input}) as manager:\n                # Use the async callback manager for the chain group\n                res = await llm.ainvoke(llm_input, {\"callbacks\": manager})\n                await manager.on_chain_end({\"output\": res})\n    \"\"\"  # noqa: E501\n    from langchain_core.tracers.context import _get_trace_callbacks\n\n    cb = _get_trace_callbacks(\n        project_name, example_id, callback_manager=callback_manager\n    )\n    cm = AsyncCallbackManager.configure(\n        inheritable_callbacks=cb, inheritable_tags=tags, inheritable_metadata=metadata\n    )\n\n    run_manager = await cm.on_chain_start(\n        {\"name\": group_name}, inputs or {}, run_id=run_id\n    )\n    child_cm = run_manager.get_child()\n    group_cm = AsyncCallbackManagerForChainGroup(\n        child_cm.handlers,\n        child_cm.inheritable_handlers,\n        child_cm.parent_run_id,\n        parent_run_manager=run_manager,\n        tags=child_cm.tags,\n        inheritable_tags=child_cm.inheritable_tags,\n        metadata=child_cm.metadata,\n        inheritable_metadata=child_cm.inheritable_metadata,\n    )\n    try:\n        yield group_cm\n    except Exception as e:\n        if not group_cm.ended:\n            await run_manager.on_chain_error(e)\n        raise\n    else:\n        if not group_cm.ended:\n            await run_manager.on_chain_end({})\n\n\nFunc = TypeVar(\"Func\", bound=Callable)\n\n\ndef shielded(func: Func) -> Func:\n    \"\"\"Makes so an awaitable method is always shielded from cancellation.\n\n    Args:\n        func (Callable): The function to shield.\n\n    Returns:\n        Callable: The shielded function\n    \"\"\"\n\n    @functools.wraps(func)\n    async def wrapped(*args: Any, **kwargs: Any) -> Any:\n        return await asyncio.shield(func(*args, **kwargs))\n\n    return cast(Func, wrapped)\n\n\ndef handle_event(\n    handlers: list[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    *args: Any,\n    **kwargs: Any,\n) -> None:\n    \"\"\"Generic event handler for CallbackManager.\n\n    Note: This function is used by LangServe to handle events.\n\n    Args:\n        handlers: The list of handlers that will handle the event.\n        event_name: The name of the event (e.g., \"on_llm_start\").\n        ignore_condition_name: Name of the attribute defined on handler\n            that if True will cause the handler to be skipped for the given event.\n        *args: The arguments to pass to the event handler.\n        **kwargs: The keyword arguments to pass to the event handler\n    \"\"\"\n    coros: list[Coroutine[Any, Any, Any]] = []\n\n    try:\n        message_strings: Optional[list[str]] = None\n        for handler in handlers:\n            try:\n                if ignore_condition_name is None or not getattr(\n                    handler, ignore_condition_name\n                ):\n                    event = getattr(handler, event_name)(*args, **kwargs)\n                    if asyncio.iscoroutine(event):\n                        coros.append(event)\n            except NotImplementedError as e:\n                if event_name == \"on_chat_model_start\":\n                    if message_strings is None:\n                        message_strings = [get_buffer_string(m) for m in args[1]]\n                    handle_event(\n                        [handler],\n                        \"on_llm_start\",\n                        \"ignore_llm\",\n                        args[0],\n                        message_strings,\n                        *args[2:],\n                        **kwargs,\n                    )\n                else:\n                    handler_name = handler.__class__.__name__\n                    logger.warning(\n                        f\"NotImplementedError in {handler_name}.{event_name}\"\n                        f\" callback: {repr(e)}\"\n                    )\n            except Exception as e:\n                logger.warning(\n                    f\"Error in {handler.__class__.__name__}.{event_name} callback:\"\n                    f\" {repr(e)}\"\n                )\n                if handler.raise_error:\n                    raise\n    finally:\n        if coros:\n            try:\n                # Raises RuntimeError if there is no current event loop.\n                asyncio.get_running_loop()\n                loop_running = True\n            except RuntimeError:\n                loop_running = False\n\n            if loop_running:\n                # If we try to submit this coroutine to the running loop\n                # we end up in a deadlock, as we'd have gotten here from a\n                # running coroutine, which we cannot interrupt to run this one.\n                # The solution is to create a new loop in a new thread.\n                with ThreadPoolExecutor(1) as executor:\n                    executor.submit(\n                        cast(Callable, copy_context().run), _run_coros, coros\n                    ).result()\n            else:\n                _run_coros(coros)\n\n\ndef _run_coros(coros: list[Coroutine[Any, Any, Any]]) -> None:\n    if hasattr(asyncio, \"Runner\"):\n        # Python 3.11+\n        # Run the coroutines in a new event loop, taking care to\n        # - install signal handlers\n        # - run pending tasks scheduled by `coros`\n        # - close asyncgens and executors\n        # - close the loop\n        with asyncio.Runner() as runner:\n            # Run the coroutine, get the result\n            for coro in coros:\n                try:\n                    runner.run(coro)\n                except Exception as e:\n                    logger.warning(f\"Error in callback coroutine: {repr(e)}\")\n\n            # Run pending tasks scheduled by coros until they are all done\n            while pending := asyncio.all_tasks(runner.get_loop()):\n                runner.run(asyncio.wait(pending))\n    else:\n        # Before Python 3.11 we need to run each coroutine in a new event loop\n        # as the Runner api is not available.\n        for coro in coros:\n            try:\n                asyncio.run(coro)\n            except Exception as e:\n                logger.warning(f\"Error in callback coroutine: {repr(e)}\")\n\n\nasync def _ahandle_event_for_handler(\n    handler: BaseCallbackHandler,\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    *args: Any,\n    **kwargs: Any,\n) -> None:\n    try:\n        if ignore_condition_name is None or not getattr(handler, ignore_condition_name):\n            event = getattr(handler, event_name)\n            if asyncio.iscoroutinefunction(event):\n                await event(*args, **kwargs)\n            else:\n                if handler.run_inline:\n                    event(*args, **kwargs)\n                else:\n                    await asyncio.get_event_loop().run_in_executor(\n                        None,\n                        cast(\n                            Callable,\n                            functools.partial(\n                                copy_context().run, event, *args, **kwargs\n                            ),\n                        ),\n                    )\n    except NotImplementedError as e:\n        if event_name == \"on_chat_model_start\":\n            message_strings = [get_buffer_string(m) for m in args[1]]\n            await _ahandle_event_for_handler(\n                handler,\n                \"on_llm_start\",\n                \"ignore_llm\",\n                args[0],\n                message_strings,\n                *args[2:],\n                **kwargs,\n            )\n        else:\n            logger.warning(\n                f\"NotImplementedError in {handler.__class__.__name__}.{event_name}\"\n                f\" callback: {repr(e)}\"\n            )\n    except Exception as e:\n        logger.warning(\n            f\"Error in {handler.__class__.__name__}.{event_name} callback: {repr(e)}\"\n        )\n        if handler.raise_error:\n            raise\n\n\nasync def ahandle_event(\n    handlers: list[BaseCallbackHandler],\n    event_name: str,\n    ignore_condition_name: Optional[str],\n    *args: Any,\n    **kwargs: Any,\n) -> None:\n    \"\"\"Async generic event handler for AsyncCallbackManager.\n\n    Note: This function is used by LangServe to handle events.\n\n    Args:\n        handlers: The list of handlers that will handle the event.\n        event_name: The name of the event (e.g., \"on_llm_start\").\n        ignore_condition_name: Name of the attribute defined on handler\n            that if True will cause the handler to be skipped for the given event.\n        *args: The arguments to pass to the event handler.\n        **kwargs: The keyword arguments to pass to the event handler.\n    \"\"\"\n    for handler in [h for h in handlers if h.run_inline]:\n        await _ahandle_event_for_handler(\n            handler, event_name, ignore_condition_name, *args, **kwargs\n        )\n    await asyncio.gather(\n        *(\n            _ahandle_event_for_handler(\n                handler,\n                event_name,\n                ignore_condition_name,\n                *args,\n                **kwargs,\n            )\n            for handler in handlers\n            if not handler.run_inline\n        )\n    )\n\n\nBRM = TypeVar(\"BRM\", bound=\"BaseRunManager\")\n\n\nclass BaseRunManager(RunManagerMixin):\n    \"\"\"Base class for run manager (a bound callback manager).\"\"\"\n\n    def __init__(\n        self,\n        *,\n        run_id: UUID,\n        handlers: list[BaseCallbackHandler],\n        inheritable_handlers: list[BaseCallbackHandler],\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        inheritable_tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        inheritable_metadata: Optional[dict[str, Any]] = None,\n    ) -> None:\n        \"\"\"Initialize the run manager.\n\n        Args:\n            run_id (UUID): The ID of the run.\n            handlers (List[BaseCallbackHandler]): The list of handlers.\n            inheritable_handlers (List[BaseCallbackHandler]):\n                The list of inheritable handlers.\n            parent_run_id (UUID, optional): The ID of the parent run.\n                Defaults to None.\n            tags (Optional[List[str]]): The list of tags. Defaults to None.\n            inheritable_tags (Optional[List[str]]): The list of inheritable tags.\n                Defaults to None.\n            metadata (Optional[Dict[str, Any]]): The metadata.\n                Defaults to None.\n            inheritable_metadata (Optional[Dict[str, Any]]): The inheritable metadata.\n                Defaults to None.\n        \"\"\"\n        self.run_id = run_id\n        self.handlers = handlers\n        self.inheritable_handlers = inheritable_handlers\n        self.parent_run_id = parent_run_id\n        self.tags = tags or []\n        self.inheritable_tags = inheritable_tags or []\n        self.metadata = metadata or {}\n        self.inheritable_metadata = inheritable_metadata or {}\n\n    @classmethod\n    def get_noop_manager(cls: type[BRM]) -> BRM:\n        \"\"\"Return a manager that doesn't perform any operations.\n\n        Returns:\n            BaseRunManager: The noop manager.\n        \"\"\"\n        return cls(\n            run_id=uuid.uuid4(),\n            handlers=[],\n            inheritable_handlers=[],\n            tags=[],\n            inheritable_tags=[],\n            metadata={},\n            inheritable_metadata={},\n        )\n\n\nclass RunManager(BaseRunManager):\n    \"\"\"Sync Run Manager.\"\"\"\n\n    def on_text(\n        self,\n        text: str,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when a text is received.\n\n        Args:\n            text (str): The received text.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: The result of the callback.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_text\",\n            None,\n            text,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    def on_retry(\n        self,\n        retry_state: RetryCallState,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when a retry is received.\n\n        Args:\n            retry_state (RetryCallState): The retry state.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_retry\",\n            \"ignore_retry\",\n            retry_state,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass ParentRunManager(RunManager):\n    \"\"\"Sync Parent Run Manager.\"\"\"\n\n    def get_child(self, tag: Optional[str] = None) -> CallbackManager:\n        \"\"\"Get a child callback manager.\n\n        Args:\n            tag (str, optional): The tag for the child callback manager.\n                Defaults to None.\n\n        Returns:\n            CallbackManager: The child callback manager.\n        \"\"\"\n        manager = CallbackManager(handlers=[], parent_run_id=self.run_id)\n        manager.set_handlers(self.inheritable_handlers)\n        manager.add_tags(self.inheritable_tags)\n        manager.add_metadata(self.inheritable_metadata)\n        if tag is not None:\n            manager.add_tags([tag], False)\n        return manager\n\n\nclass AsyncRunManager(BaseRunManager, ABC):\n    \"\"\"Async Run Manager.\"\"\"\n\n    @abstractmethod\n    def get_sync(self) -> RunManager:\n        \"\"\"Get the equivalent sync RunManager.\n\n        Returns:\n            RunManager: The sync RunManager.\n        \"\"\"\n\n    async def on_text(\n        self,\n        text: str,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run when a text is received.\n\n        Args:\n            text (str): The received text.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: The result of the callback.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_text\",\n            None,\n            text,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    async def on_retry(\n        self,\n        retry_state: RetryCallState,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Async run when a retry is received.\n\n        Args:\n            retry_state (RetryCallState): The retry state.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_retry\",\n            \"ignore_retry\",\n            retry_state,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass AsyncParentRunManager(AsyncRunManager):\n    \"\"\"Async Parent Run Manager.\"\"\"\n\n    def get_child(self, tag: Optional[str] = None) -> AsyncCallbackManager:\n        \"\"\"Get a child callback manager.\n\n        Args:\n            tag (str, optional): The tag for the child callback manager.\n                Defaults to None.\n\n        Returns:\n            AsyncCallbackManager: The child callback manager.\n        \"\"\"\n        manager = AsyncCallbackManager(handlers=[], parent_run_id=self.run_id)\n        manager.set_handlers(self.inheritable_handlers)\n        manager.add_tags(self.inheritable_tags)\n        manager.add_metadata(self.inheritable_metadata)\n        if tag is not None:\n            manager.add_tags([tag], False)\n        return manager\n\n\nclass CallbackManagerForLLMRun(RunManager, LLMManagerMixin):\n    \"\"\"Callback manager for LLM run.\"\"\"\n\n    def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when LLM generates a new token.\n\n        Args:\n            token (str): The new token.\n            chunk (Optional[Union[GenerationChunk, ChatGenerationChunk]], optional):\n                The chunk. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_llm_new_token\",\n            \"ignore_llm\",\n            token=token,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            chunk=chunk,\n            **kwargs,\n        )\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        \"\"\"Run when LLM ends running.\n\n        Args:\n            response (LLMResult): The LLM result.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_llm_end\",\n            \"ignore_llm\",\n            response,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    def on_llm_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when LLM errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            kwargs (Any): Additional keyword arguments.\n                - response (LLMResult): The response which was generated before\n                    the error occurred.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_llm_error\",\n            \"ignore_llm\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass AsyncCallbackManagerForLLMRun(AsyncRunManager, LLMManagerMixin):\n    \"\"\"Async callback manager for LLM run.\"\"\"\n\n    def get_sync(self) -> CallbackManagerForLLMRun:\n        \"\"\"Get the equivalent sync RunManager.\n\n        Returns:\n            CallbackManagerForLLMRun: The sync RunManager.\n        \"\"\"\n        return CallbackManagerForLLMRun(\n            run_id=self.run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    @shielded\n    async def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when LLM generates a new token.\n\n        Args:\n            token (str): The new token.\n            chunk (Optional[Union[GenerationChunk, ChatGenerationChunk]], optional):\n                The chunk. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_llm_new_token\",\n            \"ignore_llm\",\n            token,\n            chunk=chunk,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    @shielded\n    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n        \"\"\"Run when LLM ends running.\n\n        Args:\n            response (LLMResult): The LLM result.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_llm_end\",\n            \"ignore_llm\",\n            response,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    @shielded\n    async def on_llm_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when LLM errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            kwargs (Any): Additional keyword arguments.\n                - response (LLMResult): The response which was generated before\n                    the error occurred.\n\n\n\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_llm_error\",\n            \"ignore_llm\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass CallbackManagerForChainRun(ParentRunManager, ChainManagerMixin):\n    \"\"\"Callback manager for chain run.\"\"\"\n\n    def on_chain_end(self, outputs: Union[dict[str, Any], Any], **kwargs: Any) -> None:\n        \"\"\"Run when chain ends running.\n\n        Args:\n            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_chain_end\",\n            \"ignore_chain\",\n            outputs,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    def on_chain_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when chain errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_chain_error\",\n            \"ignore_chain\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n        \"\"\"Run when agent action is received.\n\n        Args:\n            action (AgentAction): The agent action.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: The result of the callback.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_agent_action\",\n            \"ignore_agent\",\n            action,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n        \"\"\"Run when agent finish is received.\n\n        Args:\n            finish (AgentFinish): The agent finish.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: The result of the callback.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_agent_finish\",\n            \"ignore_agent\",\n            finish,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass AsyncCallbackManagerForChainRun(AsyncParentRunManager, ChainManagerMixin):\n    \"\"\"Async callback manager for chain run.\"\"\"\n\n    def get_sync(self) -> CallbackManagerForChainRun:\n        \"\"\"Get the equivalent sync RunManager.\n\n        Returns:\n            CallbackManagerForChainRun: The sync RunManager.\n        \"\"\"\n        return CallbackManagerForChainRun(\n            run_id=self.run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    @shielded\n    async def on_chain_end(\n        self, outputs: Union[dict[str, Any], Any], **kwargs: Any\n    ) -> None:\n        \"\"\"Run when a chain ends running.\n\n        Args:\n            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_chain_end\",\n            \"ignore_chain\",\n            outputs,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    @shielded\n    async def on_chain_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when chain errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_chain_error\",\n            \"ignore_chain\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    @shielded\n    async def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n        \"\"\"Run when agent action is received.\n\n        Args:\n            action (AgentAction): The agent action.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: The result of the callback.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_agent_action\",\n            \"ignore_agent\",\n            action,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    @shielded\n    async def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n        \"\"\"Run when agent finish is received.\n\n        Args:\n            finish (AgentFinish): The agent finish.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            Any: The result of the callback.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_agent_finish\",\n            \"ignore_agent\",\n            finish,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass CallbackManagerForToolRun(ParentRunManager, ToolManagerMixin):\n    \"\"\"Callback manager for tool run.\"\"\"\n\n    def on_tool_end(\n        self,\n        output: Any,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when the tool ends running.\n\n        Args:\n            output (Any): The output of the tool.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_tool_end\",\n            \"ignore_agent\",\n            output,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    def on_tool_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when tool errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_tool_error\",\n            \"ignore_agent\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass AsyncCallbackManagerForToolRun(AsyncParentRunManager, ToolManagerMixin):\n    \"\"\"Async callback manager for tool run.\"\"\"\n\n    def get_sync(self) -> CallbackManagerForToolRun:\n        \"\"\"Get the equivalent sync RunManager.\n\n        Returns:\n            CallbackManagerForToolRun: The sync RunManager.\n        \"\"\"\n        return CallbackManagerForToolRun(\n            run_id=self.run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    @shielded\n    async def on_tool_end(self, output: Any, **kwargs: Any) -> None:\n        \"\"\"Async run when the tool ends running.\n\n        Args:\n            output (Any): The output of the tool.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_tool_end\",\n            \"ignore_agent\",\n            output,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    @shielded\n    async def on_tool_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when tool errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_tool_error\",\n            \"ignore_agent\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass CallbackManagerForRetrieverRun(ParentRunManager, RetrieverManagerMixin):\n    \"\"\"Callback manager for retriever run.\"\"\"\n\n    def on_retriever_end(\n        self,\n        documents: Sequence[Document],\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when retriever ends running.\n\n        Args:\n            documents (Sequence[Document]): The retrieved documents.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_retriever_end\",\n            \"ignore_retriever\",\n            documents,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    def on_retriever_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when retriever errors.\n\n        Args:\n            error (BaseException): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        handle_event(\n            self.handlers,\n            \"on_retriever_error\",\n            \"ignore_retriever\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass AsyncCallbackManagerForRetrieverRun(\n    AsyncParentRunManager,\n    RetrieverManagerMixin,\n):\n    \"\"\"Async callback manager for retriever run.\"\"\"\n\n    def get_sync(self) -> CallbackManagerForRetrieverRun:\n        \"\"\"Get the equivalent sync RunManager.\n\n        Returns:\n            CallbackManagerForRetrieverRun: The sync RunManager.\n        \"\"\"\n        return CallbackManagerForRetrieverRun(\n            run_id=self.run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    @shielded\n    async def on_retriever_end(\n        self, documents: Sequence[Document], **kwargs: Any\n    ) -> None:\n        \"\"\"Run when the retriever ends running.\n\n        Args:\n            documents (Sequence[Document]): The retrieved documents.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_retriever_end\",\n            \"ignore_retriever\",\n            documents,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n    @shielded\n    async def on_retriever_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when retriever errors.\n\n        Args:\n            error (BaseException): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        await ahandle_event(\n            self.handlers,\n            \"on_retriever_error\",\n            \"ignore_retriever\",\n            error,\n            run_id=self.run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            **kwargs,\n        )\n\n\nclass CallbackManager(BaseCallbackManager):\n    \"\"\"Callback manager for LangChain.\"\"\"\n\n    def on_llm_start(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> list[CallbackManagerForLLMRun]:\n        \"\"\"Run when LLM starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized LLM.\n            prompts (List[str]): The list of prompts.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            List[CallbackManagerForLLMRun]: A callback manager for each\n                prompt as an LLM run.\n        \"\"\"\n        managers = []\n        for i, prompt in enumerate(prompts):\n            # Can't have duplicate runs with the same run ID (if provided)\n            run_id_ = run_id if i == 0 and run_id is not None else uuid.uuid4()\n            handle_event(\n                self.handlers,\n                \"on_llm_start\",\n                \"ignore_llm\",\n                serialized,\n                [prompt],\n                run_id=run_id_,\n                parent_run_id=self.parent_run_id,\n                tags=self.tags,\n                metadata=self.metadata,\n                **kwargs,\n            )\n\n            managers.append(\n                CallbackManagerForLLMRun(\n                    run_id=run_id_,\n                    handlers=self.handlers,\n                    inheritable_handlers=self.inheritable_handlers,\n                    parent_run_id=self.parent_run_id,\n                    tags=self.tags,\n                    inheritable_tags=self.inheritable_tags,\n                    metadata=self.metadata,\n                    inheritable_metadata=self.inheritable_metadata,\n                )\n            )\n\n        return managers\n\n    def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> list[CallbackManagerForLLMRun]:\n        \"\"\"Run when chat model starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized LLM.\n            messages (List[List[BaseMessage]]): The list of messages.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            List[CallbackManagerForLLMRun]: A callback manager for each\n                list of messages as an LLM run.\n        \"\"\"\n        managers = []\n        for message_list in messages:\n            if run_id is not None:\n                run_id_ = run_id\n                run_id = None\n            else:\n                run_id_ = uuid.uuid4()\n            handle_event(\n                self.handlers,\n                \"on_chat_model_start\",\n                \"ignore_chat_model\",\n                serialized,\n                [message_list],\n                run_id=run_id_,\n                parent_run_id=self.parent_run_id,\n                tags=self.tags,\n                metadata=self.metadata,\n                **kwargs,\n            )\n\n            managers.append(\n                CallbackManagerForLLMRun(\n                    run_id=run_id_,\n                    handlers=self.handlers,\n                    inheritable_handlers=self.inheritable_handlers,\n                    parent_run_id=self.parent_run_id,\n                    tags=self.tags,\n                    inheritable_tags=self.inheritable_tags,\n                    metadata=self.metadata,\n                    inheritable_metadata=self.inheritable_metadata,\n                )\n            )\n\n        return managers\n\n    def on_chain_start(\n        self,\n        serialized: Optional[dict[str, Any]],\n        inputs: Union[dict[str, Any], Any],\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> CallbackManagerForChainRun:\n        \"\"\"Run when chain starts running.\n\n        Args:\n            serialized (Optional[Dict[str, Any]]): The serialized chain.\n            inputs (Union[Dict[str, Any], Any]): The inputs to the chain.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            CallbackManagerForChainRun: The callback manager for the chain run.\n        \"\"\"\n        if run_id is None:\n            run_id = uuid.uuid4()\n        handle_event(\n            self.handlers,\n            \"on_chain_start\",\n            \"ignore_chain\",\n            serialized,\n            inputs,\n            run_id=run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n            **kwargs,\n        )\n\n        return CallbackManagerForChainRun(\n            run_id=run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    def on_tool_start(\n        self,\n        serialized: Optional[dict[str, Any]],\n        input_str: str,\n        run_id: Optional[UUID] = None,\n        parent_run_id: Optional[UUID] = None,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> CallbackManagerForToolRun:\n        \"\"\"Run when tool starts running.\n\n        Args:\n            serialized: Serialized representation of the tool.\n            input_str: The  input to the tool as a string.\n                Non-string inputs are cast to strings.\n            run_id: ID for the run. Defaults to None.\n            parent_run_id: The ID of the parent run. Defaults to None.\n            inputs: The original input to the tool if provided.\n                Recommended for usage instead of input_str when the original\n                input is needed.\n                If provided, the inputs are expected to be formatted as a dict.\n                The keys will correspond to the named-arguments in the tool.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            CallbackManagerForToolRun: The callback manager for the tool run.\n        \"\"\"\n        if run_id is None:\n            run_id = uuid.uuid4()\n\n        handle_event(\n            self.handlers,\n            \"on_tool_start\",\n            \"ignore_agent\",\n            serialized,\n            input_str,\n            run_id=run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n            inputs=inputs,\n            **kwargs,\n        )\n\n        return CallbackManagerForToolRun(\n            run_id=run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    def on_retriever_start(\n        self,\n        serialized: Optional[dict[str, Any]],\n        query: str,\n        run_id: Optional[UUID] = None,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> CallbackManagerForRetrieverRun:\n        \"\"\"Run when the retriever starts running.\n\n        Args:\n            serialized (Optional[Dict[str, Any]]): The serialized retriever.\n            query (str): The query.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            parent_run_id (UUID, optional): The ID of the parent run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if run_id is None:\n            run_id = uuid.uuid4()\n\n        handle_event(\n            self.handlers,\n            \"on_retriever_start\",\n            \"ignore_retriever\",\n            serialized,\n            query,\n            run_id=run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n            **kwargs,\n        )\n\n        return CallbackManagerForRetrieverRun(\n            run_id=run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    def on_custom_event(\n        self,\n        name: str,\n        data: Any,\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Dispatch an adhoc event to the handlers (async version).\n\n        This event should NOT be used in any internal LangChain code. The event\n        is meant specifically for users of the library to dispatch custom\n        events that are tailored to their application.\n\n        Args:\n            name: The name of the adhoc event.\n            data: The data for the adhoc event.\n            run_id: The ID of the run. Defaults to None.\n\n        .. versionadded:: 0.2.14\n        \"\"\"\n        if kwargs:\n            msg = (\n                \"The dispatcher API does not accept additional keyword arguments.\"\n                \"Please do not pass any additional keyword arguments, instead \"\n                \"include them in the data field.\"\n            )\n            raise ValueError(msg)\n        if run_id is None:\n            run_id = uuid.uuid4()\n\n        handle_event(\n            self.handlers,\n            \"on_custom_event\",\n            \"ignore_custom_event\",\n            name,\n            data,\n            run_id=run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n        )\n\n    @classmethod\n    def configure(\n        cls,\n        inheritable_callbacks: Callbacks = None,\n        local_callbacks: Callbacks = None,\n        verbose: bool = False,\n        inheritable_tags: Optional[list[str]] = None,\n        local_tags: Optional[list[str]] = None,\n        inheritable_metadata: Optional[dict[str, Any]] = None,\n        local_metadata: Optional[dict[str, Any]] = None,\n    ) -> CallbackManager:\n        \"\"\"Configure the callback manager.\n\n        Args:\n            inheritable_callbacks (Optional[Callbacks], optional): The inheritable\n                callbacks. Defaults to None.\n            local_callbacks (Optional[Callbacks], optional): The local callbacks.\n                Defaults to None.\n            verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\n            inheritable_tags (Optional[List[str]], optional): The inheritable tags.\n                Defaults to None.\n            local_tags (Optional[List[str]], optional): The local tags.\n                Defaults to None.\n            inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\n                metadata. Defaults to None.\n            local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\n                Defaults to None.\n\n        Returns:\n            CallbackManager: The configured callback manager.\n        \"\"\"\n        return _configure(\n            cls,\n            inheritable_callbacks,\n            local_callbacks,\n            verbose,\n            inheritable_tags,\n            local_tags,\n            inheritable_metadata,\n            local_metadata,\n        )\n\n\nclass CallbackManagerForChainGroup(CallbackManager):\n    \"\"\"Callback manager for the chain group.\"\"\"\n\n    def __init__(\n        self,\n        handlers: list[BaseCallbackHandler],\n        inheritable_handlers: Optional[list[BaseCallbackHandler]] = None,\n        parent_run_id: Optional[UUID] = None,\n        *,\n        parent_run_manager: CallbackManagerForChainRun,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize the callback manager.\n\n        Args:\n            handlers (List[BaseCallbackHandler]): The list of handlers.\n            inheritable_handlers (Optional[List[BaseCallbackHandler]]): The list of\n                inheritable handlers. Defaults to None.\n            parent_run_id (Optional[UUID]): The ID of the parent run. Defaults to None.\n            parent_run_manager (CallbackManagerForChainRun): The parent run manager.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            handlers,\n            inheritable_handlers,\n            parent_run_id,\n            **kwargs,\n        )\n        self.parent_run_manager = parent_run_manager\n        self.ended = False\n\n    def copy(self) -> CallbackManagerForChainGroup:\n        \"\"\"Copy the callback manager.\"\"\"\n        return self.__class__(\n            handlers=self.handlers.copy(),\n            inheritable_handlers=self.inheritable_handlers.copy(),\n            parent_run_id=self.parent_run_id,\n            tags=self.tags.copy(),\n            inheritable_tags=self.inheritable_tags.copy(),\n            metadata=self.metadata.copy(),\n            inheritable_metadata=self.inheritable_metadata.copy(),\n            parent_run_manager=self.parent_run_manager,\n        )\n\n    def merge(\n        self: CallbackManagerForChainGroup, other: BaseCallbackManager\n    ) -> CallbackManagerForChainGroup:\n        \"\"\"Merge the group callback manager with another callback manager.\n\n        Overwrites the merge method in the base class to ensure that the\n        parent run manager is preserved. Keeps the parent_run_manager\n        from the current object.\n\n        Returns:\n            CallbackManagerForChainGroup: A copy of the current object with the\n                handlers, tags, and other attributes merged from the other object.\n\n        Example: Merging two callback managers.\n\n            .. code-block:: python\n\n                from langchain_core.callbacks.manager import CallbackManager, trace_as_chain_group\n                from langchain_core.callbacks.stdout import StdOutCallbackHandler\n\n                manager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=[\"tag2\"])\n                with trace_as_chain_group(\"My Group Name\", tags=[\"tag1\"]) as group_manager:\n                    merged_manager = group_manager.merge(manager)\n                    print(type(merged_manager))\n                    # <class 'langchain_core.callbacks.manager.CallbackManagerForChainGroup'>\n\n                    print(merged_manager.handlers)\n                    # [\n                    #    <langchain_core.callbacks.stdout.LangChainTracer object at ...>,\n                    #    <langchain_core.callbacks.streaming_stdout.StdOutCallbackHandler object at ...>,\n                    # ]\n\n                    print(merged_manager.tags)\n                    #    ['tag2', 'tag1']\n\n        \"\"\"  # noqa: E501\n        manager = self.__class__(\n            parent_run_id=self.parent_run_id or other.parent_run_id,\n            handlers=[],\n            inheritable_handlers=[],\n            tags=list(set(self.tags + other.tags)),\n            inheritable_tags=list(set(self.inheritable_tags + other.inheritable_tags)),\n            metadata={\n                **self.metadata,\n                **other.metadata,\n            },\n            parent_run_manager=self.parent_run_manager,\n        )\n\n        handlers = self.handlers + other.handlers\n        inheritable_handlers = self.inheritable_handlers + other.inheritable_handlers\n\n        for handler in handlers:\n            manager.add_handler(handler)\n\n        for handler in inheritable_handlers:\n            manager.add_handler(handler, inherit=True)\n        return manager\n\n    def on_chain_end(self, outputs: Union[dict[str, Any], Any], **kwargs: Any) -> None:\n        \"\"\"Run when traced chain group ends.\n\n        Args:\n            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        self.ended = True\n        return self.parent_run_manager.on_chain_end(outputs, **kwargs)\n\n    def on_chain_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when chain errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        self.ended = True\n        return self.parent_run_manager.on_chain_error(error, **kwargs)\n\n\nclass AsyncCallbackManager(BaseCallbackManager):\n    \"\"\"Async callback manager that handles callbacks from LangChain.\"\"\"\n\n    @property\n    def is_async(self) -> bool:\n        \"\"\"Return whether the handler is async.\"\"\"\n        return True\n\n    async def on_llm_start(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> list[AsyncCallbackManagerForLLMRun]:\n        \"\"\"Run when LLM starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized LLM.\n            prompts (List[str]): The list of prompts.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            List[AsyncCallbackManagerForLLMRun]: The list of async\n                callback managers, one for each LLM Run corresponding\n                to each prompt.\n        \"\"\"\n        inline_tasks = []\n        non_inline_tasks = []\n        inline_handlers = [handler for handler in self.handlers if handler.run_inline]\n        non_inline_handlers = [\n            handler for handler in self.handlers if not handler.run_inline\n        ]\n        managers = []\n\n        for prompt in prompts:\n            if run_id is not None:\n                run_id_ = run_id\n                run_id = None\n            else:\n                run_id_ = uuid.uuid4()\n\n            if inline_handlers:\n                inline_tasks.append(\n                    ahandle_event(\n                        inline_handlers,\n                        \"on_llm_start\",\n                        \"ignore_llm\",\n                        serialized,\n                        [prompt],\n                        run_id=run_id_,\n                        parent_run_id=self.parent_run_id,\n                        tags=self.tags,\n                        metadata=self.metadata,\n                        **kwargs,\n                    )\n                )\n            else:\n                non_inline_tasks.append(\n                    ahandle_event(\n                        non_inline_handlers,\n                        \"on_llm_start\",\n                        \"ignore_llm\",\n                        serialized,\n                        [prompt],\n                        run_id=run_id_,\n                        parent_run_id=self.parent_run_id,\n                        tags=self.tags,\n                        metadata=self.metadata,\n                        **kwargs,\n                    )\n                )\n\n            managers.append(\n                AsyncCallbackManagerForLLMRun(\n                    run_id=run_id_,\n                    handlers=self.handlers,\n                    inheritable_handlers=self.inheritable_handlers,\n                    parent_run_id=self.parent_run_id,\n                    tags=self.tags,\n                    inheritable_tags=self.inheritable_tags,\n                    metadata=self.metadata,\n                    inheritable_metadata=self.inheritable_metadata,\n                )\n            )\n\n        # Run inline tasks sequentially\n        for inline_task in inline_tasks:\n            await inline_task\n\n        # Run non-inline tasks concurrently\n        if non_inline_tasks:\n            await asyncio.gather(*non_inline_tasks)\n\n        return managers\n\n    async def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> list[AsyncCallbackManagerForLLMRun]:\n        \"\"\"Async run when LLM starts running.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized LLM.\n            messages (List[List[BaseMessage]]): The list of messages.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            List[AsyncCallbackManagerForLLMRun]: The list of\n                async callback managers, one for each LLM Run\n                corresponding to each inner  message list.\n        \"\"\"\n        inline_tasks = []\n        non_inline_tasks = []\n        managers = []\n\n        for message_list in messages:\n            if run_id is not None:\n                run_id_ = run_id\n                run_id = None\n            else:\n                run_id_ = uuid.uuid4()\n\n            for handler in self.handlers:\n                task = ahandle_event(\n                    [handler],\n                    \"on_chat_model_start\",\n                    \"ignore_chat_model\",\n                    serialized,\n                    [message_list],\n                    run_id=run_id_,\n                    parent_run_id=self.parent_run_id,\n                    tags=self.tags,\n                    metadata=self.metadata,\n                    **kwargs,\n                )\n                if handler.run_inline:\n                    inline_tasks.append(task)\n                else:\n                    non_inline_tasks.append(task)\n\n            managers.append(\n                AsyncCallbackManagerForLLMRun(\n                    run_id=run_id_,\n                    handlers=self.handlers,\n                    inheritable_handlers=self.inheritable_handlers,\n                    parent_run_id=self.parent_run_id,\n                    tags=self.tags,\n                    inheritable_tags=self.inheritable_tags,\n                    metadata=self.metadata,\n                    inheritable_metadata=self.inheritable_metadata,\n                )\n            )\n\n        # Run inline tasks sequentially\n        for task in inline_tasks:\n            await task\n\n        # Run non-inline tasks concurrently\n        if non_inline_tasks:\n            await asyncio.gather(*non_inline_tasks)\n\n        return managers\n\n    async def on_chain_start(\n        self,\n        serialized: Optional[dict[str, Any]],\n        inputs: Union[dict[str, Any], Any],\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> AsyncCallbackManagerForChainRun:\n        \"\"\"Async run when chain starts running.\n\n        Args:\n            serialized (Optional[Dict[str, Any]]): The serialized chain.\n            inputs (Union[Dict[str, Any], Any]): The inputs to the chain.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            AsyncCallbackManagerForChainRun: The async callback manager\n                for the chain run.\n        \"\"\"\n        if run_id is None:\n            run_id = uuid.uuid4()\n\n        await ahandle_event(\n            self.handlers,\n            \"on_chain_start\",\n            \"ignore_chain\",\n            serialized,\n            inputs,\n            run_id=run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n            **kwargs,\n        )\n\n        return AsyncCallbackManagerForChainRun(\n            run_id=run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    async def on_tool_start(\n        self,\n        serialized: Optional[dict[str, Any]],\n        input_str: str,\n        run_id: Optional[UUID] = None,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> AsyncCallbackManagerForToolRun:\n        \"\"\"Run when the tool starts running.\n\n        Args:\n            serialized (Optional[Dict[str, Any]]): The serialized tool.\n            input_str (str): The input to the tool.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            parent_run_id (UUID, optional): The ID of the parent run.\n                Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            AsyncCallbackManagerForToolRun: The async callback manager\n                for the tool run.\n        \"\"\"\n        if run_id is None:\n            run_id = uuid.uuid4()\n\n        await ahandle_event(\n            self.handlers,\n            \"on_tool_start\",\n            \"ignore_agent\",\n            serialized,\n            input_str,\n            run_id=run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n            **kwargs,\n        )\n\n        return AsyncCallbackManagerForToolRun(\n            run_id=run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    async def on_custom_event(\n        self,\n        name: str,\n        data: Any,\n        run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Dispatch an adhoc event to the handlers (async version).\n\n        This event should NOT be used in any internal LangChain code. The event\n        is meant specifically for users of the library to dispatch custom\n        events that are tailored to their application.\n\n        Args:\n            name: The name of the adhoc event.\n            data: The data for the adhoc event.\n            run_id: The ID of the run. Defaults to None.\n\n        .. versionadded:: 0.2.14\n        \"\"\"\n        if run_id is None:\n            run_id = uuid.uuid4()\n\n        if kwargs:\n            msg = (\n                \"The dispatcher API does not accept additional keyword arguments.\"\n                \"Please do not pass any additional keyword arguments, instead \"\n                \"include them in the data field.\"\n            )\n            raise ValueError(msg)\n        await ahandle_event(\n            self.handlers,\n            \"on_custom_event\",\n            \"ignore_custom_event\",\n            name,\n            data,\n            run_id=run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n        )\n\n    async def on_retriever_start(\n        self,\n        serialized: Optional[dict[str, Any]],\n        query: str,\n        run_id: Optional[UUID] = None,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> AsyncCallbackManagerForRetrieverRun:\n        \"\"\"Run when the retriever starts running.\n\n        Args:\n            serialized (Optional[Dict[str, Any]]): The serialized retriever.\n            query (str): The query.\n            run_id (UUID, optional): The ID of the run. Defaults to None.\n            parent_run_id (UUID, optional): The ID of the parent run. Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            AsyncCallbackManagerForRetrieverRun: The async callback manager\n                for the retriever run.\n        \"\"\"\n        if run_id is None:\n            run_id = uuid.uuid4()\n\n        await ahandle_event(\n            self.handlers,\n            \"on_retriever_start\",\n            \"ignore_retriever\",\n            serialized,\n            query,\n            run_id=run_id,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            metadata=self.metadata,\n            **kwargs,\n        )\n\n        return AsyncCallbackManagerForRetrieverRun(\n            run_id=run_id,\n            handlers=self.handlers,\n            inheritable_handlers=self.inheritable_handlers,\n            parent_run_id=self.parent_run_id,\n            tags=self.tags,\n            inheritable_tags=self.inheritable_tags,\n            metadata=self.metadata,\n            inheritable_metadata=self.inheritable_metadata,\n        )\n\n    @classmethod\n    def configure(\n        cls,\n        inheritable_callbacks: Callbacks = None,\n        local_callbacks: Callbacks = None,\n        verbose: bool = False,\n        inheritable_tags: Optional[list[str]] = None,\n        local_tags: Optional[list[str]] = None,\n        inheritable_metadata: Optional[dict[str, Any]] = None,\n        local_metadata: Optional[dict[str, Any]] = None,\n    ) -> AsyncCallbackManager:\n        \"\"\"Configure the async callback manager.\n\n        Args:\n            inheritable_callbacks (Optional[Callbacks], optional): The inheritable\n                callbacks. Defaults to None.\n            local_callbacks (Optional[Callbacks], optional): The local callbacks.\n                Defaults to None.\n            verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\n            inheritable_tags (Optional[List[str]], optional): The inheritable tags.\n                Defaults to None.\n            local_tags (Optional[List[str]], optional): The local tags.\n                Defaults to None.\n            inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\n                metadata. Defaults to None.\n            local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\n                Defaults to None.\n\n        Returns:\n            AsyncCallbackManager: The configured async callback manager.\n        \"\"\"\n        return _configure(\n            cls,\n            inheritable_callbacks,\n            local_callbacks,\n            verbose,\n            inheritable_tags,\n            local_tags,\n            inheritable_metadata,\n            local_metadata,\n        )\n\n\nclass AsyncCallbackManagerForChainGroup(AsyncCallbackManager):\n    \"\"\"Async callback manager for the chain group.\"\"\"\n\n    def __init__(\n        self,\n        handlers: list[BaseCallbackHandler],\n        inheritable_handlers: Optional[list[BaseCallbackHandler]] = None,\n        parent_run_id: Optional[UUID] = None,\n        *,\n        parent_run_manager: AsyncCallbackManagerForChainRun,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize the async callback manager.\n\n        Args:\n            handlers (List[BaseCallbackHandler]): The list of handlers.\n            inheritable_handlers (Optional[List[BaseCallbackHandler]]): The list of\n                inheritable handlers. Defaults to None.\n            parent_run_id (Optional[UUID]): The ID of the parent run. Defaults to None.\n            parent_run_manager (AsyncCallbackManagerForChainRun):\n                The parent run manager.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            handlers,\n            inheritable_handlers,\n            parent_run_id,\n            **kwargs,\n        )\n        self.parent_run_manager = parent_run_manager\n        self.ended = False\n\n    def copy(self) -> AsyncCallbackManagerForChainGroup:\n        \"\"\"Copy the async callback manager.\"\"\"\n        return self.__class__(\n            handlers=self.handlers.copy(),\n            inheritable_handlers=self.inheritable_handlers.copy(),\n            parent_run_id=self.parent_run_id,\n            tags=self.tags.copy(),\n            inheritable_tags=self.inheritable_tags.copy(),\n            metadata=self.metadata.copy(),\n            inheritable_metadata=self.inheritable_metadata.copy(),\n            parent_run_manager=self.parent_run_manager,\n        )\n\n    def merge(\n        self: AsyncCallbackManagerForChainGroup, other: BaseCallbackManager\n    ) -> AsyncCallbackManagerForChainGroup:\n        \"\"\"Merge the group callback manager with another callback manager.\n\n        Overwrites the merge method in the base class to ensure that the\n        parent run manager is preserved. Keeps the parent_run_manager\n        from the current object.\n\n        Returns:\n            AsyncCallbackManagerForChainGroup: A copy of the current AsyncCallbackManagerForChainGroup\n                with the handlers, tags, etc. of the other callback manager merged in.\n\n        Example: Merging two callback managers.\n\n            .. code-block:: python\n\n                from langchain_core.callbacks.manager import CallbackManager, atrace_as_chain_group\n                from langchain_core.callbacks.stdout import StdOutCallbackHandler\n\n                manager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=[\"tag2\"])\n                async with atrace_as_chain_group(\"My Group Name\", tags=[\"tag1\"]) as group_manager:\n                    merged_manager = group_manager.merge(manager)\n                    print(type(merged_manager))\n                    # <class 'langchain_core.callbacks.manager.AsyncCallbackManagerForChainGroup'>\n\n                    print(merged_manager.handlers)\n                    # [\n                    #    <langchain_core.callbacks.stdout.LangChainTracer object at ...>,\n                    #    <langchain_core.callbacks.streaming_stdout.StdOutCallbackHandler object at ...>,\n                    # ]\n\n                    print(merged_manager.tags)\n                    #    ['tag2', 'tag1']\n\n        \"\"\"  # noqa: E501\n        manager = self.__class__(\n            parent_run_id=self.parent_run_id or other.parent_run_id,\n            handlers=[],\n            inheritable_handlers=[],\n            tags=list(set(self.tags + other.tags)),\n            inheritable_tags=list(set(self.inheritable_tags + other.inheritable_tags)),\n            metadata={\n                **self.metadata,\n                **other.metadata,\n            },\n            parent_run_manager=self.parent_run_manager,\n        )\n\n        handlers = self.handlers + other.handlers\n        inheritable_handlers = self.inheritable_handlers + other.inheritable_handlers\n\n        for handler in handlers:\n            manager.add_handler(handler)\n\n        for handler in inheritable_handlers:\n            manager.add_handler(handler, inherit=True)\n        return manager\n\n    async def on_chain_end(\n        self, outputs: Union[dict[str, Any], Any], **kwargs: Any\n    ) -> None:\n        \"\"\"Run when traced chain group ends.\n\n        Args:\n            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        self.ended = True\n        await self.parent_run_manager.on_chain_end(outputs, **kwargs)\n\n    async def on_chain_error(\n        self,\n        error: BaseException,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when chain errors.\n\n        Args:\n            error (Exception or KeyboardInterrupt): The error.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        self.ended = True\n        await self.parent_run_manager.on_chain_error(error, **kwargs)\n\n\nT = TypeVar(\"T\", CallbackManager, AsyncCallbackManager)\n\n\nH = TypeVar(\"H\", bound=BaseCallbackHandler, covariant=True)\n\n\ndef _configure(\n    callback_manager_cls: type[T],\n    inheritable_callbacks: Callbacks = None,\n    local_callbacks: Callbacks = None,\n    verbose: bool = False,\n    inheritable_tags: Optional[list[str]] = None,\n    local_tags: Optional[list[str]] = None,\n    inheritable_metadata: Optional[dict[str, Any]] = None,\n    local_metadata: Optional[dict[str, Any]] = None,\n) -> T:\n    \"\"\"Configure the callback manager.\n\n    Args:\n        callback_manager_cls (Type[T]): The callback manager class.\n        inheritable_callbacks (Optional[Callbacks], optional): The inheritable\n            callbacks. Defaults to None.\n        local_callbacks (Optional[Callbacks], optional): The local callbacks.\n            Defaults to None.\n        verbose (bool, optional): Whether to enable verbose mode. Defaults to False.\n        inheritable_tags (Optional[List[str]], optional): The inheritable tags.\n            Defaults to None.\n        local_tags (Optional[List[str]], optional): The local tags. Defaults to None.\n        inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable\n            metadata. Defaults to None.\n        local_metadata (Optional[Dict[str, Any]], optional): The local metadata.\n            Defaults to None.\n\n    Returns:\n        T: The configured callback manager.\n    \"\"\"\n    from langchain_core.tracers.context import (\n        _configure_hooks,\n        _get_tracer_project,\n        _tracing_v2_is_enabled,\n        tracing_v2_callback_var,\n    )\n\n    tracing_context = get_tracing_context()\n    tracing_metadata = tracing_context[\"metadata\"]\n    tracing_tags = tracing_context[\"tags\"]\n    run_tree: Optional[Run] = tracing_context[\"parent\"]\n    parent_run_id = None if run_tree is None else run_tree.id\n    callback_manager = callback_manager_cls(\n        handlers=[],\n        parent_run_id=parent_run_id,\n    )\n    if inheritable_callbacks or local_callbacks:\n        if isinstance(inheritable_callbacks, list) or inheritable_callbacks is None:\n            inheritable_callbacks_ = inheritable_callbacks or []\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks_.copy(),\n                inheritable_handlers=inheritable_callbacks_.copy(),\n                parent_run_id=parent_run_id,\n            )\n        else:\n            parent_run_id_ = inheritable_callbacks.parent_run_id\n            # Break ties between the external tracing context and inherited context\n            if parent_run_id is not None and (\n                parent_run_id_ is None\n                # If the LC parent has already been reflected\n                # in the run tree, we know the run_tree is either the\n                # same parent or a child of the parent.\n                or (run_tree and str(parent_run_id_) in run_tree.dotted_order)\n            ):\n                parent_run_id_ = parent_run_id\n                # Otherwise, we assume the LC context has progressed\n                # beyond the run tree and we should not inherit the parent.\n            callback_manager = callback_manager_cls(\n                handlers=inheritable_callbacks.handlers.copy(),\n                inheritable_handlers=inheritable_callbacks.inheritable_handlers.copy(),\n                parent_run_id=parent_run_id_,\n                tags=inheritable_callbacks.tags.copy(),\n                inheritable_tags=inheritable_callbacks.inheritable_tags.copy(),\n                metadata=inheritable_callbacks.metadata.copy(),\n                inheritable_metadata=inheritable_callbacks.inheritable_metadata.copy(),\n            )\n        local_handlers_ = (\n            local_callbacks\n            if isinstance(local_callbacks, list)\n            else (local_callbacks.handlers if local_callbacks else [])\n        )\n        for handler in local_handlers_:\n            callback_manager.add_handler(handler, False)\n    if inheritable_tags or local_tags:\n        callback_manager.add_tags(inheritable_tags or [])\n        callback_manager.add_tags(local_tags or [], False)\n    if inheritable_metadata or local_metadata:\n        callback_manager.add_metadata(inheritable_metadata or {})\n        callback_manager.add_metadata(local_metadata or {}, False)\n    if tracing_metadata:\n        callback_manager.add_metadata(tracing_metadata.copy())\n    if tracing_tags:\n        callback_manager.add_tags(tracing_tags.copy())\n\n    v1_tracing_enabled_ = env_var_is_set(\"LANGCHAIN_TRACING\") or env_var_is_set(\n        \"LANGCHAIN_HANDLER\"\n    )\n\n    tracer_v2 = tracing_v2_callback_var.get()\n    tracing_v2_enabled_ = _tracing_v2_is_enabled()\n\n    if v1_tracing_enabled_ and not tracing_v2_enabled_:\n        # if both are enabled, can silently ignore the v1 tracer\n        msg = (\n            \"Tracing using LangChainTracerV1 is no longer supported. \"\n            \"Please set the LANGCHAIN_TRACING_V2 environment variable to enable \"\n            \"tracing instead.\"\n        )\n        raise RuntimeError(msg)\n\n    tracer_project = _get_tracer_project()\n    debug = _get_debug()\n    if verbose or debug or tracing_v2_enabled_:\n        from langchain_core.tracers.langchain import LangChainTracer\n        from langchain_core.tracers.stdout import ConsoleCallbackHandler\n\n        if verbose and not any(\n            isinstance(handler, StdOutCallbackHandler)\n            for handler in callback_manager.handlers\n        ):\n            if debug:\n                pass\n            else:\n                callback_manager.add_handler(StdOutCallbackHandler(), False)\n        if debug and not any(\n            isinstance(handler, ConsoleCallbackHandler)\n            for handler in callback_manager.handlers\n        ):\n            callback_manager.add_handler(ConsoleCallbackHandler(), True)\n        if tracing_v2_enabled_ and not any(\n            isinstance(handler, LangChainTracer)\n            for handler in callback_manager.handlers\n        ):\n            if tracer_v2:\n                callback_manager.add_handler(tracer_v2, True)\n            else:\n                try:\n                    handler = LangChainTracer(\n                        project_name=tracer_project,\n                        client=(\n                            run_tree.client\n                            if run_tree is not None\n                            else tracing_context[\"client\"]\n                        ),\n                        tags=tracing_tags,\n                    )\n                    callback_manager.add_handler(handler, True)\n                except Exception as e:\n                    logger.warning(\n                        \"Unable to load requested LangChainTracer.\"\n                        \" To disable this warning,\"\n                        \" unset the LANGCHAIN_TRACING_V2 environment variables.\\n\"\n                        f\"{repr(e)}\",\n                    )\n        if run_tree is not None:\n            for handler in callback_manager.handlers:\n                if isinstance(handler, LangChainTracer):\n                    handler.order_map[run_tree.id] = (\n                        run_tree.trace_id,\n                        run_tree.dotted_order,\n                    )\n                    handler.run_map[str(run_tree.id)] = cast(Run, run_tree)\n    for var, inheritable, handler_class, env_var in _configure_hooks:\n        create_one = (\n            env_var is not None\n            and env_var_is_set(env_var)\n            and handler_class is not None\n        )\n        if var.get() is not None or create_one:\n            var_handler = var.get() or cast(type[BaseCallbackHandler], handler_class)()\n            if handler_class is None:\n                if not any(\n                    handler is var_handler  # direct pointer comparison\n                    for handler in callback_manager.handlers\n                ):\n                    callback_manager.add_handler(var_handler, inheritable)\n            else:\n                if not any(\n                    isinstance(handler, handler_class)\n                    for handler in callback_manager.handlers\n                ):\n                    callback_manager.add_handler(var_handler, inheritable)\n    return callback_manager\n\n\nasync def adispatch_custom_event(\n    name: str, data: Any, *, config: Optional[RunnableConfig] = None\n) -> None:\n    \"\"\"Dispatch an adhoc event to the handlers.\n\n    Args:\n        name: The name of the adhoc event.\n        data: The data for the adhoc event. Free form data. Ideally should be\n              JSON serializable to avoid serialization issues downstream, but\n              this is not enforced.\n        config: Optional config object. Mirrors the async API but not strictly needed.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.callbacks import (\n                AsyncCallbackHandler,\n                adispatch_custom_event\n            )\n            from langchain_core.runnable import RunnableLambda\n\n            class CustomCallbackManager(AsyncCallbackHandler):\n                async def on_custom_event(\n                    self,\n                    name: str,\n                    data: Any,\n                    *,\n                    run_id: UUID,\n                    tags: Optional[List[str]] = None,\n                    metadata: Optional[Dict[str, Any]] = None,\n                    **kwargs: Any,\n                ) -> None:\n                    print(f\"Received custom event: {name} with data: {data}\")\n\n            callback = CustomCallbackManager()\n\n            async def foo(inputs):\n                await adispatch_custom_event(\"my_event\", {\"bar\": \"buzz})\n                return inputs\n\n            foo_ = RunnableLambda(foo)\n            await foo_.ainvoke({\"a\": \"1\"}, {\"callbacks\": [CustomCallbackManager()]})\n\n    Example: Use with astream events\n\n        .. code-block:: python\n\n            from langchain_core.callbacks import (\n                AsyncCallbackHandler,\n                adispatch_custom_event\n            )\n            from langchain_core.runnable import RunnableLambda\n\n            class CustomCallbackManager(AsyncCallbackHandler):\n                async def on_custom_event(\n                    self,\n                    name: str,\n                    data: Any,\n                    *,\n                    run_id: UUID,\n                    tags: Optional[List[str]] = None,\n                    metadata: Optional[Dict[str, Any]] = None,\n                    **kwargs: Any,\n                ) -> None:\n                    print(f\"Received custom event: {name} with data: {data}\")\n\n            callback = CustomCallbackManager()\n\n            async def foo(inputs):\n                await adispatch_custom_event(\"event_type_1\", {\"bar\": \"buzz})\n                await adispatch_custom_event(\"event_type_2\", 5)\n                return inputs\n\n            foo_ = RunnableLambda(foo)\n\n            async for event in foo_.ainvoke_stream(\n                {\"a\": \"1\"},\n                version=\"v2\",\n                config={\"callbacks\": [CustomCallbackManager()]}\n            ):\n                print(event)\n\n    .. warning::\n        If using python <= 3.10 and async, you MUST\n        specify the `config` parameter or the function will raise an error.\n        This is due to a limitation in asyncio for python <= 3.10 that prevents\n        LangChain from automatically propagating the config object on the user's\n        behalf.\n\n    .. versionadded:: 0.2.15\n    \"\"\"\n    from langchain_core.runnables.config import (\n        ensure_config,\n        get_async_callback_manager_for_config,\n    )\n\n    config = ensure_config(config)\n    callback_manager = get_async_callback_manager_for_config(config)\n    # We want to get the callback manager for the parent run.\n    # This is a work-around for now to be able to dispatch adhoc events from\n    # within a tool or a lambda and have the metadata events associated\n    # with the parent run rather than have a new run id generated for each.\n    if callback_manager.parent_run_id is None:\n        msg = (\n            \"Unable to dispatch an adhoc event without a parent run id.\"\n            \"This function can only be called from within an existing run (e.g.,\"\n            \"inside a tool or a RunnableLambda or a RunnableGenerator.)\"\n            \"If you are doing that and still seeing this error, try explicitly\"\n            \"passing the config parameter to this function.\"\n        )\n        raise RuntimeError(msg)\n\n    await callback_manager.on_custom_event(\n        name,\n        data,\n        run_id=callback_manager.parent_run_id,\n    )\n\n\ndef dispatch_custom_event(\n    name: str, data: Any, *, config: Optional[RunnableConfig] = None\n) -> None:\n    \"\"\"Dispatch an adhoc event.\n\n    Args:\n        name: The name of the adhoc event.\n        data: The data for the adhoc event. Free form data. Ideally should be\n              JSON serializable to avoid serialization issues downstream, but\n              this is not enforced.\n        config: Optional config object. Mirrors the async API but not strictly needed.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.callbacks import BaseCallbackHandler\n            from langchain_core.callbacks import dispatch_custom_event\n            from langchain_core.runnable import RunnableLambda\n\n            class CustomCallbackManager(BaseCallbackHandler):\n                def on_custom_event(\n                    self,\n                    name: str,\n                    data: Any,\n                    *,\n                    run_id: UUID,\n                    tags: Optional[List[str]] = None,\n                    metadata: Optional[Dict[str, Any]] = None,\n                    **kwargs: Any,\n                ) -> None:\n                    print(f\"Received custom event: {name} with data: {data}\")\n\n            def foo(inputs):\n                dispatch_custom_event(\"my_event\", {\"bar\": \"buzz})\n                return inputs\n\n            foo_ = RunnableLambda(foo)\n            foo_.invoke({\"a\": \"1\"}, {\"callbacks\": [CustomCallbackManager()]})\n\n    .. versionadded:: 0.2.15\n    \"\"\"\n    from langchain_core.runnables.config import (\n        ensure_config,\n        get_callback_manager_for_config,\n    )\n\n    config = ensure_config(config)\n    callback_manager = get_callback_manager_for_config(config)\n    # We want to get the callback manager for the parent run.\n    # This is a work-around for now to be able to dispatch adhoc events from\n    # within a tool or a lambda and have the metadata events associated\n    # with the parent run rather than have a new run id generated for each.\n    if callback_manager.parent_run_id is None:\n        msg = (\n            \"Unable to dispatch an adhoc event without a parent run id.\"\n            \"This function can only be called from within an existing run (e.g.,\"\n            \"inside a tool or a RunnableLambda or a RunnableGenerator.)\"\n            \"If you are doing that and still seeing this error, try explicitly\"\n            \"passing the config parameter to this function.\"\n        )\n        raise RuntimeError(msg)\n    callback_manager.on_custom_event(\n        name,\n        data,\n        run_id=callback_manager.parent_run_id,\n    )\n",
        "patch": "@@ -5,7 +5,6 @@\n import logging\n import uuid\n from abc import ABC, abstractmethod\n-from collections.abc import AsyncGenerator, Coroutine, Generator, Sequence\n from concurrent.futures import ThreadPoolExecutor\n from contextlib import asynccontextmanager, contextmanager\n from contextvars import copy_context\n@@ -21,7 +20,6 @@\n from uuid import UUID\n \n from langsmith.run_helpers import get_tracing_context\n-from tenacity import RetryCallState\n \n from langchain_core.callbacks.base import (\n     BaseCallbackHandler,\n@@ -39,6 +37,10 @@\n from langchain_core.utils.env import env_var_is_set\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncGenerator, Coroutine, Generator, Sequence\n+\n+    from tenacity import RetryCallState\n+\n     from langchain_core.agents import AgentAction, AgentFinish\n     from langchain_core.documents import Document\n     from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult"
      },
      {
        "filename": "libs/core/langchain_core/chat_history.py",
        "content_before": "\"\"\"**Chat message history** stores a history of the message interactions in a chat.\n\n**Class hierarchy:**\n\n.. code-block::\n\n    BaseChatMessageHistory --> <name>ChatMessageHistory  # Examples: FileChatMessageHistory, PostgresChatMessageHistory\n\n**Main helpers:**\n\n.. code-block::\n\n    AIMessage, HumanMessage, BaseMessage\n\n\"\"\"  # noqa: E501\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom typing import Union\n\nfrom pydantic import BaseModel, Field\n\nfrom langchain_core.messages import (\n    AIMessage,\n    BaseMessage,\n    HumanMessage,\n    get_buffer_string,\n)\n\n\nclass BaseChatMessageHistory(ABC):\n    \"\"\"Abstract base class for storing chat message history.\n\n    Implementations guidelines:\n\n    Implementations are expected to over-ride all or some of the following methods:\n\n    * add_messages: sync variant for bulk addition of messages\n    * aadd_messages: async variant for bulk addition of messages\n    * messages: sync variant for getting messages\n    * aget_messages: async variant for getting messages\n    * clear: sync variant for clearing messages\n    * aclear: async variant for clearing messages\n\n    add_messages contains a default implementation that calls add_message\n    for each message in the sequence. This is provided for backwards compatibility\n    with existing implementations which only had add_message.\n\n    Async variants all have default implementations that call the sync variants.\n    Implementers can choose to over-ride the async implementations to provide\n    truly async implementations.\n\n    Usage guidelines:\n\n    When used for updating history, users should favor usage of `add_messages`\n    over `add_message` or other variants like `add_user_message` and `add_ai_message`\n    to avoid unnecessary round-trips to the underlying persistence layer.\n\n    Example: Shows a default implementation.\n\n        .. code-block:: python\n\n            class FileChatMessageHistory(BaseChatMessageHistory):\n                storage_path:  str\n                session_id: str\n\n               @property\n               def messages(self):\n                   with open(os.path.join(storage_path, session_id), 'r:utf-8') as f:\n                       messages = json.loads(f.read())\n                    return messages_from_dict(messages)\n\n               def add_messages(self, messages: Sequence[BaseMessage]) -> None:\n                   all_messages = list(self.messages) # Existing messages\n                   all_messages.extend(messages) # Add new messages\n\n                   serialized = [message_to_dict(message) for message in all_messages]\n                   # Can be further optimized by only writing new messages\n                   # using append mode.\n                   with open(os.path.join(storage_path, session_id), 'w') as f:\n                       json.dump(f, messages)\n\n               def clear(self):\n                   with open(os.path.join(storage_path, session_id), 'w') as f:\n                       f.write(\"[]\")\n    \"\"\"\n\n    messages: list[BaseMessage]\n    \"\"\"A property or attribute that returns a list of messages.\n\n    In general, getting the messages may involve IO to the underlying\n    persistence layer, so this operation is expected to incur some\n    latency.\n    \"\"\"\n\n    async def aget_messages(self) -> list[BaseMessage]:\n        \"\"\"Async version of getting messages.\n\n        Can over-ride this method to provide an efficient async implementation.\n\n        In general, fetching messages may involve IO to the underlying\n        persistence layer.\n        \"\"\"\n        from langchain_core.runnables.config import run_in_executor\n\n        return await run_in_executor(None, lambda: self.messages)\n\n    def add_user_message(self, message: Union[HumanMessage, str]) -> None:\n        \"\"\"Convenience method for adding a human message string to the store.\n\n        Please note that this is a convenience method. Code should favor the\n        bulk add_messages interface instead to save on round-trips to the underlying\n        persistence layer.\n\n        This method may be deprecated in a future release.\n\n        Args:\n            message: The human message to add to the store.\n        \"\"\"\n        if isinstance(message, HumanMessage):\n            self.add_message(message)\n        else:\n            self.add_message(HumanMessage(content=message))\n\n    def add_ai_message(self, message: Union[AIMessage, str]) -> None:\n        \"\"\"Convenience method for adding an AI message string to the store.\n\n        Please note that this is a convenience method. Code should favor the bulk\n        add_messages interface instead to save on round-trips to the underlying\n        persistence layer.\n\n        This method may be deprecated in a future release.\n\n        Args:\n            message: The AI message to add.\n        \"\"\"\n        if isinstance(message, AIMessage):\n            self.add_message(message)\n        else:\n            self.add_message(AIMessage(content=message))\n\n    def add_message(self, message: BaseMessage) -> None:\n        \"\"\"Add a Message object to the store.\n\n        Args:\n            message: A BaseMessage object to store.\n\n        Raises:\n            NotImplementedError: If the sub-class has not implemented an efficient\n                add_messages method.\n        \"\"\"\n        if type(self).add_messages != BaseChatMessageHistory.add_messages:\n            # This means that the sub-class has implemented an efficient add_messages\n            # method, so we should use it.\n            self.add_messages([message])\n        else:\n            msg = (\n                \"add_message is not implemented for this class. \"\n                \"Please implement add_message or add_messages.\"\n            )\n            raise NotImplementedError(msg)\n\n    def add_messages(self, messages: Sequence[BaseMessage]) -> None:\n        \"\"\"Add a list of messages.\n\n        Implementations should over-ride this method to handle bulk addition of messages\n        in an efficient manner to avoid unnecessary round-trips to the underlying store.\n\n        Args:\n            messages: A sequence of BaseMessage objects to store.\n        \"\"\"\n        for message in messages:\n            self.add_message(message)\n\n    async def aadd_messages(self, messages: Sequence[BaseMessage]) -> None:\n        \"\"\"Async add a list of messages.\n\n        Args:\n            messages: A sequence of BaseMessage objects to store.\n        \"\"\"\n        from langchain_core.runnables.config import run_in_executor\n\n        await run_in_executor(None, self.add_messages, messages)\n\n    @abstractmethod\n    def clear(self) -> None:\n        \"\"\"Remove all messages from the store.\"\"\"\n\n    async def aclear(self) -> None:\n        \"\"\"Async remove all messages from the store.\"\"\"\n        from langchain_core.runnables.config import run_in_executor\n\n        await run_in_executor(None, self.clear)\n\n    def __str__(self) -> str:\n        \"\"\"Return a string representation of the chat history.\"\"\"\n        return get_buffer_string(self.messages)\n\n\nclass InMemoryChatMessageHistory(BaseChatMessageHistory, BaseModel):\n    \"\"\"In memory implementation of chat message history.\n\n    Stores messages in a memory list.\n    \"\"\"\n\n    messages: list[BaseMessage] = Field(default_factory=list)\n    \"\"\"A list of messages stored in memory.\"\"\"\n\n    async def aget_messages(self) -> list[BaseMessage]:\n        \"\"\"Async version of getting messages.\n\n        Can over-ride this method to provide an efficient async implementation.\n        In general, fetching messages may involve IO to the underlying\n        persistence layer.\n\n        Returns:\n            List of messages.\n        \"\"\"\n        return self.messages\n\n    def add_message(self, message: BaseMessage) -> None:\n        \"\"\"Add a self-created message to the store.\n\n        Args:\n            message: The message to add.\n        \"\"\"\n        self.messages.append(message)\n\n    async def aadd_messages(self, messages: Sequence[BaseMessage]) -> None:\n        \"\"\"Async add messages to the store.\n\n        Args:\n            messages: The messages to add.\n        \"\"\"\n        self.add_messages(messages)\n\n    def clear(self) -> None:\n        \"\"\"Clear all messages from the store.\"\"\"\n        self.messages = []\n\n    async def aclear(self) -> None:\n        \"\"\"Async clear all messages from the store.\"\"\"\n        self.clear()\n",
        "patch": "@@ -17,8 +17,7 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n-from typing import Union\n+from typing import TYPE_CHECKING, Union\n \n from pydantic import BaseModel, Field\n \n@@ -29,6 +28,9 @@\n     get_buffer_string,\n )\n \n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n \n class BaseChatMessageHistory(ABC):\n     \"\"\"Abstract base class for storing chat message history."
      },
      {
        "filename": "libs/core/langchain_core/document_loaders/base.py",
        "content_before": "\"\"\"Abstract interface for document loader implementations.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import AsyncIterator, Iterator\nfrom typing import TYPE_CHECKING, Optional\n\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import run_in_executor\n\nif TYPE_CHECKING:\n    from langchain_text_splitters import TextSplitter\n\nfrom langchain_core.documents.base import Blob\n\n\nclass BaseLoader(ABC):  # noqa: B024\n    \"\"\"Interface for Document Loader.\n\n    Implementations should implement the lazy-loading method using generators\n    to avoid loading all Documents into memory at once.\n\n    `load` is provided just for user convenience and should not be overridden.\n    \"\"\"\n\n    # Sub-classes should not implement this method directly. Instead, they\n    # should implement the lazy load method.\n    def load(self) -> list[Document]:\n        \"\"\"Load data into Document objects.\"\"\"\n        return list(self.lazy_load())\n\n    async def aload(self) -> list[Document]:\n        \"\"\"Load data into Document objects.\"\"\"\n        return [document async for document in self.alazy_load()]\n\n    def load_and_split(\n        self, text_splitter: Optional[TextSplitter] = None\n    ) -> list[Document]:\n        \"\"\"Load Documents and split into chunks. Chunks are returned as Documents.\n\n        Do not override this method. It should be considered to be deprecated!\n\n        Args:\n            text_splitter: TextSplitter instance to use for splitting documents.\n              Defaults to RecursiveCharacterTextSplitter.\n\n        Returns:\n            List of Documents.\n        \"\"\"\n        if text_splitter is None:\n            try:\n                from langchain_text_splitters import RecursiveCharacterTextSplitter\n            except ImportError as e:\n                msg = (\n                    \"Unable to import from langchain_text_splitters. Please specify \"\n                    \"text_splitter or install langchain_text_splitters with \"\n                    \"`pip install -U langchain-text-splitters`.\"\n                )\n                raise ImportError(msg) from e\n\n            _text_splitter: TextSplitter = RecursiveCharacterTextSplitter()\n        else:\n            _text_splitter = text_splitter\n        docs = self.load()\n        return _text_splitter.split_documents(docs)\n\n    # Attention: This method will be upgraded into an abstractmethod once it's\n    #            implemented in all the existing subclasses.\n    def lazy_load(self) -> Iterator[Document]:\n        \"\"\"A lazy loader for Documents.\"\"\"\n        if type(self).load != BaseLoader.load:\n            return iter(self.load())\n        msg = f\"{self.__class__.__name__} does not implement lazy_load()\"\n        raise NotImplementedError(msg)\n\n    async def alazy_load(self) -> AsyncIterator[Document]:\n        \"\"\"A lazy loader for Documents.\"\"\"\n        iterator = await run_in_executor(None, self.lazy_load)\n        done = object()\n        while True:\n            doc = await run_in_executor(None, next, iterator, done)  # type: ignore[call-arg, arg-type]\n            if doc is done:\n                break\n            yield doc  # type: ignore[misc]\n\n\nclass BaseBlobParser(ABC):\n    \"\"\"Abstract interface for blob parsers.\n\n    A blob parser provides a way to parse raw data stored in a blob into one\n    or more documents.\n\n    The parser can be composed with blob loaders, making it easy to reuse\n    a parser independent of how the blob was originally loaded.\n    \"\"\"\n\n    @abstractmethod\n    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n        \"\"\"Lazy parsing interface.\n\n        Subclasses are required to implement this method.\n\n        Args:\n            blob: Blob instance\n\n        Returns:\n            Generator of documents\n        \"\"\"\n\n    def parse(self, blob: Blob) -> list[Document]:\n        \"\"\"Eagerly parse the blob into a document or documents.\n\n        This is a convenience method for interactive development environment.\n\n        Production applications should favor the lazy_parse method instead.\n\n        Subclasses should generally not over-ride this parse method.\n\n        Args:\n            blob: Blob instance\n\n        Returns:\n            List of documents\n        \"\"\"\n        return list(self.lazy_parse(blob))\n",
        "patch": "@@ -3,16 +3,17 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from collections.abc import AsyncIterator, Iterator\n from typing import TYPE_CHECKING, Optional\n \n-from langchain_core.documents import Document\n from langchain_core.runnables import run_in_executor\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Iterator\n+\n     from langchain_text_splitters import TextSplitter\n \n-from langchain_core.documents.base import Blob\n+    from langchain_core.documents import Document\n+    from langchain_core.documents.base import Blob\n \n \n class BaseLoader(ABC):  # noqa: B024"
      },
      {
        "filename": "libs/core/langchain_core/document_loaders/blob_loaders.py",
        "content_before": "\"\"\"Schema for Blobs and Blob Loaders.\n\nThe goal is to facilitate decoupling of content loading from content parsing code.\n\nIn addition, content loading code should provide a lazy loading interface by default.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable\n\n# Re-export Blob and PathLike for backwards compatibility\nfrom langchain_core.documents.base import Blob as Blob\nfrom langchain_core.documents.base import PathLike as PathLike\n\n\nclass BlobLoader(ABC):\n    \"\"\"Abstract interface for blob loaders implementation.\n\n    Implementer should be able to load raw content from a storage system according\n    to some criteria and return the raw content lazily as a stream of blobs.\n    \"\"\"\n\n    @abstractmethod\n    def yield_blobs(\n        self,\n    ) -> Iterable[Blob]:\n        \"\"\"A lazy loader for raw data represented by LangChain's Blob object.\n\n        Returns:\n            A generator over blobs\n        \"\"\"\n\n\n# Re-export Blob and Pathlike for backwards compatibility\n__all__ = [\"Blob\", \"BlobLoader\", \"PathLike\"]\n",
        "patch": "@@ -8,12 +8,15 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from collections.abc import Iterable\n+from typing import TYPE_CHECKING\n \n # Re-export Blob and PathLike for backwards compatibility\n from langchain_core.documents.base import Blob as Blob\n from langchain_core.documents.base import PathLike as PathLike\n \n+if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n \n class BlobLoader(ABC):\n     \"\"\"Abstract interface for blob loaders implementation."
      },
      {
        "filename": "libs/core/langchain_core/documents/base.py",
        "content_before": "from __future__ import annotations\n\nimport contextlib\nimport mimetypes\nfrom collections.abc import Generator\nfrom io import BufferedReader, BytesIO\nfrom pathlib import PurePath\nfrom typing import Any, Literal, Optional, Union, cast\n\nfrom pydantic import ConfigDict, Field, field_validator, model_validator\n\nfrom langchain_core.load.serializable import Serializable\n\nPathLike = Union[str, PurePath]\n\n\nclass BaseMedia(Serializable):\n    \"\"\"Use to represent media content.\n\n    Media objects can be used to represent raw data, such as text or binary data.\n\n    LangChain Media objects allow associating metadata and an optional identifier\n    with the content.\n\n    The presence of an ID and metadata make it easier to store, index, and search\n    over the content in a structured way.\n    \"\"\"\n\n    # The ID field is optional at the moment.\n    # It will likely become required in a future major release after\n    # it has been adopted by enough vectorstore implementations.\n    id: Optional[str] = None\n    \"\"\"An optional identifier for the document.\n\n    Ideally this should be unique across the document collection and formatted\n    as a UUID, but this will not be enforced.\n\n    .. versionadded:: 0.2.11\n    \"\"\"\n\n    metadata: dict = Field(default_factory=dict)\n    \"\"\"Arbitrary metadata associated with the content.\"\"\"\n\n    @field_validator(\"id\", mode=\"before\")\n    def cast_id_to_str(cls, id_value: Any) -> Optional[str]:\n        if id_value is not None:\n            return str(id_value)\n        else:\n            return id_value\n\n\nclass Blob(BaseMedia):\n    \"\"\"Blob represents raw data by either reference or value.\n\n    Provides an interface to materialize the blob in different representations, and\n    help to decouple the development of data loaders from the downstream parsing of\n    the raw data.\n\n    Inspired by: https://developer.mozilla.org/en-US/docs/Web/API/Blob\n\n    Example: Initialize a blob from in-memory data\n\n        .. code-block:: python\n\n            from langchain_core.documents import Blob\n\n            blob = Blob.from_data(\"Hello, world!\")\n\n            # Read the blob as a string\n            print(blob.as_string())\n\n            # Read the blob as bytes\n            print(blob.as_bytes())\n\n            # Read the blob as a byte stream\n            with blob.as_bytes_io() as f:\n                print(f.read())\n\n    Example: Load from memory and specify mime-type and metadata\n\n        .. code-block:: python\n\n            from langchain_core.documents import Blob\n\n            blob = Blob.from_data(\n                data=\"Hello, world!\",\n                mime_type=\"text/plain\",\n                metadata={\"source\": \"https://example.com\"}\n            )\n\n    Example: Load the blob from a file\n\n        .. code-block:: python\n\n            from langchain_core.documents import Blob\n\n            blob = Blob.from_path(\"path/to/file.txt\")\n\n            # Read the blob as a string\n            print(blob.as_string())\n\n            # Read the blob as bytes\n            print(blob.as_bytes())\n\n            # Read the blob as a byte stream\n            with blob.as_bytes_io() as f:\n                print(f.read())\n    \"\"\"\n\n    data: Union[bytes, str, None] = None\n    \"\"\"Raw data associated with the blob.\"\"\"\n    mimetype: Optional[str] = None\n    \"\"\"MimeType not to be confused with a file extension.\"\"\"\n    encoding: str = \"utf-8\"\n    \"\"\"Encoding to use if decoding the bytes into a string.\n\n    Use utf-8 as default encoding, if decoding to string.\n    \"\"\"\n    path: Optional[PathLike] = None\n    \"\"\"Location where the original content was found.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        frozen=True,\n    )\n\n    @property\n    def source(self) -> Optional[str]:\n        \"\"\"The source location of the blob as string if known otherwise none.\n\n        If a path is associated with the blob, it will default to the path location.\n\n        Unless explicitly set via a metadata field called \"source\", in which\n        case that value will be used instead.\n        \"\"\"\n        if self.metadata and \"source\" in self.metadata:\n            return cast(Optional[str], self.metadata[\"source\"])\n        return str(self.path) if self.path else None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_blob_is_valid(cls, values: dict[str, Any]) -> Any:\n        \"\"\"Verify that either data or path is provided.\"\"\"\n        if \"data\" not in values and \"path\" not in values:\n            msg = \"Either data or path must be provided\"\n            raise ValueError(msg)\n        return values\n\n    def as_string(self) -> str:\n        \"\"\"Read data as a string.\"\"\"\n        if self.data is None and self.path:\n            with open(str(self.path), encoding=self.encoding) as f:\n                return f.read()\n        elif isinstance(self.data, bytes):\n            return self.data.decode(self.encoding)\n        elif isinstance(self.data, str):\n            return self.data\n        else:\n            msg = f\"Unable to get string for blob {self}\"\n            raise ValueError(msg)\n\n    def as_bytes(self) -> bytes:\n        \"\"\"Read data as bytes.\"\"\"\n        if isinstance(self.data, bytes):\n            return self.data\n        elif isinstance(self.data, str):\n            return self.data.encode(self.encoding)\n        elif self.data is None and self.path:\n            with open(str(self.path), \"rb\") as f:\n                return f.read()\n        else:\n            msg = f\"Unable to get bytes for blob {self}\"\n            raise ValueError(msg)\n\n    @contextlib.contextmanager\n    def as_bytes_io(self) -> Generator[Union[BytesIO, BufferedReader], None, None]:\n        \"\"\"Read data as a byte stream.\"\"\"\n        if isinstance(self.data, bytes):\n            yield BytesIO(self.data)\n        elif self.data is None and self.path:\n            with open(str(self.path), \"rb\") as f:\n                yield f\n        else:\n            msg = f\"Unable to convert blob {self}\"\n            raise NotImplementedError(msg)\n\n    @classmethod\n    def from_path(\n        cls,\n        path: PathLike,\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        guess_type: bool = True,\n        metadata: Optional[dict] = None,\n    ) -> Blob:\n        \"\"\"Load the blob from a path like object.\n\n        Args:\n            path: path like object to file to be read\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            guess_type: If True, the mimetype will be guessed from the file extension,\n                        if a mime-type was not provided\n            metadata: Metadata to associate with the blob\n\n        Returns:\n            Blob instance\n        \"\"\"\n        if mime_type is None and guess_type:\n            _mimetype = mimetypes.guess_type(path)[0] if guess_type else None\n        else:\n            _mimetype = mime_type\n        # We do not load the data immediately, instead we treat the blob as a\n        # reference to the underlying data.\n        return cls(\n            data=None,\n            mimetype=_mimetype,\n            encoding=encoding,\n            path=path,\n            metadata=metadata if metadata is not None else {},\n        )\n\n    @classmethod\n    def from_data(\n        cls,\n        data: Union[str, bytes],\n        *,\n        encoding: str = \"utf-8\",\n        mime_type: Optional[str] = None,\n        path: Optional[str] = None,\n        metadata: Optional[dict] = None,\n    ) -> Blob:\n        \"\"\"Initialize the blob from in-memory data.\n\n        Args:\n            data: the in-memory data associated with the blob\n            encoding: Encoding to use if decoding the bytes into a string\n            mime_type: if provided, will be set as the mime-type of the data\n            path: if provided, will be set as the source from which the data came\n            metadata: Metadata to associate with the blob\n\n        Returns:\n            Blob instance\n        \"\"\"\n        return cls(\n            data=data,\n            mimetype=mime_type,\n            encoding=encoding,\n            path=path,\n            metadata=metadata if metadata is not None else {},\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Define the blob representation.\"\"\"\n        str_repr = f\"Blob {id(self)}\"\n        if self.source:\n            str_repr += f\" {self.source}\"\n        return str_repr\n\n\nclass Document(BaseMedia):\n    \"\"\"Class for storing a piece of text and associated metadata.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.documents import Document\n\n            document = Document(\n                page_content=\"Hello, world!\",\n                metadata={\"source\": \"https://example.com\"}\n            )\n    \"\"\"\n\n    page_content: str\n    \"\"\"String text.\"\"\"\n    type: Literal[\"Document\"] = \"Document\"\n\n    def __init__(self, page_content: str, **kwargs: Any) -> None:\n        \"\"\"Pass page_content in as positional or named arg.\"\"\"\n        # my-py is complaining that page_content is not defined on the base class.\n        # Here, we're relying on pydantic base class to handle the validation.\n        super().__init__(page_content=page_content, **kwargs)  # type: ignore[call-arg]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this class is serializable.\"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"document\"]\n\n    def __str__(self) -> str:\n        \"\"\"Override __str__ to restrict it to page_content and metadata.\"\"\"\n        # The format matches pydantic format for __str__.\n        #\n        # The purpose of this change is to make sure that user code that\n        # feeds Document objects directly into prompts remains unchanged\n        # due to the addition of the id field (or any other fields in the future).\n        #\n        # This override will likely be removed in the future in favor of\n        # a more general solution of formatting content directly inside the prompts.\n        if self.metadata:\n            return f\"page_content='{self.page_content}' metadata={self.metadata}\"\n        else:\n            return f\"page_content='{self.page_content}'\"\n",
        "patch": "@@ -2,15 +2,17 @@\n \n import contextlib\n import mimetypes\n-from collections.abc import Generator\n from io import BufferedReader, BytesIO\n from pathlib import PurePath\n-from typing import Any, Literal, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, Literal, Optional, Union, cast\n \n from pydantic import ConfigDict, Field, field_validator, model_validator\n \n from langchain_core.load.serializable import Serializable\n \n+if TYPE_CHECKING:\n+    from collections.abc import Generator\n+\n PathLike = Union[str, PurePath]\n \n "
      },
      {
        "filename": "libs/core/langchain_core/documents/compressor.py",
        "content_before": "from __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom typing import Optional\n\nfrom pydantic import BaseModel\n\nfrom langchain_core.callbacks import Callbacks\nfrom langchain_core.documents import Document\nfrom langchain_core.runnables import run_in_executor\n\n\nclass BaseDocumentCompressor(BaseModel, ABC):\n    \"\"\"Base class for document compressors.\n\n    This abstraction is primarily used for\n    post-processing of retrieved documents.\n\n    Documents matching a given query are first retrieved.\n    Then the list of documents can be further processed.\n\n    For example, one could re-rank the retrieved documents\n    using an LLM.\n\n    **Note** users should favor using a RunnableLambda\n    instead of sub-classing from this interface.\n    \"\"\"\n\n    @abstractmethod\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[Callbacks] = None,\n    ) -> Sequence[Document]:\n        \"\"\"Compress retrieved documents given the query context.\n\n        Args:\n            documents: The retrieved documents.\n            query: The query context.\n            callbacks: Optional callbacks to run during compression.\n\n        Returns:\n            The compressed documents.\n        \"\"\"\n\n    async def acompress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[Callbacks] = None,\n    ) -> Sequence[Document]:\n        \"\"\"Async compress retrieved documents given the query context.\n\n        Args:\n            documents: The retrieved documents.\n            query: The query context.\n            callbacks: Optional callbacks to run during compression.\n\n        Returns:\n            The compressed documents.\n        \"\"\"\n        return await run_in_executor(\n            None, self.compress_documents, documents, query, callbacks\n        )\n",
        "patch": "@@ -1,15 +1,18 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n-from typing import Optional\n+from typing import TYPE_CHECKING, Optional\n \n from pydantic import BaseModel\n \n-from langchain_core.callbacks import Callbacks\n-from langchain_core.documents import Document\n from langchain_core.runnables import run_in_executor\n \n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n+    from langchain_core.callbacks import Callbacks\n+    from langchain_core.documents import Document\n+\n \n class BaseDocumentCompressor(BaseModel, ABC):\n     \"\"\"Base class for document compressors."
      },
      {
        "filename": "libs/core/langchain_core/documents/transformers.py",
        "content_before": "from __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\n\nfrom langchain_core.runnables.config import run_in_executor\n\nif TYPE_CHECKING:\n    from langchain_core.documents import Document\n\n\nclass BaseDocumentTransformer(ABC):\n    \"\"\"Abstract base class for document transformation.\n\n    A document transformation takes a sequence of Documents and returns a\n    sequence of transformed Documents.\n\n    Example:\n        .. code-block:: python\n\n            class EmbeddingsRedundantFilter(BaseDocumentTransformer, BaseModel):\n                embeddings: Embeddings\n                similarity_fn: Callable = cosine_similarity\n                similarity_threshold: float = 0.95\n\n                class Config:\n                    arbitrary_types_allowed = True\n\n                def transform_documents(\n                    self, documents: Sequence[Document], **kwargs: Any\n                ) -> Sequence[Document]:\n                    stateful_documents = get_stateful_documents(documents)\n                    embedded_documents = _get_embeddings_from_stateful_docs(\n                        self.embeddings, stateful_documents\n                    )\n                    included_idxs = _filter_similar_embeddings(\n                        embedded_documents, self.similarity_fn, self.similarity_threshold\n                    )\n                    return [stateful_documents[i] for i in sorted(included_idxs)]\n\n                async def atransform_documents(\n                    self, documents: Sequence[Document], **kwargs: Any\n                ) -> Sequence[Document]:\n                    raise NotImplementedError\n\n    \"\"\"  # noqa: E501\n\n    @abstractmethod\n    def transform_documents(\n        self, documents: Sequence[Document], **kwargs: Any\n    ) -> Sequence[Document]:\n        \"\"\"Transform a list of documents.\n\n        Args:\n            documents: A sequence of Documents to be transformed.\n\n        Returns:\n            A sequence of transformed Documents.\n        \"\"\"\n\n    async def atransform_documents(\n        self, documents: Sequence[Document], **kwargs: Any\n    ) -> Sequence[Document]:\n        \"\"\"Asynchronously transform a list of documents.\n\n        Args:\n            documents: A sequence of Documents to be transformed.\n\n        Returns:\n            A sequence of transformed Documents.\n        \"\"\"\n        return await run_in_executor(\n            None, self.transform_documents, documents, **kwargs\n        )\n",
        "patch": "@@ -1,12 +1,13 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n from typing import TYPE_CHECKING, Any\n \n from langchain_core.runnables.config import run_in_executor\n \n if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n     from langchain_core.documents import Document\n \n "
      },
      {
        "filename": "libs/core/langchain_core/example_selectors/semantic_similarity.py",
        "content_before": "\"\"\"Example selector that selects examples based on SemanticSimilarity.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC\nfrom typing import TYPE_CHECKING, Any, Optional\n\nfrom pydantic import BaseModel, ConfigDict\n\nfrom langchain_core.documents import Document\nfrom langchain_core.example_selectors.base import BaseExampleSelector\nfrom langchain_core.vectorstores import VectorStore\n\nif TYPE_CHECKING:\n    from langchain_core.embeddings import Embeddings\n\n\ndef sorted_values(values: dict[str, str]) -> list[Any]:\n    \"\"\"Return a list of values in dict sorted by key.\n\n    Args:\n        values: A dictionary with keys as input variables\n            and values as their values.\n\n    Returns:\n        A list of values in dict sorted by key.\n    \"\"\"\n    return [values[val] for val in sorted(values)]\n\n\nclass _VectorStoreExampleSelector(BaseExampleSelector, BaseModel, ABC):\n    \"\"\"Example selector that selects examples based on SemanticSimilarity.\"\"\"\n\n    vectorstore: VectorStore\n    \"\"\"VectorStore that contains information about examples.\"\"\"\n    k: int = 4\n    \"\"\"Number of examples to select.\"\"\"\n    example_keys: Optional[list[str]] = None\n    \"\"\"Optional keys to filter examples to.\"\"\"\n    input_keys: Optional[list[str]] = None\n    \"\"\"Optional keys to filter input to. If provided, the search is based on\n    the input variables instead of all variables.\"\"\"\n    vectorstore_kwargs: Optional[dict[str, Any]] = None\n    \"\"\"Extra arguments passed to similarity_search function of the vectorstore.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"forbid\",\n    )\n\n    @staticmethod\n    def _example_to_text(\n        example: dict[str, str], input_keys: Optional[list[str]]\n    ) -> str:\n        if input_keys:\n            return \" \".join(sorted_values({key: example[key] for key in input_keys}))\n        else:\n            return \" \".join(sorted_values(example))\n\n    def _documents_to_examples(self, documents: list[Document]) -> list[dict]:\n        # Get the examples from the metadata.\n        # This assumes that examples are stored in metadata.\n        examples = [dict(e.metadata) for e in documents]\n        # If example keys are provided, filter examples to those keys.\n        if self.example_keys:\n            examples = [{k: eg[k] for k in self.example_keys} for eg in examples]\n        return examples\n\n    def add_example(self, example: dict[str, str]) -> str:\n        \"\"\"Add a new example to vectorstore.\n\n        Args:\n            example: A dictionary with keys as input variables\n                and values as their values.\n\n        Returns:\n            The ID of the added example.\n        \"\"\"\n        ids = self.vectorstore.add_texts(\n            [self._example_to_text(example, self.input_keys)], metadatas=[example]\n        )\n        return ids[0]\n\n    async def aadd_example(self, example: dict[str, str]) -> str:\n        \"\"\"Async add new example to vectorstore.\n\n        Args:\n            example: A dictionary with keys as input variables\n                and values as their values.\n\n        Returns:\n            The ID of the added example.\n        \"\"\"\n        ids = await self.vectorstore.aadd_texts(\n            [self._example_to_text(example, self.input_keys)], metadatas=[example]\n        )\n        return ids[0]\n\n\nclass SemanticSimilarityExampleSelector(_VectorStoreExampleSelector):\n    \"\"\"Select examples based on semantic similarity.\"\"\"\n\n    def select_examples(self, input_variables: dict[str, str]) -> list[dict]:\n        \"\"\"Select examples based on semantic similarity.\n\n        Args:\n            input_variables: The input variables to use for search.\n\n        Returns:\n            The selected examples.\n        \"\"\"\n        # Get the docs with the highest similarity.\n        vectorstore_kwargs = self.vectorstore_kwargs or {}\n        example_docs = self.vectorstore.similarity_search(\n            self._example_to_text(input_variables, self.input_keys),\n            k=self.k,\n            **vectorstore_kwargs,\n        )\n        return self._documents_to_examples(example_docs)\n\n    async def aselect_examples(self, input_variables: dict[str, str]) -> list[dict]:\n        \"\"\"Asynchronously select examples based on semantic similarity.\n\n        Args:\n            input_variables: The input variables to use for search.\n\n        Returns:\n            The selected examples.\n        \"\"\"\n        # Get the docs with the highest similarity.\n        vectorstore_kwargs = self.vectorstore_kwargs or {}\n        example_docs = await self.vectorstore.asimilarity_search(\n            self._example_to_text(input_variables, self.input_keys),\n            k=self.k,\n            **vectorstore_kwargs,\n        )\n        return self._documents_to_examples(example_docs)\n\n    @classmethod\n    def from_examples(\n        cls,\n        examples: list[dict],\n        embeddings: Embeddings,\n        vectorstore_cls: type[VectorStore],\n        k: int = 4,\n        input_keys: Optional[list[str]] = None,\n        *,\n        example_keys: Optional[list[str]] = None,\n        vectorstore_kwargs: Optional[dict] = None,\n        **vectorstore_cls_kwargs: Any,\n    ) -> SemanticSimilarityExampleSelector:\n        \"\"\"Create k-shot example selector using example list and embeddings.\n\n        Reshuffles examples dynamically based on query similarity.\n\n        Args:\n            examples: List of examples to use in the prompt.\n            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().\n            vectorstore_cls: A vector store DB interface class, e.g. FAISS.\n            k: Number of examples to select. Default is 4.\n            input_keys: If provided, the search is based on the input variables\n                instead of all variables.\n            example_keys: If provided, keys to filter examples to.\n            vectorstore_kwargs: Extra arguments passed to similarity_search function\n                of the vectorstore.\n            vectorstore_cls_kwargs: optional kwargs containing url for vector store\n\n        Returns:\n            The ExampleSelector instantiated, backed by a vector store.\n        \"\"\"\n        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]\n        vectorstore = vectorstore_cls.from_texts(\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\n        )\n        return cls(\n            vectorstore=vectorstore,\n            k=k,\n            input_keys=input_keys,\n            example_keys=example_keys,\n            vectorstore_kwargs=vectorstore_kwargs,\n        )\n\n    @classmethod\n    async def afrom_examples(\n        cls,\n        examples: list[dict],\n        embeddings: Embeddings,\n        vectorstore_cls: type[VectorStore],\n        k: int = 4,\n        input_keys: Optional[list[str]] = None,\n        *,\n        example_keys: Optional[list[str]] = None,\n        vectorstore_kwargs: Optional[dict] = None,\n        **vectorstore_cls_kwargs: Any,\n    ) -> SemanticSimilarityExampleSelector:\n        \"\"\"Async create k-shot example selector using example list and embeddings.\n\n        Reshuffles examples dynamically based on query similarity.\n\n        Args:\n            examples: List of examples to use in the prompt.\n            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().\n            vectorstore_cls: A vector store DB interface class, e.g. FAISS.\n            k: Number of examples to select. Default is 4.\n            input_keys: If provided, the search is based on the input variables\n                instead of all variables.\n            example_keys: If provided, keys to filter examples to.\n            vectorstore_kwargs: Extra arguments passed to similarity_search function\n                of the vectorstore.\n            vectorstore_cls_kwargs: optional kwargs containing url for vector store\n\n        Returns:\n            The ExampleSelector instantiated, backed by a vector store.\n        \"\"\"\n        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]\n        vectorstore = await vectorstore_cls.afrom_texts(\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\n        )\n        return cls(\n            vectorstore=vectorstore,\n            k=k,\n            input_keys=input_keys,\n            example_keys=example_keys,\n            vectorstore_kwargs=vectorstore_kwargs,\n        )\n\n\nclass MaxMarginalRelevanceExampleSelector(_VectorStoreExampleSelector):\n    \"\"\"Select examples based on Max Marginal Relevance.\n\n    This was shown to improve performance in this paper:\n    https://arxiv.org/pdf/2211.13892.pdf\n    \"\"\"\n\n    fetch_k: int = 20\n    \"\"\"Number of examples to fetch to rerank.\"\"\"\n\n    def select_examples(self, input_variables: dict[str, str]) -> list[dict]:\n        \"\"\"Select examples based on Max Marginal Relevance.\n\n        Args:\n            input_variables: The input variables to use for search.\n\n        Returns:\n            The selected examples.\n        \"\"\"\n        example_docs = self.vectorstore.max_marginal_relevance_search(\n            self._example_to_text(input_variables, self.input_keys),\n            k=self.k,\n            fetch_k=self.fetch_k,\n        )\n        return self._documents_to_examples(example_docs)\n\n    async def aselect_examples(self, input_variables: dict[str, str]) -> list[dict]:\n        \"\"\"Asynchronously select examples based on Max Marginal Relevance.\n\n        Args:\n            input_variables: The input variables to use for search.\n\n        Returns:\n            The selected examples.\n        \"\"\"\n        example_docs = await self.vectorstore.amax_marginal_relevance_search(\n            self._example_to_text(input_variables, self.input_keys),\n            k=self.k,\n            fetch_k=self.fetch_k,\n        )\n        return self._documents_to_examples(example_docs)\n\n    @classmethod\n    def from_examples(\n        cls,\n        examples: list[dict],\n        embeddings: Embeddings,\n        vectorstore_cls: type[VectorStore],\n        k: int = 4,\n        input_keys: Optional[list[str]] = None,\n        fetch_k: int = 20,\n        example_keys: Optional[list[str]] = None,\n        vectorstore_kwargs: Optional[dict] = None,\n        **vectorstore_cls_kwargs: Any,\n    ) -> MaxMarginalRelevanceExampleSelector:\n        \"\"\"Create k-shot example selector using example list and embeddings.\n\n        Reshuffles examples dynamically based on Max Marginal Relevance.\n\n        Args:\n            examples: List of examples to use in the prompt.\n            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().\n            vectorstore_cls: A vector store DB interface class, e.g. FAISS.\n            k: Number of examples to select. Default is 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n                Default is 20.\n            input_keys: If provided, the search is based on the input variables\n                instead of all variables.\n            example_keys: If provided, keys to filter examples to.\n            vectorstore_kwargs: Extra arguments passed to similarity_search function\n                of the vectorstore.\n            vectorstore_cls_kwargs: optional kwargs containing url for vector store\n\n        Returns:\n            The ExampleSelector instantiated, backed by a vector store.\n        \"\"\"\n        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]\n        vectorstore = vectorstore_cls.from_texts(\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\n        )\n        return cls(\n            vectorstore=vectorstore,\n            k=k,\n            fetch_k=fetch_k,\n            input_keys=input_keys,\n            example_keys=example_keys,\n            vectorstore_kwargs=vectorstore_kwargs,\n        )\n\n    @classmethod\n    async def afrom_examples(\n        cls,\n        examples: list[dict],\n        embeddings: Embeddings,\n        vectorstore_cls: type[VectorStore],\n        *,\n        k: int = 4,\n        input_keys: Optional[list[str]] = None,\n        fetch_k: int = 20,\n        example_keys: Optional[list[str]] = None,\n        vectorstore_kwargs: Optional[dict] = None,\n        **vectorstore_cls_kwargs: Any,\n    ) -> MaxMarginalRelevanceExampleSelector:\n        \"\"\"Asynchronously create k-shot example selector using example list and\n        embeddings.\n\n        Reshuffles examples dynamically based on Max Marginal Relevance.\n\n        Args:\n            examples: List of examples to use in the prompt.\n            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().\n            vectorstore_cls: A vector store DB interface class, e.g. FAISS.\n            k: Number of examples to select. Default is 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n                Default is 20.\n            input_keys: If provided, the search is based on the input variables\n                instead of all variables.\n            example_keys: If provided, keys to filter examples to.\n            vectorstore_kwargs: Extra arguments passed to similarity_search function\n                of the vectorstore.\n            vectorstore_cls_kwargs: optional kwargs containing url for vector store\n\n        Returns:\n            The ExampleSelector instantiated, backed by a vector store.\n        \"\"\"\n        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]\n        vectorstore = await vectorstore_cls.afrom_texts(\n            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\n        )\n        return cls(\n            vectorstore=vectorstore,\n            k=k,\n            fetch_k=fetch_k,\n            input_keys=input_keys,\n            example_keys=example_keys,\n            vectorstore_kwargs=vectorstore_kwargs,\n        )\n",
        "patch": "@@ -7,11 +7,11 @@\n \n from pydantic import BaseModel, ConfigDict\n \n-from langchain_core.documents import Document\n from langchain_core.example_selectors.base import BaseExampleSelector\n from langchain_core.vectorstores import VectorStore\n \n if TYPE_CHECKING:\n+    from langchain_core.documents import Document\n     from langchain_core.embeddings import Embeddings\n \n "
      },
      {
        "filename": "libs/core/langchain_core/indexing/base.py",
        "content_before": "from __future__ import annotations\n\nimport abc\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom typing import Any, Optional, TypedDict\n\nfrom langchain_core._api import beta\nfrom langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.runnables import run_in_executor\n\n\nclass RecordManager(ABC):\n    \"\"\"Abstract base class representing the interface for a record manager.\n\n    The record manager abstraction is used by the langchain indexing API.\n\n    The record manager keeps track of which documents have been\n    written into a vectorstore and when they were written.\n\n    The indexing API computes hashes for each document and stores the hash\n    together with the write time and the source id in the record manager.\n\n    On subsequent indexing runs, the indexing API can check the record manager\n    to determine which documents have already been indexed and which have not.\n\n    This allows the indexing API to avoid re-indexing documents that have\n    already been indexed, and to only index new documents.\n\n    The main benefit of this abstraction is that it works across many vectorstores.\n    To be supported, a vectorstore needs to only support the ability to add and\n    delete documents by ID. Using the record manager, the indexing API will\n    be able to delete outdated documents and avoid redundant indexing of documents\n    that have already been indexed.\n\n    The main constraints of this abstraction are:\n\n    1. It relies on the time-stamps to determine which documents have been\n       indexed and which have not. This means that the time-stamps must be\n       monotonically increasing. The timestamp should be the timestamp\n       as measured by the server to minimize issues.\n    2. The record manager is currently implemented separately from the\n       vectorstore, which means that the overall system becomes distributed\n       and may create issues with consistency. For example, writing to\n       record manager succeeds, but corresponding writing to vectorstore fails.\n    \"\"\"\n\n    def __init__(\n        self,\n        namespace: str,\n    ) -> None:\n        \"\"\"Initialize the record manager.\n\n        Args:\n            namespace (str): The namespace for the record manager.\n        \"\"\"\n        self.namespace = namespace\n\n    @abstractmethod\n    def create_schema(self) -> None:\n        \"\"\"Create the database schema for the record manager.\"\"\"\n\n    @abstractmethod\n    async def acreate_schema(self) -> None:\n        \"\"\"Asynchronously create the database schema for the record manager.\"\"\"\n\n    @abstractmethod\n    def get_time(self) -> float:\n        \"\"\"Get the current server time as a high resolution timestamp!\n\n        It's important to get this from the server to ensure a monotonic clock,\n        otherwise there may be data loss when cleaning up old documents!\n\n        Returns:\n            The current server time as a float timestamp.\n        \"\"\"\n\n    @abstractmethod\n    async def aget_time(self) -> float:\n        \"\"\"Asynchronously get the current server time as a high resolution timestamp.\n\n        It's important to get this from the server to ensure a monotonic clock,\n        otherwise there may be data loss when cleaning up old documents!\n\n        Returns:\n            The current server time as a float timestamp.\n        \"\"\"\n\n    @abstractmethod\n    def update(\n        self,\n        keys: Sequence[str],\n        *,\n        group_ids: Optional[Sequence[Optional[str]]] = None,\n        time_at_least: Optional[float] = None,\n    ) -> None:\n        \"\"\"Upsert records into the database.\n\n        Args:\n            keys: A list of record keys to upsert.\n            group_ids: A list of group IDs corresponding to the keys.\n            time_at_least: Optional timestamp. Implementation can use this\n                to optionally verify that the timestamp IS at least this time\n                in the system that stores the data.\n\n                e.g., use to validate that the time in the postgres database\n                is equal to or larger than the given timestamp, if not\n                raise an error.\n\n                This is meant to help prevent time-drift issues since\n                time may not be monotonically increasing!\n\n        Raises:\n            ValueError: If the length of keys doesn't match the length of group_ids.\n        \"\"\"\n\n    @abstractmethod\n    async def aupdate(\n        self,\n        keys: Sequence[str],\n        *,\n        group_ids: Optional[Sequence[Optional[str]]] = None,\n        time_at_least: Optional[float] = None,\n    ) -> None:\n        \"\"\"Asynchronously upsert records into the database.\n\n        Args:\n            keys: A list of record keys to upsert.\n            group_ids: A list of group IDs corresponding to the keys.\n            time_at_least: Optional timestamp. Implementation can use this\n                to optionally verify that the timestamp IS at least this time\n                in the system that stores the data.\n\n                e.g., use to validate that the time in the postgres database\n                is equal to or larger than the given timestamp, if not\n                raise an error.\n\n                This is meant to help prevent time-drift issues since\n                time may not be monotonically increasing!\n\n        Raises:\n            ValueError: If the length of keys doesn't match the length of group_ids.\n        \"\"\"\n\n    @abstractmethod\n    def exists(self, keys: Sequence[str]) -> list[bool]:\n        \"\"\"Check if the provided keys exist in the database.\n\n        Args:\n            keys: A list of keys to check.\n\n        Returns:\n            A list of boolean values indicating the existence of each key.\n        \"\"\"\n\n    @abstractmethod\n    async def aexists(self, keys: Sequence[str]) -> list[bool]:\n        \"\"\"Asynchronously check if the provided keys exist in the database.\n\n        Args:\n            keys: A list of keys to check.\n\n        Returns:\n            A list of boolean values indicating the existence of each key.\n        \"\"\"\n\n    @abstractmethod\n    def list_keys(\n        self,\n        *,\n        before: Optional[float] = None,\n        after: Optional[float] = None,\n        group_ids: Optional[Sequence[str]] = None,\n        limit: Optional[int] = None,\n    ) -> list[str]:\n        \"\"\"List records in the database based on the provided filters.\n\n        Args:\n            before: Filter to list records updated before this time.\n            after: Filter to list records updated after this time.\n            group_ids: Filter to list records with specific group IDs.\n            limit: optional limit on the number of records to return.\n\n        Returns:\n            A list of keys for the matching records.\n        \"\"\"\n\n    @abstractmethod\n    async def alist_keys(\n        self,\n        *,\n        before: Optional[float] = None,\n        after: Optional[float] = None,\n        group_ids: Optional[Sequence[str]] = None,\n        limit: Optional[int] = None,\n    ) -> list[str]:\n        \"\"\"Asynchronously list records in the database based on the provided filters.\n\n        Args:\n            before: Filter to list records updated before this time.\n            after: Filter to list records updated after this time.\n            group_ids: Filter to list records with specific group IDs.\n            limit: optional limit on the number of records to return.\n\n        Returns:\n            A list of keys for the matching records.\n        \"\"\"\n\n    @abstractmethod\n    def delete_keys(self, keys: Sequence[str]) -> None:\n        \"\"\"Delete specified records from the database.\n\n        Args:\n            keys: A list of keys to delete.\n        \"\"\"\n\n    @abstractmethod\n    async def adelete_keys(self, keys: Sequence[str]) -> None:\n        \"\"\"Asynchronously delete specified records from the database.\n\n        Args:\n            keys: A list of keys to delete.\n        \"\"\"\n\n\nclass _Record(TypedDict):\n    group_id: Optional[str]\n    updated_at: float\n\n\nclass InMemoryRecordManager(RecordManager):\n    \"\"\"An in-memory record manager for testing purposes.\"\"\"\n\n    def __init__(self, namespace: str) -> None:\n        \"\"\"Initialize the in-memory record manager.\n\n        Args:\n            namespace (str): The namespace for the record manager.\n        \"\"\"\n        super().__init__(namespace)\n        # Each key points to a dictionary\n        # of {'group_id': group_id, 'updated_at': timestamp}\n        self.records: dict[str, _Record] = {}\n        self.namespace = namespace\n\n    def create_schema(self) -> None:\n        \"\"\"In-memory schema creation is simply ensuring the structure is initialized.\"\"\"\n\n    async def acreate_schema(self) -> None:\n        \"\"\"Async in-memory schema creation is simply ensuring\n        the structure is initialized.\n        \"\"\"\n\n    def get_time(self) -> float:\n        \"\"\"Get the current server time as a high resolution timestamp!\"\"\"\n        return time.time()\n\n    async def aget_time(self) -> float:\n        \"\"\"Async get the current server time as a high resolution timestamp!\"\"\"\n        return self.get_time()\n\n    def update(\n        self,\n        keys: Sequence[str],\n        *,\n        group_ids: Optional[Sequence[Optional[str]]] = None,\n        time_at_least: Optional[float] = None,\n    ) -> None:\n        \"\"\"Upsert records into the database.\n\n        Args:\n            keys: A list of record keys to upsert.\n            group_ids: A list of group IDs corresponding to the keys.\n                Defaults to None.\n            time_at_least: Optional timestamp. Implementation can use this\n                to optionally verify that the timestamp IS at least this time\n                in the system that stores. Defaults to None.\n                E.g., use to validate that the time in the postgres database\n                is equal to or larger than the given timestamp, if not\n                raise an error.\n                This is meant to help prevent time-drift issues since\n                time may not be monotonically increasing!\n\n        Raises:\n            ValueError: If the length of keys doesn't match the length of group\n                ids.\n            ValueError: If time_at_least is in the future.\n        \"\"\"\n        if group_ids and len(keys) != len(group_ids):\n            msg = \"Length of keys must match length of group_ids\"\n            raise ValueError(msg)\n        for index, key in enumerate(keys):\n            group_id = group_ids[index] if group_ids else None\n            if time_at_least and time_at_least > self.get_time():\n                msg = \"time_at_least must be in the past\"\n                raise ValueError(msg)\n            self.records[key] = {\"group_id\": group_id, \"updated_at\": self.get_time()}\n\n    async def aupdate(\n        self,\n        keys: Sequence[str],\n        *,\n        group_ids: Optional[Sequence[Optional[str]]] = None,\n        time_at_least: Optional[float] = None,\n    ) -> None:\n        \"\"\"Async upsert records into the database.\n\n        Args:\n            keys: A list of record keys to upsert.\n            group_ids: A list of group IDs corresponding to the keys.\n                Defaults to None.\n            time_at_least: Optional timestamp. Implementation can use this\n                to optionally verify that the timestamp IS at least this time\n                in the system that stores. Defaults to None.\n                E.g., use to validate that the time in the postgres database\n                is equal to or larger than the given timestamp, if not\n                raise an error.\n                This is meant to help prevent time-drift issues since\n                time may not be monotonically increasing!\n\n        Raises:\n            ValueError: If the length of keys doesn't match the length of group\n                ids.\n            ValueError: If time_at_least is in the future.\n        \"\"\"\n        self.update(keys, group_ids=group_ids, time_at_least=time_at_least)\n\n    def exists(self, keys: Sequence[str]) -> list[bool]:\n        \"\"\"Check if the provided keys exist in the database.\n\n        Args:\n            keys: A list of keys to check.\n\n        Returns:\n            A list of boolean values indicating the existence of each key.\n        \"\"\"\n        return [key in self.records for key in keys]\n\n    async def aexists(self, keys: Sequence[str]) -> list[bool]:\n        \"\"\"Async check if the provided keys exist in the database.\n\n        Args:\n            keys: A list of keys to check.\n\n        Returns:\n            A list of boolean values indicating the existence of each key.\n        \"\"\"\n        return self.exists(keys)\n\n    def list_keys(\n        self,\n        *,\n        before: Optional[float] = None,\n        after: Optional[float] = None,\n        group_ids: Optional[Sequence[str]] = None,\n        limit: Optional[int] = None,\n    ) -> list[str]:\n        \"\"\"List records in the database based on the provided filters.\n\n        Args:\n            before: Filter to list records updated before this time.\n                Defaults to None.\n            after: Filter to list records updated after this time.\n                Defaults to None.\n            group_ids: Filter to list records with specific group IDs.\n                Defaults to None.\n            limit: optional limit on the number of records to return.\n                Defaults to None.\n\n        Returns:\n            A list of keys for the matching records.\n        \"\"\"\n        result = []\n        for key, data in self.records.items():\n            if before and data[\"updated_at\"] >= before:\n                continue\n            if after and data[\"updated_at\"] <= after:\n                continue\n            if group_ids and data[\"group_id\"] not in group_ids:\n                continue\n            result.append(key)\n        if limit:\n            return result[:limit]\n        return result\n\n    async def alist_keys(\n        self,\n        *,\n        before: Optional[float] = None,\n        after: Optional[float] = None,\n        group_ids: Optional[Sequence[str]] = None,\n        limit: Optional[int] = None,\n    ) -> list[str]:\n        \"\"\"Async list records in the database based on the provided filters.\n\n        Args:\n            before: Filter to list records updated before this time.\n                Defaults to None.\n            after: Filter to list records updated after this time.\n                Defaults to None.\n            group_ids: Filter to list records with specific group IDs.\n                Defaults to None.\n            limit: optional limit on the number of records to return.\n                Defaults to None.\n\n        Returns:\n            A list of keys for the matching records.\n        \"\"\"\n        return self.list_keys(\n            before=before, after=after, group_ids=group_ids, limit=limit\n        )\n\n    def delete_keys(self, keys: Sequence[str]) -> None:\n        \"\"\"Delete specified records from the database.\n\n        Args:\n            keys: A list of keys to delete.\n        \"\"\"\n        for key in keys:\n            if key in self.records:\n                del self.records[key]\n\n    async def adelete_keys(self, keys: Sequence[str]) -> None:\n        \"\"\"Async delete specified records from the database.\n\n        Args:\n            keys: A list of keys to delete.\n        \"\"\"\n        self.delete_keys(keys)\n\n\nclass UpsertResponse(TypedDict):\n    \"\"\"A generic response for upsert operations.\n\n    The upsert response will be used by abstractions that implement an upsert\n    operation for content that can be upserted by ID.\n\n    Upsert APIs that accept inputs with IDs and generate IDs internally\n    will return a response that includes the IDs that succeeded and the IDs\n    that failed.\n\n    If there are no failures, the failed list will be empty, and the order\n    of the IDs in the succeeded list will match the order of the input documents.\n\n    If there are failures, the response becomes ill defined, and a user of the API\n    cannot determine which generated ID corresponds to which input document.\n\n    It is recommended for users explicitly attach the IDs to the items being\n    indexed to avoid this issue.\n    \"\"\"\n\n    succeeded: list[str]\n    \"\"\"The IDs that were successfully indexed.\"\"\"\n    failed: list[str]\n    \"\"\"The IDs that failed to index.\"\"\"\n\n\nclass DeleteResponse(TypedDict, total=False):\n    \"\"\"A generic response for delete operation.\n\n    The fields in this response are optional and whether the vectorstore\n    returns them or not is up to the implementation.\n    \"\"\"\n\n    num_deleted: int\n    \"\"\"The number of items that were successfully deleted.\n\n    If returned, this should only include *actual* deletions.\n\n    If the ID did not exist to begin with,\n    it should not be included in this count.\n    \"\"\"\n\n    succeeded: Sequence[str]\n    \"\"\"The IDs that were successfully deleted.\n\n    If returned, this should only include *actual* deletions.\n\n    If the ID did not exist to begin with,\n    it should not be included in this list.\n    \"\"\"\n\n    failed: Sequence[str]\n    \"\"\"The IDs that failed to be deleted.\n\n    Please note that deleting an ID that\n    does not exist is **NOT** considered a failure.\n    \"\"\"\n\n    num_failed: int\n    \"\"\"The number of items that failed to be deleted.\"\"\"\n\n\n@beta(message=\"Added in 0.2.29. The abstraction is subject to change.\")\nclass DocumentIndex(BaseRetriever):\n    \"\"\"A document retriever that supports indexing operations.\n\n    This indexing interface is designed to be a generic abstraction for storing and\n    querying documents that has an ID and metadata associated with it.\n\n    The interface is designed to be agnostic to the underlying implementation of the\n    indexing system.\n\n    The interface is designed to support the following operations:\n\n    1. Storing document in the index.\n    2. Fetching document by ID.\n    3. Searching for document using a query.\n\n    .. versionadded:: 0.2.29\n    \"\"\"\n\n    @abc.abstractmethod\n    def upsert(self, items: Sequence[Document], /, **kwargs: Any) -> UpsertResponse:\n        \"\"\"Upsert documents into the index.\n\n        The upsert functionality should utilize the ID field of the content object\n        if it is provided. If the ID is not provided, the upsert method is free\n        to generate an ID for the content.\n\n        When an ID is specified and the content already exists in the vectorstore,\n        the upsert method should update the content with the new data. If the content\n        does not exist, the upsert method should add the item to the vectorstore.\n\n        Args:\n            items: Sequence of documents to add to the vectorstore.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            UpsertResponse: A response object that contains the list of IDs that were\n            successfully added or updated in the vectorstore and the list of IDs that\n            failed to be added or updated.\n        \"\"\"\n\n    async def aupsert(\n        self, items: Sequence[Document], /, **kwargs: Any\n    ) -> UpsertResponse:\n        \"\"\"Add or update documents in the vectorstore. Async version of upsert.\n\n        The upsert functionality should utilize the ID field of the item\n        if it is provided. If the ID is not provided, the upsert method is free\n        to generate an ID for the item.\n\n        When an ID is specified and the item already exists in the vectorstore,\n        the upsert method should update the item with the new data. If the item\n        does not exist, the upsert method should add the item to the vectorstore.\n\n        Args:\n            items: Sequence of documents to add to the vectorstore.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            UpsertResponse: A response object that contains the list of IDs that were\n            successfully added or updated in the vectorstore and the list of IDs that\n            failed to be added or updated.\n        \"\"\"\n        return await run_in_executor(\n            None,\n            self.upsert,\n            items,\n            **kwargs,\n        )\n\n    @abc.abstractmethod\n    def delete(self, ids: Optional[list[str]] = None, **kwargs: Any) -> DeleteResponse:\n        \"\"\"Delete by IDs or other criteria.\n\n        Calling delete without any input parameters should raise a ValueError!\n\n        Args:\n            ids: List of ids to delete.\n            kwargs: Additional keyword arguments. This is up to the implementation.\n                For example, can include an option to delete the entire index,\n                or else issue a non-blocking delete etc.\n\n        Returns:\n            DeleteResponse: A response object that contains the list of IDs that were\n            successfully deleted and the list of IDs that failed to be deleted.\n        \"\"\"\n\n    async def adelete(\n        self, ids: Optional[list[str]] = None, **kwargs: Any\n    ) -> DeleteResponse:\n        \"\"\"Delete by IDs or other criteria. Async variant.\n\n        Calling adelete without any input parameters should raise a ValueError!\n\n        Args:\n            ids: List of ids to delete.\n            kwargs: Additional keyword arguments. This is up to the implementation.\n                For example, can include an option to delete the entire index.\n\n        Returns:\n            DeleteResponse: A response object that contains the list of IDs that were\n            successfully deleted and the list of IDs that failed to be deleted.\n        \"\"\"\n        return await run_in_executor(\n            None,\n            self.delete,\n            ids,\n            **kwargs,\n        )\n\n    @abc.abstractmethod\n    def get(\n        self,\n        ids: Sequence[str],\n        /,\n        **kwargs: Any,\n    ) -> list[Document]:\n        \"\"\"Get documents by id.\n\n        Fewer documents may be returned than requested if some IDs are not found or\n        if there are duplicated IDs.\n\n        Users should not assume that the order of the returned documents matches\n        the order of the input IDs. Instead, users should rely on the ID field of the\n        returned documents.\n\n        This method should **NOT** raise exceptions if no documents are found for\n        some IDs.\n\n        Args:\n            ids: List of IDs to get.\n            kwargs: Additional keyword arguments. These are up to the implementation.\n\n        Returns:\n            List[Document]: List of documents that were found.\n        \"\"\"\n\n    async def aget(\n        self,\n        ids: Sequence[str],\n        /,\n        **kwargs: Any,\n    ) -> list[Document]:\n        \"\"\"Get documents by id.\n\n        Fewer documents may be returned than requested if some IDs are not found or\n        if there are duplicated IDs.\n\n        Users should not assume that the order of the returned documents matches\n        the order of the input IDs. Instead, users should rely on the ID field of the\n        returned documents.\n\n        This method should **NOT** raise exceptions if no documents are found for\n        some IDs.\n\n        Args:\n            ids: List of IDs to get.\n            kwargs: Additional keyword arguments. These are up to the implementation.\n\n        Returns:\n            List[Document]: List of documents that were found.\n        \"\"\"\n        return await run_in_executor(\n            None,\n            self.get,\n            ids,\n            **kwargs,\n        )\n",
        "patch": "@@ -3,14 +3,17 @@\n import abc\n import time\n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n-from typing import Any, Optional, TypedDict\n+from typing import TYPE_CHECKING, Any, Optional, TypedDict\n \n from langchain_core._api import beta\n-from langchain_core.documents import Document\n from langchain_core.retrievers import BaseRetriever\n from langchain_core.runnables import run_in_executor\n \n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n+    from langchain_core.documents import Document\n+\n \n class RecordManager(ABC):\n     \"\"\"Abstract base class representing the interface for a record manager."
      },
      {
        "filename": "libs/core/langchain_core/language_models/chat_models.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport inspect\nimport json\nimport typing\nimport uuid\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import AsyncIterator, Iterator, Sequence\nfrom functools import cached_property\nfrom operator import itemgetter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    model_validator,\n)\nfrom typing_extensions import override\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.callbacks import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForLLMRun,\n    BaseCallbackManager,\n    CallbackManager,\n    CallbackManagerForLLMRun,\n    Callbacks,\n)\nfrom langchain_core.globals import get_llm_cache\nfrom langchain_core.language_models.base import (\n    BaseLanguageModel,\n    LangSmithParams,\n    LanguageModelInput,\n)\nfrom langchain_core.load import dumpd, dumps\nfrom langchain_core.messages import (\n    AIMessage,\n    AnyMessage,\n    BaseMessage,\n    BaseMessageChunk,\n    HumanMessage,\n    convert_to_messages,\n    message_chunk_to_message,\n)\nfrom langchain_core.outputs import (\n    ChatGeneration,\n    ChatGenerationChunk,\n    ChatResult,\n    LLMResult,\n    RunInfo,\n)\nfrom langchain_core.prompt_values import ChatPromptValue, PromptValue, StringPromptValue\nfrom langchain_core.rate_limiters import BaseRateLimiter\nfrom langchain_core.runnables import RunnableMap, RunnablePassthrough\nfrom langchain_core.runnables.config import ensure_config, run_in_executor\nfrom langchain_core.tracers._streaming import _StreamingCallbackHandler\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import TypeBaseModel, is_basemodel_subclass\n\nif TYPE_CHECKING:\n    from langchain_core.output_parsers.base import OutputParserLike\n    from langchain_core.runnables import Runnable, RunnableConfig\n    from langchain_core.tools import BaseTool\n\n\ndef generate_from_stream(stream: Iterator[ChatGenerationChunk]) -> ChatResult:\n    \"\"\"Generate from a stream.\n\n    Args:\n        stream: Iterator of ChatGenerationChunk.\n\n    Returns:\n        ChatResult: Chat result.\n    \"\"\"\n    generation = next(stream, None)\n    if generation:\n        generation += list(stream)\n    if generation is None:\n        msg = \"No generations found in stream.\"\n        raise ValueError(msg)\n    return ChatResult(\n        generations=[\n            ChatGeneration(\n                message=message_chunk_to_message(generation.message),\n                generation_info=generation.generation_info,\n            )\n        ]\n    )\n\n\nasync def agenerate_from_stream(\n    stream: AsyncIterator[ChatGenerationChunk],\n) -> ChatResult:\n    \"\"\"Async generate from a stream.\n\n    Args:\n        stream: Iterator of ChatGenerationChunk.\n\n    Returns:\n        ChatResult: Chat result.\n    \"\"\"\n    chunks = [chunk async for chunk in stream]\n    return await run_in_executor(None, generate_from_stream, iter(chunks))\n\n\nclass BaseChatModel(BaseLanguageModel[BaseMessage], ABC):\n    \"\"\"Base class for chat models.\n\n    Key imperative methods:\n        Methods that actually call the underlying model.\n\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | Method                    | Input                                                          | Output                                                              | Description                                                                                      |\n        +===========================+================================================================+=====================================================================+==================================================================================================+\n        | `invoke`                  | str | List[dict | tuple | BaseMessage] | PromptValue           | BaseMessage                                                         | A single chat model call.                                                                        |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `ainvoke`                 | '''                                                            | BaseMessage                                                         | Defaults to running invoke in an async executor.                                                 |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `stream`                  | '''                                                            | Iterator[BaseMessageChunk]                                          | Defaults to yielding output of invoke.                                                           |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `astream`                 | '''                                                            | AsyncIterator[BaseMessageChunk]                                     | Defaults to yielding output of ainvoke.                                                          |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `astream_events`          | '''                                                            | AsyncIterator[StreamEvent]                                          | Event types: 'on_chat_model_start', 'on_chat_model_stream', 'on_chat_model_end'.                 |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `batch`                   | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running invoke in concurrent threads.                                                |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `abatch`                  | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running ainvoke in concurrent threads.                                               |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `batch_as_completed`      | List[''']                                                      | Iterator[Tuple[int, Union[BaseMessage, Exception]]]                 | Defaults to running invoke in concurrent threads.                                                |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `abatch_as_completed`     | List[''']                                                      | AsyncIterator[Tuple[int, Union[BaseMessage, Exception]]]            | Defaults to running ainvoke in concurrent threads.                                               |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n\n        This table provides a brief overview of the main imperative methods. Please see the base Runnable reference for full documentation.\n\n    Key declarative methods:\n        Methods for creating another Runnable using the ChatModel.\n\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | Method                           | Description                                                                                               |\n        +==================================+===========================================================================================================+\n        | `bind_tools`                     | Create ChatModel that can call tools.                                                                     |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `with_structured_output`         | Create wrapper that structures model output using schema.                                                 |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `with_retry`                     | Create wrapper that retries model calls on failure.                                                       |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `with_fallbacks`                 | Create wrapper that falls back to other models on failure.                                                |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `configurable_fields`            | Specify init args of the model that can be configured at runtime via the RunnableConfig.                  |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `configurable_alternatives`      | Specify alternative models which can be swapped in at runtime via the RunnableConfig.                     |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n\n        This table provides a brief overview of the main declarative methods. Please see the reference for each method for full documentation.\n\n    Creating custom chat model:\n        Custom chat model implementations should inherit from this class.\n        Please reference the table below for information about which\n        methods and properties are required or optional for implementations.\n\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | Method/Property                  | Description                                                        | Required/Optional |\n        +==================================+====================================================================+===================+\n        | `_generate`                      | Use to generate a chat result from a prompt                        | Required          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_llm_type` (property)           | Used to uniquely identify the type of the model. Used for logging. | Required          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_identifying_params` (property) | Represent model parameterization for tracing purposes.             | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_stream`                        | Use to implement streaming                                         | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_agenerate`                     | Use to implement a native async method                             | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_astream`                       | Use to implement async version of `_stream`                        | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n\n        Follow the guide for more information on how to implement a custom Chat Model:\n        [Guide](https://python.langchain.com/docs/how_to/custom_chat_model/).\n\n    \"\"\"  # noqa: E501\n\n    callback_manager: Optional[BaseCallbackManager] = deprecated(\n        name=\"callback_manager\", since=\"0.1.7\", removal=\"1.0\", alternative=\"callbacks\"\n    )(\n        Field(\n            default=None,\n            exclude=True,\n            description=\"Callback manager to add to the run trace.\",\n        )\n    )\n\n    rate_limiter: Optional[BaseRateLimiter] = Field(default=None, exclude=True)\n    \"An optional rate limiter to use for limiting the number of requests.\"\n\n    disable_streaming: Union[bool, Literal[\"tool_calling\"]] = False\n    \"\"\"Whether to disable streaming for this model.\n\n    If streaming is bypassed, then ``stream()``/``astream()``/``astream_events()`` will\n    defer to ``invoke()``/``ainvoke()``.\n\n    - If True, will always bypass streaming case.\n    - If \"tool_calling\", will bypass streaming case only when the model is called\n      with a ``tools`` keyword argument.\n    - If False (default), will always use streaming case if available.\n    \"\"\"\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def raise_deprecation(cls, values: dict) -> Any:\n        \"\"\"Raise deprecation warning if callback_manager is used.\n\n        Args:\n            values (Dict): Values to validate.\n\n        Returns:\n            Dict: Validated values.\n\n        Raises:\n            DeprecationWarning: If callback_manager is used.\n        \"\"\"\n        if values.get(\"callback_manager\") is not None:\n            warnings.warn(\n                \"callback_manager is deprecated. Please use callbacks instead.\",\n                DeprecationWarning,\n                stacklevel=5,\n            )\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\n        return values\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @cached_property\n    def _serialized(self) -> dict[str, Any]:\n        return dumpd(self)\n\n    # --- Runnable methods ---\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        \"\"\"Get the output type for this runnable.\"\"\"\n        return AnyMessage\n\n    def _convert_input(self, input: LanguageModelInput) -> PromptValue:\n        if isinstance(input, PromptValue):\n            return input\n        elif isinstance(input, str):\n            return StringPromptValue(text=input)\n        elif isinstance(input, Sequence):\n            return ChatPromptValue(messages=convert_to_messages(input))\n        else:\n            msg = (\n                f\"Invalid input type {type(input)}. \"\n                \"Must be a PromptValue, str, or list of BaseMessages.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n\n    def invoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        config = ensure_config(config)\n        return cast(\n            ChatGeneration,\n            self.generate_prompt(\n                [self._convert_input(input)],\n                stop=stop,\n                callbacks=config.get(\"callbacks\"),\n                tags=config.get(\"tags\"),\n                metadata=config.get(\"metadata\"),\n                run_name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                **kwargs,\n            ).generations[0][0],\n        ).message\n\n    async def ainvoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        config = ensure_config(config)\n        llm_result = await self.agenerate_prompt(\n            [self._convert_input(input)],\n            stop=stop,\n            callbacks=config.get(\"callbacks\"),\n            tags=config.get(\"tags\"),\n            metadata=config.get(\"metadata\"),\n            run_name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            **kwargs,\n        )\n        return cast(ChatGeneration, llm_result.generations[0][0]).message\n\n    def _should_stream(\n        self,\n        *,\n        async_api: bool,\n        run_manager: Optional[\n            Union[CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun]\n        ] = None,\n        **kwargs: Any,\n    ) -> bool:\n        \"\"\"Determine if a given model call should hit the streaming API.\"\"\"\n        sync_not_implemented = type(self)._stream == BaseChatModel._stream\n        async_not_implemented = type(self)._astream == BaseChatModel._astream\n\n        # Check if streaming is implemented.\n        if (not async_api) and sync_not_implemented:\n            return False\n        # Note, since async falls back to sync we check both here.\n        if async_api and async_not_implemented and sync_not_implemented:\n            return False\n\n        # Check if streaming has been disabled on this instance.\n        if self.disable_streaming is True:\n            return False\n        # We assume tools are passed in via \"tools\" kwarg in all models.\n        if self.disable_streaming == \"tool_calling\" and kwargs.get(\"tools\"):\n            return False\n\n        # Check if a runtime streaming flag has been passed in.\n        if \"stream\" in kwargs:\n            return kwargs[\"stream\"]\n\n        # Check if any streaming callback handlers have been passed in.\n        handlers = run_manager.handlers if run_manager else []\n        return any(isinstance(h, _StreamingCallbackHandler) for h in handlers)\n\n    def stream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> Iterator[BaseMessageChunk]:\n        if not self._should_stream(async_api=False, **{**kwargs, \"stream\": True}):\n            # model doesn't implement streaming, so use default implementation\n            yield cast(\n                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)\n            )\n        else:\n            config = ensure_config(config)\n            messages = self._convert_input(input).to_messages()\n            structured_output_format = kwargs.pop(\"structured_output_format\", None)\n            if structured_output_format:\n                try:\n                    structured_output_format_dict = {\n                        \"structured_output_format\": {\n                            \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                            \"schema\": convert_to_openai_tool(\n                                structured_output_format[\"schema\"]\n                            ),\n                        }\n                    }\n                except ValueError:\n                    structured_output_format_dict = {}\n            else:\n                structured_output_format_dict = {}\n\n            params = self._get_invocation_params(stop=stop, **kwargs)\n            options = {\"stop\": stop, **kwargs}\n            inheritable_metadata = {\n                **(config.get(\"metadata\") or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n                **structured_output_format_dict,\n            }\n            callback_manager = CallbackManager.configure(\n                config.get(\"callbacks\"),\n                self.callbacks,\n                self.verbose,\n                config.get(\"tags\"),\n                self.tags,\n                inheritable_metadata,\n                self.metadata,\n            )\n            (run_manager,) = callback_manager.on_chat_model_start(\n                self._serialized,\n                [messages],\n                invocation_params=params,\n                options=options,\n                name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                batch_size=1,\n            )\n            generation: Optional[ChatGenerationChunk] = None\n\n            if self.rate_limiter:\n                self.rate_limiter.acquire(blocking=True)\n\n            try:\n                for chunk in self._stream(messages, stop=stop, **kwargs):\n                    if chunk.message.id is None:\n                        chunk.message.id = f\"run-{run_manager.run_id}\"\n                    chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                    run_manager.on_llm_new_token(\n                        cast(str, chunk.message.content), chunk=chunk\n                    )\n                    yield chunk.message\n                    if generation is None:\n                        generation = chunk\n                    else:\n                        generation += chunk\n            except BaseException as e:\n                run_manager.on_llm_error(\n                    e,\n                    response=LLMResult(\n                        generations=[[generation]] if generation else []\n                    ),\n                )\n                raise\n\n            if generation is None:\n                err = ValueError(\"No generation chunks were returned\")\n                run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n                raise err\n\n            run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n\n    async def astream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[BaseMessageChunk]:\n        if not self._should_stream(async_api=True, **{**kwargs, \"stream\": True}):\n            # No async or sync stream is implemented, so fall back to ainvoke\n            yield cast(\n                BaseMessageChunk,\n                await self.ainvoke(input, config=config, stop=stop, **kwargs),\n            )\n            return\n\n        config = ensure_config(config)\n        messages = self._convert_input(input).to_messages()\n\n        structured_output_format = kwargs.pop(\"structured_output_format\", None)\n        if structured_output_format:\n            try:\n                structured_output_format_dict = {\n                    \"structured_output_format\": {\n                        \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                        \"schema\": convert_to_openai_tool(\n                            structured_output_format[\"schema\"]\n                        ),\n                    }\n                }\n            except ValueError:\n                structured_output_format_dict = {}\n        else:\n            structured_output_format_dict = {}\n\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        options = {\"stop\": stop, **kwargs}\n        inheritable_metadata = {\n            **(config.get(\"metadata\") or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n            **structured_output_format_dict,\n        }\n        callback_manager = AsyncCallbackManager.configure(\n            config.get(\"callbacks\"),\n            self.callbacks,\n            self.verbose,\n            config.get(\"tags\"),\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n        (run_manager,) = await callback_manager.on_chat_model_start(\n            self._serialized,\n            [messages],\n            invocation_params=params,\n            options=options,\n            name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            batch_size=1,\n        )\n\n        if self.rate_limiter:\n            await self.rate_limiter.aacquire(blocking=True)\n\n        generation: Optional[ChatGenerationChunk] = None\n        try:\n            async for chunk in self._astream(\n                messages,\n                stop=stop,\n                **kwargs,\n            ):\n                if chunk.message.id is None:\n                    chunk.message.id = f\"run-{run_manager.run_id}\"\n                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                await run_manager.on_llm_new_token(\n                    cast(str, chunk.message.content), chunk=chunk\n                )\n                yield chunk.message\n                if generation is None:\n                    generation = chunk\n                else:\n                    generation += chunk\n        except BaseException as e:\n            await run_manager.on_llm_error(\n                e,\n                response=LLMResult(generations=[[generation]] if generation else []),\n            )\n            raise\n\n        if generation is None:\n            err = ValueError(\"No generation chunks were returned\")\n            await run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n            raise err\n\n        await run_manager.on_llm_end(\n            LLMResult(generations=[[generation]]),\n        )\n\n    # --- Custom methods ---\n\n    def _combine_llm_outputs(self, llm_outputs: list[Optional[dict]]) -> dict:\n        return {}\n\n    def _get_invocation_params(\n        self,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        params = self.dict()\n        params[\"stop\"] = stop\n        return {**params, **kwargs}\n\n    def _get_ls_params(\n        self,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        # get default provider from class name\n        default_provider = self.__class__.__name__\n        if default_provider.startswith(\"Chat\"):\n            default_provider = default_provider[4:].lower()\n        elif default_provider.endswith(\"Chat\"):\n            default_provider = default_provider[:-4]\n        default_provider = default_provider.lower()\n\n        ls_params = LangSmithParams(ls_provider=default_provider, ls_model_type=\"chat\")\n        if stop:\n            ls_params[\"ls_stop\"] = stop\n\n        # model\n        if hasattr(self, \"model\") and isinstance(self.model, str):\n            ls_params[\"ls_model_name\"] = self.model\n        elif hasattr(self, \"model_name\") and isinstance(self.model_name, str):\n            ls_params[\"ls_model_name\"] = self.model_name\n\n        # temperature\n        if \"temperature\" in kwargs and isinstance(kwargs[\"temperature\"], float):\n            ls_params[\"ls_temperature\"] = kwargs[\"temperature\"]\n        elif hasattr(self, \"temperature\") and isinstance(self.temperature, float):\n            ls_params[\"ls_temperature\"] = self.temperature\n\n        # max_tokens\n        if \"max_tokens\" in kwargs and isinstance(kwargs[\"max_tokens\"], int):\n            ls_params[\"ls_max_tokens\"] = kwargs[\"max_tokens\"]\n        elif hasattr(self, \"max_tokens\") and isinstance(self.max_tokens, int):\n            ls_params[\"ls_max_tokens\"] = self.max_tokens\n\n        return ls_params\n\n    def _get_llm_string(self, stop: Optional[list[str]] = None, **kwargs: Any) -> str:\n        if self.is_lc_serializable():\n            params = {**kwargs, \"stop\": stop}\n            param_string = str(sorted(params.items()))\n            # This code is not super efficient as it goes back and forth between\n            # json and dict.\n            serialized_repr = self._serialized\n            _cleanup_llm_representation(serialized_repr, 1)\n            llm_string = json.dumps(serialized_repr, sort_keys=True)\n            return llm_string + \"---\" + param_string\n        else:\n            params = self._get_invocation_params(stop=stop, **kwargs)\n            params = {**params, **kwargs}\n            return str(sorted(params.items()))\n\n    def generate(\n        self,\n        messages: list[list[BaseMessage]],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Pass a sequence of prompts to the model and return model generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            messages: List of list of messages.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        structured_output_format = kwargs.pop(\"structured_output_format\", None)\n        if structured_output_format:\n            try:\n                structured_output_format_dict = {\n                    \"structured_output_format\": {\n                        \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                        \"schema\": convert_to_openai_tool(\n                            structured_output_format[\"schema\"]\n                        ),\n                    }\n                }\n            except ValueError:\n                structured_output_format_dict = {}\n        else:\n            structured_output_format_dict = {}\n\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        options = {\"stop\": stop}\n        inheritable_metadata = {\n            **(metadata or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n            **structured_output_format_dict,\n        }\n\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n        run_managers = callback_manager.on_chat_model_start(\n            self._serialized,\n            messages,\n            invocation_params=params,\n            options=options,\n            name=run_name,\n            run_id=run_id,\n            batch_size=len(messages),\n        )\n        results = []\n        for i, m in enumerate(messages):\n            try:\n                results.append(\n                    self._generate_with_cache(\n                        m,\n                        stop=stop,\n                        run_manager=run_managers[i] if run_managers else None,\n                        **kwargs,\n                    )\n                )\n            except BaseException as e:\n                if run_managers:\n                    run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n                raise\n        flattened_outputs = [\n            LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]\n            for res in results\n        ]\n        llm_output = self._combine_llm_outputs([res.llm_output for res in results])\n        generations = [res.generations for res in results]\n        output = LLMResult(generations=generations, llm_output=llm_output)  # type: ignore[arg-type]\n        if run_managers:\n            run_infos = []\n            for manager, flattened_output in zip(run_managers, flattened_outputs):\n                manager.on_llm_end(flattened_output)\n                run_infos.append(RunInfo(run_id=manager.run_id))\n            output.run = run_infos\n        return output\n\n    async def agenerate(\n        self,\n        messages: list[list[BaseMessage]],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Asynchronously pass a sequence of prompts to a model and return generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            messages: List of list of messages.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        structured_output_format = kwargs.pop(\"structured_output_format\", None)\n        if structured_output_format:\n            try:\n                structured_output_format_dict = {\n                    \"structured_output_format\": {\n                        \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                        \"schema\": convert_to_openai_tool(\n                            structured_output_format[\"schema\"]\n                        ),\n                    }\n                }\n            except ValueError:\n                structured_output_format_dict = {}\n        else:\n            structured_output_format_dict = {}\n\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        options = {\"stop\": stop}\n        inheritable_metadata = {\n            **(metadata or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n            **structured_output_format_dict,\n        }\n\n        callback_manager = AsyncCallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n\n        run_managers = await callback_manager.on_chat_model_start(\n            self._serialized,\n            messages,\n            invocation_params=params,\n            options=options,\n            name=run_name,\n            batch_size=len(messages),\n            run_id=run_id,\n        )\n\n        results = await asyncio.gather(\n            *[\n                self._agenerate_with_cache(\n                    m,\n                    stop=stop,\n                    run_manager=run_managers[i] if run_managers else None,\n                    **kwargs,\n                )\n                for i, m in enumerate(messages)\n            ],\n            return_exceptions=True,\n        )\n        exceptions = []\n        for i, res in enumerate(results):\n            if isinstance(res, BaseException):\n                if run_managers:\n                    await run_managers[i].on_llm_error(\n                        res, response=LLMResult(generations=[])\n                    )\n                exceptions.append(res)\n        if exceptions:\n            if run_managers:\n                await asyncio.gather(\n                    *[\n                        run_manager.on_llm_end(\n                            LLMResult(\n                                generations=[res.generations],  # type: ignore[list-item, union-attr]\n                                llm_output=res.llm_output,  # type: ignore[list-item, union-attr]\n                            )\n                        )\n                        for run_manager, res in zip(run_managers, results)\n                        if not isinstance(res, Exception)\n                    ]\n                )\n            raise exceptions[0]\n        flattened_outputs = [\n            LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item, union-attr]\n            for res in results\n        ]\n        llm_output = self._combine_llm_outputs([res.llm_output for res in results])  # type: ignore[union-attr]\n        generations = [res.generations for res in results]  # type: ignore[union-attr]\n        output = LLMResult(generations=generations, llm_output=llm_output)  # type: ignore[arg-type]\n        await asyncio.gather(\n            *[\n                run_manager.on_llm_end(flattened_output)\n                for run_manager, flattened_output in zip(\n                    run_managers, flattened_outputs\n                )\n            ]\n        )\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\n\n    def generate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_messages = [p.to_messages() for p in prompts]\n        return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n\n    async def agenerate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_messages = [p.to_messages() for p in prompts]\n        return await self.agenerate(\n            prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n        )\n\n    def _generate_with_cache(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        llm_cache = self.cache if isinstance(self.cache, BaseCache) else get_llm_cache()\n        # We should check the cache unless it's explicitly set to False\n        # A None cache means we should use the default global cache\n        # if it's configured.\n        check_cache = self.cache or self.cache is None\n        if check_cache:\n            if llm_cache:\n                llm_string = self._get_llm_string(stop=stop, **kwargs)\n                prompt = dumps(messages)\n                cache_val = llm_cache.lookup(prompt, llm_string)\n                if isinstance(cache_val, list):\n                    return ChatResult(generations=cache_val)\n            elif self.cache is None:\n                pass\n            else:\n                msg = \"Asked to cache, but no cache found at `langchain.cache`.\"\n                raise ValueError(msg)\n\n        # Apply the rate limiter after checking the cache, since\n        # we usually don't want to rate limit cache lookups, but\n        # we do want to rate limit API requests.\n        if self.rate_limiter:\n            self.rate_limiter.acquire(blocking=True)\n\n        # If stream is not explicitly set, check if implicitly requested by\n        # astream_events() or astream_log(). Bail out if _stream not implemented\n        if self._should_stream(\n            async_api=False,\n            run_manager=run_manager,\n            **kwargs,\n        ):\n            chunks: list[ChatGenerationChunk] = []\n            for chunk in self._stream(messages, stop=stop, **kwargs):\n                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                if run_manager:\n                    if chunk.message.id is None:\n                        chunk.message.id = f\"run-{run_manager.run_id}\"\n                    run_manager.on_llm_new_token(\n                        cast(str, chunk.message.content), chunk=chunk\n                    )\n                chunks.append(chunk)\n            result = generate_from_stream(iter(chunks))\n        else:\n            if inspect.signature(self._generate).parameters.get(\"run_manager\"):\n                result = self._generate(\n                    messages, stop=stop, run_manager=run_manager, **kwargs\n                )\n            else:\n                result = self._generate(messages, stop=stop, **kwargs)\n\n        # Add response metadata to each generation\n        for idx, generation in enumerate(result.generations):\n            if run_manager and generation.message.id is None:\n                generation.message.id = f\"run-{run_manager.run_id}-{idx}\"\n            generation.message.response_metadata = _gen_info_and_msg_metadata(\n                generation\n            )\n        if len(result.generations) == 1 and result.llm_output is not None:\n            result.generations[0].message.response_metadata = {\n                **result.llm_output,\n                **result.generations[0].message.response_metadata,\n            }\n        if check_cache and llm_cache:\n            llm_cache.update(prompt, llm_string, result.generations)\n        return result\n\n    async def _agenerate_with_cache(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        llm_cache = self.cache if isinstance(self.cache, BaseCache) else get_llm_cache()\n        # We should check the cache unless it's explicitly set to False\n        # A None cache means we should use the default global cache\n        # if it's configured.\n        check_cache = self.cache or self.cache is None\n        if check_cache:\n            if llm_cache:\n                llm_string = self._get_llm_string(stop=stop, **kwargs)\n                prompt = dumps(messages)\n                cache_val = await llm_cache.alookup(prompt, llm_string)\n                if isinstance(cache_val, list):\n                    return ChatResult(generations=cache_val)\n            elif self.cache is None:\n                pass\n            else:\n                msg = \"Asked to cache, but no cache found at `langchain.cache`.\"\n                raise ValueError(msg)\n\n        # Apply the rate limiter after checking the cache, since\n        # we usually don't want to rate limit cache lookups, but\n        # we do want to rate limit API requests.\n        if self.rate_limiter:\n            await self.rate_limiter.aacquire(blocking=True)\n\n        # If stream is not explicitly set, check if implicitly requested by\n        # astream_events() or astream_log(). Bail out if _astream not implemented\n        if self._should_stream(\n            async_api=True,\n            run_manager=run_manager,\n            **kwargs,\n        ):\n            chunks: list[ChatGenerationChunk] = []\n            async for chunk in self._astream(messages, stop=stop, **kwargs):\n                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                if run_manager:\n                    if chunk.message.id is None:\n                        chunk.message.id = f\"run-{run_manager.run_id}\"\n                    await run_manager.on_llm_new_token(\n                        cast(str, chunk.message.content), chunk=chunk\n                    )\n                chunks.append(chunk)\n            result = generate_from_stream(iter(chunks))\n        else:\n            if inspect.signature(self._agenerate).parameters.get(\"run_manager\"):\n                result = await self._agenerate(\n                    messages, stop=stop, run_manager=run_manager, **kwargs\n                )\n            else:\n                result = await self._agenerate(messages, stop=stop, **kwargs)\n\n        # Add response metadata to each generation\n        for idx, generation in enumerate(result.generations):\n            if run_manager and generation.message.id is None:\n                generation.message.id = f\"run-{run_manager.run_id}-{idx}\"\n            generation.message.response_metadata = _gen_info_and_msg_metadata(\n                generation\n            )\n        if len(result.generations) == 1 and result.llm_output is not None:\n            result.generations[0].message.response_metadata = {\n                **result.llm_output,\n                **result.generations[0].message.response_metadata,\n            }\n        if check_cache and llm_cache:\n            await llm_cache.aupdate(prompt, llm_string, result.generations)\n        return result\n\n    @abstractmethod\n    def _generate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        \"\"\"Top Level call.\"\"\"\n\n    async def _agenerate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        \"\"\"Top Level call.\"\"\"\n        return await run_in_executor(\n            None,\n            self._generate,\n            messages,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n    def _stream(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        raise NotImplementedError\n\n    async def _astream(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        iterator = await run_in_executor(\n            None,\n            self._stream,\n            messages,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n        done = object()\n        while True:\n            item = await run_in_executor(\n                None,\n                next,\n                iterator,\n                done,  # type: ignore[call-arg, arg-type]\n            )\n            if item is done:\n                break\n            yield item  # type: ignore[misc]\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def __call__(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        generation = self.generate(\n            [messages], stop=stop, callbacks=callbacks, **kwargs\n        ).generations[0][0]\n        if isinstance(generation, ChatGeneration):\n            return generation.message\n        else:\n            msg = \"Unexpected generation type\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    async def _call_async(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        result = await self.agenerate(\n            [messages], stop=stop, callbacks=callbacks, **kwargs\n        )\n        generation = result.generations[0][0]\n        if isinstance(generation, ChatGeneration):\n            return generation.message\n        else:\n            msg = \"Unexpected generation type\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def call_as_llm(\n        self, message: str, stop: Optional[list[str]] = None, **kwargs: Any\n    ) -> str:\n        return self.predict(message, stop=stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        result = self([HumanMessage(content=text)], stop=_stop, **kwargs)\n        if isinstance(result.content, str):\n            return result.content\n        else:\n            msg = \"Cannot use predict when output is not a string.\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        _stop = None if stop is None else list(stop)\n        return self(messages, stop=_stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        result = await self._call_async(\n            [HumanMessage(content=text)], stop=_stop, **kwargs\n        )\n        if isinstance(result.content, str):\n            return result.content\n        else:\n            msg = \"Cannot use predict when output is not a string.\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        _stop = None if stop is None else list(stop)\n        return await self._call_async(messages, stop=_stop, **kwargs)\n\n    @property\n    @abstractmethod\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n\n    def dict(self, **kwargs: Any) -> dict:\n        \"\"\"Return a dictionary of the LLM.\"\"\"\n        starter_dict = dict(self._identifying_params)\n        starter_dict[\"_type\"] = self._llm_type\n        return starter_dict\n\n    def bind_tools(\n        self,\n        tools: Sequence[\n            Union[typing.Dict[str, Any], type, Callable, BaseTool]  # noqa: UP006\n        ],\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        raise NotImplementedError\n\n    def with_structured_output(\n        self,\n        schema: Union[typing.Dict, type],  # noqa: UP006\n        *,\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[typing.Dict, BaseModel]]:  # noqa: UP006\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n                    - an OpenAI function/tool schema,\n                    - a JSON Schema,\n                    - a TypedDict class,\n                    - or a Pydantic class.\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: Pydantic schema (include_raw=False):\n            .. code-block:: python\n\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatModel(model=\"model-name\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example: Pydantic schema (include_raw=True):\n            .. code-block:: python\n\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatModel(model=\"model-name\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: Dict schema (include_raw=False):\n            .. code-block:: python\n\n                from pydantic import BaseModel\n                from langchain_core.utils.function_calling import convert_to_openai_tool\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                dict_schema = convert_to_openai_tool(AnswerWithJustification)\n                llm = ChatModel(model=\"model-name\", temperature=0)\n                structured_llm = llm.with_structured_output(dict_schema)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. versionchanged:: 0.2.26\n\n                Added support for TypedDict class.\n        \"\"\"  # noqa: E501\n        if kwargs:\n            msg = f\"Received unsupported arguments {kwargs}\"\n            raise ValueError(msg)\n\n        from langchain_core.output_parsers.openai_tools import (\n            JsonOutputKeyToolsParser,\n            PydanticToolsParser,\n        )\n\n        if self.bind_tools is BaseChatModel.bind_tools:\n            msg = \"with_structured_output is not implemented for this model.\"\n            raise NotImplementedError(msg)\n\n        llm = self.bind_tools(\n            [schema],\n            tool_choice=\"any\",\n            structured_output_format={\"kwargs\": {}, \"schema\": schema},\n        )\n        if isinstance(schema, type) and is_basemodel_subclass(schema):\n            output_parser: OutputParserLike = PydanticToolsParser(\n                tools=[cast(TypeBaseModel, schema)], first_tool_only=True\n            )\n        else:\n            key_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n            output_parser = JsonOutputKeyToolsParser(\n                key_name=key_name, first_tool_only=True\n            )\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n\nclass SimpleChatModel(BaseChatModel):\n    \"\"\"Simplified implementation for a chat model to inherit from.\n\n    **Note** This implementation is primarily here for backwards compatibility.\n        For new implementations, please use `BaseChatModel` directly.\n    \"\"\"\n\n    def _generate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        output_str = self._call(messages, stop=stop, run_manager=run_manager, **kwargs)\n        message = AIMessage(content=output_str)\n        generation = ChatGeneration(message=message)\n        return ChatResult(generations=[generation])\n\n    @abstractmethod\n    def _call(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Simpler interface.\"\"\"\n\n    async def _agenerate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        return await run_in_executor(\n            None,\n            self._generate,\n            messages,\n            stop=stop,\n            run_manager=run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n\ndef _gen_info_and_msg_metadata(\n    generation: Union[ChatGeneration, ChatGenerationChunk],\n) -> dict:\n    return {\n        **(generation.generation_info or {}),\n        **generation.message.response_metadata,\n    }\n\n\ndef _cleanup_llm_representation(serialized: Any, depth: int) -> None:\n    \"\"\"Remove non-serializable objects from a serialized object.\"\"\"\n    if depth > 100:  # Don't cooperate for pathological cases\n        return\n\n    if not isinstance(serialized, dict):\n        return\n\n    if (\n        \"type\" in serialized\n        and serialized[\"type\"] == \"not_implemented\"\n        and \"repr\" in serialized\n    ):\n        del serialized[\"repr\"]\n\n    if \"graph\" in serialized:\n        del serialized[\"graph\"]\n\n    if \"kwargs\" in serialized:\n        kwargs = serialized[\"kwargs\"]\n\n        for value in kwargs.values():\n            _cleanup_llm_representation(value, depth + 1)\n",
        "patch": "@@ -4,7 +4,6 @@\n import inspect\n import json\n import typing\n-import uuid\n import warnings\n from abc import ABC, abstractmethod\n from collections.abc import AsyncIterator, Iterator, Sequence\n@@ -70,6 +69,8 @@\n from langchain_core.utils.pydantic import TypeBaseModel, is_basemodel_subclass\n \n if TYPE_CHECKING:\n+    import uuid\n+\n     from langchain_core.output_parsers.base import OutputParserLike\n     from langchain_core.runnables import Runnable, RunnableConfig\n     from langchain_core.tools import BaseTool"
      },
      {
        "filename": "libs/core/langchain_core/language_models/llms.py",
        "content_before": "\"\"\"Base interface for large language models to expose.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport logging\nimport uuid\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import AsyncIterator, Iterator, Sequence\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Callable,\n    Optional,\n    Union,\n    cast,\n)\n\nimport yaml\nfrom pydantic import ConfigDict, Field, model_validator\nfrom tenacity import (\n    RetryCallState,\n    before_sleep_log,\n    retry,\n    retry_base,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\nfrom typing_extensions import override\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.callbacks import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForLLMRun,\n    BaseCallbackManager,\n    CallbackManager,\n    CallbackManagerForLLMRun,\n    Callbacks,\n)\nfrom langchain_core.globals import get_llm_cache\nfrom langchain_core.language_models.base import (\n    BaseLanguageModel,\n    LangSmithParams,\n    LanguageModelInput,\n)\nfrom langchain_core.load import dumpd\nfrom langchain_core.messages import (\n    AIMessage,\n    BaseMessage,\n    convert_to_messages,\n    get_buffer_string,\n)\nfrom langchain_core.outputs import Generation, GenerationChunk, LLMResult, RunInfo\nfrom langchain_core.prompt_values import ChatPromptValue, PromptValue, StringPromptValue\nfrom langchain_core.runnables import RunnableConfig, ensure_config, get_config_list\nfrom langchain_core.runnables.config import run_in_executor\n\nlogger = logging.getLogger(__name__)\n\n\n@functools.lru_cache\ndef _log_error_once(msg: str) -> None:\n    \"\"\"Log an error once.\"\"\"\n    logger.error(msg)\n\n\ndef create_base_retry_decorator(\n    error_types: list[type[BaseException]],\n    max_retries: int = 1,\n    run_manager: Optional[\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n    ] = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Create a retry decorator for a given LLM and provided\n     a list of error types.\n\n    Args:\n        error_types: List of error types to retry on.\n        max_retries: Number of retries. Default is 1.\n        run_manager: Callback manager for the run. Default is None.\n\n    Returns:\n        A retry decorator.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    _logging = before_sleep_log(logger, logging.WARNING)\n\n    def _before_sleep(retry_state: RetryCallState) -> None:\n        _logging(retry_state)\n        if run_manager:\n            if isinstance(run_manager, AsyncCallbackManagerForLLMRun):\n                coro = run_manager.on_retry(retry_state)\n                try:\n                    loop = asyncio.get_event_loop()\n                    if loop.is_running():\n                        loop.create_task(coro)\n                    else:\n                        asyncio.run(coro)\n                except Exception as e:\n                    _log_error_once(f\"Error in on_retry: {e}\")\n            else:\n                run_manager.on_retry(retry_state)\n\n    min_seconds = 4\n    max_seconds = 10\n    # Wait 2^x * 1 second between each retry starting with\n    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards\n    retry_instance: retry_base = retry_if_exception_type(error_types[0])\n    for error in error_types[1:]:\n        retry_instance = retry_instance | retry_if_exception_type(error)\n    return retry(\n        reraise=True,\n        stop=stop_after_attempt(max_retries),\n        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),\n        retry=retry_instance,\n        before_sleep=_before_sleep,\n    )\n\n\ndef _resolve_cache(cache: Union[BaseCache, bool, None]) -> Optional[BaseCache]:\n    \"\"\"Resolve the cache.\"\"\"\n    if isinstance(cache, BaseCache):\n        llm_cache = cache\n    elif cache is None:\n        llm_cache = get_llm_cache()\n    elif cache is True:\n        llm_cache = get_llm_cache()\n        if llm_cache is None:\n            msg = (\n                \"No global cache was configured. Use `set_llm_cache`.\"\n                \"to set a global cache if you want to use a global cache.\"\n                \"Otherwise either pass a cache object or set cache to False/None\"\n            )\n            raise ValueError(msg)\n    elif cache is False:\n        llm_cache = None\n    else:\n        msg = f\"Unsupported cache value {cache}\"\n        raise ValueError(msg)\n    return llm_cache\n\n\ndef get_prompts(\n    params: dict[str, Any],\n    prompts: list[str],\n    cache: Optional[Union[BaseCache, bool, None]] = None,\n) -> tuple[dict[int, list], str, list[int], list[str]]:\n    \"\"\"Get prompts that are already cached.\n\n    Args:\n        params: Dictionary of parameters.\n        prompts: List of prompts.\n        cache: Cache object. Default is None.\n\n    Returns:\n        A tuple of existing prompts, llm_string, missing prompt indexes,\n            and missing prompts.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_string = str(sorted(params.items()))\n    missing_prompts = []\n    missing_prompt_idxs = []\n    existing_prompts = {}\n\n    llm_cache = _resolve_cache(cache)\n    for i, prompt in enumerate(prompts):\n        if llm_cache:\n            cache_val = llm_cache.lookup(prompt, llm_string)\n            if isinstance(cache_val, list):\n                existing_prompts[i] = cache_val\n            else:\n                missing_prompts.append(prompt)\n                missing_prompt_idxs.append(i)\n    return existing_prompts, llm_string, missing_prompt_idxs, missing_prompts\n\n\nasync def aget_prompts(\n    params: dict[str, Any],\n    prompts: list[str],\n    cache: Optional[Union[BaseCache, bool, None]] = None,\n) -> tuple[dict[int, list], str, list[int], list[str]]:\n    \"\"\"Get prompts that are already cached. Async version.\n\n    Args:\n        params: Dictionary of parameters.\n        prompts: List of prompts.\n        cache: Cache object. Default is None.\n\n    Returns:\n        A tuple of existing prompts, llm_string, missing prompt indexes,\n            and missing prompts.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_string = str(sorted(params.items()))\n    missing_prompts = []\n    missing_prompt_idxs = []\n    existing_prompts = {}\n    llm_cache = _resolve_cache(cache)\n    for i, prompt in enumerate(prompts):\n        if llm_cache:\n            cache_val = await llm_cache.alookup(prompt, llm_string)\n            if isinstance(cache_val, list):\n                existing_prompts[i] = cache_val\n            else:\n                missing_prompts.append(prompt)\n                missing_prompt_idxs.append(i)\n    return existing_prompts, llm_string, missing_prompt_idxs, missing_prompts\n\n\ndef update_cache(\n    cache: Union[BaseCache, bool, None],\n    existing_prompts: dict[int, list],\n    llm_string: str,\n    missing_prompt_idxs: list[int],\n    new_results: LLMResult,\n    prompts: list[str],\n) -> Optional[dict]:\n    \"\"\"Update the cache and get the LLM output.\n\n    Args:\n        cache: Cache object.\n        existing_prompts: Dictionary of existing prompts.\n        llm_string: LLM string.\n        missing_prompt_idxs: List of missing prompt indexes.\n        new_results: LLMResult object.\n        prompts: List of prompts.\n\n    Returns:\n        LLM output.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_cache = _resolve_cache(cache)\n    for i, result in enumerate(new_results.generations):\n        existing_prompts[missing_prompt_idxs[i]] = result\n        prompt = prompts[missing_prompt_idxs[i]]\n        if llm_cache is not None:\n            llm_cache.update(prompt, llm_string, result)\n    llm_output = new_results.llm_output\n    return llm_output\n\n\nasync def aupdate_cache(\n    cache: Union[BaseCache, bool, None],\n    existing_prompts: dict[int, list],\n    llm_string: str,\n    missing_prompt_idxs: list[int],\n    new_results: LLMResult,\n    prompts: list[str],\n) -> Optional[dict]:\n    \"\"\"Update the cache and get the LLM output. Async version.\n\n    Args:\n        cache: Cache object.\n        existing_prompts: Dictionary of existing prompts.\n        llm_string: LLM string.\n        missing_prompt_idxs: List of missing prompt indexes.\n        new_results: LLMResult object.\n        prompts: List of prompts.\n\n    Returns:\n        LLM output.\n\n    Raises:\n        ValueError: If the cache is not set and cache is True.\n    \"\"\"\n    llm_cache = _resolve_cache(cache)\n    for i, result in enumerate(new_results.generations):\n        existing_prompts[missing_prompt_idxs[i]] = result\n        prompt = prompts[missing_prompt_idxs[i]]\n        if llm_cache:\n            await llm_cache.aupdate(prompt, llm_string, result)\n    llm_output = new_results.llm_output\n    return llm_output\n\n\nclass BaseLLM(BaseLanguageModel[str], ABC):\n    \"\"\"Base LLM abstract interface.\n\n    It should take in a prompt and return a string.\n    \"\"\"\n\n    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)\n    \"\"\"[DEPRECATED]\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def raise_deprecation(cls, values: dict) -> Any:\n        \"\"\"Raise deprecation warning if callback_manager is used.\"\"\"\n        if values.get(\"callback_manager\") is not None:\n            warnings.warn(\n                \"callback_manager is deprecated. Please use callbacks instead.\",\n                DeprecationWarning,\n                stacklevel=5,\n            )\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\n        return values\n\n    @functools.cached_property\n    def _serialized(self) -> dict[str, Any]:\n        return dumpd(self)\n\n    # --- Runnable methods ---\n\n    @property\n    @override\n    def OutputType(self) -> type[str]:\n        \"\"\"Get the input type for this runnable.\"\"\"\n        return str\n\n    def _convert_input(self, input: LanguageModelInput) -> PromptValue:\n        if isinstance(input, PromptValue):\n            return input\n        elif isinstance(input, str):\n            return StringPromptValue(text=input)\n        elif isinstance(input, Sequence):\n            return ChatPromptValue(messages=convert_to_messages(input))\n        else:\n            msg = (\n                f\"Invalid input type {type(input)}. \"\n                \"Must be a PromptValue, str, or list of BaseMessages.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n\n    def _get_ls_params(\n        self,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        # get default provider from class name\n        default_provider = self.__class__.__name__\n        default_provider = default_provider.removesuffix(\"LLM\")\n        default_provider = default_provider.lower()\n\n        ls_params = LangSmithParams(ls_provider=default_provider, ls_model_type=\"llm\")\n        if stop:\n            ls_params[\"ls_stop\"] = stop\n\n        # model\n        if hasattr(self, \"model\") and isinstance(self.model, str):\n            ls_params[\"ls_model_name\"] = self.model\n        elif hasattr(self, \"model_name\") and isinstance(self.model_name, str):\n            ls_params[\"ls_model_name\"] = self.model_name\n\n        # temperature\n        if \"temperature\" in kwargs and isinstance(kwargs[\"temperature\"], float):\n            ls_params[\"ls_temperature\"] = kwargs[\"temperature\"]\n        elif hasattr(self, \"temperature\") and isinstance(self.temperature, float):\n            ls_params[\"ls_temperature\"] = self.temperature\n\n        # max_tokens\n        if \"max_tokens\" in kwargs and isinstance(kwargs[\"max_tokens\"], int):\n            ls_params[\"ls_max_tokens\"] = kwargs[\"max_tokens\"]\n        elif hasattr(self, \"max_tokens\") and isinstance(self.max_tokens, int):\n            ls_params[\"ls_max_tokens\"] = self.max_tokens\n\n        return ls_params\n\n    def invoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> str:\n        config = ensure_config(config)\n        return (\n            self.generate_prompt(\n                [self._convert_input(input)],\n                stop=stop,\n                callbacks=config.get(\"callbacks\"),\n                tags=config.get(\"tags\"),\n                metadata=config.get(\"metadata\"),\n                run_name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                **kwargs,\n            )\n            .generations[0][0]\n            .text\n        )\n\n    async def ainvoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> str:\n        config = ensure_config(config)\n        llm_result = await self.agenerate_prompt(\n            [self._convert_input(input)],\n            stop=stop,\n            callbacks=config.get(\"callbacks\"),\n            tags=config.get(\"tags\"),\n            metadata=config.get(\"metadata\"),\n            run_name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            **kwargs,\n        )\n        return llm_result.generations[0][0].text\n\n    def batch(\n        self,\n        inputs: list[LanguageModelInput],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Any,\n    ) -> list[str]:\n        if not inputs:\n            return []\n\n        config = get_config_list(config, len(inputs))\n        max_concurrency = config[0].get(\"max_concurrency\")\n\n        if max_concurrency is None:\n            try:\n                llm_result = self.generate_prompt(\n                    [self._convert_input(input) for input in inputs],\n                    callbacks=[c.get(\"callbacks\") for c in config],\n                    tags=[c.get(\"tags\") for c in config],\n                    metadata=[c.get(\"metadata\") for c in config],\n                    run_name=[c.get(\"run_name\") for c in config],\n                    **kwargs,\n                )\n                return [g[0].text for g in llm_result.generations]\n            except Exception as e:\n                if return_exceptions:\n                    return cast(list[str], [e for _ in inputs])\n                else:\n                    raise\n        else:\n            batches = [\n                inputs[i : i + max_concurrency]\n                for i in range(0, len(inputs), max_concurrency)\n            ]\n            config = [{**c, \"max_concurrency\": None} for c in config]  # type: ignore[misc]\n            return [\n                output\n                for i, batch in enumerate(batches)\n                for output in self.batch(\n                    batch,\n                    config=config[i * max_concurrency : (i + 1) * max_concurrency],\n                    return_exceptions=return_exceptions,\n                    **kwargs,\n                )\n            ]\n\n    async def abatch(\n        self,\n        inputs: list[LanguageModelInput],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Any,\n    ) -> list[str]:\n        if not inputs:\n            return []\n        config = get_config_list(config, len(inputs))\n        max_concurrency = config[0].get(\"max_concurrency\")\n\n        if max_concurrency is None:\n            try:\n                llm_result = await self.agenerate_prompt(\n                    [self._convert_input(input) for input in inputs],\n                    callbacks=[c.get(\"callbacks\") for c in config],\n                    tags=[c.get(\"tags\") for c in config],\n                    metadata=[c.get(\"metadata\") for c in config],\n                    run_name=[c.get(\"run_name\") for c in config],\n                    **kwargs,\n                )\n                return [g[0].text for g in llm_result.generations]\n            except Exception as e:\n                if return_exceptions:\n                    return cast(list[str], [e for _ in inputs])\n                else:\n                    raise\n        else:\n            batches = [\n                inputs[i : i + max_concurrency]\n                for i in range(0, len(inputs), max_concurrency)\n            ]\n            config = [{**c, \"max_concurrency\": None} for c in config]  # type: ignore[misc]\n            return [\n                output\n                for i, batch in enumerate(batches)\n                for output in await self.abatch(\n                    batch,\n                    config=config[i * max_concurrency : (i + 1) * max_concurrency],\n                    return_exceptions=return_exceptions,\n                    **kwargs,\n                )\n            ]\n\n    def stream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> Iterator[str]:\n        if type(self)._stream == BaseLLM._stream:\n            # model doesn't implement streaming, so use default implementation\n            yield self.invoke(input, config=config, stop=stop, **kwargs)\n        else:\n            prompt = self._convert_input(input).to_string()\n            config = ensure_config(config)\n            params = self.dict()\n            params[\"stop\"] = stop\n            params = {**params, **kwargs}\n            options = {\"stop\": stop}\n            inheritable_metadata = {\n                **(config.get(\"metadata\") or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n            }\n            callback_manager = CallbackManager.configure(\n                config.get(\"callbacks\"),\n                self.callbacks,\n                self.verbose,\n                config.get(\"tags\"),\n                self.tags,\n                inheritable_metadata,\n                self.metadata,\n            )\n            (run_manager,) = callback_manager.on_llm_start(\n                self._serialized,\n                [prompt],\n                invocation_params=params,\n                options=options,\n                name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                batch_size=1,\n            )\n            generation: Optional[GenerationChunk] = None\n            try:\n                for chunk in self._stream(\n                    prompt, stop=stop, run_manager=run_manager, **kwargs\n                ):\n                    yield chunk.text\n                    if generation is None:\n                        generation = chunk\n                    else:\n                        generation += chunk\n            except BaseException as e:\n                run_manager.on_llm_error(\n                    e,\n                    response=LLMResult(\n                        generations=[[generation]] if generation else []\n                    ),\n                )\n                raise\n\n            if generation is None:\n                err = ValueError(\"No generation chunks were returned\")\n                run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n                raise err\n\n            run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n\n    async def astream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[str]:\n        if (\n            type(self)._astream is BaseLLM._astream\n            and type(self)._stream is BaseLLM._stream\n        ):\n            yield await self.ainvoke(input, config=config, stop=stop, **kwargs)\n            return\n\n        prompt = self._convert_input(input).to_string()\n        config = ensure_config(config)\n        params = self.dict()\n        params[\"stop\"] = stop\n        params = {**params, **kwargs}\n        options = {\"stop\": stop}\n        inheritable_metadata = {\n            **(config.get(\"metadata\") or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n        }\n        callback_manager = AsyncCallbackManager.configure(\n            config.get(\"callbacks\"),\n            self.callbacks,\n            self.verbose,\n            config.get(\"tags\"),\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n        (run_manager,) = await callback_manager.on_llm_start(\n            self._serialized,\n            [prompt],\n            invocation_params=params,\n            options=options,\n            name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            batch_size=1,\n        )\n        generation: Optional[GenerationChunk] = None\n        try:\n            async for chunk in self._astream(\n                prompt,\n                stop=stop,\n                run_manager=run_manager,\n                **kwargs,\n            ):\n                yield chunk.text\n                if generation is None:\n                    generation = chunk\n                else:\n                    generation += chunk\n        except BaseException as e:\n            await run_manager.on_llm_error(\n                e,\n                response=LLMResult(generations=[[generation]] if generation else []),\n            )\n            raise\n\n        if generation is None:\n            err = ValueError(\"No generation chunks were returned\")\n            await run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n            raise err\n\n        await run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n\n    # --- Custom methods ---\n\n    @abstractmethod\n    def _generate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Run the LLM on the given prompts.\"\"\"\n\n    async def _agenerate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Run the LLM on the given prompts.\"\"\"\n        return await run_in_executor(\n            None,\n            self._generate,\n            prompts,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n    def _stream(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[GenerationChunk]:\n        \"\"\"Stream the LLM on the given prompt.\n\n        This method should be overridden by subclasses that support streaming.\n\n        If not implemented, the default behavior of calls to stream will be to\n        fallback to the non-streaming version of the model and return\n        the output as a single chunk.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An iterator of GenerationChunks.\n        \"\"\"\n        raise NotImplementedError\n\n    async def _astream(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[GenerationChunk]:\n        \"\"\"An async version of the _stream method.\n\n        The default implementation uses the synchronous _stream method and wraps it in\n        an async iterator. Subclasses that need to provide a true async implementation\n        should override this method.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An async iterator of GenerationChunks.\n        \"\"\"\n        iterator = await run_in_executor(\n            None,\n            self._stream,\n            prompt,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n        done = object()\n        while True:\n            item = await run_in_executor(\n                None,\n                next,\n                iterator,\n                done,  # type: ignore[call-arg, arg-type]\n            )\n            if item is done:\n                break\n            yield item  # type: ignore[misc]\n\n    def generate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_strings = [p.to_string() for p in prompts]\n        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n\n    async def agenerate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_strings = [p.to_string() for p in prompts]\n        return await self.agenerate(\n            prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n        )\n\n    def _generate_helper(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]],\n        run_managers: list[CallbackManagerForLLMRun],\n        new_arg_supported: bool,\n        **kwargs: Any,\n    ) -> LLMResult:\n        try:\n            output = (\n                self._generate(\n                    prompts,\n                    stop=stop,\n                    # TODO: support multiple run managers\n                    run_manager=run_managers[0] if run_managers else None,\n                    **kwargs,\n                )\n                if new_arg_supported\n                else self._generate(prompts, stop=stop)\n            )\n        except BaseException as e:\n            for run_manager in run_managers:\n                run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n            raise\n        flattened_outputs = output.flatten()\n        for manager, flattened_output in zip(run_managers, flattened_outputs):\n            manager.on_llm_end(flattened_output)\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\n\n    def generate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        *,\n        tags: Optional[Union[list[str], list[list[str]]]] = None,\n        metadata: Optional[Union[dict[str, Any], list[dict[str, Any]]]] = None,\n        run_name: Optional[Union[str, list[str]]] = None,\n        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Pass a sequence of prompts to a model and return generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            prompts: List of string prompts.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            tags: List of tags to associate with each prompt. If provided, the length\n                of the list must match the length of the prompts list.\n            metadata: List of metadata dictionaries to associate with each prompt. If\n                provided, the length of the list must match the length of the prompts\n                list.\n            run_name: List of run names to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            run_id: List of run IDs to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        if not isinstance(prompts, list):\n            msg = (\n                \"Argument 'prompts' is expected to be of type List[str], received\"\n                f\" argument of type {type(prompts)}.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n        # Create callback managers\n        if isinstance(metadata, list):\n            metadata = [\n                {\n                    **(meta or {}),\n                    **self._get_ls_params(stop=stop, **kwargs),\n                }\n                for meta in metadata\n            ]\n        elif isinstance(metadata, dict):\n            metadata = {\n                **(metadata or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n            }\n        else:\n            pass\n        if (\n            isinstance(callbacks, list)\n            and callbacks\n            and (\n                isinstance(callbacks[0], (list, BaseCallbackManager))\n                or callbacks[0] is None\n            )\n        ):\n            # We've received a list of callbacks args to apply to each input\n            if len(callbacks) != len(prompts):\n                msg = \"callbacks must be the same length as prompts\"\n                raise ValueError(msg)\n            if tags is not None and not (\n                isinstance(tags, list) and len(tags) == len(prompts)\n            ):\n                msg = \"tags must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if metadata is not None and not (\n                isinstance(metadata, list) and len(metadata) == len(prompts)\n            ):\n                msg = \"metadata must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if run_name is not None and not (\n                isinstance(run_name, list) and len(run_name) == len(prompts)\n            ):\n                msg = \"run_name must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            callbacks = cast(list[Callbacks], callbacks)\n            tags_list = cast(list[Optional[list[str]]], tags or ([None] * len(prompts)))\n            metadata_list = cast(\n                list[Optional[dict[str, Any]]], metadata or ([{}] * len(prompts))\n            )\n            run_name_list = run_name or cast(\n                list[Optional[str]], ([None] * len(prompts))\n            )\n            callback_managers = [\n                CallbackManager.configure(\n                    callback,\n                    self.callbacks,\n                    self.verbose,\n                    tag,\n                    self.tags,\n                    meta,\n                    self.metadata,\n                )\n                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)\n            ]\n        else:\n            # We've received a single callbacks arg to apply to all inputs\n            callback_managers = [\n                CallbackManager.configure(\n                    cast(Callbacks, callbacks),\n                    self.callbacks,\n                    self.verbose,\n                    cast(list[str], tags),\n                    self.tags,\n                    cast(dict[str, Any], metadata),\n                    self.metadata,\n                )\n            ] * len(prompts)\n            run_name_list = [cast(Optional[str], run_name)] * len(prompts)\n        run_ids_list = self._get_run_ids_list(run_id, prompts)\n        params = self.dict()\n        params[\"stop\"] = stop\n        options = {\"stop\": stop}\n        (\n            existing_prompts,\n            llm_string,\n            missing_prompt_idxs,\n            missing_prompts,\n        ) = get_prompts(params, prompts, self.cache)\n        new_arg_supported = inspect.signature(self._generate).parameters.get(\n            \"run_manager\"\n        )\n        if (self.cache is None and get_llm_cache() is None) or self.cache is False:\n            run_managers = [\n                callback_manager.on_llm_start(\n                    self._serialized,\n                    [prompt],\n                    invocation_params=params,\n                    options=options,\n                    name=run_name,\n                    batch_size=len(prompts),\n                    run_id=run_id_,\n                )[0]\n                for callback_manager, prompt, run_name, run_id_ in zip(\n                    callback_managers, prompts, run_name_list, run_ids_list\n                )\n            ]\n            output = self._generate_helper(\n                prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            return output\n        if len(missing_prompts) > 0:\n            run_managers = [\n                callback_managers[idx].on_llm_start(\n                    self._serialized,\n                    [prompts[idx]],\n                    invocation_params=params,\n                    options=options,\n                    name=run_name_list[idx],\n                    batch_size=len(missing_prompts),\n                )[0]\n                for idx in missing_prompt_idxs\n            ]\n            new_results = self._generate_helper(\n                missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs\n            )\n            llm_output = update_cache(\n                self.cache,\n                existing_prompts,\n                llm_string,\n                missing_prompt_idxs,\n                new_results,\n                prompts,\n            )\n            run_info = (\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]\n                if run_managers\n                else None\n            )\n        else:\n            llm_output = {}\n            run_info = None\n        generations = [existing_prompts[i] for i in range(len(prompts))]\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)\n\n    @staticmethod\n    def _get_run_ids_list(\n        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]], prompts: list\n    ) -> list:\n        if run_id is None:\n            return [None] * len(prompts)\n        if isinstance(run_id, list):\n            if len(run_id) != len(prompts):\n                msg = (\n                    \"Number of manually provided run_id's does not match batch length.\"\n                    f\" {len(run_id)} != {len(prompts)}\"\n                )\n                raise ValueError(msg)\n            return run_id\n        return [run_id] + [None] * (len(prompts) - 1)\n\n    async def _agenerate_helper(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]],\n        run_managers: list[AsyncCallbackManagerForLLMRun],\n        new_arg_supported: bool,\n        **kwargs: Any,\n    ) -> LLMResult:\n        try:\n            output = (\n                await self._agenerate(\n                    prompts,\n                    stop=stop,\n                    run_manager=run_managers[0] if run_managers else None,\n                    **kwargs,\n                )\n                if new_arg_supported\n                else await self._agenerate(prompts, stop=stop)\n            )\n        except BaseException as e:\n            await asyncio.gather(\n                *[\n                    run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n                    for run_manager in run_managers\n                ]\n            )\n            raise\n        flattened_outputs = output.flatten()\n        await asyncio.gather(\n            *[\n                run_manager.on_llm_end(flattened_output)\n                for run_manager, flattened_output in zip(\n                    run_managers, flattened_outputs\n                )\n            ]\n        )\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\n\n    async def agenerate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,\n        *,\n        tags: Optional[Union[list[str], list[list[str]]]] = None,\n        metadata: Optional[Union[dict[str, Any], list[dict[str, Any]]]] = None,\n        run_name: Optional[Union[str, list[str]]] = None,\n        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Asynchronously pass a sequence of prompts to a model and return generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            prompts: List of string prompts.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            tags: List of tags to associate with each prompt. If provided, the length\n                of the list must match the length of the prompts list.\n            metadata: List of metadata dictionaries to associate with each prompt. If\n                provided, the length of the list must match the length of the prompts\n                list.\n            run_name: List of run names to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            run_id: List of run IDs to associate with each prompt. If provided, the\n                length of the list must match the length of the prompts list.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        if isinstance(metadata, list):\n            metadata = [\n                {\n                    **(meta or {}),\n                    **self._get_ls_params(stop=stop, **kwargs),\n                }\n                for meta in metadata\n            ]\n        elif isinstance(metadata, dict):\n            metadata = {\n                **(metadata or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n            }\n        else:\n            pass\n        # Create callback managers\n        if isinstance(callbacks, list) and (\n            isinstance(callbacks[0], (list, BaseCallbackManager))\n            or callbacks[0] is None\n        ):\n            # We've received a list of callbacks args to apply to each input\n            if len(callbacks) != len(prompts):\n                msg = \"callbacks must be the same length as prompts\"\n                raise ValueError(msg)\n            if tags is not None and not (\n                isinstance(tags, list) and len(tags) == len(prompts)\n            ):\n                msg = \"tags must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if metadata is not None and not (\n                isinstance(metadata, list) and len(metadata) == len(prompts)\n            ):\n                msg = \"metadata must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            if run_name is not None and not (\n                isinstance(run_name, list) and len(run_name) == len(prompts)\n            ):\n                msg = \"run_name must be a list of the same length as prompts\"\n                raise ValueError(msg)\n            callbacks = cast(list[Callbacks], callbacks)\n            tags_list = cast(list[Optional[list[str]]], tags or ([None] * len(prompts)))\n            metadata_list = cast(\n                list[Optional[dict[str, Any]]], metadata or ([{}] * len(prompts))\n            )\n            run_name_list = run_name or cast(\n                list[Optional[str]], ([None] * len(prompts))\n            )\n            callback_managers = [\n                AsyncCallbackManager.configure(\n                    callback,\n                    self.callbacks,\n                    self.verbose,\n                    tag,\n                    self.tags,\n                    meta,\n                    self.metadata,\n                )\n                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)\n            ]\n        else:\n            # We've received a single callbacks arg to apply to all inputs\n            callback_managers = [\n                AsyncCallbackManager.configure(\n                    cast(Callbacks, callbacks),\n                    self.callbacks,\n                    self.verbose,\n                    cast(list[str], tags),\n                    self.tags,\n                    cast(dict[str, Any], metadata),\n                    self.metadata,\n                )\n            ] * len(prompts)\n            run_name_list = [cast(Optional[str], run_name)] * len(prompts)\n        run_ids_list = self._get_run_ids_list(run_id, prompts)\n        params = self.dict()\n        params[\"stop\"] = stop\n        options = {\"stop\": stop}\n        (\n            existing_prompts,\n            llm_string,\n            missing_prompt_idxs,\n            missing_prompts,\n        ) = await aget_prompts(params, prompts, self.cache)\n\n        # Verify whether the cache is set, and if the cache is set,\n        # verify whether the cache is available.\n        new_arg_supported = inspect.signature(self._agenerate).parameters.get(\n            \"run_manager\"\n        )\n        if (self.cache is None and get_llm_cache() is None) or self.cache is False:\n            run_managers = await asyncio.gather(\n                *[\n                    callback_manager.on_llm_start(\n                        self._serialized,\n                        [prompt],\n                        invocation_params=params,\n                        options=options,\n                        name=run_name,\n                        batch_size=len(prompts),\n                        run_id=run_id_,\n                    )\n                    for callback_manager, prompt, run_name, run_id_ in zip(\n                        callback_managers, prompts, run_name_list, run_ids_list\n                    )\n                ]\n            )\n            run_managers = [r[0] for r in run_managers]  # type: ignore[misc]\n            output = await self._agenerate_helper(\n                prompts,\n                stop,\n                run_managers,  # type: ignore[arg-type]\n                bool(new_arg_supported),\n                **kwargs,  # type: ignore[arg-type]\n            )\n            return output\n        if len(missing_prompts) > 0:\n            run_managers = await asyncio.gather(\n                *[\n                    callback_managers[idx].on_llm_start(\n                        self._serialized,\n                        [prompts[idx]],\n                        invocation_params=params,\n                        options=options,\n                        name=run_name_list[idx],\n                        batch_size=len(missing_prompts),\n                    )\n                    for idx in missing_prompt_idxs\n                ]\n            )\n            run_managers = [r[0] for r in run_managers]  # type: ignore[misc]\n            new_results = await self._agenerate_helper(\n                missing_prompts,\n                stop,\n                run_managers,  # type: ignore[arg-type]\n                bool(new_arg_supported),\n                **kwargs,  # type: ignore[arg-type]\n            )\n            llm_output = await aupdate_cache(\n                self.cache,\n                existing_prompts,\n                llm_string,\n                missing_prompt_idxs,\n                new_results,\n                prompts,\n            )\n            run_info = (\n                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]  # type: ignore[attr-defined]\n                if run_managers\n                else None\n            )\n        else:\n            llm_output = {}\n            run_info = None\n        generations = [existing_prompts[i] for i in range(len(prompts))]\n        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def __call__(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Check Cache and run the LLM on the given prompt and input.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            tags: List of tags to associate with the prompt.\n            metadata: Metadata to associate with the prompt.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            The generated text.\n\n        Raises:\n            ValueError: If the prompt is not a string.\n        \"\"\"\n        if not isinstance(prompt, str):\n            msg = (\n                \"Argument `prompt` is expected to be a string. Instead found \"\n                f\"{type(prompt)}. If you want to run the LLM on multiple prompts, use \"\n                \"`generate` instead.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n        return (\n            self.generate(\n                [prompt],\n                stop=stop,\n                callbacks=callbacks,\n                tags=tags,\n                metadata=metadata,\n                **kwargs,\n            )\n            .generations[0][0]\n            .text\n        )\n\n    async def _call_async(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\n        result = await self.agenerate(\n            [prompt],\n            stop=stop,\n            callbacks=callbacks,\n            tags=tags,\n            metadata=metadata,\n            **kwargs,\n        )\n        return result.generations[0][0].text\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        return self(text, stop=_stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        text = get_buffer_string(messages)\n        _stop = None if stop is None else list(stop)\n        content = self(text, stop=_stop, **kwargs)\n        return AIMessage(content=content)\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        return await self._call_async(text, stop=_stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        text = get_buffer_string(messages)\n        _stop = None if stop is None else list(stop)\n        content = await self._call_async(text, stop=_stop, **kwargs)\n        return AIMessage(content=content)\n\n    def __str__(self) -> str:\n        \"\"\"Get a string representation of the object for printing.\"\"\"\n        cls_name = f\"\\033[1m{self.__class__.__name__}\\033[0m\"\n        return f\"{cls_name}\\nParams: {self._identifying_params}\"\n\n    @property\n    @abstractmethod\n    def _llm_type(self) -> str:\n        \"\"\"Return type of llm.\"\"\"\n\n    def dict(self, **kwargs: Any) -> dict:\n        \"\"\"Return a dictionary of the LLM.\"\"\"\n        starter_dict = dict(self._identifying_params)\n        starter_dict[\"_type\"] = self._llm_type\n        return starter_dict\n\n    def save(self, file_path: Union[Path, str]) -> None:\n        \"\"\"Save the LLM.\n\n        Args:\n            file_path: Path to file to save the LLM to.\n\n        Raises:\n            ValueError: If the file path is not a string or Path object.\n\n        Example:\n        .. code-block:: python\n\n            llm.save(file_path=\"path/llm.yaml\")\n        \"\"\"\n        # Convert file to Path object.\n        save_path = Path(file_path) if isinstance(file_path, str) else file_path\n\n        directory_path = save_path.parent\n        directory_path.mkdir(parents=True, exist_ok=True)\n\n        # Fetch dictionary to save\n        prompt_dict = self.dict()\n\n        if save_path.suffix == \".json\":\n            with open(file_path, \"w\") as f:\n                json.dump(prompt_dict, f, indent=4)\n        elif save_path.suffix.endswith((\".yaml\", \".yml\")):\n            with open(file_path, \"w\") as f:\n                yaml.dump(prompt_dict, f, default_flow_style=False)\n        else:\n            msg = f\"{save_path} must be json or yaml\"\n            raise ValueError(msg)\n\n\nclass LLM(BaseLLM):\n    \"\"\"Simple interface for implementing a custom LLM.\n\n    You should subclass this class and implement the following:\n\n    - `_call` method: Run the LLM on the given prompt and input (used by `invoke`).\n    - `_identifying_params` property: Return a dictionary of the identifying parameters\n        This is critical for caching and tracing purposes. Identifying parameters\n        is a dict that identifies the LLM.\n        It should mostly include a `model_name`.\n\n    Optional: Override the following methods to provide more optimizations:\n\n    - `_acall`: Provide a native async version of the `_call` method.\n        If not provided, will delegate to the synchronous version using\n        `run_in_executor`. (Used by `ainvoke`).\n    - `_stream`: Stream the LLM on the given prompt and input.\n        `stream` will use `_stream` if provided, otherwise it\n        use `_call` and output will arrive in one chunk.\n    - `_astream`: Override to provide a native async version of the `_stream` method.\n        `astream` will use `_astream` if provided, otherwise it will implement\n        a fallback behavior that will use `_stream` if `_stream` is implemented,\n        and use `_acall` if `_stream` is not implemented.\n\n    Please see the following guide for more information on how to\n    implement a custom LLM:\n\n    https://python.langchain.com/docs/how_to/custom_llm/\n    \"\"\"\n\n    @abstractmethod\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Run the LLM on the given input.\n\n        Override this method to implement the LLM logic.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of the stop substrings.\n                If stop tokens are not supported consider raising NotImplementedError.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            The model output as a string. SHOULD NOT include the prompt.\n        \"\"\"\n\n    async def _acall(\n        self,\n        prompt: str,\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Async version of the _call method.\n\n        The default implementation delegates to the synchronous _call method using\n        `run_in_executor`. Subclasses that need to provide a true async implementation\n        should override this method to reduce the overhead of using `run_in_executor`.\n\n        Args:\n            prompt: The prompt to generate from.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of the stop substrings.\n                If stop tokens are not supported consider raising NotImplementedError.\n            run_manager: Callback manager for the run.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            The model output as a string. SHOULD NOT include the prompt.\n        \"\"\"\n        return await run_in_executor(\n            None,\n            self._call,\n            prompt,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n    def _generate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Run the LLM on the given prompt and input.\"\"\"\n        # TODO: add caching here.\n        generations = []\n        new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\n        for prompt in prompts:\n            text = (\n                self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n                if new_arg_supported\n                else self._call(prompt, stop=stop, **kwargs)\n            )\n            generations.append([Generation(text=text)])\n        return LLMResult(generations=generations)\n\n    async def _agenerate(\n        self,\n        prompts: list[str],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Async run the LLM on the given prompt and input.\"\"\"\n        generations = []\n        new_arg_supported = inspect.signature(self._acall).parameters.get(\"run_manager\")\n        for prompt in prompts:\n            text = (\n                await self._acall(prompt, stop=stop, run_manager=run_manager, **kwargs)\n                if new_arg_supported\n                else await self._acall(prompt, stop=stop, **kwargs)\n            )\n            generations.append([Generation(text=text)])\n        return LLMResult(generations=generations)\n",
        "patch": "@@ -7,12 +7,12 @@\n import inspect\n import json\n import logging\n-import uuid\n import warnings\n from abc import ABC, abstractmethod\n from collections.abc import AsyncIterator, Iterator, Sequence\n from pathlib import Path\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     Optional,\n@@ -61,6 +61,9 @@\n from langchain_core.runnables import RunnableConfig, ensure_config, get_config_list\n from langchain_core.runnables.config import run_in_executor\n \n+if TYPE_CHECKING:\n+    import uuid\n+\n logger = logging.getLogger(__name__)\n \n "
      },
      {
        "filename": "libs/core/langchain_core/messages/base.py",
        "content_before": "from __future__ import annotations\n\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, Optional, Union, cast\n\nfrom pydantic import ConfigDict, Field, field_validator\n\nfrom langchain_core.load.serializable import Serializable\nfrom langchain_core.utils import get_bolded_text\nfrom langchain_core.utils._merge import merge_dicts, merge_lists\nfrom langchain_core.utils.interactive_env import is_interactive_env\n\nif TYPE_CHECKING:\n    from langchain_core.prompts.chat import ChatPromptTemplate\n\n\nclass BaseMessage(Serializable):\n    \"\"\"Base abstract message class.\n\n    Messages are the inputs and outputs of ChatModels.\n    \"\"\"\n\n    content: Union[str, list[Union[str, dict]]]\n    \"\"\"The string contents of the message.\"\"\"\n\n    additional_kwargs: dict = Field(default_factory=dict)\n    \"\"\"Reserved for additional payload data associated with the message.\n\n    For example, for a message from an AI, this could include tool calls as\n    encoded by the model provider.\n    \"\"\"\n\n    response_metadata: dict = Field(default_factory=dict)\n    \"\"\"Response metadata. For example: response headers, logprobs, token counts.\"\"\"\n\n    type: str\n    \"\"\"The type of the message. Must be a string that is unique to the message type.\n\n    The purpose of this field is to allow for easy identification of the message type\n    when deserializing messages.\n    \"\"\"\n\n    name: Optional[str] = None\n    \"\"\"An optional name for the message.\n\n    This can be used to provide a human-readable name for the message.\n\n    Usage of this field is optional, and whether it's used or not is up to the\n    model implementation.\n    \"\"\"\n\n    id: Optional[str] = None\n    \"\"\"An optional unique identifier for the message. This should ideally be\n    provided by the provider/model which created the message.\"\"\"\n\n    model_config = ConfigDict(\n        extra=\"allow\",\n    )\n\n    @field_validator(\"id\", mode=\"before\")\n    def cast_id_to_str(cls, id_value: Any) -> Optional[str]:\n        if id_value is not None:\n            return str(id_value)\n        else:\n            return id_value\n\n    def __init__(\n        self, content: Union[str, list[Union[str, dict]]], **kwargs: Any\n    ) -> None:\n        \"\"\"Pass in content as positional arg.\n\n        Args:\n            content: The string contents of the message.\n            kwargs: Additional fields to pass to the\n        \"\"\"\n        super().__init__(content=content, **kwargs)\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this class is serializable. This is used to determine\n        whether the class should be included in the langchain schema.\n\n        Returns:\n            True if the class is serializable, False otherwise.\n        \"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\n        Default is [\"langchain\", \"schema\", \"messages\"].\n        \"\"\"\n        return [\"langchain\", \"schema\", \"messages\"]\n\n    def text(self) -> str:\n        \"\"\"Get the text content of the message.\n\n        Returns:\n            The text content of the message.\n        \"\"\"\n        if isinstance(self.content, str):\n            return self.content\n\n        # must be a list\n        blocks = [\n            block\n            for block in self.content\n            if isinstance(block, str)\n            or block.get(\"type\") == \"text\"\n            and isinstance(block.get(\"text\"), str)\n        ]\n        return \"\".join(\n            block if isinstance(block, str) else block[\"text\"] for block in blocks\n        )\n\n    def __add__(self, other: Any) -> ChatPromptTemplate:\n        \"\"\"Concatenate this message with another message.\"\"\"\n        from langchain_core.prompts.chat import ChatPromptTemplate\n\n        prompt = ChatPromptTemplate(messages=[self])  # type: ignore[call-arg]\n        return prompt + other\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Get a pretty representation of the message.\n\n        Args:\n            html: Whether to format the message as HTML. If True, the message will be\n                formatted with HTML tags. Default is False.\n\n        Returns:\n            A pretty representation of the message.\n        \"\"\"\n        title = get_msg_title_repr(self.type.title() + \" Message\", bold=html)\n        # TODO: handle non-string content.\n        if self.name is not None:\n            title += f\"\\nName: {self.name}\"\n        return f\"{title}\\n\\n{self.content}\"\n\n    def pretty_print(self) -> None:\n        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201\n\n\ndef merge_content(\n    first_content: Union[str, list[Union[str, dict]]],\n    *contents: Union[str, list[Union[str, dict]]],\n) -> Union[str, list[Union[str, dict]]]:\n    \"\"\"Merge two message contents.\n\n    Args:\n        first_content: The first content. Can be a string or a list.\n        second_content: The second content. Can be a string or a list.\n\n    Returns:\n        The merged content.\n    \"\"\"\n    merged = first_content\n    for content in contents:\n        # If current is a string\n        if isinstance(merged, str):\n            # If the next chunk is also a string, then merge them naively\n            if isinstance(content, str):\n                merged = cast(str, merged) + content\n            # If the next chunk is a list, add the current to the start of the list\n            else:\n                merged = [merged] + content  # type: ignore\n        elif isinstance(content, list):\n            # If both are lists\n            merged = merge_lists(cast(list, merged), content)  # type: ignore\n        # If the first content is a list, and the second content is a string\n        else:\n            # If the last element of the first content is a string\n            # Add the second content to the last element\n            if merged and isinstance(merged[-1], str):\n                merged[-1] += content\n            # If second content is an empty string, treat as a no-op\n            elif content == \"\":\n                pass\n            else:\n                # Otherwise, add the second content as a new element of the list\n                merged.append(content)\n    return merged\n\n\nclass BaseMessageChunk(BaseMessage):\n    \"\"\"Message chunk, which can be concatenated with other Message chunks.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\n        Default is [\"langchain\", \"schema\", \"messages\"].\n        \"\"\"\n        return [\"langchain\", \"schema\", \"messages\"]\n\n    def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore\n        \"\"\"Message chunks support concatenation with other message chunks.\n\n        This functionality is useful to combine message chunks yielded from\n        a streaming model into a complete message.\n\n        Args:\n            other: Another message chunk to concatenate with this one.\n\n        Returns:\n            A new message chunk that is the concatenation of this message chunk\n            and the other message chunk.\n\n        Raises:\n            TypeError: If the other object is not a message chunk.\n\n        For example,\n\n        `AIMessageChunk(content=\"Hello\") + AIMessageChunk(content=\" World\")`\n\n        will give `AIMessageChunk(content=\"Hello World\")`\n        \"\"\"\n        if isinstance(other, BaseMessageChunk):\n            # If both are (subclasses of) BaseMessageChunk,\n            # concat into a single BaseMessageChunk\n\n            return self.__class__(  # type: ignore[call-arg]\n                id=self.id,\n                type=self.type,\n                content=merge_content(self.content, other.content),\n                additional_kwargs=merge_dicts(\n                    self.additional_kwargs, other.additional_kwargs\n                ),\n                response_metadata=merge_dicts(\n                    self.response_metadata, other.response_metadata\n                ),\n            )\n        elif isinstance(other, list) and all(\n            isinstance(o, BaseMessageChunk) for o in other\n        ):\n            content = merge_content(self.content, *(o.content for o in other))\n            additional_kwargs = merge_dicts(\n                self.additional_kwargs, *(o.additional_kwargs for o in other)\n            )\n            response_metadata = merge_dicts(\n                self.response_metadata, *(o.response_metadata for o in other)\n            )\n            return self.__class__(  # type: ignore[call-arg]\n                id=self.id,\n                content=content,\n                additional_kwargs=additional_kwargs,\n                response_metadata=response_metadata,\n            )\n        else:\n            msg = (\n                'unsupported operand type(s) for +: \"'\n                f\"{self.__class__.__name__}\"\n                f'\" and \"{other.__class__.__name__}\"'\n            )\n            raise TypeError(msg)\n\n\ndef message_to_dict(message: BaseMessage) -> dict:\n    \"\"\"Convert a Message to a dictionary.\n\n    Args:\n        message: Message to convert.\n\n    Returns:\n        Message as a dict. The dict will have a \"type\" key with the message type\n        and a \"data\" key with the message data as a dict.\n    \"\"\"\n    return {\"type\": message.type, \"data\": message.model_dump()}\n\n\ndef messages_to_dict(messages: Sequence[BaseMessage]) -> list[dict]:\n    \"\"\"Convert a sequence of Messages to a list of dictionaries.\n\n    Args:\n        messages: Sequence of messages (as BaseMessages) to convert.\n\n    Returns:\n        List of messages as dicts.\n    \"\"\"\n    return [message_to_dict(m) for m in messages]\n\n\ndef get_msg_title_repr(title: str, *, bold: bool = False) -> str:\n    \"\"\"Get a title representation for a message.\n\n    Args:\n        title: The title.\n        bold: Whether to bold the title. Default is False.\n\n    Returns:\n        The title representation.\n    \"\"\"\n    padded = \" \" + title + \" \"\n    sep_len = (80 - len(padded)) // 2\n    sep = \"=\" * sep_len\n    second_sep = sep + \"=\" if len(padded) % 2 else sep\n    if bold:\n        padded = get_bolded_text(padded)\n    return f\"{sep}{padded}{second_sep}\"\n",
        "patch": "@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-from collections.abc import Sequence\n from typing import TYPE_CHECKING, Any, Optional, Union, cast\n \n from pydantic import ConfigDict, Field, field_validator\n@@ -11,6 +10,8 @@\n from langchain_core.utils.interactive_env import is_interactive_env\n \n if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n     from langchain_core.prompts.chat import ChatPromptTemplate\n \n "
      },
      {
        "filename": "libs/core/langchain_core/output_parsers/list.py",
        "content_before": "from __future__ import annotations\n\nimport csv\nimport re\nfrom abc import abstractmethod\nfrom collections import deque\nfrom collections.abc import AsyncIterator, Iterator\nfrom io import StringIO\nfrom typing import Optional as Optional\nfrom typing import TypeVar, Union\n\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.output_parsers.transform import BaseTransformOutputParser\n\nT = TypeVar(\"T\")\n\n\ndef droplastn(iter: Iterator[T], n: int) -> Iterator[T]:\n    \"\"\"Drop the last n elements of an iterator.\n\n    Args:\n        iter: The iterator to drop elements from.\n        n: The number of elements to drop.\n\n    Yields:\n        The elements of the iterator, except the last n elements.\n    \"\"\"\n    buffer: deque[T] = deque()\n    for item in iter:\n        buffer.append(item)\n        if len(buffer) > n:\n            yield buffer.popleft()\n\n\nclass ListOutputParser(BaseTransformOutputParser[list[str]]):\n    \"\"\"Parse the output of an LLM call to a list.\"\"\"\n\n    @property\n    def _type(self) -> str:\n        return \"list\"\n\n    @abstractmethod\n    def parse(self, text: str) -> list[str]:\n        \"\"\"Parse the output of an LLM call.\n\n        Args:\n            text: The output of an LLM call.\n\n        Returns:\n            A list of strings.\n        \"\"\"\n\n    def parse_iter(self, text: str) -> Iterator[re.Match]:\n        \"\"\"Parse the output of an LLM call.\n\n        Args:\n            text: The output of an LLM call.\n\n        Yields:\n            A match object for each part of the output.\n        \"\"\"\n        raise NotImplementedError\n\n    def _transform(\n        self, input: Iterator[Union[str, BaseMessage]]\n    ) -> Iterator[list[str]]:\n        buffer = \"\"\n        for chunk in input:\n            if isinstance(chunk, BaseMessage):\n                # extract text\n                chunk_content = chunk.content\n                if not isinstance(chunk_content, str):\n                    continue\n                chunk = chunk_content\n            # add current chunk to buffer\n            buffer += chunk\n            # parse buffer into a list of parts\n            try:\n                done_idx = 0\n                # yield only complete parts\n                for m in droplastn(self.parse_iter(buffer), 1):\n                    done_idx = m.end()\n                    yield [m.group(1)]\n                buffer = buffer[done_idx:]\n            except NotImplementedError:\n                parts = self.parse(buffer)\n                # yield only complete parts\n                if len(parts) > 1:\n                    for part in parts[:-1]:\n                        yield [part]\n                    buffer = parts[-1]\n        # yield the last part\n        for part in self.parse(buffer):\n            yield [part]\n\n    async def _atransform(\n        self, input: AsyncIterator[Union[str, BaseMessage]]\n    ) -> AsyncIterator[list[str]]:\n        buffer = \"\"\n        async for chunk in input:\n            if isinstance(chunk, BaseMessage):\n                # extract text\n                chunk_content = chunk.content\n                if not isinstance(chunk_content, str):\n                    continue\n                chunk = chunk_content\n            # add current chunk to buffer\n            buffer += chunk\n            # parse buffer into a list of parts\n            try:\n                done_idx = 0\n                # yield only complete parts\n                for m in droplastn(self.parse_iter(buffer), 1):\n                    done_idx = m.end()\n                    yield [m.group(1)]\n                buffer = buffer[done_idx:]\n            except NotImplementedError:\n                parts = self.parse(buffer)\n                # yield only complete parts\n                if len(parts) > 1:\n                    for part in parts[:-1]:\n                        yield [part]\n                    buffer = parts[-1]\n        # yield the last part\n        for part in self.parse(buffer):\n            yield [part]\n\n\nListOutputParser.model_rebuild()\n\n\nclass CommaSeparatedListOutputParser(ListOutputParser):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Check if the langchain object is serializable.\n\n        Returns True.\n        \"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\n\n        Returns:\n            A list of strings.\n            Default is [\"langchain\", \"output_parsers\", \"list\"].\n        \"\"\"\n        return [\"langchain\", \"output_parsers\", \"list\"]\n\n    def get_format_instructions(self) -> str:\n        \"\"\"Return the format instructions for the comma-separated list output.\"\"\"\n        return (\n            \"Your response should be a list of comma separated values, \"\n            \"eg: `foo, bar, baz` or `foo,bar,baz`\"\n        )\n\n    def parse(self, text: str) -> list[str]:\n        \"\"\"Parse the output of an LLM call.\n\n        Args:\n            text: The output of an LLM call.\n\n        Returns:\n            A list of strings.\n        \"\"\"\n        try:\n            reader = csv.reader(\n                StringIO(text), quotechar='\"', delimiter=\",\", skipinitialspace=True\n            )\n            return [item for sublist in reader for item in sublist]\n        except csv.Error:\n            # keep old logic for backup\n            return [part.strip() for part in text.split(\",\")]\n\n    @property\n    def _type(self) -> str:\n        return \"comma-separated-list\"\n\n\nclass NumberedListOutputParser(ListOutputParser):\n    \"\"\"Parse a numbered list.\"\"\"\n\n    pattern: str = r\"\\d+\\.\\s([^\\n]+)\"\n    \"\"\"The pattern to match a numbered list item.\"\"\"\n\n    def get_format_instructions(self) -> str:\n        return (\n            \"Your response should be a numbered list with each item on a new line. \"\n            \"For example: \\n\\n1. foo\\n\\n2. bar\\n\\n3. baz\"\n        )\n\n    def parse(self, text: str) -> list[str]:\n        \"\"\"Parse the output of an LLM call.\n\n        Args:\n            text: The output of an LLM call.\n\n        Returns:\n            A list of strings.\n        \"\"\"\n        return re.findall(self.pattern, text)\n\n    def parse_iter(self, text: str) -> Iterator[re.Match]:\n        \"\"\"Parse the output of an LLM call.\n\n        Args:\n            text: The output of an LLM call.\n\n        Yields:\n            A match object for each part of the output.\n        \"\"\"\n        return re.finditer(self.pattern, text)\n\n    @property\n    def _type(self) -> str:\n        return \"numbered-list\"\n\n\nclass MarkdownListOutputParser(ListOutputParser):\n    \"\"\"Parse a Markdown list.\"\"\"\n\n    pattern: str = r\"^\\s*[-*]\\s([^\\n]+)$\"\n    \"\"\"The pattern to match a Markdown list item.\"\"\"\n\n    def get_format_instructions(self) -> str:\n        \"\"\"Return the format instructions for the Markdown list output.\"\"\"\n        return \"Your response should be a markdown list, eg: `- foo\\n- bar\\n- baz`\"\n\n    def parse(self, text: str) -> list[str]:\n        \"\"\"Parse the output of an LLM call.\n\n        Args:\n            text: The output of an LLM call.\n\n        Returns:\n            A list of strings.\n        \"\"\"\n        return re.findall(self.pattern, text, re.MULTILINE)\n\n    def parse_iter(self, text: str) -> Iterator[re.Match]:\n        \"\"\"Parse the output of an LLM call.\n\n        Args:\n            text: The output of an LLM call.\n\n        Yields:\n            A match object for each part of the output.\n        \"\"\"\n        return re.finditer(self.pattern, text, re.MULTILINE)\n\n    @property\n    def _type(self) -> str:\n        return \"markdown-list\"\n",
        "patch": "@@ -4,14 +4,16 @@\n import re\n from abc import abstractmethod\n from collections import deque\n-from collections.abc import AsyncIterator, Iterator\n from io import StringIO\n+from typing import TYPE_CHECKING, TypeVar, Union\n from typing import Optional as Optional\n-from typing import TypeVar, Union\n \n from langchain_core.messages import BaseMessage\n from langchain_core.output_parsers.transform import BaseTransformOutputParser\n \n+if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Iterator\n+\n T = TypeVar(\"T\")\n \n "
      },
      {
        "filename": "libs/core/langchain_core/output_parsers/transform.py",
        "content_before": "from __future__ import annotations\n\nfrom collections.abc import AsyncIterator, Iterator\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Optional,\n    Union,\n)\n\nfrom langchain_core.messages import BaseMessage, BaseMessageChunk\nfrom langchain_core.output_parsers.base import BaseOutputParser, T\nfrom langchain_core.outputs import (\n    ChatGeneration,\n    ChatGenerationChunk,\n    Generation,\n    GenerationChunk,\n)\nfrom langchain_core.runnables.config import run_in_executor\n\nif TYPE_CHECKING:\n    from langchain_core.runnables import RunnableConfig\n\n\nclass BaseTransformOutputParser(BaseOutputParser[T]):\n    \"\"\"Base class for an output parser that can handle streaming input.\"\"\"\n\n    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[T]:\n        for chunk in input:\n            if isinstance(chunk, BaseMessage):\n                yield self.parse_result([ChatGeneration(message=chunk)])\n            else:\n                yield self.parse_result([Generation(text=chunk)])\n\n    async def _atransform(\n        self, input: AsyncIterator[Union[str, BaseMessage]]\n    ) -> AsyncIterator[T]:\n        async for chunk in input:\n            if isinstance(chunk, BaseMessage):\n                yield await run_in_executor(\n                    None, self.parse_result, [ChatGeneration(message=chunk)]\n                )\n            else:\n                yield await run_in_executor(\n                    None, self.parse_result, [Generation(text=chunk)]\n                )\n\n    def transform(\n        self,\n        input: Iterator[Union[str, BaseMessage]],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[T]:\n        \"\"\"Transform the input into the output format.\n\n        Args:\n            input: The input to transform.\n            config: The configuration to use for the transformation.\n            kwargs: Additional keyword arguments.\n\n        Yields:\n            The transformed output.\n        \"\"\"\n        yield from self._transform_stream_with_config(\n            input, self._transform, config, run_type=\"parser\"\n        )\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Union[str, BaseMessage]],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[T]:\n        \"\"\"Async transform the input into the output format.\n\n        Args:\n            input: The input to transform.\n            config: The configuration to use for the transformation.\n            kwargs: Additional keyword arguments.\n\n        Yields:\n            The transformed output.\n        \"\"\"\n        async for chunk in self._atransform_stream_with_config(\n            input, self._atransform, config, run_type=\"parser\"\n        ):\n            yield chunk\n\n\nclass BaseCumulativeTransformOutputParser(BaseTransformOutputParser[T]):\n    \"\"\"Base class for an output parser that can handle streaming input.\"\"\"\n\n    diff: bool = False\n    \"\"\"In streaming mode, whether to yield diffs between the previous and current\n    parsed output, or just the current parsed output.\n    \"\"\"\n\n    def _diff(self, prev: Optional[T], next: T) -> T:\n        \"\"\"Convert parsed outputs into a diff format. The semantics of this are\n        up to the output parser.\n\n        Args:\n            prev: The previous parsed output.\n            next: The current parsed output.\n\n        Returns:\n            The diff between the previous and current parsed output.\n        \"\"\"\n        raise NotImplementedError\n\n    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Any]:\n        prev_parsed = None\n        acc_gen: Union[GenerationChunk, ChatGenerationChunk, None] = None\n        for chunk in input:\n            chunk_gen: Union[GenerationChunk, ChatGenerationChunk]\n            if isinstance(chunk, BaseMessageChunk):\n                chunk_gen = ChatGenerationChunk(message=chunk)\n            elif isinstance(chunk, BaseMessage):\n                chunk_gen = ChatGenerationChunk(\n                    message=BaseMessageChunk(**chunk.dict())\n                )\n            else:\n                chunk_gen = GenerationChunk(text=chunk)\n\n            acc_gen = chunk_gen if acc_gen is None else acc_gen + chunk_gen  # type: ignore[operator]\n\n            parsed = self.parse_result([acc_gen], partial=True)\n            if parsed is not None and parsed != prev_parsed:\n                if self.diff:\n                    yield self._diff(prev_parsed, parsed)\n                else:\n                    yield parsed\n                prev_parsed = parsed\n\n    async def _atransform(\n        self, input: AsyncIterator[Union[str, BaseMessage]]\n    ) -> AsyncIterator[T]:\n        prev_parsed = None\n        acc_gen: Union[GenerationChunk, ChatGenerationChunk, None] = None\n        async for chunk in input:\n            chunk_gen: Union[GenerationChunk, ChatGenerationChunk]\n            if isinstance(chunk, BaseMessageChunk):\n                chunk_gen = ChatGenerationChunk(message=chunk)\n            elif isinstance(chunk, BaseMessage):\n                chunk_gen = ChatGenerationChunk(\n                    message=BaseMessageChunk(**chunk.dict())\n                )\n            else:\n                chunk_gen = GenerationChunk(text=chunk)\n\n            acc_gen = chunk_gen if acc_gen is None else acc_gen + chunk_gen  # type: ignore[operator]\n\n            parsed = await self.aparse_result([acc_gen], partial=True)\n            if parsed is not None and parsed != prev_parsed:\n                if self.diff:\n                    yield await run_in_executor(None, self._diff, prev_parsed, parsed)\n                else:\n                    yield parsed\n                prev_parsed = parsed\n",
        "patch": "@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-from collections.abc import AsyncIterator, Iterator\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -19,6 +18,8 @@\n from langchain_core.runnables.config import run_in_executor\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Iterator\n+\n     from langchain_core.runnables import RunnableConfig\n \n "
      },
      {
        "filename": "libs/core/langchain_core/outputs/chat_generation.py",
        "content_before": "from __future__ import annotations\n\nfrom typing import Literal, Union\n\nfrom pydantic import model_validator\nfrom typing_extensions import Self\n\nfrom langchain_core.messages import BaseMessage, BaseMessageChunk\nfrom langchain_core.outputs.generation import Generation\nfrom langchain_core.utils._merge import merge_dicts\n\n\nclass ChatGeneration(Generation):\n    \"\"\"A single chat generation output.\n\n    A subclass of Generation that represents the response from a chat model\n    that generates chat messages.\n\n    The `message` attribute is a structured representation of the chat message.\n    Most of the time, the message will be of type `AIMessage`.\n\n    Users working with chat models will usually access information via either\n    `AIMessage` (returned from runnable interfaces) or `LLMResult` (available\n    via callbacks).\n    \"\"\"\n\n    text: str = \"\"\n    \"\"\"*SHOULD NOT BE SET DIRECTLY* The text contents of the output message.\"\"\"\n    message: BaseMessage\n    \"\"\"The message output by the chat model.\"\"\"\n    # Override type to be ChatGeneration, ignore mypy error as this is intentional\n    type: Literal[\"ChatGeneration\"] = \"ChatGeneration\"  # type: ignore[assignment]\n    \"\"\"Type is used exclusively for serialization purposes.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def set_text(self) -> Self:\n        \"\"\"Set the text attribute to be the contents of the message.\n\n        Args:\n            values: The values of the object.\n\n        Returns:\n            The values of the object with the text attribute set.\n\n        Raises:\n            ValueError: If the message is not a string or a list.\n        \"\"\"\n        try:\n            text = \"\"\n            if isinstance(self.message.content, str):\n                text = self.message.content\n            # HACK: Assumes text in content blocks in OpenAI format.\n            # Uses first text block.\n            elif isinstance(self.message.content, list):\n                for block in self.message.content:\n                    if isinstance(block, str):\n                        text = block\n                        break\n                    elif isinstance(block, dict) and \"text\" in block:\n                        text = block[\"text\"]\n                        break\n                    else:\n                        pass\n            else:\n                pass\n            self.text = text\n        except (KeyError, AttributeError) as e:\n            msg = \"Error while initializing ChatGeneration\"\n            raise ValueError(msg) from e\n        return self\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"output\"]\n\n\nclass ChatGenerationChunk(ChatGeneration):\n    \"\"\"ChatGeneration chunk, which can be concatenated with other\n    ChatGeneration chunks.\n    \"\"\"\n\n    message: BaseMessageChunk\n    \"\"\"The message chunk output by the chat model.\"\"\"\n    # Override type to be ChatGeneration, ignore mypy error as this is intentional\n    type: Literal[\"ChatGenerationChunk\"] = \"ChatGenerationChunk\"  # type: ignore[assignment]\n    \"\"\"Type is used exclusively for serialization purposes.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"output\"]\n\n    def __add__(\n        self, other: Union[ChatGenerationChunk, list[ChatGenerationChunk]]\n    ) -> ChatGenerationChunk:\n        if isinstance(other, ChatGenerationChunk):\n            generation_info = merge_dicts(\n                self.generation_info or {},\n                other.generation_info or {},\n            )\n            return ChatGenerationChunk(\n                message=self.message + other.message,\n                generation_info=generation_info or None,\n            )\n        elif isinstance(other, list) and all(\n            isinstance(x, ChatGenerationChunk) for x in other\n        ):\n            generation_info = merge_dicts(\n                self.generation_info or {},\n                *[chunk.generation_info for chunk in other if chunk.generation_info],\n            )\n            return ChatGenerationChunk(\n                message=self.message + [chunk.message for chunk in other],\n                generation_info=generation_info or None,\n            )\n        else:\n            msg = (\n                f\"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'\"\n            )\n            raise TypeError(msg)\n",
        "patch": "@@ -1,14 +1,16 @@\n from __future__ import annotations\n \n-from typing import Literal, Union\n+from typing import TYPE_CHECKING, Literal, Union\n \n from pydantic import model_validator\n-from typing_extensions import Self\n \n from langchain_core.messages import BaseMessage, BaseMessageChunk\n from langchain_core.outputs.generation import Generation\n from langchain_core.utils._merge import merge_dicts\n \n+if TYPE_CHECKING:\n+    from typing_extensions import Self\n+\n \n class ChatGeneration(Generation):\n     \"\"\"A single chat generation output."
      },
      {
        "filename": "libs/core/langchain_core/prompts/chat.py",
        "content_before": "\"\"\"Chat prompt template.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom typing import (\n    Annotated,\n    Any,\n    Optional,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\n\nfrom pydantic import (\n    Field,\n    PositiveInt,\n    SkipValidation,\n    model_validator,\n)\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.load import Serializable\nfrom langchain_core.messages import (\n    AIMessage,\n    AnyMessage,\n    BaseMessage,\n    ChatMessage,\n    HumanMessage,\n    SystemMessage,\n    convert_to_messages,\n)\nfrom langchain_core.messages.base import get_msg_title_repr\nfrom langchain_core.prompt_values import ChatPromptValue, ImageURL, PromptValue\nfrom langchain_core.prompts.base import BasePromptTemplate\nfrom langchain_core.prompts.image import ImagePromptTemplate\nfrom langchain_core.prompts.prompt import PromptTemplate\nfrom langchain_core.prompts.string import (\n    PromptTemplateFormat,\n    StringPromptTemplate,\n    get_template_variables,\n)\nfrom langchain_core.utils import get_colored_text\nfrom langchain_core.utils.interactive_env import is_interactive_env\n\n\nclass BaseMessagePromptTemplate(Serializable, ABC):\n    \"\"\"Base class for message prompt templates.\"\"\"\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether or not the class is serializable.\n        Returns: True.\n        \"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    @abstractmethod\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs. Should return a list of BaseMessages.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format messages from kwargs.\n        Should return a list of BaseMessages.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return self.format_messages(**kwargs)\n\n    @property\n    @abstractmethod\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variables.\n        \"\"\"\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        raise NotImplementedError\n\n    def pretty_print(self) -> None:\n        \"\"\"Print a human-readable representation.\"\"\"\n        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201\n\n    def __add__(self, other: Any) -> ChatPromptTemplate:\n        \"\"\"Combine two prompt templates.\n\n        Args:\n            other: Another prompt template.\n\n        Returns:\n            Combined prompt template.\n        \"\"\"\n        prompt = ChatPromptTemplate(messages=[self])  # type: ignore[call-arg]\n        return prompt + other\n\n\nclass MessagesPlaceholder(BaseMessagePromptTemplate):\n    \"\"\"Prompt template that assumes variable is already list of messages.\n\n    A placeholder which can be used to pass in a list of messages.\n\n    Direct usage:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import MessagesPlaceholder\n\n            prompt = MessagesPlaceholder(\"history\")\n            prompt.format_messages() # raises KeyError\n\n            prompt = MessagesPlaceholder(\"history\", optional=True)\n            prompt.format_messages() # returns empty list []\n\n            prompt.format_messages(\n                history=[\n                    (\"system\", \"You are an AI assistant.\"),\n                    (\"human\", \"Hello!\"),\n                ]\n            )\n            # -> [\n            #     SystemMessage(content=\"You are an AI assistant.\"),\n            #     HumanMessage(content=\"Hello!\"),\n            # ]\n\n    Building a prompt with chat history:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n            prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", \"You are a helpful assistant.\"),\n                    MessagesPlaceholder(\"history\"),\n                    (\"human\", \"{question}\")\n                ]\n            )\n            prompt.invoke(\n               {\n                   \"history\": [(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n                   \"question\": \"now multiply that by 4\"\n               }\n            )\n            # -> ChatPromptValue(messages=[\n            #     SystemMessage(content=\"You are a helpful assistant.\"),\n            #     HumanMessage(content=\"what's 5 + 2\"),\n            #     AIMessage(content=\"5 + 2 is 7\"),\n            #     HumanMessage(content=\"now multiply that by 4\"),\n            # ])\n\n    Limiting the number of messages:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import MessagesPlaceholder\n\n            prompt = MessagesPlaceholder(\"history\", n_messages=1)\n\n            prompt.format_messages(\n                history=[\n                    (\"system\", \"You are an AI assistant.\"),\n                    (\"human\", \"Hello!\"),\n                ]\n            )\n            # -> [\n            #     HumanMessage(content=\"Hello!\"),\n            # ]\n    \"\"\"\n\n    variable_name: str\n    \"\"\"Name of variable to use as messages.\"\"\"\n\n    optional: bool = False\n    \"\"\"If True format_messages can be called with no arguments and will return an empty\n        list. If False then a named argument with name `variable_name` must be passed\n        in, even if the value is an empty list.\"\"\"\n\n    n_messages: Optional[PositiveInt] = None\n    \"\"\"Maximum number of messages to include. If None, then will include all.\n    Defaults to None.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    def __init__(\n        self, variable_name: str, *, optional: bool = False, **kwargs: Any\n    ) -> None:\n        # mypy can't detect the init which is defined in the parent class\n        # b/c these are BaseModel classes.\n        super().__init__(  # type: ignore\n            variable_name=variable_name, optional=optional, **kwargs\n        )\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessage.\n\n        Raises:\n            ValueError: If variable is not a list of messages.\n        \"\"\"\n        value = (\n            kwargs.get(self.variable_name, [])\n            if self.optional\n            else kwargs[self.variable_name]\n        )\n        if not isinstance(value, list):\n            msg = (\n                f\"variable {self.variable_name} should be a list of base messages, \"\n                f\"got {value} of type {type(value)}\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n        value = convert_to_messages(value)\n        if self.n_messages:\n            value = value[-self.n_messages :]\n        return value\n\n    @property\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variable names.\n        \"\"\"\n        return [self.variable_name] if not self.optional else []\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        var = \"{\" + self.variable_name + \"}\"\n        if html:\n            title = get_msg_title_repr(\"Messages Placeholder\", bold=True)\n            var = get_colored_text(var, \"yellow\")\n        else:\n            title = get_msg_title_repr(\"Messages Placeholder\")\n        return f\"{title}\\n\\n{var}\"\n\n\nMessagePromptTemplateT = TypeVar(\n    \"MessagePromptTemplateT\", bound=\"BaseStringMessagePromptTemplate\"\n)\n\"\"\"Type variable for message prompt templates.\"\"\"\n\n\nclass BaseStringMessagePromptTemplate(BaseMessagePromptTemplate, ABC):\n    \"\"\"Base class for message prompt templates that use a string prompt template.\"\"\"\n\n    prompt: StringPromptTemplate\n    \"\"\"String prompt template.\"\"\"\n    additional_kwargs: dict = Field(default_factory=dict)\n    \"\"\"Additional keyword arguments to pass to the prompt template.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    @classmethod\n    def from_template(\n        cls: type[MessagePromptTemplateT],\n        template: str,\n        template_format: PromptTemplateFormat = \"f-string\",\n        partial_variables: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> MessagePromptTemplateT:\n        \"\"\"Create a class from a string template.\n\n        Args:\n            template: a template.\n            template_format: format of the template. Defaults to \"f-string\".\n            partial_variables: A dictionary of variables that can be used to partially\n                               fill in the template. For example, if the template is\n                              `\"{variable1} {variable2}\"`, and `partial_variables` is\n                              `{\"variable1\": \"foo\"}`, then the final prompt will be\n                              `\"foo {variable2}\"`.\n                              Defaults to None.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        prompt = PromptTemplate.from_template(\n            template,\n            template_format=template_format,\n            partial_variables=partial_variables,\n        )\n        return cls(prompt=prompt, **kwargs)\n\n    @classmethod\n    def from_template_file(\n        cls: type[MessagePromptTemplateT],\n        template_file: Union[str, Path],\n        input_variables: list[str],\n        **kwargs: Any,\n    ) -> MessagePromptTemplateT:\n        \"\"\"Create a class from a template file.\n\n        Args:\n            template_file: path to a template file. String or Path.\n            input_variables: list of input variables.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        prompt = PromptTemplate.from_file(template_file, input_variables)\n        return cls(prompt=prompt, **kwargs)\n\n    @abstractmethod\n    def format(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n\n    async def aformat(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Async format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        return self.format(**kwargs)\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [self.format(**kwargs)]\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [await self.aformat(**kwargs)]\n\n    @property\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variable names.\n        \"\"\"\n        return self.prompt.input_variables\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        # TODO: Handle partials\n        title = self.__class__.__name__.replace(\"MessagePromptTemplate\", \" Message\")\n        title = get_msg_title_repr(title, bold=html)\n        return f\"{title}\\n\\n{self.prompt.pretty_repr(html=html)}\"\n\n\nclass ChatMessagePromptTemplate(BaseStringMessagePromptTemplate):\n    \"\"\"Chat message prompt template.\"\"\"\n\n    role: str\n    \"\"\"Role of the message.\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    def format(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        text = self.prompt.format(**kwargs)\n        return ChatMessage(\n            content=text, role=self.role, additional_kwargs=self.additional_kwargs\n        )\n\n    async def aformat(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Async format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        text = await self.prompt.aformat(**kwargs)\n        return ChatMessage(\n            content=text, role=self.role, additional_kwargs=self.additional_kwargs\n        )\n\n\n_StringImageMessagePromptTemplateT = TypeVar(\n    \"_StringImageMessagePromptTemplateT\", bound=\"_StringImageMessagePromptTemplate\"\n)\n\n\nclass _TextTemplateParam(TypedDict, total=False):\n    text: Union[str, dict]\n\n\nclass _ImageTemplateParam(TypedDict, total=False):\n    image_url: Union[str, dict]\n\n\nclass _StringImageMessagePromptTemplate(BaseMessagePromptTemplate):\n    \"\"\"Human message prompt template. This is a message sent from the user.\"\"\"\n\n    prompt: Union[\n        StringPromptTemplate, list[Union[StringPromptTemplate, ImagePromptTemplate]]\n    ]\n    \"\"\"Prompt template.\"\"\"\n    additional_kwargs: dict = Field(default_factory=dict)\n    \"\"\"Additional keyword arguments to pass to the prompt template.\"\"\"\n\n    _msg_class: type[BaseMessage]\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    @classmethod\n    def from_template(\n        cls: type[_StringImageMessagePromptTemplateT],\n        template: Union[str, list[Union[str, _TextTemplateParam, _ImageTemplateParam]]],\n        template_format: PromptTemplateFormat = \"f-string\",\n        *,\n        partial_variables: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> _StringImageMessagePromptTemplateT:\n        \"\"\"Create a class from a string template.\n\n        Args:\n            template: a template.\n            template_format: format of the template.\n                Options are: 'f-string', 'mustache', 'jinja2'. Defaults to \"f-string\".\n            partial_variables: A dictionary of variables that can be used too partially.\n                Defaults to None.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n\n        Raises:\n            ValueError: If the template is not a string or list of strings.\n        \"\"\"\n        if isinstance(template, str):\n            prompt: Union[StringPromptTemplate, list] = PromptTemplate.from_template(\n                template,\n                template_format=template_format,\n                partial_variables=partial_variables,\n            )\n            return cls(prompt=prompt, **kwargs)\n        elif isinstance(template, list):\n            if (partial_variables is not None) and len(partial_variables) > 0:\n                msg = \"Partial variables are not supported for list of templates.\"\n                raise ValueError(msg)\n            prompt = []\n            for tmpl in template:\n                if isinstance(tmpl, str) or isinstance(tmpl, dict) and \"text\" in tmpl:\n                    if isinstance(tmpl, str):\n                        text: str = tmpl\n                    else:\n                        text = cast(_TextTemplateParam, tmpl)[\"text\"]  # type: ignore[assignment]\n                    prompt.append(\n                        PromptTemplate.from_template(\n                            text, template_format=template_format\n                        )\n                    )\n                elif isinstance(tmpl, dict) and \"image_url\" in tmpl:\n                    img_template = cast(_ImageTemplateParam, tmpl)[\"image_url\"]\n                    input_variables = []\n                    if isinstance(img_template, str):\n                        vars = get_template_variables(img_template, template_format)\n                        if vars:\n                            if len(vars) > 1:\n                                msg = (\n                                    \"Only one format variable allowed per image\"\n                                    f\" template.\\nGot: {vars}\"\n                                    f\"\\nFrom: {tmpl}\"\n                                )\n                                raise ValueError(msg)\n                            input_variables = [vars[0]]\n                        img_template = {\"url\": img_template}\n                        img_template_obj = ImagePromptTemplate(\n                            input_variables=input_variables,\n                            template=img_template,\n                            template_format=template_format,\n                        )\n                    elif isinstance(img_template, dict):\n                        img_template = dict(img_template)\n                        for key in [\"url\", \"path\", \"detail\"]:\n                            if key in img_template:\n                                input_variables.extend(\n                                    get_template_variables(\n                                        img_template[key], template_format\n                                    )\n                                )\n                        img_template_obj = ImagePromptTemplate(\n                            input_variables=input_variables,\n                            template=img_template,\n                            template_format=template_format,\n                        )\n                    else:\n                        msg = f\"Invalid image template: {tmpl}\"\n                        raise ValueError(msg)\n                    prompt.append(img_template_obj)\n                else:\n                    msg = f\"Invalid template: {tmpl}\"\n                    raise ValueError(msg)\n            return cls(prompt=prompt, **kwargs)\n        else:\n            msg = f\"Invalid template: {template}\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @classmethod\n    def from_template_file(\n        cls: type[_StringImageMessagePromptTemplateT],\n        template_file: Union[str, Path],\n        input_variables: list[str],\n        **kwargs: Any,\n    ) -> _StringImageMessagePromptTemplateT:\n        \"\"\"Create a class from a template file.\n\n        Args:\n            template_file: path to a template file. String or Path.\n            input_variables: list of input variables.\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        with open(str(template_file)) as f:\n            template = f.read()\n        return cls.from_template(template, input_variables=input_variables, **kwargs)\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [self.format(**kwargs)]\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format messages from kwargs.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            List of BaseMessages.\n        \"\"\"\n        return [await self.aformat(**kwargs)]\n\n    @property\n    def input_variables(self) -> list[str]:\n        \"\"\"Input variables for this prompt template.\n\n        Returns:\n            List of input variable names.\n        \"\"\"\n        prompts = self.prompt if isinstance(self.prompt, list) else [self.prompt]\n        input_variables = [iv for prompt in prompts for iv in prompt.input_variables]\n        return input_variables\n\n    def format(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        if isinstance(self.prompt, StringPromptTemplate):\n            text = self.prompt.format(**kwargs)\n            return self._msg_class(\n                content=text, additional_kwargs=self.additional_kwargs\n            )\n        else:\n            content: list = []\n            for prompt in self.prompt:\n                inputs = {var: kwargs[var] for var in prompt.input_variables}\n                if isinstance(prompt, StringPromptTemplate):\n                    formatted: Union[str, ImageURL] = prompt.format(**inputs)\n                    content.append({\"type\": \"text\", \"text\": formatted})\n                elif isinstance(prompt, ImagePromptTemplate):\n                    formatted = prompt.format(**inputs)\n                    content.append({\"type\": \"image_url\", \"image_url\": formatted})\n            return self._msg_class(\n                content=content, additional_kwargs=self.additional_kwargs\n            )\n\n    async def aformat(self, **kwargs: Any) -> BaseMessage:\n        \"\"\"Async format the prompt template.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            Formatted message.\n        \"\"\"\n        if isinstance(self.prompt, StringPromptTemplate):\n            text = await self.prompt.aformat(**kwargs)\n            return self._msg_class(\n                content=text, additional_kwargs=self.additional_kwargs\n            )\n        else:\n            content: list = []\n            for prompt in self.prompt:\n                inputs = {var: kwargs[var] for var in prompt.input_variables}\n                if isinstance(prompt, StringPromptTemplate):\n                    formatted: Union[str, ImageURL] = await prompt.aformat(**inputs)\n                    content.append({\"type\": \"text\", \"text\": formatted})\n                elif isinstance(prompt, ImagePromptTemplate):\n                    formatted = await prompt.aformat(**inputs)\n                    content.append({\"type\": \"image_url\", \"image_url\": formatted})\n            return self._msg_class(\n                content=content, additional_kwargs=self.additional_kwargs\n            )\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        # TODO: Handle partials\n        title = self.__class__.__name__.replace(\"MessagePromptTemplate\", \" Message\")\n        title = get_msg_title_repr(title, bold=html)\n        prompts = self.prompt if isinstance(self.prompt, list) else [self.prompt]\n        prompt_reprs = \"\\n\\n\".join(prompt.pretty_repr(html=html) for prompt in prompts)\n        return f\"{title}\\n\\n{prompt_reprs}\"\n\n\nclass HumanMessagePromptTemplate(_StringImageMessagePromptTemplate):\n    \"\"\"Human message prompt template. This is a message sent from the user.\"\"\"\n\n    _msg_class: type[BaseMessage] = HumanMessage\n\n\nclass AIMessagePromptTemplate(_StringImageMessagePromptTemplate):\n    \"\"\"AI message prompt template. This is a message sent from the AI.\"\"\"\n\n    _msg_class: type[BaseMessage] = AIMessage\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n\nclass SystemMessagePromptTemplate(_StringImageMessagePromptTemplate):\n    \"\"\"System message prompt template.\n    This is a message that is not sent to the user.\n    \"\"\"\n\n    _msg_class: type[BaseMessage] = SystemMessage\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n\nclass BaseChatPromptTemplate(BasePromptTemplate, ABC):\n    \"\"\"Base class for chat prompt templates.\"\"\"\n\n    @property\n    def lc_attributes(self) -> dict:\n        \"\"\"Return a list of attribute names that should be included in the\n        serialized kwargs. These attributes must be accepted by the\n        constructor.\n        \"\"\"\n        return {\"input_variables\": self.input_variables}\n\n    def format(self, **kwargs: Any) -> str:\n        \"\"\"Format the chat template into a string.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            formatted string.\n        \"\"\"\n        return self.format_prompt(**kwargs).to_string()\n\n    async def aformat(self, **kwargs: Any) -> str:\n        \"\"\"Async format the chat template into a string.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            formatted string.\n        \"\"\"\n        return (await self.aformat_prompt(**kwargs)).to_string()\n\n    def format_prompt(self, **kwargs: Any) -> PromptValue:\n        \"\"\"Format prompt. Should return a PromptValue.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            PromptValue.\n        \"\"\"\n        messages = self.format_messages(**kwargs)\n        return ChatPromptValue(messages=messages)\n\n    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:\n        \"\"\"Async format prompt. Should return a PromptValue.\n\n        Args:\n            **kwargs: Keyword arguments to use for formatting.\n\n        Returns:\n            PromptValue.\n        \"\"\"\n        messages = await self.aformat_messages(**kwargs)\n        return ChatPromptValue(messages=messages)\n\n    @abstractmethod\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format kwargs into a list of messages.\"\"\"\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format kwargs into a list of messages.\"\"\"\n        return self.format_messages(**kwargs)\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        raise NotImplementedError\n\n    def pretty_print(self) -> None:\n        \"\"\"Print a human-readable representation.\"\"\"\n        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201\n\n\nMessageLike = Union[BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate]\n\nMessageLikeRepresentation = Union[\n    MessageLike,\n    tuple[\n        Union[str, type],\n        Union[str, list[dict], list[object]],\n    ],\n    str,\n    dict,\n]\n\n\nclass ChatPromptTemplate(BaseChatPromptTemplate):\n    \"\"\"Prompt template for chat models.\n\n    Use to create flexible templated prompts for chat models.\n\n    Examples:\n\n        .. versionchanged:: 0.2.24\n\n            You can pass any Message-like formats supported by\n            ``ChatPromptTemplate.from_messages()`` directly to ``ChatPromptTemplate()``\n            init.\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n\n            template = ChatPromptTemplate([\n                (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n                (\"human\", \"Hello, how are you doing?\"),\n                (\"ai\", \"I'm doing well, thanks!\"),\n                (\"human\", \"{user_input}\"),\n            ])\n\n            prompt_value = template.invoke(\n                {\n                    \"name\": \"Bob\",\n                    \"user_input\": \"What is your name?\"\n                }\n            )\n            # Output:\n            # ChatPromptValue(\n            #    messages=[\n            #        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n            #        HumanMessage(content='Hello, how are you doing?'),\n            #        AIMessage(content=\"I'm doing well, thanks!\"),\n            #        HumanMessage(content='What is your name?')\n            #    ]\n            #)\n\n    Messages Placeholder:\n\n        .. code-block:: python\n\n            # In addition to Human/AI/Tool/Function messages,\n            # you can initialize the template with a MessagesPlaceholder\n            # either using the class directly or with the shorthand tuple syntax:\n\n            template = ChatPromptTemplate([\n                (\"system\", \"You are a helpful AI bot.\"),\n                # Means the template will receive an optional list of messages under\n                # the \"conversation\" key\n                (\"placeholder\", \"{conversation}\")\n                # Equivalently:\n                # MessagesPlaceholder(variable_name=\"conversation\", optional=True)\n            ])\n\n            prompt_value = template.invoke(\n                {\n                    \"conversation\": [\n                        (\"human\", \"Hi!\"),\n                        (\"ai\", \"How can I assist you today?\"),\n                        (\"human\", \"Can you make me an ice cream sundae?\"),\n                        (\"ai\", \"No.\")\n                    ]\n                }\n            )\n\n            # Output:\n            # ChatPromptValue(\n            #    messages=[\n            #        SystemMessage(content='You are a helpful AI bot.'),\n            #        HumanMessage(content='Hi!'),\n            #        AIMessage(content='How can I assist you today?'),\n            #        HumanMessage(content='Can you make me an ice cream sundae?'),\n            #        AIMessage(content='No.'),\n            #    ]\n            #)\n\n    Single-variable template:\n\n        If your prompt has only a single input variable (i.e., 1 instance of \"{variable_nams}\"),\n        and you invoke the template with a non-dict object, the prompt template will\n        inject the provided argument into that variable location.\n\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n\n            template = ChatPromptTemplate([\n                (\"system\", \"You are a helpful AI bot. Your name is Carl.\"),\n                (\"human\", \"{user_input}\"),\n            ])\n\n            prompt_value = template.invoke(\"Hello, there!\")\n            # Equivalent to\n            # prompt_value = template.invoke({\"user_input\": \"Hello, there!\"})\n\n            # Output:\n            #  ChatPromptValue(\n            #     messages=[\n            #         SystemMessage(content='You are a helpful AI bot. Your name is Carl.'),\n            #         HumanMessage(content='Hello, there!'),\n            #     ]\n            # )\n\n    \"\"\"  # noqa: E501\n\n    messages: Annotated[list[MessageLike], SkipValidation()]\n    \"\"\"List of messages consisting of either message prompt templates or messages.\"\"\"\n    validate_template: bool = False\n    \"\"\"Whether or not to try validating the template.\"\"\"\n\n    def __init__(\n        self,\n        messages: Sequence[MessageLikeRepresentation],\n        *,\n        template_format: PromptTemplateFormat = \"f-string\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a chat prompt template from a variety of message formats.\n\n        Args:\n            messages: sequence of message representations.\n                  A message can be represented using the following formats:\n                  (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\n                  (message type, template); e.g., (\"human\", \"{user_input}\"),\n                  (4) 2-tuple of (message class, template), (5) a string which is\n                  shorthand for (\"human\", template); e.g., \"{user_input}\".\n            template_format: format of the template. Defaults to \"f-string\".\n            input_variables: A list of the names of the variables whose values are\n                required as inputs to the prompt.\n            optional_variables: A list of the names of the variables for placeholder\n            or MessagePlaceholder that are optional. These variables are auto inferred\n            from the prompt and user need not provide them.\n            partial_variables: A dictionary of the partial variables the prompt\n                template carries. Partial variables populate the template so that you\n                don't need to pass them in every time you call the prompt.\n            validate_template: Whether to validate the template.\n            input_types: A dictionary of the types of the variables the prompt template\n                expects. If not provided, all variables are assumed to be strings.\n\n        Returns:\n            A chat prompt template.\n\n        Examples:\n            Instantiation from a list of message templates:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate([\n                    (\"human\", \"Hello, how are you?\"),\n                    (\"ai\", \"I'm doing well, thanks!\"),\n                    (\"human\", \"That's good to hear.\"),\n                ])\n\n            Instantiation from mixed message formats:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate([\n                    SystemMessage(content=\"hello\"),\n                    (\"human\", \"Hello, how are you?\"),\n                ])\n\n        \"\"\"\n        _messages = [\n            _convert_to_message(message, template_format) for message in messages\n        ]\n\n        # Automatically infer input variables from messages\n        input_vars: set[str] = set()\n        optional_variables: set[str] = set()\n        partial_vars: dict[str, Any] = {}\n        for _message in _messages:\n            if isinstance(_message, MessagesPlaceholder) and _message.optional:\n                partial_vars[_message.variable_name] = []\n                optional_variables.add(_message.variable_name)\n            elif isinstance(\n                _message, (BaseChatPromptTemplate, BaseMessagePromptTemplate)\n            ):\n                input_vars.update(_message.input_variables)\n\n        kwargs = {\n            \"input_variables\": sorted(input_vars),\n            \"optional_variables\": sorted(optional_variables),\n            \"partial_variables\": partial_vars,\n            **kwargs,\n        }\n        cast(type[ChatPromptTemplate], super()).__init__(messages=_messages, **kwargs)\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"chat\"]\n\n    def __add__(self, other: Any) -> ChatPromptTemplate:\n        \"\"\"Combine two prompt templates.\n\n        Args:\n            other: Another prompt template.\n\n        Returns:\n            Combined prompt template.\n        \"\"\"\n        # Allow for easy combining\n        if isinstance(other, ChatPromptTemplate):\n            return ChatPromptTemplate(messages=self.messages + other.messages)  # type: ignore[call-arg]\n        elif isinstance(\n            other, (BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate)\n        ):\n            return ChatPromptTemplate(messages=self.messages + [other])  # type: ignore[call-arg]\n        elif isinstance(other, (list, tuple)):\n            _other = ChatPromptTemplate.from_messages(other)\n            return ChatPromptTemplate(messages=self.messages + _other.messages)  # type: ignore[call-arg]\n        elif isinstance(other, str):\n            prompt = HumanMessagePromptTemplate.from_template(other)\n            return ChatPromptTemplate(messages=self.messages + [prompt])  # type: ignore[call-arg]\n        else:\n            msg = f\"Unsupported operand type for +: {type(other)}\"\n            raise NotImplementedError(msg)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_input_variables(cls, values: dict) -> Any:\n        \"\"\"Validate input variables.\n\n        If input_variables is not set, it will be set to the union of\n        all input variables in the messages.\n\n        Args:\n            values: values to validate.\n\n        Returns:\n            Validated values.\n\n        Raises:\n            ValueError: If input variables do not match.\n        \"\"\"\n        messages = values[\"messages\"]\n        input_vars = set()\n        optional_variables = set()\n        input_types: dict[str, Any] = values.get(\"input_types\", {})\n        for message in messages:\n            if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):\n                input_vars.update(message.input_variables)\n            if isinstance(message, MessagesPlaceholder):\n                if \"partial_variables\" not in values:\n                    values[\"partial_variables\"] = {}\n                if (\n                    message.optional\n                    and message.variable_name not in values[\"partial_variables\"]\n                ):\n                    values[\"partial_variables\"][message.variable_name] = []\n                    optional_variables.add(message.variable_name)\n                if message.variable_name not in input_types:\n                    input_types[message.variable_name] = list[AnyMessage]\n        if \"partial_variables\" in values:\n            input_vars = input_vars - set(values[\"partial_variables\"])\n        if optional_variables:\n            input_vars = input_vars - optional_variables\n        if \"input_variables\" in values and values.get(\"validate_template\"):\n            if input_vars != set(values[\"input_variables\"]):\n                msg = (\n                    \"Got mismatched input_variables. \"\n                    f\"Expected: {input_vars}. \"\n                    f\"Got: {values['input_variables']}\"\n                )\n                raise ValueError(msg)\n        else:\n            values[\"input_variables\"] = sorted(input_vars)\n        if optional_variables:\n            values[\"optional_variables\"] = sorted(optional_variables)\n        values[\"input_types\"] = input_types\n        return values\n\n    @classmethod\n    def from_template(cls, template: str, **kwargs: Any) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a template string.\n\n        Creates a chat template consisting of a single message assumed to be from\n        the human.\n\n        Args:\n            template: template string\n            **kwargs: keyword arguments to pass to the constructor.\n\n        Returns:\n            A new instance of this class.\n        \"\"\"\n        prompt_template = PromptTemplate.from_template(template, **kwargs)\n        message = HumanMessagePromptTemplate(prompt=prompt_template)\n        return cls.from_messages([message])\n\n    @classmethod\n    @deprecated(\"0.0.1\", alternative=\"from_messages classmethod\", pending=True)\n    def from_role_strings(\n        cls, string_messages: list[tuple[str, str]]\n    ) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a list of (role, template) tuples.\n\n        Args:\n            string_messages: list of (role, template) tuples.\n\n        Returns:\n            a chat prompt template.\n        \"\"\"\n        return cls(  # type: ignore[call-arg]\n            messages=[\n                ChatMessagePromptTemplate.from_template(template, role=role)\n                for role, template in string_messages\n            ]\n        )\n\n    @classmethod\n    @deprecated(\"0.0.1\", alternative=\"from_messages classmethod\", pending=True)\n    def from_strings(\n        cls, string_messages: list[tuple[type[BaseMessagePromptTemplate], str]]\n    ) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a list of (role class, template) tuples.\n\n        Args:\n            string_messages: list of (role class, template) tuples.\n\n        Returns:\n            a chat prompt template.\n        \"\"\"\n        return cls.from_messages(string_messages)\n\n    @classmethod\n    def from_messages(\n        cls,\n        messages: Sequence[MessageLikeRepresentation],\n        template_format: PromptTemplateFormat = \"f-string\",\n    ) -> ChatPromptTemplate:\n        \"\"\"Create a chat prompt template from a variety of message formats.\n\n        Examples:\n            Instantiation from a list of message templates:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate.from_messages([\n                    (\"human\", \"Hello, how are you?\"),\n                    (\"ai\", \"I'm doing well, thanks!\"),\n                    (\"human\", \"That's good to hear.\"),\n                ])\n\n            Instantiation from mixed message formats:\n\n            .. code-block:: python\n\n                template = ChatPromptTemplate.from_messages([\n                    SystemMessage(content=\"hello\"),\n                    (\"human\", \"Hello, how are you?\"),\n                ])\n\n        Args:\n            messages: sequence of message representations.\n                  A message can be represented using the following formats:\n                  (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of\n                  (message type, template); e.g., (\"human\", \"{user_input}\"),\n                  (4) 2-tuple of (message class, template), (5) a string which is\n                  shorthand for (\"human\", template); e.g., \"{user_input}\".\n            template_format: format of the template. Defaults to \"f-string\".\n\n        Returns:\n            a chat prompt template.\n        \"\"\"\n        return cls(messages, template_format=template_format)\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format the chat template into a list of finalized messages.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            list of formatted messages.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        result = []\n        for message_template in self.messages:\n            if isinstance(message_template, BaseMessage):\n                result.extend([message_template])\n            elif isinstance(\n                message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n            ):\n                message = message_template.format_messages(**kwargs)\n                result.extend(message)\n            else:\n                msg = f\"Unexpected input: {message_template}\"\n                raise ValueError(msg)  # noqa: TRY004\n        return result\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format the chat template into a list of finalized messages.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables\n                      in all the template messages in this chat template.\n\n        Returns:\n            list of formatted messages.\n\n        Raises:\n            ValueError: If unexpected input.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        result = []\n        for message_template in self.messages:\n            if isinstance(message_template, BaseMessage):\n                result.extend([message_template])\n            elif isinstance(\n                message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n            ):\n                message = await message_template.aformat_messages(**kwargs)\n                result.extend(message)\n            else:\n                msg = f\"Unexpected input: {message_template}\"\n                raise ValueError(msg)  # noqa:TRY004\n        return result\n\n    def partial(self, **kwargs: Any) -> ChatPromptTemplate:\n        \"\"\"Get a new ChatPromptTemplate with some input variables already filled in.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in template variables. Ought\n                        to be a subset of the input variables.\n\n        Returns:\n            A new ChatPromptTemplate.\n\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.prompts import ChatPromptTemplate\n\n                template = ChatPromptTemplate.from_messages(\n                    [\n                        (\"system\", \"You are an AI assistant named {name}.\"),\n                        (\"human\", \"Hi I'm {user}\"),\n                        (\"ai\", \"Hi there, {user}, I'm {name}.\"),\n                        (\"human\", \"{input}\"),\n                    ]\n                )\n                template2 = template.partial(user=\"Lucy\", name=\"R2D2\")\n\n                template2.format_messages(input=\"hello\")\n        \"\"\"\n        prompt_dict = self.__dict__.copy()\n        prompt_dict[\"input_variables\"] = list(\n            set(self.input_variables).difference(kwargs)\n        )\n        prompt_dict[\"partial_variables\"] = {**self.partial_variables, **kwargs}\n        return type(self)(**prompt_dict)\n\n    def append(self, message: MessageLikeRepresentation) -> None:\n        \"\"\"Append a message to the end of the chat template.\n\n        Args:\n            message: representation of a message to append.\n        \"\"\"\n        self.messages.append(_convert_to_message(message))\n\n    def extend(self, messages: Sequence[MessageLikeRepresentation]) -> None:\n        \"\"\"Extend the chat template with a sequence of messages.\n\n        Args:\n            messages: sequence of message representations to append.\n        \"\"\"\n        self.messages.extend([_convert_to_message(message) for message in messages])\n\n    @overload\n    def __getitem__(self, index: int) -> MessageLike: ...\n\n    @overload\n    def __getitem__(self, index: slice) -> ChatPromptTemplate: ...\n\n    def __getitem__(\n        self, index: Union[int, slice]\n    ) -> Union[MessageLike, ChatPromptTemplate]:\n        \"\"\"Use to index into the chat template.\"\"\"\n        if isinstance(index, slice):\n            start, stop, step = index.indices(len(self.messages))\n            messages = self.messages[start:stop:step]\n            return ChatPromptTemplate.from_messages(messages)\n        else:\n            return self.messages[index]\n\n    def __len__(self) -> int:\n        \"\"\"Get the length of the chat template.\"\"\"\n        return len(self.messages)\n\n    @property\n    def _prompt_type(self) -> str:\n        \"\"\"Name of prompt type. Used for serialization.\"\"\"\n        return \"chat\"\n\n    def save(self, file_path: Union[Path, str]) -> None:\n        \"\"\"Save prompt to file.\n\n        Args:\n            file_path: path to file.\n        \"\"\"\n        raise NotImplementedError\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Human-readable representation.\n\n        Args:\n            html: Whether to format as HTML. Defaults to False.\n\n        Returns:\n            Human-readable representation.\n        \"\"\"\n        # TODO: handle partials\n        return \"\\n\\n\".join(msg.pretty_repr(html=html) for msg in self.messages)\n\n\ndef _create_template_from_message_type(\n    message_type: str,\n    template: Union[str, list],\n    template_format: PromptTemplateFormat = \"f-string\",\n) -> BaseMessagePromptTemplate:\n    \"\"\"Create a message prompt template from a message type and template string.\n\n    Args:\n        message_type: str the type of the message template (e.g., \"human\", \"ai\", etc.)\n        template: str the template string.\n        template_format: format of the template. Defaults to \"f-string\".\n\n    Returns:\n        a message prompt template of the appropriate type.\n\n    Raises:\n        ValueError: If unexpected message type.\n    \"\"\"\n    if message_type in (\"human\", \"user\"):\n        message: BaseMessagePromptTemplate = HumanMessagePromptTemplate.from_template(\n            template, template_format=template_format\n        )\n    elif message_type in (\"ai\", \"assistant\"):\n        message = AIMessagePromptTemplate.from_template(\n            cast(str, template), template_format=template_format\n        )\n    elif message_type == \"system\":\n        message = SystemMessagePromptTemplate.from_template(\n            cast(str, template), template_format=template_format\n        )\n    elif message_type == \"placeholder\":\n        if isinstance(template, str):\n            if template[0] != \"{\" or template[-1] != \"}\":\n                msg = (\n                    f\"Invalid placeholder template: {template}.\"\n                    \" Expected a variable name surrounded by curly braces.\"\n                )\n                raise ValueError(msg)\n            var_name = template[1:-1]\n            message = MessagesPlaceholder(variable_name=var_name, optional=True)\n        elif len(template) == 2 and isinstance(template[1], bool):\n            var_name_wrapped, is_optional = template\n            if not isinstance(var_name_wrapped, str):\n                msg = f\"Expected variable name to be a string. Got: {var_name_wrapped}\"\n                raise ValueError(msg)  # noqa:TRY004\n            if var_name_wrapped[0] != \"{\" or var_name_wrapped[-1] != \"}\":\n                msg = (\n                    f\"Invalid placeholder template: {var_name_wrapped}.\"\n                    \" Expected a variable name surrounded by curly braces.\"\n                )\n                raise ValueError(msg)\n            var_name = var_name_wrapped[1:-1]\n\n            message = MessagesPlaceholder(variable_name=var_name, optional=is_optional)\n        else:\n            msg = (\n                \"Unexpected arguments for placeholder message type.\"\n                \" Expected either a single string variable name\"\n                \" or a list of [variable_name: str, is_optional: bool].\"\n                f\" Got: {template}\"\n            )\n            raise ValueError(msg)\n    else:\n        msg = (\n            f\"Unexpected message type: {message_type}. Use one of 'human',\"\n            f\" 'user', 'ai', 'assistant', or 'system'.\"\n        )\n        raise ValueError(msg)\n    return message\n\n\ndef _convert_to_message(\n    message: MessageLikeRepresentation,\n    template_format: PromptTemplateFormat = \"f-string\",\n) -> Union[BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate]:\n    \"\"\"Instantiate a message from a variety of message formats.\n\n    The message format can be one of the following:\n\n    - BaseMessagePromptTemplate\n    - BaseMessage\n    - 2-tuple of (role string, template); e.g., (\"human\", \"{user_input}\")\n    - 2-tuple of (message class, template)\n    - string: shorthand for (\"human\", template); e.g., \"{user_input}\"\n\n    Args:\n        message: a representation of a message in one of the supported formats.\n        template_format: format of the template. Defaults to \"f-string\".\n\n    Returns:\n        an instance of a message or a message template.\n\n    Raises:\n        ValueError: If unexpected message type.\n        ValueError: If 2-tuple does not have 2 elements.\n    \"\"\"\n    if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):\n        _message: Union[\n            BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate\n        ] = message\n    elif isinstance(message, BaseMessage):\n        _message = message\n    elif isinstance(message, str):\n        _message = _create_template_from_message_type(\n            \"human\", message, template_format=template_format\n        )\n    elif isinstance(message, (tuple, dict)):\n        if isinstance(message, dict):\n            if set(message.keys()) != {\"content\", \"role\"}:\n                msg = (\n                    \"Expected dict to have exact keys 'role' and 'content'.\"\n                    f\" Got: {message}\"\n                )\n                raise ValueError(msg)\n            message = (message[\"role\"], message[\"content\"])\n        if len(message) != 2:\n            msg = f\"Expected 2-tuple of (role, template), got {message}\"\n            raise ValueError(msg)\n        message_type_str, template = message\n        if isinstance(message_type_str, str):\n            _message = _create_template_from_message_type(\n                message_type_str, template, template_format=template_format\n            )\n        else:\n            _message = message_type_str(\n                prompt=PromptTemplate.from_template(\n                    cast(str, template), template_format=template_format\n                )\n            )\n    else:\n        msg = f\"Unsupported message type: {type(message)}\"\n        raise NotImplementedError(msg)\n\n    return _message\n",
        "patch": "@@ -3,9 +3,8 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n-from pathlib import Path\n from typing import (\n+    TYPE_CHECKING,\n     Annotated,\n     Any,\n     Optional,\n@@ -47,6 +46,10 @@\n from langchain_core.utils import get_colored_text\n from langchain_core.utils.interactive_env import is_interactive_env\n \n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+    from pathlib import Path\n+\n \n class BaseMessagePromptTemplate(Serializable, ABC):\n     \"\"\"Base class for message prompt templates.\"\"\""
      },
      {
        "filename": "libs/core/langchain_core/prompts/few_shot.py",
        "content_before": "\"\"\"Prompt template that contains few shot examples.\"\"\"\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Any, Literal, Optional, Union\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    model_validator,\n)\nfrom typing_extensions import Self\n\nfrom langchain_core.example_selectors import BaseExampleSelector\nfrom langchain_core.messages import BaseMessage, get_buffer_string\nfrom langchain_core.prompts.chat import (\n    BaseChatPromptTemplate,\n    BaseMessagePromptTemplate,\n)\nfrom langchain_core.prompts.prompt import PromptTemplate\nfrom langchain_core.prompts.string import (\n    DEFAULT_FORMATTER_MAPPING,\n    StringPromptTemplate,\n    check_valid_template,\n    get_template_variables,\n)\n\n\nclass _FewShotPromptTemplateMixin(BaseModel):\n    \"\"\"Prompt template that contains few shot examples.\"\"\"\n\n    examples: Optional[list[dict]] = None\n    \"\"\"Examples to format into the prompt.\n    Either this or example_selector should be provided.\"\"\"\n\n    example_selector: Optional[BaseExampleSelector] = None\n    \"\"\"ExampleSelector to choose the examples to format into the prompt.\n    Either this or examples should be provided.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"forbid\",\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_examples_and_selector(cls, values: dict) -> Any:\n        \"\"\"Check that one and only one of examples/example_selector are provided.\n\n        Args:\n            values: The values to check.\n\n        Returns:\n            The values if they are valid.\n\n        Raises:\n            ValueError: If neither or both examples and example_selector are provided.\n            ValueError: If both examples and example_selector are provided.\n        \"\"\"\n        examples = values.get(\"examples\")\n        example_selector = values.get(\"example_selector\")\n        if examples and example_selector:\n            msg = \"Only one of 'examples' and 'example_selector' should be provided\"\n            raise ValueError(msg)\n\n        if examples is None and example_selector is None:\n            msg = \"One of 'examples' and 'example_selector' should be provided\"\n            raise ValueError(msg)\n\n        return values\n\n    def _get_examples(self, **kwargs: Any) -> list[dict]:\n        \"\"\"Get the examples to use for formatting the prompt.\n\n        Args:\n            **kwargs: Keyword arguments to be passed to the example selector.\n\n        Returns:\n            List of examples.\n\n        Raises:\n            ValueError: If neither examples nor example_selector are provided.\n        \"\"\"\n        if self.examples is not None:\n            return self.examples\n        elif self.example_selector is not None:\n            return self.example_selector.select_examples(kwargs)\n        else:\n            msg = \"One of 'examples' and 'example_selector' should be provided\"\n            raise ValueError(msg)\n\n    async def _aget_examples(self, **kwargs: Any) -> list[dict]:\n        \"\"\"Async get the examples to use for formatting the prompt.\n\n        Args:\n            **kwargs: Keyword arguments to be passed to the example selector.\n\n        Returns:\n            List of examples.\n\n        Raises:\n            ValueError: If neither examples nor example_selector are provided.\n        \"\"\"\n        if self.examples is not None:\n            return self.examples\n        elif self.example_selector is not None:\n            return await self.example_selector.aselect_examples(kwargs)\n        else:\n            msg = \"One of 'examples' and 'example_selector' should be provided\"\n            raise ValueError(msg)\n\n\nclass FewShotPromptTemplate(_FewShotPromptTemplateMixin, StringPromptTemplate):\n    \"\"\"Prompt template that contains few shot examples.\"\"\"\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether or not the class is serializable.\"\"\"\n        return False\n\n    validate_template: bool = False\n    \"\"\"Whether or not to try validating the template.\"\"\"\n\n    example_prompt: PromptTemplate\n    \"\"\"PromptTemplate used to format an individual example.\"\"\"\n\n    suffix: str\n    \"\"\"A prompt template string to put after the examples.\"\"\"\n\n    example_separator: str = \"\\n\\n\"\n    \"\"\"String separator used to join the prefix, the examples, and suffix.\"\"\"\n\n    prefix: str = \"\"\n    \"\"\"A prompt template string to put before the examples.\"\"\"\n\n    template_format: Literal[\"f-string\", \"jinja2\"] = \"f-string\"\n    \"\"\"The format of the prompt template. Options are: 'f-string', 'jinja2'.\"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Initialize the few shot prompt template.\"\"\"\n        if \"input_variables\" not in kwargs and \"example_prompt\" in kwargs:\n            kwargs[\"input_variables\"] = kwargs[\"example_prompt\"].input_variables\n        super().__init__(**kwargs)\n\n    @model_validator(mode=\"after\")\n    def template_is_valid(self) -> Self:\n        \"\"\"Check that prefix, suffix, and input variables are consistent.\"\"\"\n        if self.validate_template:\n            check_valid_template(\n                self.prefix + self.suffix,\n                self.template_format,\n                self.input_variables + list(self.partial_variables),\n            )\n        elif self.template_format or None:\n            self.input_variables = [\n                var\n                for var in get_template_variables(\n                    self.prefix + self.suffix, self.template_format\n                )\n                if var not in self.partial_variables\n            ]\n        return self\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"forbid\",\n    )\n\n    def format(self, **kwargs: Any) -> str:\n        \"\"\"Format the prompt with inputs generating a string.\n\n        Use this method to generate a string representation of a prompt.\n\n        Args:\n            **kwargs: keyword arguments to use for formatting.\n\n        Returns:\n            A string representation of the prompt.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        # Get the examples to use.\n        examples = self._get_examples(**kwargs)\n        examples = [\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\n        ]\n        # Format the examples.\n        example_strings = [\n            self.example_prompt.format(**example) for example in examples\n        ]\n        # Create the overall template.\n        pieces = [self.prefix, *example_strings, self.suffix]\n        template = self.example_separator.join([piece for piece in pieces if piece])\n\n        # Format the template with the input variables.\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)\n\n    async def aformat(self, **kwargs: Any) -> str:\n        \"\"\"Async format the prompt with inputs generating a string.\n\n        Use this method to generate a string representation of a prompt.\n\n        Args:\n            **kwargs: keyword arguments to use for formatting.\n\n        Returns:\n            A string representation of the prompt.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        # Get the examples to use.\n        examples = await self._aget_examples(**kwargs)\n        examples = [\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\n        ]\n        # Format the examples.\n        example_strings = [\n            await self.example_prompt.aformat(**example) for example in examples\n        ]\n        # Create the overall template.\n        pieces = [self.prefix, *example_strings, self.suffix]\n        template = self.example_separator.join([piece for piece in pieces if piece])\n\n        # Format the template with the input variables.\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)\n\n    @property\n    def _prompt_type(self) -> str:\n        \"\"\"Return the prompt type key.\"\"\"\n        return \"few_shot\"\n\n    def save(self, file_path: Union[Path, str]) -> None:\n        \"\"\"Save the prompt template to a file.\n\n        Args:\n            file_path: The path to save the prompt template to.\n\n        Raises:\n            ValueError: If example_selector is provided.\n        \"\"\"\n        if self.example_selector:\n            msg = \"Saving an example selector is not currently supported\"\n            raise ValueError(msg)\n        return super().save(file_path)\n\n\nclass FewShotChatMessagePromptTemplate(\n    BaseChatPromptTemplate, _FewShotPromptTemplateMixin\n):\n    \"\"\"Chat prompt template that supports few-shot examples.\n\n    The high level structure of produced by this prompt template is a list of messages\n    consisting of prefix message(s), example message(s), and suffix message(s).\n\n    This structure enables creating a conversation with intermediate examples like:\n\n        System: You are a helpful AI Assistant\n        Human: What is 2+2?\n        AI: 4\n        Human: What is 2+3?\n        AI: 5\n        Human: What is 4+4?\n\n    This prompt template can be used to generate a fixed list of examples or else\n    to dynamically select examples based on the input.\n\n    Examples:\n        Prompt template with a fixed list of examples (matching the sample\n        conversation above):\n\n        .. code-block:: python\n\n            from langchain_core.prompts import (\n                FewShotChatMessagePromptTemplate,\n                ChatPromptTemplate\n            )\n\n            examples = [\n                {\"input\": \"2+2\", \"output\": \"4\"},\n                {\"input\": \"2+3\", \"output\": \"5\"},\n            ]\n\n            example_prompt = ChatPromptTemplate.from_messages(\n            [('human', 'What is {input}?'), ('ai', '{output}')]\n            )\n\n            few_shot_prompt = FewShotChatMessagePromptTemplate(\n                examples=examples,\n                # This is a prompt template used to format each individual example.\n                example_prompt=example_prompt,\n            )\n\n            final_prompt = ChatPromptTemplate.from_messages(\n                [\n                    ('system', 'You are a helpful AI Assistant'),\n                    few_shot_prompt,\n                    ('human', '{input}'),\n                ]\n            )\n            final_prompt.format(input=\"What is 4+4?\")\n\n        Prompt template with dynamically selected examples:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import SemanticSimilarityExampleSelector\n            from langchain_core.embeddings import OpenAIEmbeddings\n            from langchain_core.vectorstores import Chroma\n\n            examples = [\n                {\"input\": \"2+2\", \"output\": \"4\"},\n                {\"input\": \"2+3\", \"output\": \"5\"},\n                {\"input\": \"2+4\", \"output\": \"6\"},\n                # ...\n            ]\n\n            to_vectorize = [\n                \" \".join(example.values())\n                for example in examples\n            ]\n            embeddings = OpenAIEmbeddings()\n            vectorstore = Chroma.from_texts(\n                to_vectorize, embeddings, metadatas=examples\n            )\n            example_selector = SemanticSimilarityExampleSelector(\n                vectorstore=vectorstore\n            )\n\n            from langchain_core import SystemMessage\n            from langchain_core.prompts import HumanMessagePromptTemplate\n            from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate\n\n            few_shot_prompt = FewShotChatMessagePromptTemplate(\n                # Which variable(s) will be passed to the example selector.\n                input_variables=[\"input\"],\n                example_selector=example_selector,\n                # Define how each example will be formatted.\n                # In this case, each example will become 2 messages:\n                # 1 human, and 1 AI\n                example_prompt=(\n                    HumanMessagePromptTemplate.from_template(\"{input}\")\n                    + AIMessagePromptTemplate.from_template(\"{output}\")\n                ),\n            )\n            # Define the overall prompt.\n            final_prompt = (\n                SystemMessagePromptTemplate.from_template(\n                    \"You are a helpful AI Assistant\"\n                )\n                + few_shot_prompt\n                + HumanMessagePromptTemplate.from_template(\"{input}\")\n            )\n            # Show the prompt\n            print(final_prompt.format_messages(input=\"What's 3+3?\"))  # noqa: T201\n\n            # Use within an LLM\n            from langchain_core.chat_models import ChatAnthropic\n            chain = final_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\")\n            chain.invoke({\"input\": \"What's 3+3?\"})\n    \"\"\"\n\n    input_variables: list[str] = Field(default_factory=list)\n    \"\"\"A list of the names of the variables the prompt template will use\n    to pass to the example_selector, if provided.\"\"\"\n\n    example_prompt: Union[BaseMessagePromptTemplate, BaseChatPromptTemplate]\n    \"\"\"The class to format each example.\"\"\"\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether or not the class is serializable.\"\"\"\n        return False\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"forbid\",\n    )\n\n    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Format kwargs into a list of messages.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in templates in messages.\n\n        Returns:\n            A list of formatted messages with all template variables filled in.\n        \"\"\"\n        # Get the examples to use.\n        examples = self._get_examples(**kwargs)\n        examples = [\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\n        ]\n        # Format the examples.\n        messages = [\n            message\n            for example in examples\n            for message in self.example_prompt.format_messages(**example)\n        ]\n        return messages\n\n    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:\n        \"\"\"Async format kwargs into a list of messages.\n\n        Args:\n            **kwargs: keyword arguments to use for filling in templates in messages.\n\n        Returns:\n            A list of formatted messages with all template variables filled in.\n        \"\"\"\n        # Get the examples to use.\n        examples = await self._aget_examples(**kwargs)\n        examples = [\n            {k: e[k] for k in self.example_prompt.input_variables} for e in examples\n        ]\n        # Format the examples.\n        messages = [\n            message\n            for example in examples\n            for message in await self.example_prompt.aformat_messages(**example)\n        ]\n        return messages\n\n    def format(self, **kwargs: Any) -> str:\n        \"\"\"Format the prompt with inputs generating a string.\n\n        Use this method to generate a string representation of a prompt consisting\n        of chat messages.\n\n        Useful for feeding into a string-based completion language model or debugging.\n\n        Args:\n            **kwargs: keyword arguments to use for formatting.\n\n        Returns:\n            A string representation of the prompt\n        \"\"\"\n        messages = self.format_messages(**kwargs)\n        return get_buffer_string(messages)\n\n    async def aformat(self, **kwargs: Any) -> str:\n        \"\"\"Async format the prompt with inputs generating a string.\n\n        Use this method to generate a string representation of a prompt consisting\n        of chat messages.\n\n        Useful for feeding into a string-based completion language model or debugging.\n\n        Args:\n            **kwargs: keyword arguments to use for formatting.\n\n        Returns:\n            A string representation of the prompt\n        \"\"\"\n        messages = await self.aformat_messages(**kwargs)\n        return get_buffer_string(messages)\n\n    def pretty_repr(self, html: bool = False) -> str:\n        \"\"\"Return a pretty representation of the prompt template.\n\n        Args:\n            html: Whether or not to return an HTML formatted string.\n\n        Returns:\n            A pretty representation of the prompt template.\n        \"\"\"\n        raise NotImplementedError\n",
        "patch": "@@ -2,16 +2,14 @@\n \n from __future__ import annotations\n \n-from pathlib import Path\n-from typing import Any, Literal, Optional, Union\n+from typing import TYPE_CHECKING, Any, Literal, Optional, Union\n \n from pydantic import (\n     BaseModel,\n     ConfigDict,\n     Field,\n     model_validator,\n )\n-from typing_extensions import Self\n \n from langchain_core.example_selectors import BaseExampleSelector\n from langchain_core.messages import BaseMessage, get_buffer_string\n@@ -27,6 +25,11 @@\n     get_template_variables,\n )\n \n+if TYPE_CHECKING:\n+    from pathlib import Path\n+\n+    from typing_extensions import Self\n+\n \n class _FewShotPromptTemplateMixin(BaseModel):\n     \"\"\"Prompt template that contains few shot examples.\"\"\""
      },
      {
        "filename": "libs/core/langchain_core/prompts/prompt.py",
        "content_before": "\"\"\"Prompt schema definition.\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom pathlib import Path\nfrom typing import Any, Optional, Union\n\nfrom pydantic import BaseModel, model_validator\n\nfrom langchain_core.prompts.string import (\n    DEFAULT_FORMATTER_MAPPING,\n    PromptTemplateFormat,\n    StringPromptTemplate,\n    check_valid_template,\n    get_template_variables,\n    mustache_schema,\n)\nfrom langchain_core.runnables.config import RunnableConfig\n\n\nclass PromptTemplate(StringPromptTemplate):\n    \"\"\"Prompt template for a language model.\n\n    A prompt template consists of a string template. It accepts a set of parameters\n    from the user that can be used to generate a prompt for a language model.\n\n    The template can be formatted using either f-strings (default), jinja2,\n    or mustache syntax.\n\n    *Security warning*:\n        Prefer using `template_format=\"f-string\"` instead of\n        `template_format=\"jinja2\"`, or make sure to NEVER accept jinja2 templates\n        from untrusted sources as they may lead to arbitrary Python code execution.\n\n        As of LangChain 0.0.329, Jinja2 templates will be rendered using\n        Jinja2's SandboxedEnvironment by default. This sand-boxing should\n        be treated as a best-effort approach rather than a guarantee of security,\n        as it is an opt-out rather than opt-in approach.\n\n        Despite the sand-boxing, we recommend to never use jinja2 templates\n        from untrusted sources.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import PromptTemplate\n\n            # Instantiation using from_template (recommended)\n            prompt = PromptTemplate.from_template(\"Say {foo}\")\n            prompt.format(foo=\"bar\")\n\n            # Instantiation using initializer\n            prompt = PromptTemplate(template=\"Say {foo}\")\n    \"\"\"\n\n    @property\n    def lc_attributes(self) -> dict[str, Any]:\n        return {\n            \"template_format\": self.template_format,\n        }\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"prompts\", \"prompt\"]\n\n    template: str\n    \"\"\"The prompt template.\"\"\"\n\n    template_format: PromptTemplateFormat = \"f-string\"\n    \"\"\"The format of the prompt template.\n    Options are: 'f-string', 'mustache', 'jinja2'.\"\"\"\n\n    validate_template: bool = False\n    \"\"\"Whether or not to try validating the template.\"\"\"\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_init_validation(cls, values: dict) -> Any:\n        \"\"\"Check that template and input variables are consistent.\"\"\"\n        if values.get(\"template\") is None:\n            # Will let pydantic fail with a ValidationError if template\n            # is not provided.\n            return values\n\n        # Set some default values based on the field defaults\n        values.setdefault(\"template_format\", \"f-string\")\n        values.setdefault(\"partial_variables\", {})\n\n        if values.get(\"validate_template\"):\n            if values[\"template_format\"] == \"mustache\":\n                msg = \"Mustache templates cannot be validated.\"\n                raise ValueError(msg)\n\n            if \"input_variables\" not in values:\n                msg = \"Input variables must be provided to validate the template.\"\n                raise ValueError(msg)\n\n            all_inputs = values[\"input_variables\"] + list(values[\"partial_variables\"])\n            check_valid_template(\n                values[\"template\"], values[\"template_format\"], all_inputs\n            )\n\n        if values[\"template_format\"]:\n            values[\"input_variables\"] = [\n                var\n                for var in get_template_variables(\n                    values[\"template\"], values[\"template_format\"]\n                )\n                if var not in values[\"partial_variables\"]\n            ]\n\n        return values\n\n    def get_input_schema(self, config: RunnableConfig | None = None) -> type[BaseModel]:\n        \"\"\"Get the input schema for the prompt.\n\n        Args:\n            config: The runnable configuration.\n\n        Returns:\n            The input schema for the prompt.\n        \"\"\"\n        if self.template_format != \"mustache\":\n            return super().get_input_schema(config)\n\n        return mustache_schema(self.template)\n\n    def __add__(self, other: Any) -> PromptTemplate:\n        \"\"\"Override the + operator to allow for combining prompt templates.\"\"\"\n        # Allow for easy combining\n        if isinstance(other, PromptTemplate):\n            if self.template_format != \"f-string\":\n                msg = \"Adding prompt templates only supported for f-strings.\"\n                raise ValueError(msg)\n            if other.template_format != \"f-string\":\n                msg = \"Adding prompt templates only supported for f-strings.\"\n                raise ValueError(msg)\n            input_variables = list(\n                set(self.input_variables) | set(other.input_variables)\n            )\n            template = self.template + other.template\n            # If any do not want to validate, then don't\n            validate_template = self.validate_template and other.validate_template\n            partial_variables = dict(self.partial_variables.items())\n            for k, v in other.partial_variables.items():\n                if k in partial_variables:\n                    msg = \"Cannot have same variable partialed twice.\"\n                    raise ValueError(msg)\n                else:\n                    partial_variables[k] = v\n            return PromptTemplate(\n                template=template,\n                input_variables=input_variables,\n                partial_variables=partial_variables,\n                template_format=\"f-string\",\n                validate_template=validate_template,\n            )\n        elif isinstance(other, str):\n            prompt = PromptTemplate.from_template(other)\n            return self + prompt\n        else:\n            msg = f\"Unsupported operand type for +: {type(other)}\"\n            raise NotImplementedError(msg)\n\n    @property\n    def _prompt_type(self) -> str:\n        \"\"\"Return the prompt type key.\"\"\"\n        return \"prompt\"\n\n    def format(self, **kwargs: Any) -> str:\n        \"\"\"Format the prompt with the inputs.\n\n        Args:\n            kwargs: Any arguments to be passed to the prompt template.\n\n        Returns:\n            A formatted string.\n        \"\"\"\n        kwargs = self._merge_partial_and_user_variables(**kwargs)\n        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)\n\n    @classmethod\n    def from_examples(\n        cls,\n        examples: list[str],\n        suffix: str,\n        input_variables: list[str],\n        example_separator: str = \"\\n\\n\",\n        prefix: str = \"\",\n        **kwargs: Any,\n    ) -> PromptTemplate:\n        \"\"\"Take examples in list format with prefix and suffix to create a prompt.\n\n        Intended to be used as a way to dynamically create a prompt from examples.\n\n        Args:\n            examples: List of examples to use in the prompt.\n            suffix: String to go after the list of examples. Should generally\n                set up the user's input.\n            input_variables: A list of variable names the final prompt template\n                will expect.\n            example_separator: The separator to use in between examples. Defaults\n                to two new line characters.\n            prefix: String that should go before any examples. Generally includes\n                examples. Default to an empty string.\n\n        Returns:\n            The final prompt generated.\n        \"\"\"\n        template = example_separator.join([prefix, *examples, suffix])\n        return cls(input_variables=input_variables, template=template, **kwargs)\n\n    @classmethod\n    def from_file(\n        cls,\n        template_file: Union[str, Path],\n        input_variables: Optional[list[str]] = None,\n        encoding: Optional[str] = None,\n        **kwargs: Any,\n    ) -> PromptTemplate:\n        \"\"\"Load a prompt from a file.\n\n        Args:\n            template_file: The path to the file containing the prompt template.\n            input_variables: [DEPRECATED] A list of variable names the final prompt\n                template will expect. Defaults to None.\n            encoding: The encoding system for opening the template file.\n                If not provided, will use the OS default.\n\n        input_variables is ignored as from_file now delegates to from_template().\n\n        Returns:\n            The prompt loaded from the file.\n        \"\"\"\n        with open(str(template_file), encoding=encoding) as f:\n            template = f.read()\n        if input_variables:\n            warnings.warn(\n                \"`input_variables' is deprecated and ignored.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        return cls.from_template(template=template, **kwargs)\n\n    @classmethod\n    def from_template(\n        cls,\n        template: str,\n        *,\n        template_format: PromptTemplateFormat = \"f-string\",\n        partial_variables: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> PromptTemplate:\n        \"\"\"Load a prompt template from a template.\n\n        *Security warning*:\n            Prefer using `template_format=\"f-string\"` instead of\n            `template_format=\"jinja2\"`, or make sure to NEVER accept jinja2 templates\n            from untrusted sources as they may lead to arbitrary Python code execution.\n\n            As of LangChain 0.0.329, Jinja2 templates will be rendered using\n            Jinja2's SandboxedEnvironment by default. This sand-boxing should\n            be treated as a best-effort approach rather than a guarantee of security,\n            as it is an opt-out rather than opt-in approach.\n\n            Despite the sand-boxing, we recommend never using jinja2 templates\n            from untrusted sources.\n\n        Args:\n            template: The template to load.\n            template_format: The format of the template. Use `jinja2` for jinja2,\n                             `mustache` for mustache, and `f-string` for f-strings.\n                             Defaults to `f-string`.\n            partial_variables: A dictionary of variables that can be used to partially\n                               fill in the template. For example, if the template is\n                              `\"{variable1} {variable2}\"`, and `partial_variables` is\n                              `{\"variable1\": \"foo\"}`, then the final prompt will be\n                              `\"foo {variable2}\"`. Defaults to None.\n            kwargs: Any other arguments to pass to the prompt template.\n\n        Returns:\n            The prompt template loaded from the template.\n        \"\"\"\n        input_variables = get_template_variables(template, template_format)\n        _partial_variables = partial_variables or {}\n\n        if _partial_variables:\n            input_variables = [\n                var for var in input_variables if var not in _partial_variables\n            ]\n\n        return cls(\n            input_variables=input_variables,\n            template=template,\n            template_format=template_format,  # type: ignore[arg-type]\n            partial_variables=_partial_variables,\n            **kwargs,\n        )\n",
        "patch": "@@ -3,8 +3,7 @@\n from __future__ import annotations\n \n import warnings\n-from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from pydantic import BaseModel, model_validator\n \n@@ -16,7 +15,11 @@\n     get_template_variables,\n     mustache_schema,\n )\n-from langchain_core.runnables.config import RunnableConfig\n+\n+if TYPE_CHECKING:\n+    from pathlib import Path\n+\n+    from langchain_core.runnables.config import RunnableConfig\n \n \n class PromptTemplate(StringPromptTemplate):"
      },
      {
        "filename": "libs/core/langchain_core/runnables/base.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport collections\nimport contextlib\nimport functools\nimport inspect\nimport threading\nfrom abc import ABC, abstractmethod\nfrom collections.abc import (\n    AsyncGenerator,\n    AsyncIterator,\n    Awaitable,\n    Coroutine,\n    Iterator,\n    Mapping,\n    Sequence,\n)\nfrom concurrent.futures import FIRST_COMPLETED, wait\nfrom contextvars import copy_context\nfrom functools import wraps\nfrom itertools import groupby, tee\nfrom operator import itemgetter\nfrom types import GenericAlias\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    Optional,\n    Protocol,\n    TypeVar,\n    Union,\n    cast,\n    get_type_hints,\n    overload,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, RootModel\nfrom typing_extensions import Literal, get_args, override\n\nfrom langchain_core._api import beta_decorator\nfrom langchain_core.load.serializable import (\n    Serializable,\n    SerializedConstructor,\n    SerializedNotImplemented,\n)\nfrom langchain_core.runnables.config import (\n    RunnableConfig,\n    _set_config_context,\n    acall_func_with_variable_args,\n    call_func_with_variable_args,\n    ensure_config,\n    get_async_callback_manager_for_config,\n    get_callback_manager_for_config,\n    get_config_list,\n    get_executor_for_config,\n    merge_configs,\n    patch_config,\n    run_in_executor,\n)\nfrom langchain_core.runnables.graph import Graph\nfrom langchain_core.runnables.schema import StreamEvent\nfrom langchain_core.runnables.utils import (\n    AddableDict,\n    AnyConfigurableField,\n    ConfigurableField,\n    ConfigurableFieldSpec,\n    Input,\n    Output,\n    accepts_config,\n    accepts_run_manager,\n    asyncio_accepts_context,\n    gated_coro,\n    gather_with_concurrency,\n    get_function_first_arg_dict_keys,\n    get_function_nonlocals,\n    get_lambda_source,\n    get_unique_config_specs,\n    indent_lines_after_first,\n    is_async_callable,\n    is_async_generator,\n)\nfrom langchain_core.utils.aiter import aclosing, atee, py_anext\nfrom langchain_core.utils.iter import safetee\nfrom langchain_core.utils.pydantic import create_model_v2\n\nif TYPE_CHECKING:\n    from langchain_core.callbacks.manager import (\n        AsyncCallbackManagerForChainRun,\n        CallbackManagerForChainRun,\n    )\n    from langchain_core.prompts.base import BasePromptTemplate\n    from langchain_core.runnables.fallbacks import (\n        RunnableWithFallbacks as RunnableWithFallbacksT,\n    )\n    from langchain_core.tools import BaseTool\n    from langchain_core.tracers.log_stream import (\n        RunLog,\n        RunLogPatch,\n    )\n    from langchain_core.tracers.root_listeners import AsyncListener\n    from langchain_core.tracers.schemas import Run\n\n\nOther = TypeVar(\"Other\")\n\n\nclass Runnable(Generic[Input, Output], ABC):\n    \"\"\"A unit of work that can be invoked, batched, streamed, transformed and composed.\n\n    Key Methods\n    ===========\n\n    - **invoke/ainvoke**: Transforms a single input into an output.\n    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n    - **stream/astream**: Streams output from a single input as it's produced.\n    - **astream_log**: Streams output and selected intermediate results from an input.\n\n    Built-in optimizations:\n\n    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n      Override to optimize batching.\n\n    - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n      the sync counterpart using asyncio's thread pool.\n      Override for native async.\n\n    All methods accept an optional config argument, which can be used to configure\n    execution, add tags and metadata for tracing and debugging etc.\n\n    Runnables expose schematic information about their input, output and config via\n    the input_schema property, the output_schema property and config_schema method.\n\n    LCEL and Composition\n    ====================\n\n    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables\n    into chains. Any chain constructed this way will automatically have sync, async,\n    batch, and streaming support.\n\n    The main composition primitives are RunnableSequence and RunnableParallel.\n\n    **RunnableSequence** invokes a series of runnables sequentially, with\n    one Runnable's output serving as the next's input. Construct using\n    the `|` operator or by passing a list of runnables to RunnableSequence.\n\n    **RunnableParallel** invokes runnables concurrently, providing the same input\n    to each. Construct it using a dict literal within a sequence or by passing a\n    dict to RunnableParallel.\n\n\n    For example,\n\n    .. code-block:: python\n\n        from langchain_core.runnables import RunnableLambda\n\n        # A RunnableSequence constructed using the `|` operator\n        sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n        sequence.invoke(1) # 4\n        sequence.batch([1, 2, 3]) # [4, 6, 8]\n\n\n        # A sequence that contains a RunnableParallel constructed using a dict literal\n        sequence = RunnableLambda(lambda x: x + 1) | {\n            'mul_2': RunnableLambda(lambda x: x * 2),\n            'mul_5': RunnableLambda(lambda x: x * 5)\n        }\n        sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}\n\n    Standard Methods\n    ================\n\n    All Runnables expose additional methods that can be used to modify their behavior\n    (e.g., add a retry policy, add lifecycle listeners, make them configurable, etc.).\n\n    These methods will work on any Runnable, including Runnable chains constructed\n    by composing other Runnables. See the individual methods for details.\n\n    For example,\n\n    .. code-block:: python\n\n        from langchain_core.runnables import RunnableLambda\n\n        import random\n\n        def add_one(x: int) -> int:\n            return x + 1\n\n\n        def buggy_double(y: int) -> int:\n            \\\"\\\"\\\"Buggy code that will fail 70% of the time\\\"\\\"\\\"\n            if random.random() > 0.3:\n                print('This code failed, and will probably be retried!')  # noqa: T201\n                raise ValueError('Triggered buggy code')\n            return y * 2\n\n        sequence = (\n            RunnableLambda(add_one) |\n            RunnableLambda(buggy_double).with_retry( # Retry on failure\n                stop_after_attempt=10,\n                wait_exponential_jitter=False\n            )\n        )\n\n        print(sequence.input_schema.model_json_schema()) # Show inferred input schema\n        print(sequence.output_schema.model_json_schema()) # Show inferred output schema\n        print(sequence.invoke(2)) # invoke the sequence (note the retry above!!)\n\n    Debugging and tracing\n    =====================\n\n    As the chains get longer, it can be useful to be able to see intermediate results\n    to debug and trace the chain.\n\n    You can set the global debug flag to True to enable debug output for all chains:\n\n        .. code-block:: python\n\n            from langchain_core.globals import set_debug\n            set_debug(True)\n\n    Alternatively, you can pass existing or custom callbacks to any given chain:\n\n        .. code-block:: python\n\n            from langchain_core.tracers import ConsoleCallbackHandler\n\n            chain.invoke(\n                ...,\n                config={'callbacks': [ConsoleCallbackHandler()]}\n            )\n\n    For a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/\n    \"\"\"  # noqa: E501\n\n    name: Optional[str]\n    \"\"\"The name of the Runnable. Used for debugging and tracing.\"\"\"\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        \"\"\"Get the name of the Runnable.\"\"\"\n        if name:\n            name_ = name\n        elif hasattr(self, \"name\") and self.name:\n            name_ = self.name\n        else:\n            # Here we handle a case where the runnable subclass is also a pydantic\n            # model.\n            cls = self.__class__\n            # Then it's a pydantic sub-class, and we have to check\n            # whether it's a generic, and if so recover the original name.\n            if (\n                hasattr(\n                    cls,\n                    \"__pydantic_generic_metadata__\",\n                )\n                and \"origin\" in cls.__pydantic_generic_metadata__\n                and cls.__pydantic_generic_metadata__[\"origin\"] is not None\n            ):\n                name_ = cls.__pydantic_generic_metadata__[\"origin\"].__name__\n            else:\n                name_ = cls.__name__\n\n        if suffix:\n            if name_[0].isupper():\n                return name_ + suffix.title()\n            else:\n                return name_ + \"_\" + suffix.lower()\n        else:\n            return name_\n\n    @property\n    def InputType(self) -> type[Input]:  # noqa: N802\n        \"\"\"The type of input this Runnable accepts specified as a type annotation.\"\"\"\n        # First loop through all parent classes and if any of them is\n        # a pydantic model, we will pick up the generic parameterization\n        # from that model via the __pydantic_generic_metadata__ attribute.\n        for base in self.__class__.mro():\n            if hasattr(base, \"__pydantic_generic_metadata__\"):\n                metadata = base.__pydantic_generic_metadata__\n                if \"args\" in metadata and len(metadata[\"args\"]) == 2:\n                    return metadata[\"args\"][0]\n\n        # If we didn't find a pydantic model in the parent classes,\n        # then loop through __orig_bases__. This corresponds to\n        # Runnables that are not pydantic models.\n        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]\n            type_args = get_args(cls)\n            if type_args and len(type_args) == 2:\n                return type_args[0]\n\n        msg = (\n            f\"Runnable {self.get_name()} doesn't have an inferable InputType. \"\n            \"Override the InputType property to specify the input type.\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def OutputType(self) -> type[Output]:  # noqa: N802\n        \"\"\"The type of output this Runnable produces specified as a type annotation.\"\"\"\n        # First loop through bases -- this will help generic\n        # any pydantic models.\n        for base in self.__class__.mro():\n            if hasattr(base, \"__pydantic_generic_metadata__\"):\n                metadata = base.__pydantic_generic_metadata__\n                if \"args\" in metadata and len(metadata[\"args\"]) == 2:\n                    return metadata[\"args\"][1]\n\n        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]\n            type_args = get_args(cls)\n            if type_args and len(type_args) == 2:\n                return type_args[1]\n\n        msg = (\n            f\"Runnable {self.get_name()} doesn't have an inferable OutputType. \"\n            \"Override the OutputType property to specify the output type.\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def input_schema(self) -> type[BaseModel]:\n        \"\"\"The type of input this Runnable accepts specified as a pydantic model.\"\"\"\n        return self.get_input_schema()\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get a pydantic model that can be used to validate input to the Runnable.\n\n        Runnables that leverage the configurable_fields and configurable_alternatives\n        methods will have a dynamic input schema that depends on which\n        configuration the Runnable is invoked with.\n\n        This method allows to get an input schema for a specific configuration.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A pydantic model that can be used to validate input.\n        \"\"\"\n        root_type = self.InputType\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=root_type,\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    def get_input_jsonschema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the input to the Runnable.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A JSON schema that represents the input to the Runnable.\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                runnable = RunnableLambda(add_one)\n\n                print(runnable.get_input_jsonschema())\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.get_input_schema(config).model_json_schema()\n\n    @property\n    def output_schema(self) -> type[BaseModel]:\n        \"\"\"The type of output this Runnable produces specified as a pydantic model.\"\"\"\n        return self.get_output_schema()\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get a pydantic model that can be used to validate output to the Runnable.\n\n        Runnables that leverage the configurable_fields and configurable_alternatives\n        methods will have a dynamic output schema that depends on which\n        configuration the Runnable is invoked with.\n\n        This method allows to get an output schema for a specific configuration.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A pydantic model that can be used to validate output.\n        \"\"\"\n        root_type = self.OutputType\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    def get_output_jsonschema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the output of the Runnable.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A JSON schema that represents the output of the Runnable.\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                runnable = RunnableLambda(add_one)\n\n                print(runnable.get_output_jsonschema())\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.get_output_schema(config).model_json_schema()\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"List configurable fields for this Runnable.\"\"\"\n        return []\n\n    def config_schema(\n        self, *, include: Optional[Sequence[str]] = None\n    ) -> type[BaseModel]:\n        \"\"\"The type of config this Runnable accepts specified as a pydantic model.\n\n        To mark a field as configurable, see the `configurable_fields`\n        and `configurable_alternatives` methods.\n\n        Args:\n            include: A list of fields to include in the config schema.\n\n        Returns:\n            A pydantic model that can be used to validate config.\n        \"\"\"\n        include = include or []\n        config_specs = self.config_specs\n        configurable = (\n            create_model_v2(  # type: ignore[call-overload]\n                \"Configurable\",\n                field_definitions={\n                    spec.id: (\n                        spec.annotation,\n                        Field(\n                            spec.default, title=spec.name, description=spec.description\n                        ),\n                    )\n                    for spec in config_specs\n                },\n            )\n            if config_specs\n            else None\n        )\n\n        # Many need to create a typed dict instead to implement NotRequired!\n        all_fields = {\n            **({\"configurable\": (configurable, None)} if configurable else {}),\n            **{\n                field_name: (field_type, None)\n                for field_name, field_type in get_type_hints(RunnableConfig).items()\n                if field_name in [i for i in include if i != \"configurable\"]\n            },\n        }\n        model = create_model_v2(  # type: ignore[call-overload]\n            self.get_name(\"Config\"), field_definitions=all_fields\n        )\n        return model\n\n    def get_config_jsonschema(\n        self, *, include: Optional[Sequence[str]] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the config of the Runnable.\n\n        Args:\n            include: A list of fields to include in the config schema.\n\n        Returns:\n            A JSON schema that represents the config of the Runnable.\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.config_schema(include=include).model_json_schema()\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Return a graph representation of this Runnable.\"\"\"\n        graph = Graph()\n        try:\n            input_node = graph.add_node(self.get_input_schema(config))\n        except TypeError:\n            input_node = graph.add_node(create_model_v2(self.get_name(\"Input\")))\n        runnable_node = graph.add_node(\n            self, metadata=config.get(\"metadata\") if config else None\n        )\n        try:\n            output_node = graph.add_node(self.get_output_schema(config))\n        except TypeError:\n            output_node = graph.add_node(create_model_v2(self.get_name(\"Output\")))\n        graph.add_edge(input_node, runnable_node)\n        graph.add_edge(runnable_node, output_node)\n        return graph\n\n    def get_prompts(\n        self, config: Optional[RunnableConfig] = None\n    ) -> list[BasePromptTemplate]:\n        \"\"\"Return a list of prompts used by this Runnable.\"\"\"\n        from langchain_core.prompts.base import BasePromptTemplate\n\n        prompts = []\n        for _, node in self.get_graph(config=config).nodes.items():\n            if isinstance(node.data, BasePromptTemplate):\n                prompts.append(node.data)\n        return prompts\n\n    def __or__(\n        self,\n        other: Union[\n            Runnable[Any, Other],\n            Callable[[Any], Other],\n            Callable[[Iterator[Any]], Iterator[Other]],\n            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],\n        ],\n    ) -> RunnableSerializable[Input, Other]:\n        \"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\n        return RunnableSequence(self, coerce_to_runnable(other))\n\n    def __ror__(\n        self,\n        other: Union[\n            Runnable[Other, Any],\n            Callable[[Other], Any],\n            Callable[[Iterator[Other]], Iterator[Any]],\n            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],\n        ],\n    ) -> RunnableSerializable[Other, Output]:\n        \"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\n        return RunnableSequence(coerce_to_runnable(other), self)\n\n    def pipe(\n        self,\n        *others: Union[Runnable[Any, Other], Callable[[Any], Other]],\n        name: Optional[str] = None,\n    ) -> RunnableSerializable[Input, Other]:\n        \"\"\"Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n\n        Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n\n        Example:\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                def mul_two(x: int) -> int:\n                    return x * 2\n\n                runnable_1 = RunnableLambda(add_one)\n                runnable_2 = RunnableLambda(mul_two)\n                sequence = runnable_1.pipe(runnable_2)\n                # Or equivalently:\n                # sequence = runnable_1 | runnable_2\n                # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n                sequence.invoke(1)\n                await sequence.ainvoke(1)\n                # -> 4\n\n                sequence.batch([1, 2, 3])\n                await sequence.abatch([1, 2, 3])\n                # -> [4, 6, 8]\n        \"\"\"\n        return RunnableSequence(self, *others, name=name)\n\n    def pick(self, keys: Union[str, list[str]]) -> RunnableSerializable[Any, Any]:\n        \"\"\"Pick keys from the output dict of this Runnable.\n\n        Pick single key:\n            .. code-block:: python\n\n                import json\n\n                from langchain_core.runnables import RunnableLambda, RunnableMap\n\n                as_str = RunnableLambda(str)\n                as_json = RunnableLambda(json.loads)\n                chain = RunnableMap(str=as_str, json=as_json)\n\n                chain.invoke(\"[1, 2, 3]\")\n                # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n\n                json_only_chain = chain.pick(\"json\")\n                json_only_chain.invoke(\"[1, 2, 3]\")\n                # -> [1, 2, 3]\n\n        Pick list of keys:\n            .. code-block:: python\n\n                from typing import Any\n\n                import json\n\n                from langchain_core.runnables import RunnableLambda, RunnableMap\n\n                as_str = RunnableLambda(str)\n                as_json = RunnableLambda(json.loads)\n                def as_bytes(x: Any) -> bytes:\n                    return bytes(x, \"utf-8\")\n\n                chain = RunnableMap(\n                    str=as_str,\n                    json=as_json,\n                    bytes=RunnableLambda(as_bytes)\n                )\n\n                chain.invoke(\"[1, 2, 3]\")\n                # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n\n                json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n                json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n                # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n\n        \"\"\"\n        from langchain_core.runnables.passthrough import RunnablePick\n\n        return self | RunnablePick(keys)\n\n    def assign(\n        self,\n        **kwargs: Union[\n            Runnable[dict[str, Any], Any],\n            Callable[[dict[str, Any]], Any],\n            Mapping[\n                str,\n                Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]],\n            ],\n        ],\n    ) -> RunnableSerializable[Any, Any]:\n        \"\"\"Assigns new fields to the dict output of this Runnable.\n        Returns a new Runnable.\n\n        .. code-block:: python\n\n            from langchain_community.llms.fake import FakeStreamingListLLM\n            from langchain_core.output_parsers import StrOutputParser\n            from langchain_core.prompts import SystemMessagePromptTemplate\n            from langchain_core.runnables import Runnable\n            from operator import itemgetter\n\n            prompt = (\n                SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n                + \"{question}\"\n            )\n            llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n            chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n\n            chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n\n            print(chain_with_assign.input_schema.model_json_schema())\n            # {'title': 'PromptInput', 'type': 'object', 'properties':\n            {'question': {'title': 'Question', 'type': 'string'}}}\n            print(chain_with_assign.output_schema.model_json_schema())\n            # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n            {'str': {'title': 'Str',\n            'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n\n        \"\"\"\n        from langchain_core.runnables.passthrough import RunnableAssign\n\n        return self | RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))\n\n    \"\"\" --- Public API --- \"\"\"\n\n    @abstractmethod\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        \"\"\"Transform a single input into an output. Override to implement.\n\n        Args:\n            input: The input to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details.\n\n        Returns:\n            The output of the Runnable.\n        \"\"\"\n\n    async def ainvoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        \"\"\"Default implementation of ainvoke, calls invoke from a thread.\n\n        The default implementation allows usage of async code even if\n        the Runnable did not implement a native async version of invoke.\n\n        Subclasses should override this method if they can run asynchronously.\n        \"\"\"\n        return await run_in_executor(config, self.invoke, input, config, **kwargs)\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Default implementation runs invoke in parallel using a thread pool executor.\n\n        The default implementation of batch works well for IO bound runnables.\n\n        Subclasses should override this method if they can batch more efficiently;\n        e.g., if the underlying Runnable uses an API which supports a batch mode.\n        \"\"\"\n        if not inputs:\n            return []\n\n        configs = get_config_list(config, len(inputs))\n\n        def invoke(input: Input, config: RunnableConfig) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return self.invoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return self.invoke(input, config, **kwargs)\n\n        # If there's only one input, don't bother with the executor\n        if len(inputs) == 1:\n            return cast(list[Output], [invoke(inputs[0], configs[0])])\n\n        with get_executor_for_config(configs[0]) as executor:\n            return cast(list[Output], list(executor.map(invoke, inputs, configs)))\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Output]]: ...\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...\n\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]:\n        \"\"\"Run invoke in parallel on a list of inputs,\n        yielding results as they complete.\n        \"\"\"\n        if not inputs:\n            return\n\n        configs = get_config_list(config, len(inputs))\n\n        def invoke(\n            i: int, input: Input, config: RunnableConfig\n        ) -> tuple[int, Union[Output, Exception]]:\n            if return_exceptions:\n                try:\n                    out: Union[Output, Exception] = self.invoke(input, config, **kwargs)\n                except Exception as e:\n                    out = e\n            else:\n                out = self.invoke(input, config, **kwargs)\n\n            return (i, out)\n\n        if len(inputs) == 1:\n            yield invoke(0, inputs[0], configs[0])\n            return\n\n        with get_executor_for_config(configs[0]) as executor:\n            futures = {\n                executor.submit(invoke, i, input, config)\n                for i, (input, config) in enumerate(zip(inputs, configs))\n            }\n\n            try:\n                while futures:\n                    done, futures = wait(futures, return_when=FIRST_COMPLETED)\n                    while done:\n                        yield done.pop().result()\n            finally:\n                for future in futures:\n                    future.cancel()\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Default implementation runs ainvoke in parallel using asyncio.gather.\n\n        The default implementation of batch works well for IO bound runnables.\n\n        Subclasses should override this method if they can batch more efficiently;\n        e.g., if the underlying Runnable uses an API which supports a batch mode.\n\n        Args:\n            inputs: A list of inputs to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details. Defaults to None.\n            return_exceptions: Whether to return exceptions instead of raising them.\n                Defaults to False.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Returns:\n            A list of outputs from the Runnable.\n        \"\"\"\n        if not inputs:\n            return []\n\n        configs = get_config_list(config, len(inputs))\n\n        async def ainvoke(\n            input: Input, config: RunnableConfig\n        ) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return await self.ainvoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return await self.ainvoke(input, config, **kwargs)\n\n        coros = map(ainvoke, inputs, configs)\n        return await gather_with_concurrency(configs[0].get(\"max_concurrency\"), *coros)\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Output]]: ...\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...\n\n    async def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:\n        \"\"\"Run ainvoke in parallel on a list of inputs,\n        yielding results as they complete.\n\n        Args:\n            inputs: A list of inputs to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details. Defaults to None. Defaults to None.\n            return_exceptions: Whether to return exceptions instead of raising them.\n                Defaults to False.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            A tuple of the index of the input and the output from the Runnable.\n        \"\"\"\n        if not inputs:\n            return\n\n        configs = get_config_list(config, len(inputs))\n        # Get max_concurrency from first config, defaulting to None (unlimited)\n        max_concurrency = configs[0].get(\"max_concurrency\") if configs else None\n        semaphore = asyncio.Semaphore(max_concurrency) if max_concurrency else None\n\n        async def ainvoke_task(\n            i: int, input: Input, config: RunnableConfig\n        ) -> tuple[int, Union[Output, Exception]]:\n            if return_exceptions:\n                try:\n                    out: Union[Output, Exception] = await self.ainvoke(\n                        input, config, **kwargs\n                    )\n                except Exception as e:\n                    out = e\n            else:\n                out = await self.ainvoke(input, config, **kwargs)\n            return (i, out)\n\n        coros = [\n            gated_coro(semaphore, ainvoke_task(i, input, config))\n            if semaphore\n            else ainvoke_task(i, input, config)\n            for i, (input, config) in enumerate(zip(inputs, configs))\n        ]\n\n        for coro in asyncio.as_completed(coros):\n            yield await coro\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Default implementation of stream, which calls invoke.\n        Subclasses should override this method if they support streaming output.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        yield self.invoke(input, config, **kwargs)\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Default implementation of astream, which calls ainvoke.\n        Subclasses should override this method if they support streaming output.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        yield await self.ainvoke(input, config, **kwargs)\n\n    @overload\n    def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: Literal[True] = True,\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[RunLogPatch]: ...\n\n    @overload\n    def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: Literal[False],\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[RunLog]: ...\n\n    async def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: bool = True,\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]:\n        \"\"\"Stream all output from a Runnable, as reported to the callback system.\n        This includes all inner runs of LLMs, Retrievers, Tools, etc.\n\n        Output is streamed as Log objects, which include a list of\n        Jsonpatch ops that describe how the state of the run has changed in each\n        step, and the final state of the run.\n\n        The Jsonpatch ops can be applied in order to construct state.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable.\n            diff: Whether to yield diffs between each step or the current state.\n            with_streamed_output_list: Whether to yield the streamed_output list.\n            include_names: Only include logs with these names.\n            include_types: Only include logs with these types.\n            include_tags: Only include logs with these tags.\n            exclude_names: Exclude logs with these names.\n            exclude_types: Exclude logs with these types.\n            exclude_tags: Exclude logs with these tags.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            A RunLogPatch or RunLog object.\n        \"\"\"\n        from langchain_core.tracers.log_stream import (\n            LogStreamCallbackHandler,\n            _astream_log_implementation,\n        )\n\n        stream = LogStreamCallbackHandler(\n            auto_close=False,\n            include_names=include_names,\n            include_types=include_types,\n            include_tags=include_tags,\n            exclude_names=exclude_names,\n            exclude_types=exclude_types,\n            exclude_tags=exclude_tags,\n            _schema_format=\"original\",\n        )\n\n        # Mypy isn't resolving the overloads here\n        # Likely an issue b/c `self` is being passed through\n        # and it's can't map it to Runnable[Input,Output]?\n        async for item in _astream_log_implementation(  # type: ignore\n            self,\n            input,\n            config,\n            diff=diff,\n            stream=stream,\n            with_streamed_output_list=with_streamed_output_list,\n            **kwargs,\n        ):\n            yield item\n\n    async def astream_events(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        version: Literal[\"v1\", \"v2\"] = \"v2\",\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[StreamEvent]:\n        \"\"\"Generate a stream of events.\n\n        Use to create an iterator over StreamEvents that provide real-time information\n        about the progress of the Runnable, including StreamEvents from intermediate\n        results.\n\n        A StreamEvent is a dictionary with the following schema:\n\n        - ``event``: **str** - Event names are of the\n            format: on_[runnable_type]_(start|stream|end).\n        - ``name``: **str** - The name of the Runnable that generated the event.\n        - ``run_id``: **str** - randomly generated ID associated with the given execution of\n            the Runnable that emitted the event.\n            A child Runnable that gets invoked as part of the execution of a\n            parent Runnable is assigned its own unique ID.\n        - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n            generated the event. The root Runnable will have an empty list.\n            The order of the parent IDs is from the root to the immediate parent.\n            Only available for v2 version of the API. The v1 version of the API\n            will return an empty list.\n        - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n            the event.\n        - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n            that generated the event.\n        - ``data``: **Dict[str, Any]**\n\n\n        Below is a table that illustrates some events that might be emitted by various\n        chains. Metadata fields have been omitted from the table for brevity.\n        Chain definitions have been included after the table.\n\n        **ATTENTION** This reference table is for the V2 version of the schema.\n\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | event                | name             | chunk                           | input                                         | output                                          |\n        +======================+==================+=================================+===============================================+=================================================+\n        | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n\n        In addition to the standard events, users can also dispatch custom events (see example below).\n\n        Custom events will be only be surfaced with in the `v2` version of the API!\n\n        A custom event has following format:\n\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n        | Attribute | Type | Description                                                                                               |\n        +===========+======+===========================================================================================================+\n        | name      | str  | A user defined name for the event.                                                                        |\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n        | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n\n        Here are declarations associated with the standard events shown above:\n\n        `format_docs`:\n\n        .. code-block:: python\n\n            def format_docs(docs: List[Document]) -> str:\n                '''Format the docs.'''\n                return \", \".join([doc.page_content for doc in docs])\n\n            format_docs = RunnableLambda(format_docs)\n\n        `some_tool`:\n\n        .. code-block:: python\n\n            @tool\n            def some_tool(x: int, y: str) -> dict:\n                '''Some_tool.'''\n                return {\"x\": x, \"y\": y}\n\n        `prompt`:\n\n        .. code-block:: python\n\n            template = ChatPromptTemplate.from_messages(\n                [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n            ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            async def reverse(s: str) -> str:\n                return s[::-1]\n\n            chain = RunnableLambda(func=reverse)\n\n            events = [\n                event async for event in chain.astream_events(\"hello\", version=\"v2\")\n            ]\n\n            # will produce the following events (run_id, and parent_ids\n            # has been omitted for brevity):\n            [\n                {\n                    \"data\": {\"input\": \"hello\"},\n                    \"event\": \"on_chain_start\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"chunk\": \"olleh\"},\n                    \"event\": \"on_chain_stream\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"output\": \"olleh\"},\n                    \"event\": \"on_chain_end\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n            ]\n\n\n        Example: Dispatch Custom Event\n\n        .. code-block:: python\n\n            from langchain_core.callbacks.manager import (\n                adispatch_custom_event,\n            )\n            from langchain_core.runnables import RunnableLambda, RunnableConfig\n            import asyncio\n\n\n            async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n                \\\"\\\"\\\"Do something that takes a long time.\\\"\\\"\\\"\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                await adispatch_custom_event(\n                    \"progress_event\",\n                    {\"message\": \"Finished step 1 of 3\"},\n                    config=config # Must be included for python < 3.10\n                )\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                await adispatch_custom_event(\n                    \"progress_event\",\n                    {\"message\": \"Finished step 2 of 3\"},\n                    config=config # Must be included for python < 3.10\n                )\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                return \"Done\"\n\n            slow_thing = RunnableLambda(slow_thing)\n\n            async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n                print(event)\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable.\n            version: The version of the schema to use either `v2` or `v1`.\n                     Users should use `v2`.\n                     `v1` is for backwards compatibility and will be deprecated\n                     in 0.4.0.\n                     No default will be assigned until the API is stabilized.\n                     custom events will only be surfaced in `v2`.\n            include_names: Only include events from runnables with matching names.\n            include_types: Only include events from runnables with matching types.\n            include_tags: Only include events from runnables with matching tags.\n            exclude_names: Exclude events from runnables with matching names.\n            exclude_types: Exclude events from runnables with matching types.\n            exclude_tags: Exclude events from runnables with matching tags.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n                These will be passed to astream_log as this implementation\n                of astream_events is built on top of astream_log.\n\n        Yields:\n            An async stream of StreamEvents.\n\n        Raises:\n            NotImplementedError: If the version is not `v1` or `v2`.\n        \"\"\"  # noqa: E501\n        from langchain_core.tracers.event_stream import (\n            _astream_events_implementation_v1,\n            _astream_events_implementation_v2,\n        )\n\n        if version == \"v2\":\n            event_stream = _astream_events_implementation_v2(\n                self,\n                input,\n                config=config,\n                include_names=include_names,\n                include_types=include_types,\n                include_tags=include_tags,\n                exclude_names=exclude_names,\n                exclude_types=exclude_types,\n                exclude_tags=exclude_tags,\n                **kwargs,\n            )\n        elif version == \"v1\":\n            # First implementation, built on top of astream_log API\n            # This implementation will be deprecated as of 0.2.0\n            event_stream = _astream_events_implementation_v1(\n                self,\n                input,\n                config=config,\n                include_names=include_names,\n                include_types=include_types,\n                include_tags=include_tags,\n                exclude_names=exclude_names,\n                exclude_types=exclude_types,\n                exclude_tags=exclude_tags,\n                **kwargs,\n            )\n        else:\n            msg = 'Only versions \"v1\" and \"v2\" of the schema is currently supported.'\n            raise NotImplementedError(msg)\n\n        async with aclosing(event_stream):\n            async for event in event_stream:\n                yield event\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Default implementation of transform, which buffers input and calls astream.\n\n        Subclasses should override this method if they can start producing output while\n        input is still being generated.\n\n        Args:\n            input: An iterator of inputs to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        final: Input\n        got_first_val = False\n\n        for ichunk in input:\n            # The default implementation of transform is to buffer input and\n            # then call stream.\n            # It'll attempt to gather all input into a single chunk using\n            # the `+` operator.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk,\n            # and we'll iterate until we get to the last chunk.\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if got_first_val:\n            yield from self.stream(final, config, **kwargs)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Default implementation of atransform, which buffers input and calls astream.\n        Subclasses should override this method if they can start producing output while\n        input is still being generated.\n\n        Args:\n            input: An async iterator of inputs to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        final: Input\n        got_first_val = False\n\n        async for ichunk in input:\n            # The default implementation of transform is to buffer input and\n            # then call stream.\n            # It'll attempt to gather all input into a single chunk using\n            # the `+` operator.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk,\n            # and we'll iterate until we get to the last chunk.\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if got_first_val:\n            async for output in self.astream(final, config, **kwargs):\n                yield output\n\n    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:\n        \"\"\"Bind arguments to a Runnable, returning a new Runnable.\n\n        Useful when a Runnable in a chain requires an argument that is not\n        in the output of the previous Runnable or included in the user input.\n\n        Args:\n            kwargs: The arguments to bind to the Runnable.\n\n        Returns:\n            A new Runnable with the arguments bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_community.chat_models import ChatOllama\n            from langchain_core.output_parsers import StrOutputParser\n\n            llm = ChatOllama(model='llama2')\n\n            # Without bind.\n            chain = (\n                llm\n                | StrOutputParser()\n            )\n\n            chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n            # Output is 'One two three four five.'\n\n            # With bind.\n            chain = (\n                llm.bind(stop=[\"three\"])\n                | StrOutputParser()\n            )\n\n            chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n            # Output is 'One two'\n\n        \"\"\"\n        return RunnableBinding(bound=self, kwargs=kwargs, config={})\n\n    def with_config(\n        self,\n        config: Optional[RunnableConfig] = None,\n        # Sadly Unpack is not well-supported by mypy so this will have to be untyped\n        **kwargs: Any,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind config to a Runnable, returning a new Runnable.\n\n        Args:\n            config: The config to bind to the Runnable.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Returns:\n            A new Runnable with the config bound.\n        \"\"\"\n        return RunnableBinding(\n            bound=self,\n            config=cast(\n                RunnableConfig,\n                {**(config or {}), **kwargs},\n            ),  # type: ignore[misc]\n            kwargs={},\n        )\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        on_start: Called before the Runnable starts running, with the Run object.\n        on_end: Called after the Runnable finishes running, with the Run object.\n        on_error: Called if the Runnable throws an error, with the Run object.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n\n        Args:\n            on_start: Called before the Runnable starts running. Defaults to None.\n            on_end: Called after the Runnable finishes running. Defaults to None.\n            on_error: Called if the Runnable throws an error. Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n            from langchain_core.tracers.schemas import Run\n\n            import time\n\n            def test_runnable(time_to_sleep : int):\n                time.sleep(time_to_sleep)\n\n            def fn_start(run_obj: Run):\n                print(\"start_time:\", run_obj.start_time)\n\n            def fn_end(run_obj: Run):\n                print(\"end_time:\", run_obj.end_time)\n\n            chain = RunnableLambda(test_runnable).with_listeners(\n                on_start=fn_start,\n                on_end=fn_end\n            )\n            chain.invoke(2)\n        \"\"\"\n        from langchain_core.tracers.root_listeners import RootListenersTracer\n\n        return RunnableBinding(\n            bound=self,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        RootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n        )\n\n    def with_alisteners(\n        self,\n        *,\n        on_start: Optional[AsyncListener] = None,\n        on_end: Optional[AsyncListener] = None,\n        on_error: Optional[AsyncListener] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n\n        on_start: Asynchronously called before the Runnable starts running.\n        on_end: Asynchronously called after the Runnable finishes running.\n        on_error: Asynchronously called if the Runnable throws an error.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n\n        Args:\n            on_start: Asynchronously called before the Runnable starts running.\n                Defaults to None.\n            on_end: Asynchronously called after the Runnable finishes running.\n                Defaults to None.\n            on_error: Asynchronously called if the Runnable throws an error.\n                Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n            import time\n\n            async def test_runnable(time_to_sleep : int):\n                print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n                await asyncio.sleep(time_to_sleep)\n                print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n\n            async def fn_start(run_obj : Runnable):\n                print(f\"on start callback starts at {format_t(time.time())}\n                await asyncio.sleep(3)\n                print(f\"on start callback ends at {format_t(time.time())}\")\n\n            async def fn_end(run_obj : Runnable):\n                print(f\"on end callback starts at {format_t(time.time())}\n                await asyncio.sleep(2)\n                print(f\"on end callback ends at {format_t(time.time())}\")\n\n            runnable = RunnableLambda(test_runnable).with_alisteners(\n                on_start=fn_start,\n                on_end=fn_end\n            )\n            async def concurrent_runs():\n                await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n\n            asyncio.run(concurrent_runs())\n            Result:\n            on start callback starts at 2024-05-16T14:20:29.637053+00:00\n            on start callback starts at 2024-05-16T14:20:29.637150+00:00\n            on start callback ends at 2024-05-16T14:20:32.638305+00:00\n            on start callback ends at 2024-05-16T14:20:32.638383+00:00\n            Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n            Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n            Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n            on end callback starts at 2024-05-16T14:20:35.640534+00:00\n            Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n            on end callback starts at 2024-05-16T14:20:37.640574+00:00\n            on end callback ends at 2024-05-16T14:20:37.640654+00:00\n            on end callback ends at 2024-05-16T14:20:39.641751+00:00\n\n        \"\"\"\n        from langchain_core.tracers.root_listeners import AsyncRootListenersTracer\n\n        return RunnableBinding(\n            bound=self,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        AsyncRootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n        )\n\n    def with_types(\n        self,\n        *,\n        input_type: Optional[type[Input]] = None,\n        output_type: Optional[type[Output]] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind input and output types to a Runnable, returning a new Runnable.\n\n        Args:\n            input_type: The input type to bind to the Runnable. Defaults to None.\n            output_type: The output type to bind to the Runnable. Defaults to None.\n\n        Returns:\n            A new Runnable with the types bound.\n        \"\"\"\n        return RunnableBinding(\n            bound=self,\n            custom_input_type=input_type,\n            custom_output_type=output_type,\n            kwargs={},\n        )\n\n    def with_retry(\n        self,\n        *,\n        retry_if_exception_type: tuple[type[BaseException], ...] = (Exception,),\n        wait_exponential_jitter: bool = True,\n        stop_after_attempt: int = 3,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Create a new Runnable that retries the original Runnable on exceptions.\n\n        Args:\n            retry_if_exception_type: A tuple of exception types to retry on.\n                Defaults to (Exception,).\n            wait_exponential_jitter: Whether to add jitter to the wait\n                time between retries. Defaults to True.\n            stop_after_attempt: The maximum number of attempts to make before\n                giving up. Defaults to 3.\n\n        Returns:\n            A new Runnable that retries the original Runnable on exceptions.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            count = 0\n\n\n            def _lambda(x: int) -> None:\n                global count\n                count = count + 1\n                if x == 1:\n                    raise ValueError(\"x is 1\")\n                else:\n                     pass\n\n\n            runnable = RunnableLambda(_lambda)\n            try:\n                runnable.with_retry(\n                    stop_after_attempt=2,\n                    retry_if_exception_type=(ValueError,),\n                ).invoke(1)\n            except ValueError:\n                pass\n\n            assert (count == 2)\n\n\n        Args:\n            retry_if_exception_type: A tuple of exception types to retry on\n            wait_exponential_jitter: Whether to add jitter to the wait time\n                                     between retries\n            stop_after_attempt: The maximum number of attempts to make before giving up\n\n        Returns:\n            A new Runnable that retries the original Runnable on exceptions.\n        \"\"\"\n        from langchain_core.runnables.retry import RunnableRetry\n\n        return RunnableRetry(\n            bound=self,\n            kwargs={},\n            config={},\n            retry_exception_types=retry_if_exception_type,\n            wait_exponential_jitter=wait_exponential_jitter,\n            max_attempt_number=stop_after_attempt,\n        )\n\n    def map(self) -> Runnable[list[Input], list[Output]]:\n        \"\"\"Return a new Runnable that maps a list of inputs to a list of outputs,\n        by calling invoke() with each input.\n\n        Returns:\n            A new Runnable that maps a list of inputs to a list of outputs.\n\n        Example:\n\n            .. code-block:: python\n\n                    from langchain_core.runnables import RunnableLambda\n\n                    def _lambda(x: int) -> int:\n                        return x + 1\n\n                    runnable = RunnableLambda(_lambda)\n                    print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n        \"\"\"\n        return RunnableEach(bound=self)\n\n    def with_fallbacks(\n        self,\n        fallbacks: Sequence[Runnable[Input, Output]],\n        *,\n        exceptions_to_handle: tuple[type[BaseException], ...] = (Exception,),\n        exception_key: Optional[str] = None,\n    ) -> RunnableWithFallbacksT[Input, Output]:\n        \"\"\"Add fallbacks to a Runnable, returning a new Runnable.\n\n        The new Runnable will try the original Runnable, and then each fallback\n        in order, upon failures.\n\n        Args:\n            fallbacks: A sequence of runnables to try if the original Runnable fails.\n            exceptions_to_handle: A tuple of exception types to handle.\n                Defaults to (Exception,).\n            exception_key: If string is specified then handled exceptions will be passed\n                to fallbacks as part of the input under the specified key. If None,\n                exceptions will not be passed to fallbacks. If used, the base Runnable\n                and its fallbacks must accept a dictionary as input. Defaults to None.\n\n        Returns:\n            A new Runnable that will try the original Runnable, and then each\n            fallback in order, upon failures.\n\n        Example:\n\n            .. code-block:: python\n\n                from typing import Iterator\n\n                from langchain_core.runnables import RunnableGenerator\n\n\n                def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n                    raise ValueError()\n                    yield \"\"\n\n\n                def _generate(input: Iterator) -> Iterator[str]:\n                    yield from \"foo bar\"\n\n\n                runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n                    [RunnableGenerator(_generate)]\n                    )\n                print(''.join(runnable.stream({}))) #foo bar\n\n        Args:\n            fallbacks: A sequence of runnables to try if the original Runnable fails.\n            exceptions_to_handle: A tuple of exception types to handle.\n            exception_key: If string is specified then handled exceptions will be passed\n                to fallbacks as part of the input under the specified key. If None,\n                exceptions will not be passed to fallbacks. If used, the base Runnable\n                and its fallbacks must accept a dictionary as input.\n\n        Returns:\n            A new Runnable that will try the original Runnable, and then each\n            fallback in order, upon failures.\n\n        \"\"\"\n        from langchain_core.runnables.fallbacks import RunnableWithFallbacks\n\n        return RunnableWithFallbacks(\n            runnable=self,\n            fallbacks=fallbacks,\n            exceptions_to_handle=exceptions_to_handle,\n            exception_key=exception_key,\n        )\n\n    \"\"\" --- Helper methods for Subclasses --- \"\"\"\n\n    def _call_with_config(\n        self,\n        func: Union[\n            Callable[[Input], Output],\n            Callable[[Input, CallbackManagerForChainRun], Output],\n            Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\n        ],\n        input: Input,\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        serialized: Optional[dict[str, Any]] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        config = ensure_config(config)\n        callback_manager = get_callback_manager_for_config(config)\n        run_manager = callback_manager.on_chain_start(\n            serialized,\n            input,\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            output = cast(\n                Output,\n                context.run(\n                    call_func_with_variable_args,  # type: ignore[arg-type]\n                    func,  # type: ignore[arg-type]\n                    input,  # type: ignore[arg-type]\n                    config,\n                    run_manager,\n                    **kwargs,\n                ),\n            )\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(output)\n            return output\n\n    async def _acall_with_config(\n        self,\n        func: Union[\n            Callable[[Input], Awaitable[Output]],\n            Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n            Callable[\n                [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                Awaitable[Output],\n            ],\n        ],\n        input: Input,\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        serialized: Optional[dict[str, Any]] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement ainvoke() in subclasses.\n        \"\"\"\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        run_manager = await callback_manager.on_chain_start(\n            serialized,\n            input,\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            coro = acall_func_with_variable_args(\n                func, input, config, run_manager, **kwargs\n            )\n            if asyncio_accepts_context():\n                output: Output = await asyncio.create_task(coro, context=context)  # type: ignore\n            else:\n                output = await coro\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(output)\n            return output\n\n    def _batch_with_config(\n        self,\n        func: Union[\n            Callable[[list[Input]], list[Union[Exception, Output]]],\n            Callable[\n                [list[Input], list[CallbackManagerForChainRun]],\n                list[Union[Exception, Output]],\n            ],\n            Callable[\n                [list[Input], list[CallbackManagerForChainRun], list[RunnableConfig]],\n                list[Union[Exception, Output]],\n            ],\n        ],\n        input: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        if not input:\n            return []\n\n        configs = get_config_list(config, len(input))\n        callback_managers = [get_callback_manager_for_config(c) for c in configs]\n        run_managers = [\n            callback_manager.on_chain_start(\n                None,\n                input,\n                run_type=run_type,\n                name=config.get(\"run_name\") or self.get_name(),\n                run_id=config.pop(\"run_id\", None),\n            )\n            for callback_manager, input, config in zip(\n                callback_managers, input, configs\n            )\n        ]\n        try:\n            if accepts_config(func):\n                kwargs[\"config\"] = [\n                    patch_config(c, callbacks=rm.get_child())\n                    for c, rm in zip(configs, run_managers)\n                ]\n            if accepts_run_manager(func):\n                kwargs[\"run_manager\"] = run_managers\n            output = func(input, **kwargs)  # type: ignore[call-arg]\n        except BaseException as e:\n            for run_manager in run_managers:\n                run_manager.on_chain_error(e)\n            if return_exceptions:\n                return cast(list[Output], [e for _ in input])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            for run_manager, out in zip(run_managers, output):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    run_manager.on_chain_error(out)\n                else:\n                    run_manager.on_chain_end(out)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], output)\n            else:\n                raise first_exception\n\n    async def _abatch_with_config(\n        self,\n        func: Union[\n            Callable[[list[Input]], Awaitable[list[Union[Exception, Output]]]],\n            Callable[\n                [list[Input], list[AsyncCallbackManagerForChainRun]],\n                Awaitable[list[Union[Exception, Output]]],\n            ],\n            Callable[\n                [\n                    list[Input],\n                    list[AsyncCallbackManagerForChainRun],\n                    list[RunnableConfig],\n                ],\n                Awaitable[list[Union[Exception, Output]]],\n            ],\n        ],\n        input: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        if not input:\n            return []\n\n        configs = get_config_list(config, len(input))\n        callback_managers = [get_async_callback_manager_for_config(c) for c in configs]\n        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(\n            *(\n                callback_manager.on_chain_start(\n                    None,\n                    input,\n                    run_type=run_type,\n                    name=config.get(\"run_name\") or self.get_name(),\n                    run_id=config.pop(\"run_id\", None),\n                )\n                for callback_manager, input, config in zip(\n                    callback_managers, input, configs\n                )\n            )\n        )\n        try:\n            if accepts_config(func):\n                kwargs[\"config\"] = [\n                    patch_config(c, callbacks=rm.get_child())\n                    for c, rm in zip(configs, run_managers)\n                ]\n            if accepts_run_manager(func):\n                kwargs[\"run_manager\"] = run_managers\n            output = await func(input, **kwargs)  # type: ignore[call-arg]\n        except BaseException as e:\n            await asyncio.gather(\n                *(run_manager.on_chain_error(e) for run_manager in run_managers)\n            )\n            if return_exceptions:\n                return cast(list[Output], [e for _ in input])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            coros: list[Awaitable[None]] = []\n            for run_manager, out in zip(run_managers, output):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    coros.append(run_manager.on_chain_error(out))\n                else:\n                    coros.append(run_manager.on_chain_end(out))\n            await asyncio.gather(*coros)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], output)\n            else:\n                raise first_exception\n\n    def _transform_stream_with_config(\n        self,\n        input: Iterator[Input],\n        transformer: Union[\n            Callable[[Iterator[Input]], Iterator[Output]],\n            Callable[[Iterator[Input], CallbackManagerForChainRun], Iterator[Output]],\n            Callable[\n                [\n                    Iterator[Input],\n                    CallbackManagerForChainRun,\n                    RunnableConfig,\n                ],\n                Iterator[Output],\n            ],\n        ],\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Helper method to transform an Iterator of Input values into an Iterator of\n        Output values, with callbacks.\n        Use this to implement `stream()` or `transform()` in Runnable subclasses.\n        \"\"\"\n        # Mixin that is used by both astream log and astream events implementation\n        from langchain_core.tracers._streaming import _StreamingCallbackHandler\n\n        # tee the input so we can iterate over it twice\n        input_for_tracing, input_for_transform = tee(input, 2)\n        # Start the input iterator to ensure the input Runnable starts before this one\n        final_input: Optional[Input] = next(input_for_tracing, None)\n        final_input_supported = True\n        final_output: Optional[Output] = None\n        final_output_supported = True\n\n        config = ensure_config(config)\n        callback_manager = get_callback_manager_for_config(config)\n        run_manager = callback_manager.on_chain_start(\n            None,\n            {\"input\": \"\"},\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            if accepts_config(transformer):\n                kwargs[\"config\"] = child_config\n            if accepts_run_manager(transformer):\n                kwargs[\"run_manager\"] = run_manager\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            iterator = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]\n            if stream_handler := next(\n                (\n                    cast(_StreamingCallbackHandler, h)\n                    for h in run_manager.handlers\n                    # instance check OK here, it's a mixin\n                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]\n                ),\n                None,\n            ):\n                # populates streamed_output in astream_log() output if needed\n                iterator = stream_handler.tap_output_iter(run_manager.run_id, iterator)\n            try:\n                while True:\n                    chunk: Output = context.run(next, iterator)  # type: ignore\n                    yield chunk\n                    if final_output_supported:\n                        if final_output is None:\n                            final_output = chunk\n                        else:\n                            try:\n                                final_output = final_output + chunk  # type: ignore\n                            except TypeError:\n                                final_output = chunk\n                                final_output_supported = False\n                    else:\n                        final_output = chunk\n            except (StopIteration, GeneratorExit):\n                pass\n            for ichunk in input_for_tracing:\n                if final_input_supported:\n                    if final_input is None:\n                        final_input = ichunk\n                    else:\n                        try:\n                            final_input = final_input + ichunk  # type: ignore\n                        except TypeError:\n                            final_input = ichunk\n                            final_input_supported = False\n                else:\n                    final_input = ichunk\n        except BaseException as e:\n            run_manager.on_chain_error(e, inputs=final_input)\n            raise\n        else:\n            run_manager.on_chain_end(final_output, inputs=final_input)\n\n    async def _atransform_stream_with_config(\n        self,\n        input: AsyncIterator[Input],\n        transformer: Union[\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n            Callable[\n                [AsyncIterator[Input], AsyncCallbackManagerForChainRun],\n                AsyncIterator[Output],\n            ],\n            Callable[\n                [\n                    AsyncIterator[Input],\n                    AsyncCallbackManagerForChainRun,\n                    RunnableConfig,\n                ],\n                AsyncIterator[Output],\n            ],\n        ],\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Helper method to transform an Async Iterator of Input values into an Async\n        Iterator of Output values, with callbacks.\n        Use this to implement `astream()` or `atransform()` in Runnable subclasses.\n        \"\"\"\n        # Mixin that is used by both astream log and astream events implementation\n        from langchain_core.tracers._streaming import _StreamingCallbackHandler\n\n        # tee the input so we can iterate over it twice\n        input_for_tracing, input_for_transform = atee(input, 2)\n        # Start the input iterator to ensure the input Runnable starts before this one\n        final_input: Optional[Input] = await py_anext(input_for_tracing, None)\n        final_input_supported = True\n        final_output: Optional[Output] = None\n        final_output_supported = True\n\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            {\"input\": \"\"},\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            if accepts_config(transformer):\n                kwargs[\"config\"] = child_config\n            if accepts_run_manager(transformer):\n                kwargs[\"run_manager\"] = run_manager\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            iterator_ = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]\n\n            if stream_handler := next(\n                (\n                    cast(_StreamingCallbackHandler, h)\n                    for h in run_manager.handlers\n                    # instance check OK here, it's a mixin\n                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]\n                ),\n                None,\n            ):\n                # populates streamed_output in astream_log() output if needed\n                iterator = stream_handler.tap_output_aiter(\n                    run_manager.run_id, iterator_\n                )\n            else:\n                iterator = iterator_\n            try:\n                while True:\n                    if asyncio_accepts_context():\n                        chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]\n                            py_anext(iterator),  # type: ignore[arg-type]\n                            context=context,\n                        )\n                    else:\n                        chunk = cast(Output, await py_anext(iterator))\n                    yield chunk\n                    if final_output_supported:\n                        if final_output is None:\n                            final_output = chunk\n                        else:\n                            try:\n                                final_output = final_output + chunk  # type: ignore\n                            except TypeError:\n                                final_output = chunk\n                                final_output_supported = False\n                    else:\n                        final_output = chunk\n            except StopAsyncIteration:\n                pass\n            async for ichunk in input_for_tracing:\n                if final_input_supported:\n                    if final_input is None:\n                        final_input = ichunk\n                    else:\n                        try:\n                            final_input = final_input + ichunk  # type: ignore[operator]\n                        except TypeError:\n                            final_input = ichunk\n                            final_input_supported = False\n                else:\n                    final_input = ichunk\n        except BaseException as e:\n            await run_manager.on_chain_error(e, inputs=final_input)\n            raise\n        else:\n            await run_manager.on_chain_end(final_output, inputs=final_input)\n        finally:\n            if iterator_ is not None and hasattr(iterator_, \"aclose\"):\n                await iterator_.aclose()\n\n    @beta_decorator.beta(message=\"This API is in beta and may change in the future.\")\n    def as_tool(\n        self,\n        args_schema: Optional[type[BaseModel]] = None,\n        *,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        arg_types: Optional[dict[str, type]] = None,\n    ) -> BaseTool:\n        \"\"\"Create a BaseTool from a Runnable.\n\n        ``as_tool`` will instantiate a BaseTool with a name, description, and\n        ``args_schema`` from a Runnable. Where possible, schemas are inferred\n        from ``runnable.get_input_schema``. Alternatively (e.g., if the\n        Runnable takes a dict as input and the specific dict keys are not typed),\n        the schema can be specified directly with ``args_schema``. You can also\n        pass ``arg_types`` to just specify the required arguments and their types.\n\n        Args:\n            args_schema: The schema for the tool. Defaults to None.\n            name: The name of the tool. Defaults to None.\n            description: The description of the tool. Defaults to None.\n            arg_types: A dictionary of argument names to types. Defaults to None.\n\n        Returns:\n            A BaseTool instance.\n\n        Typed dict input:\n\n        .. code-block:: python\n\n            from typing import List\n            from typing_extensions import TypedDict\n            from langchain_core.runnables import RunnableLambda\n\n            class Args(TypedDict):\n                a: int\n                b: List[int]\n\n            def f(x: Args) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool()\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        ``dict`` input, specifying schema via ``args_schema``:\n\n        .. code-block:: python\n\n            from typing import Any, Dict, List\n            from pydantic import BaseModel, Field\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: Dict[str, Any]) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            class FSchema(BaseModel):\n                \\\"\\\"\\\"Apply a function to an integer and list of integers.\\\"\\\"\\\"\n\n                a: int = Field(..., description=\"Integer\")\n                b: List[int] = Field(..., description=\"List of ints\")\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool(FSchema)\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        ``dict`` input, specifying schema via ``arg_types``:\n\n        .. code-block:: python\n\n            from typing import Any, Dict, List\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: Dict[str, Any]) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        String input:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: str) -> str:\n                return x + \"a\"\n\n            def g(x: str) -> str:\n                return x + \"z\"\n\n            runnable = RunnableLambda(f) | g\n            as_tool = runnable.as_tool()\n            as_tool.invoke(\"b\")\n\n        .. versionadded:: 0.2.14\n        \"\"\"\n        # Avoid circular import\n        from langchain_core.tools import convert_runnable_to_tool\n\n        return convert_runnable_to_tool(\n            self,\n            args_schema=args_schema,\n            name=name,\n            description=description,\n            arg_types=arg_types,\n        )\n\n\nclass RunnableSerializable(Serializable, Runnable[Input, Output]):\n    \"\"\"Runnable that can be serialized to JSON.\"\"\"\n\n    name: Optional[str] = None\n\n    model_config = ConfigDict(\n        # Suppress warnings from pydantic protected namespaces\n        # (e.g., `model_`)\n        protected_namespaces=(),\n    )\n\n    def to_json(self) -> Union[SerializedConstructor, SerializedNotImplemented]:\n        \"\"\"Serialize the Runnable to JSON.\n\n        Returns:\n            A JSON-serializable representation of the Runnable.\n        \"\"\"\n        dumped = super().to_json()\n        with contextlib.suppress(Exception):\n            dumped[\"name\"] = self.get_name()\n        return dumped\n\n    def configurable_fields(\n        self, **kwargs: AnyConfigurableField\n    ) -> RunnableSerializable[Input, Output]:\n        \"\"\"Configure particular Runnable fields at runtime.\n\n        Args:\n            **kwargs: A dictionary of ConfigurableField instances to configure.\n\n        Returns:\n            A new Runnable with the fields configured.\n\n        .. code-block:: python\n\n            from langchain_core.runnables import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            model = ChatOpenAI(max_tokens=20).configurable_fields(\n                max_tokens=ConfigurableField(\n                    id=\"output_token_number\",\n                    name=\"Max tokens in the output\",\n                    description=\"The maximum number of tokens in the output\",\n                )\n            )\n\n            # max_tokens = 20\n            print(\n                \"max_tokens_20: \",\n                model.invoke(\"tell me something about chess\").content\n            )\n\n            # max_tokens = 200\n            print(\"max_tokens_200: \", model.with_config(\n                configurable={\"output_token_number\": 200}\n                ).invoke(\"tell me something about chess\").content\n            )\n        \"\"\"\n        from langchain_core.runnables.configurable import RunnableConfigurableFields\n\n        for key in kwargs:\n            if key not in self.model_fields:\n                msg = (\n                    f\"Configuration key {key} not found in {self}: \"\n                    f\"available keys are {self.model_fields.keys()}\"\n                )\n                raise ValueError(msg)\n\n        return RunnableConfigurableFields(default=self, fields=kwargs)\n\n    def configurable_alternatives(\n        self,\n        which: ConfigurableField,\n        *,\n        default_key: str = \"default\",\n        prefix_keys: bool = False,\n        **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],\n    ) -> RunnableSerializable[Input, Output]:\n        \"\"\"Configure alternatives for Runnables that can be set at runtime.\n\n        Args:\n            which: The ConfigurableField instance that will be used to select the\n                alternative.\n            default_key: The default key to use if no alternative is selected.\n                Defaults to \"default\".\n            prefix_keys: Whether to prefix the keys with the ConfigurableField id.\n                Defaults to False.\n            **kwargs: A dictionary of keys to Runnable instances or callables that\n                return Runnable instances.\n\n        Returns:\n            A new Runnable with the alternatives configured.\n\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n            from langchain_core.runnables.utils import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            model = ChatAnthropic(\n                model_name=\"claude-3-sonnet-20240229\"\n            ).configurable_alternatives(\n                ConfigurableField(id=\"llm\"),\n                default_key=\"anthropic\",\n                openai=ChatOpenAI()\n            )\n\n            # uses the default model ChatAnthropic\n            print(model.invoke(\"which organization created you?\").content)\n\n            # uses ChatOpenAI\n            print(\n                model.with_config(\n                    configurable={\"llm\": \"openai\"}\n                ).invoke(\"which organization created you?\").content\n            )\n        \"\"\"\n        from langchain_core.runnables.configurable import (\n            RunnableConfigurableAlternatives,\n        )\n\n        return RunnableConfigurableAlternatives(\n            which=which,\n            default=self,\n            alternatives=kwargs,\n            default_key=default_key,\n            prefix_keys=prefix_keys,\n        )\n\n\ndef _seq_input_schema(\n    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]\n) -> type[BaseModel]:\n    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick\n\n    first = steps[0]\n    if len(steps) == 1:\n        return first.get_input_schema(config)\n    elif isinstance(first, RunnableAssign):\n        next_input_schema = _seq_input_schema(steps[1:], config)\n        if not issubclass(next_input_schema, RootModel):\n            # it's a dict as expected\n            return create_model_v2(  # type: ignore[call-overload]\n                \"RunnableSequenceInput\",\n                field_definitions={\n                    k: (v.annotation, v.default)\n                    for k, v in next_input_schema.model_fields.items()\n                    if k not in first.mapper.steps__\n                },\n            )\n    elif isinstance(first, RunnablePick):\n        return _seq_input_schema(steps[1:], config)\n\n    return first.get_input_schema(config)\n\n\ndef _seq_output_schema(\n    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]\n) -> type[BaseModel]:\n    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick\n\n    last = steps[-1]\n    if len(steps) == 1:\n        return last.get_input_schema(config)\n    elif isinstance(last, RunnableAssign):\n        mapper_output_schema = last.mapper.get_output_schema(config)\n        prev_output_schema = _seq_output_schema(steps[:-1], config)\n        if not issubclass(prev_output_schema, RootModel):\n            # it's a dict as expected\n            return create_model_v2(  # type: ignore[call-overload]\n                \"RunnableSequenceOutput\",\n                field_definitions={\n                    **{\n                        k: (v.annotation, v.default)\n                        for k, v in prev_output_schema.model_fields.items()\n                    },\n                    **{\n                        k: (v.annotation, v.default)\n                        for k, v in mapper_output_schema.model_fields.items()\n                    },\n                },\n            )\n    elif isinstance(last, RunnablePick):\n        prev_output_schema = _seq_output_schema(steps[:-1], config)\n        if not issubclass(prev_output_schema, RootModel):\n            # it's a dict as expected\n            if isinstance(last.keys, list):\n                return create_model_v2(  # type: ignore[call-overload]\n                    \"RunnableSequenceOutput\",\n                    field_definitions={\n                        k: (v.annotation, v.default)\n                        for k, v in prev_output_schema.model_fields.items()\n                        if k in last.keys\n                    },\n                )\n            else:\n                field = prev_output_schema.model_fields[last.keys]\n                return create_model_v2(  # type: ignore[call-overload]\n                    \"RunnableSequenceOutput\", root=(field.annotation, field.default)\n                )\n\n    return last.get_output_schema(config)\n\n\nclass RunnableSequence(RunnableSerializable[Input, Output]):\n    \"\"\"Sequence of Runnables, where the output of each is the input of the next.\n\n    **RunnableSequence** is the most important composition operator in LangChain\n    as it is used in virtually every chain.\n\n    A RunnableSequence can be instantiated directly or more commonly by using the `|`\n    operator where either the left or right operands (or both) must be a Runnable.\n\n    Any RunnableSequence automatically supports sync, async, batch.\n\n    The default implementations of `batch` and `abatch` utilize threadpools and\n    asyncio gather and will be faster than naive invocation of invoke or ainvoke\n    for IO bound Runnables.\n\n    Batching is implemented by invoking the batch method on each component of the\n    RunnableSequence in order.\n\n    A RunnableSequence preserves the streaming properties of its components, so if all\n    components of the sequence implement a `transform` method -- which\n    is the method that implements the logic to map a streaming input to a streaming\n    output -- then the sequence will be able to stream input to output!\n\n    If any component of the sequence does not implement transform then the\n    streaming will only begin after this component is run. If there are\n    multiple blocking components, streaming begins after the last one.\n\n    Please note: RunnableLambdas do not support `transform` by default! So if\n        you need to use a RunnableLambdas be careful about where you place them in a\n        RunnableSequence (if you need to use the .stream()/.astream() methods).\n\n        If you need arbitrary logic and need streaming, you can subclass\n        Runnable, and implement `transform` for whatever logic you need.\n\n    Here is a simple example that uses simple functions to illustrate the use of\n    RunnableSequence:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            def mul_two(x: int) -> int:\n                return x * 2\n\n            runnable_1 = RunnableLambda(add_one)\n            runnable_2 = RunnableLambda(mul_two)\n            sequence = runnable_1 | runnable_2\n            # Or equivalently:\n            # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n            sequence.invoke(1)\n            await sequence.ainvoke(1)\n\n            sequence.batch([1, 2, 3])\n            await sequence.abatch([1, 2, 3])\n\n    Here's an example that uses streams JSON output generated by an LLM:\n\n        .. code-block:: python\n\n            from langchain_core.output_parsers.json import SimpleJsonOutputParser\n            from langchain_openai import ChatOpenAI\n\n            prompt = PromptTemplate.from_template(\n                'In JSON format, give me a list of {topic} and their '\n                'corresponding names in French, Spanish and in a '\n                'Cat Language.'\n            )\n\n            model = ChatOpenAI()\n            chain = prompt | model | SimpleJsonOutputParser()\n\n            async for chunk in chain.astream({'topic': 'colors'}):\n                print('-')  # noqa: T201\n                print(chunk, sep='', flush=True)  # noqa: T201\n    \"\"\"\n\n    # The steps are broken into first, middle and last, solely for type checking\n    # purposes. It allows specifying the `Input` on the first type, the `Output` of\n    # the last type.\n    first: Runnable[Input, Any]\n    \"\"\"The first Runnable in the sequence.\"\"\"\n    middle: list[Runnable[Any, Any]] = Field(default_factory=list)\n    \"\"\"The middle Runnables in the sequence.\"\"\"\n    last: Runnable[Any, Output]\n    \"\"\"The last Runnable in the sequence.\"\"\"\n\n    def __init__(\n        self,\n        *steps: RunnableLike,\n        name: Optional[str] = None,\n        first: Optional[Runnable[Any, Any]] = None,\n        middle: Optional[list[Runnable[Any, Any]]] = None,\n        last: Optional[Runnable[Any, Any]] = None,\n    ) -> None:\n        \"\"\"Create a new RunnableSequence.\n\n        Args:\n            steps: The steps to include in the sequence.\n            name: The name of the Runnable. Defaults to None.\n            first: The first Runnable in the sequence. Defaults to None.\n            middle: The middle Runnables in the sequence. Defaults to None.\n            last: The last Runnable in the sequence. Defaults to None.\n\n        Raises:\n            ValueError: If the sequence has less than 2 steps.\n        \"\"\"\n        steps_flat: list[Runnable] = []\n        if not steps and first is not None and last is not None:\n            steps_flat = [first] + (middle or []) + [last]\n        for step in steps:\n            if isinstance(step, RunnableSequence):\n                steps_flat.extend(step.steps)\n            else:\n                steps_flat.append(coerce_to_runnable(step))\n        if len(steps_flat) < 2:\n            msg = f\"RunnableSequence must have at least 2 steps, got {len(steps_flat)}\"\n            raise ValueError(msg)\n        super().__init__(  # type: ignore[call-arg]\n            first=steps_flat[0],\n            middle=list(steps_flat[1:-1]),\n            last=steps_flat[-1],\n            name=name,\n        )\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    @property\n    def steps(self) -> list[Runnable[Any, Any]]:\n        \"\"\"All the Runnables that make up the sequence in order.\n\n        Returns:\n            A list of Runnables.\n        \"\"\"\n        return [self.first] + self.middle + [self.last]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Check if the object is serializable.\n\n        Returns:\n            True if the object is serializable, False otherwise.\n                Defaults to True.\n        \"\"\"\n        return True\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    @override\n    def InputType(self) -> type[Input]:\n        \"\"\"The type of the input to the Runnable.\"\"\"\n        return self.first.InputType\n\n    @property\n    @override\n    def OutputType(self) -> type[Output]:\n        \"\"\"The type of the output of the Runnable.\"\"\"\n        return self.last.OutputType\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the input schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema of the Runnable.\n        \"\"\"\n        return _seq_input_schema(self.steps, config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the output schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The output schema of the Runnable.\n        \"\"\"\n        return _seq_output_schema(self.steps, config)\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"Get the config specs of the Runnable.\n\n        Returns:\n            The config specs of the Runnable.\n        \"\"\"\n        from langchain_core.beta.runnables.context import (\n            CONTEXT_CONFIG_PREFIX,\n            _key_from_id,\n        )\n\n        # get all specs\n        all_specs = [\n            (spec, idx)\n            for idx, step in enumerate(self.steps)\n            for spec in step.config_specs\n        ]\n        # calculate context dependencies\n        specs_by_pos = groupby(\n            [tup for tup in all_specs if tup[0].id.startswith(CONTEXT_CONFIG_PREFIX)],\n            itemgetter(1),\n        )\n        next_deps: set[str] = set()\n        deps_by_pos: dict[int, set[str]] = {}\n        for pos, specs in specs_by_pos:\n            deps_by_pos[pos] = next_deps\n            next_deps = next_deps | {spec[0].id for spec in specs}\n        # assign context dependencies\n        for pos, (spec, idx) in enumerate(all_specs):\n            if spec.id.startswith(CONTEXT_CONFIG_PREFIX):\n                all_specs[pos] = (\n                    ConfigurableFieldSpec(\n                        id=spec.id,\n                        annotation=spec.annotation,\n                        name=spec.name,\n                        default=spec.default,\n                        description=spec.description,\n                        is_shared=spec.is_shared,\n                        dependencies=[\n                            d\n                            for d in deps_by_pos[idx]\n                            if _key_from_id(d) != _key_from_id(spec.id)\n                        ]\n                        + (spec.dependencies or []),\n                    ),\n                    idx,\n                )\n\n        return get_unique_config_specs(spec for spec, _ in all_specs)\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Get the graph representation of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The graph representation of the Runnable.\n\n        Raises:\n            ValueError: If a Runnable has no first or last node.\n        \"\"\"\n        from langchain_core.runnables.graph import Graph\n\n        graph = Graph()\n        for step in self.steps:\n            current_last_node = graph.last_node()\n            step_graph = step.get_graph(config)\n            if step is not self.first:\n                step_graph.trim_first_node()\n            if step is not self.last:\n                step_graph.trim_last_node()\n            step_first_node, _ = graph.extend(step_graph)\n            if not step_first_node:\n                msg = f\"Runnable {step} has no first node\"\n                raise ValueError(msg)\n            if current_last_node:\n                graph.add_edge(current_last_node, step_first_node)\n\n        return graph\n\n    def __repr__(self) -> str:\n        return \"\\n| \".join(\n            repr(s) if i == 0 else indent_lines_after_first(repr(s), \"| \")\n            for i, s in enumerate(self.steps)\n        )\n\n    def __or__(\n        self,\n        other: Union[\n            Runnable[Any, Other],\n            Callable[[Any], Other],\n            Callable[[Iterator[Any]], Iterator[Other]],\n            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],\n        ],\n    ) -> RunnableSerializable[Input, Other]:\n        if isinstance(other, RunnableSequence):\n            return RunnableSequence(\n                self.first,\n                *self.middle,\n                self.last,\n                other.first,\n                *other.middle,\n                other.last,\n                name=self.name or other.name,\n            )\n        else:\n            return RunnableSequence(\n                self.first,\n                *self.middle,\n                self.last,\n                coerce_to_runnable(other),\n                name=self.name,\n            )\n\n    def __ror__(\n        self,\n        other: Union[\n            Runnable[Other, Any],\n            Callable[[Other], Any],\n            Callable[[Iterator[Other]], Iterator[Any]],\n            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],\n        ],\n    ) -> RunnableSerializable[Other, Output]:\n        if isinstance(other, RunnableSequence):\n            return RunnableSequence(\n                other.first,\n                *other.middle,\n                other.last,\n                self.first,\n                *self.middle,\n                self.last,\n                name=other.name or self.name,\n            )\n        else:\n            return RunnableSequence(\n                coerce_to_runnable(other),\n                self.first,\n                *self.middle,\n                self.last,\n                name=self.name,\n            )\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        from langchain_core.beta.runnables.context import config_with_context\n\n        # setup callbacks and context\n        config = config_with_context(ensure_config(config), self.steps)\n        callback_manager = get_callback_manager_for_config(config)\n        # start the root run\n        run_manager = callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        # invoke all steps in sequence\n        try:\n            for i, step in enumerate(self.steps):\n                # mark each step as a child run\n                config = patch_config(\n                    config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n                )\n                context = copy_context()\n                context.run(_set_config_context, config)\n                if i == 0:\n                    input = context.run(step.invoke, input, config, **kwargs)\n                else:\n                    input = context.run(step.invoke, input, config)\n        # finish the root run\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(input)\n            return cast(Output, input)\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n\n        # setup callbacks and context\n        config = aconfig_with_context(ensure_config(config), self.steps)\n        callback_manager = get_async_callback_manager_for_config(config)\n        # start the root run\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        # invoke all steps in sequence\n        try:\n            for i, step in enumerate(self.steps):\n                # mark each step as a child run\n                config = patch_config(\n                    config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n                )\n                context = copy_context()\n                context.run(_set_config_context, config)\n                if i == 0:\n                    part = functools.partial(step.ainvoke, input, config, **kwargs)\n                else:\n                    part = functools.partial(step.ainvoke, input, config)\n                if asyncio_accepts_context():\n                    input = await asyncio.create_task(part(), context=context)  # type: ignore\n                else:\n                    input = await asyncio.create_task(part())\n        # finish the root run\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(input)\n            return cast(Output, input)\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        from langchain_core.beta.runnables.context import config_with_context\n        from langchain_core.callbacks.manager import CallbackManager\n\n        if not inputs:\n            return []\n\n        # setup callbacks and context\n        configs = [\n            config_with_context(c, self.steps)\n            for c in get_config_list(config, len(inputs))\n        ]\n        callback_managers = [\n            CallbackManager.configure(\n                inheritable_callbacks=config.get(\"callbacks\"),\n                local_callbacks=None,\n                verbose=False,\n                inheritable_tags=config.get(\"tags\"),\n                local_tags=None,\n                inheritable_metadata=config.get(\"metadata\"),\n                local_metadata=None,\n            )\n            for config in configs\n        ]\n        # start the root runs, one per input\n        run_managers = [\n            cm.on_chain_start(\n                None,\n                input,\n                name=config.get(\"run_name\") or self.get_name(),\n                run_id=config.pop(\"run_id\", None),\n            )\n            for cm, input, config in zip(callback_managers, inputs, configs)\n        ]\n\n        # invoke\n        try:\n            if return_exceptions:\n                # Track which inputs (by index) failed so far\n                # If an input has failed it will be present in this map,\n                # and the value will be the exception that was raised.\n                failed_inputs_map: dict[int, Exception] = {}\n                for stepidx, step in enumerate(self.steps):\n                    # Assemble the original indexes of the remaining inputs\n                    # (i.e. the ones that haven't failed yet)\n                    remaining_idxs = [\n                        i for i in range(len(configs)) if i not in failed_inputs_map\n                    ]\n                    # Invoke the step on the remaining inputs\n                    inputs = step.batch(\n                        [\n                            inp\n                            for i, inp in zip(remaining_idxs, inputs)\n                            if i not in failed_inputs_map\n                        ],\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config,\n                                callbacks=rm.get_child(f\"seq:step:{stepidx + 1}\"),\n                            )\n                            for i, (rm, config) in enumerate(zip(run_managers, configs))\n                            if i not in failed_inputs_map\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if stepidx == 0 else {}),\n                    )\n                    # If an input failed, add it to the map\n                    for i, inp in zip(remaining_idxs, inputs):\n                        if isinstance(inp, Exception):\n                            failed_inputs_map[i] = inp\n                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]\n                    # If all inputs have failed, stop processing\n                    if len(failed_inputs_map) == len(configs):\n                        break\n\n                # Reassemble the outputs, inserting Exceptions for failed inputs\n                inputs_copy = inputs.copy()\n                inputs = []\n                for i in range(len(configs)):\n                    if i in failed_inputs_map:\n                        inputs.append(cast(Input, failed_inputs_map[i]))\n                    else:\n                        inputs.append(inputs_copy.pop(0))\n            else:\n                for i, step in enumerate(self.steps):\n                    inputs = step.batch(\n                        inputs,\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config, callbacks=rm.get_child(f\"seq:step:{i + 1}\")\n                            )\n                            for rm, config in zip(run_managers, configs)\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if i == 0 else {}),\n                    )\n\n        # finish the root runs\n        except BaseException as e:\n            for rm in run_managers:\n                rm.on_chain_error(e)\n            if return_exceptions:\n                return cast(list[Output], [e for _ in inputs])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            for run_manager, out in zip(run_managers, inputs):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    run_manager.on_chain_error(out)\n                else:\n                    run_manager.on_chain_end(out)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], inputs)\n            else:\n                raise first_exception\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n        from langchain_core.callbacks.manager import AsyncCallbackManager\n\n        if not inputs:\n            return []\n\n        # setup callbacks and context\n        configs = [\n            aconfig_with_context(c, self.steps)\n            for c in get_config_list(config, len(inputs))\n        ]\n        callback_managers = [\n            AsyncCallbackManager.configure(\n                inheritable_callbacks=config.get(\"callbacks\"),\n                local_callbacks=None,\n                verbose=False,\n                inheritable_tags=config.get(\"tags\"),\n                local_tags=None,\n                inheritable_metadata=config.get(\"metadata\"),\n                local_metadata=None,\n            )\n            for config in configs\n        ]\n        # start the root runs, one per input\n        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(\n            *(\n                cm.on_chain_start(\n                    None,\n                    input,\n                    name=config.get(\"run_name\") or self.get_name(),\n                    run_id=config.pop(\"run_id\", None),\n                )\n                for cm, input, config in zip(callback_managers, inputs, configs)\n            )\n        )\n\n        # invoke .batch() on each step\n        # this uses batching optimizations in Runnable subclasses, like LLM\n        try:\n            if return_exceptions:\n                # Track which inputs (by index) failed so far\n                # If an input has failed it will be present in this map,\n                # and the value will be the exception that was raised.\n                failed_inputs_map: dict[int, Exception] = {}\n                for stepidx, step in enumerate(self.steps):\n                    # Assemble the original indexes of the remaining inputs\n                    # (i.e. the ones that haven't failed yet)\n                    remaining_idxs = [\n                        i for i in range(len(configs)) if i not in failed_inputs_map\n                    ]\n                    # Invoke the step on the remaining inputs\n                    inputs = await step.abatch(\n                        [\n                            inp\n                            for i, inp in zip(remaining_idxs, inputs)\n                            if i not in failed_inputs_map\n                        ],\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config,\n                                callbacks=rm.get_child(f\"seq:step:{stepidx + 1}\"),\n                            )\n                            for i, (rm, config) in enumerate(zip(run_managers, configs))\n                            if i not in failed_inputs_map\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if stepidx == 0 else {}),\n                    )\n                    # If an input failed, add it to the map\n                    for i, inp in zip(remaining_idxs, inputs):\n                        if isinstance(inp, Exception):\n                            failed_inputs_map[i] = inp\n                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]\n                    # If all inputs have failed, stop processing\n                    if len(failed_inputs_map) == len(configs):\n                        break\n\n                # Reassemble the outputs, inserting Exceptions for failed inputs\n                inputs_copy = inputs.copy()\n                inputs = []\n                for i in range(len(configs)):\n                    if i in failed_inputs_map:\n                        inputs.append(cast(Input, failed_inputs_map[i]))\n                    else:\n                        inputs.append(inputs_copy.pop(0))\n            else:\n                for i, step in enumerate(self.steps):\n                    inputs = await step.abatch(\n                        inputs,\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config, callbacks=rm.get_child(f\"seq:step:{i + 1}\")\n                            )\n                            for rm, config in zip(run_managers, configs)\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if i == 0 else {}),\n                    )\n        # finish the root runs\n        except BaseException as e:\n            await asyncio.gather(*(rm.on_chain_error(e) for rm in run_managers))\n            if return_exceptions:\n                return cast(list[Output], [e for _ in inputs])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            coros: list[Awaitable[None]] = []\n            for run_manager, out in zip(run_managers, inputs):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    coros.append(run_manager.on_chain_error(out))\n                else:\n                    coros.append(run_manager.on_chain_end(out))\n            await asyncio.gather(*coros)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], inputs)\n            else:\n                raise first_exception\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        from langchain_core.beta.runnables.context import config_with_context\n\n        steps = [self.first] + self.middle + [self.last]\n        config = config_with_context(config, self.steps)\n\n        # transform the input stream of each step with the next\n        # steps that don't natively support transforming an input stream will\n        # buffer input in memory until all available, and then start emitting output\n        final_pipeline = cast(Iterator[Output], input)\n        for idx, step in enumerate(steps):\n            config = patch_config(\n                config, callbacks=run_manager.get_child(f\"seq:step:{idx + 1}\")\n            )\n            if idx == 0:\n                final_pipeline = step.transform(final_pipeline, config, **kwargs)\n            else:\n                final_pipeline = step.transform(final_pipeline, config)\n\n        yield from final_pipeline\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n\n        steps = [self.first] + self.middle + [self.last]\n        config = aconfig_with_context(config, self.steps)\n\n        # stream the last steps\n        # transform the input stream of each step with the next\n        # steps that don't natively support transforming an input stream will\n        # buffer input in memory until all available, and then start emitting output\n        final_pipeline = cast(AsyncIterator[Output], input)\n        for idx, step in enumerate(steps):\n            config = patch_config(\n                config,\n                callbacks=run_manager.get_child(f\"seq:step:{idx + 1}\"),\n            )\n            if idx == 0:\n                final_pipeline = step.atransform(final_pipeline, config, **kwargs)\n            else:\n                final_pipeline = step.atransform(final_pipeline, config)\n        async for output in final_pipeline:\n            yield output\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self._transform_stream_with_config(\n            input,\n            self._transform,\n            patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),\n            **kwargs,\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self.transform(iter([input]), config, **kwargs)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for chunk in self._atransform_stream_with_config(\n            input,\n            self._atransform,\n            patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),\n            **kwargs,\n        ):\n            yield chunk\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\nclass RunnableParallel(RunnableSerializable[Input, dict[str, Any]]):\n    \"\"\"Runnable that runs a mapping of Runnables in parallel, and returns a mapping\n    of their outputs.\n\n    RunnableParallel is one of the two main composition primitives for the LCEL,\n    alongside RunnableSequence. It invokes Runnables concurrently, providing the same\n    input to each.\n\n    A RunnableParallel can be instantiated directly or by using a dict literal within a\n    sequence.\n\n    Here is a simple example that uses functions to illustrate the use of\n    RunnableParallel:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            def mul_two(x: int) -> int:\n                return x * 2\n\n            def mul_three(x: int) -> int:\n                return x * 3\n\n            runnable_1 = RunnableLambda(add_one)\n            runnable_2 = RunnableLambda(mul_two)\n            runnable_3 = RunnableLambda(mul_three)\n\n            sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n                \"mul_two\": runnable_2,\n                \"mul_three\": runnable_3,\n            }\n            # Or equivalently:\n            # sequence = runnable_1 | RunnableParallel(\n            #     {\"mul_two\": runnable_2, \"mul_three\": runnable_3}\n            # )\n            # Also equivalently:\n            # sequence = runnable_1 | RunnableParallel(\n            #     mul_two=runnable_2,\n            #     mul_three=runnable_3,\n            # )\n\n            sequence.invoke(1)\n            await sequence.ainvoke(1)\n\n            sequence.batch([1, 2, 3])\n            await sequence.abatch([1, 2, 3])\n\n    RunnableParallel makes it easy to run Runnables in parallel. In the below example,\n    we simultaneously stream output from two different Runnables:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.runnables import RunnableParallel\n            from langchain_openai import ChatOpenAI\n\n            model = ChatOpenAI()\n            joke_chain = (\n                ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n                | model\n            )\n            poem_chain = (\n                ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n                | model\n            )\n\n            runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)\n\n            # Display stream\n            output = {key: \"\" for key, _ in runnable.output_schema()}\n            for chunk in runnable.stream({\"topic\": \"bear\"}):\n                for key in chunk:\n                    output[key] = output[key] + chunk[key].content\n                print(output)  # noqa: T201\n    \"\"\"\n\n    steps__: Mapping[str, Runnable[Input, Any]]\n\n    def __init__(\n        self,\n        steps__: Optional[\n            Mapping[\n                str,\n                Union[\n                    Runnable[Input, Any],\n                    Callable[[Input], Any],\n                    Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],\n                ],\n            ]\n        ] = None,\n        **kwargs: Union[\n            Runnable[Input, Any],\n            Callable[[Input], Any],\n            Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],\n        ],\n    ) -> None:\n        merged = {**steps__} if steps__ is not None else {}\n        merged.update(kwargs)\n        super().__init__(  # type: ignore[call-arg]\n            steps__={key: coerce_to_runnable(r) for key, r in merged.items()}\n        )\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        \"\"\"Get the name of the Runnable.\n\n        Args:\n            suffix: The suffix to use. Defaults to None.\n            name: The name to use. Defaults to None.\n\n        Returns:\n            The name of the Runnable.\n        \"\"\"\n        name = name or self.name or f\"RunnableParallel<{','.join(self.steps__.keys())}>\"\n        return super().get_name(suffix, name=name)\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        \"\"\"The type of the input to the Runnable.\"\"\"\n        for step in self.steps__.values():\n            if step.InputType:\n                return step.InputType\n\n        return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the input schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema of the Runnable.\n        \"\"\"\n        if all(\n            s.get_input_schema(config).model_json_schema().get(\"type\", \"object\")\n            == \"object\"\n            for s in self.steps__.values()\n        ):\n            # This is correct, but pydantic typings/mypy don't think so.\n            return create_model_v2(  # type: ignore[call-overload]\n                self.get_name(\"Input\"),\n                field_definitions={\n                    k: (v.annotation, v.default)\n                    for step in self.steps__.values()\n                    for k, v in step.get_input_schema(config).model_fields.items()\n                    if k != \"__root__\"\n                },\n            )\n\n        return super().get_input_schema(config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the output schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The output schema of the Runnable.\n        \"\"\"\n        fields = {k: (v.OutputType, ...) for k, v in self.steps__.items()}\n        return create_model_v2(self.get_name(\"Output\"), field_definitions=fields)\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"Get the config specs of the Runnable.\n\n        Returns:\n            The config specs of the Runnable.\n        \"\"\"\n        return get_unique_config_specs(\n            spec for step in self.steps__.values() for spec in step.config_specs\n        )\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Get the graph representation of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The graph representation of the Runnable.\n\n        Raises:\n            ValueError: If a Runnable has no first or last node.\n        \"\"\"\n        from langchain_core.runnables.graph import Graph\n\n        graph = Graph()\n        input_node = graph.add_node(self.get_input_schema(config))\n        output_node = graph.add_node(self.get_output_schema(config))\n        for step in self.steps__.values():\n            step_graph = step.get_graph()\n            step_graph.trim_first_node()\n            step_graph.trim_last_node()\n            if not step_graph:\n                graph.add_edge(input_node, output_node)\n            else:\n                step_first_node, step_last_node = graph.extend(step_graph)\n                if not step_first_node:\n                    msg = f\"Runnable {step} has no first node\"\n                    raise ValueError(msg)\n                if not step_last_node:\n                    msg = f\"Runnable {step} has no last node\"\n                    raise ValueError(msg)\n                graph.add_edge(input_node, step_first_node)\n                graph.add_edge(step_last_node, output_node)\n\n        return graph\n\n    def __repr__(self) -> str:\n        map_for_repr = \",\\n  \".join(\n            f\"{k}: {indent_lines_after_first(repr(v), '  ' + k + ': ')}\"\n            for k, v in self.steps__.items()\n        )\n        return \"{\\n  \" + map_for_repr + \"\\n}\"\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> dict[str, Any]:\n        from langchain_core.callbacks.manager import CallbackManager\n\n        # setup callbacks\n        config = ensure_config(config)\n        callback_manager = CallbackManager.configure(\n            inheritable_callbacks=config.get(\"callbacks\"),\n            local_callbacks=None,\n            verbose=False,\n            inheritable_tags=config.get(\"tags\"),\n            local_tags=None,\n            inheritable_metadata=config.get(\"metadata\"),\n            local_metadata=None,\n        )\n        # start the root run\n        run_manager = callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        def _invoke_step(\n            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n                # mark each step as a child run\n                callbacks=run_manager.get_child(f\"map:key:{key}\"),\n            )\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            return context.run(\n                step.invoke,\n                input,\n                child_config,\n            )\n\n        # gather results from all steps\n        try:\n            # copy to avoid issues from the caller mutating the steps during invoke()\n            steps = dict(self.steps__)\n\n            with get_executor_for_config(config) as executor:\n                futures = [\n                    executor.submit(_invoke_step, step, input, config, key)\n                    for key, step in steps.items()\n                ]\n                output = {key: future.result() for key, future in zip(steps, futures)}\n        # finish the root run\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(output)\n            return output\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> dict[str, Any]:\n        # setup callbacks\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        # start the root run\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        async def _ainvoke_step(\n            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n                callbacks=run_manager.get_child(f\"map:key:{key}\"),\n            )\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            if asyncio_accepts_context():\n                return await asyncio.create_task(  # type: ignore\n                    step.ainvoke(input, child_config), context=context\n                )\n            else:\n                return await asyncio.create_task(step.ainvoke(input, child_config))\n\n        # gather results from all steps\n        try:\n            # copy to avoid issues from the caller mutating the steps during invoke()\n            steps = dict(self.steps__)\n            results = await asyncio.gather(\n                *(\n                    _ainvoke_step(\n                        step,\n                        input,\n                        # mark each step as a child run\n                        config,\n                        key,\n                    )\n                    for key, step in steps.items()\n                )\n            )\n            output = dict(zip(steps, results))\n        # finish the root run\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(output)\n            return output\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n    ) -> Iterator[AddableDict]:\n        # Shallow copy steps to ignore mutations while in progress\n        steps = dict(self.steps__)\n        # Each step gets a copy of the input iterator,\n        # which is consumed in parallel in a separate thread.\n        input_copies = list(safetee(input, len(steps), lock=threading.Lock()))\n        with get_executor_for_config(config) as executor:\n            # Create the transform() generator for each step\n            named_generators = [\n                (\n                    name,\n                    step.transform(\n                        input_copies.pop(),\n                        patch_config(\n                            config, callbacks=run_manager.get_child(f\"map:key:{name}\")\n                        ),\n                    ),\n                )\n                for name, step in steps.items()\n            ]\n            # Start the first iteration of each generator\n            futures = {\n                executor.submit(next, generator): (step_name, generator)\n                for step_name, generator in named_generators\n            }\n            # Yield chunks from each as they become available,\n            # and start the next iteration of that generator that yielded it.\n            # When all generators are exhausted, stop.\n            while futures:\n                completed_futures, _ = wait(futures, return_when=FIRST_COMPLETED)\n                for future in completed_futures:\n                    (step_name, generator) = futures.pop(future)\n                    try:\n                        chunk = AddableDict({step_name: future.result()})\n                        yield chunk\n                        futures[executor.submit(next, generator)] = (\n                            step_name,\n                            generator,\n                        )\n                    except StopIteration:\n                        pass\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[dict[str, Any]]:\n        yield from self._transform_stream_with_config(\n            input, self._transform, config, **kwargs\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[dict[str, Any]]:\n        yield from self.transform(iter([input]), config)\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n    ) -> AsyncIterator[AddableDict]:\n        # Shallow copy steps to ignore mutations while in progress\n        steps = dict(self.steps__)\n        # Each step gets a copy of the input iterator,\n        # which is consumed in parallel in a separate thread.\n        input_copies = list(atee(input, len(steps), lock=asyncio.Lock()))\n        # Create the transform() generator for each step\n        named_generators = [\n            (\n                name,\n                step.atransform(\n                    input_copies.pop(),\n                    patch_config(\n                        config, callbacks=run_manager.get_child(f\"map:key:{name}\")\n                    ),\n                ),\n            )\n            for name, step in steps.items()\n        ]\n\n        # Wrap in a coroutine to satisfy linter\n        async def get_next_chunk(generator: AsyncIterator) -> Optional[Output]:\n            return await py_anext(generator)\n\n        # Start the first iteration of each generator\n        tasks = {\n            asyncio.create_task(get_next_chunk(generator)): (step_name, generator)\n            for step_name, generator in named_generators\n        }\n        # Yield chunks from each as they become available,\n        # and start the next iteration of the generator that yielded it.\n        # When all generators are exhausted, stop.\n        while tasks:\n            completed_tasks, _ = await asyncio.wait(\n                tasks, return_when=asyncio.FIRST_COMPLETED\n            )\n            for task in completed_tasks:\n                (step_name, generator) = tasks.pop(task)\n                try:\n                    chunk = AddableDict({step_name: task.result()})\n                    yield chunk\n                    new_task = asyncio.create_task(get_next_chunk(generator))\n                    tasks[new_task] = (step_name, generator)\n                except StopAsyncIteration:\n                    pass\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        async for chunk in self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        ):\n            yield chunk\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[dict[str, Any]]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config):\n            yield chunk\n\n\n# We support both names\nRunnableMap = RunnableParallel\n\n\nclass RunnableGenerator(Runnable[Input, Output]):\n    \"\"\"Runnable that runs a generator function.\n\n    RunnableGenerators can be instantiated directly or by using a generator within\n    a sequence.\n\n    RunnableGenerators can be used to implement custom behavior, such as custom output\n    parsers, while preserving streaming capabilities. Given a generator function with\n    a signature Iterator[A] -> Iterator[B], wrapping it in a RunnableGenerator allows\n    it to emit output chunks as soon as they are streamed in from the previous step.\n\n    Note that if a generator function has a signature A -> Iterator[B], such that it\n    requires its input from the previous step to be completed before emitting chunks\n    (e.g., most LLMs need the entire prompt available to start generating), it can\n    instead be wrapped in a RunnableLambda.\n\n    Here is an example to show the basic mechanics of a RunnableGenerator:\n\n        .. code-block:: python\n\n            from typing import Any, AsyncIterator, Iterator\n\n            from langchain_core.runnables import RunnableGenerator\n\n\n            def gen(input: Iterator[Any]) -> Iterator[str]:\n                for token in [\"Have\", \" a\", \" nice\", \" day\"]:\n                    yield token\n\n\n            runnable = RunnableGenerator(gen)\n            runnable.invoke(None)  # \"Have a nice day\"\n            list(runnable.stream(None))  # [\"Have\", \" a\", \" nice\", \" day\"]\n            runnable.batch([None, None])  # [\"Have a nice day\", \"Have a nice day\"]\n\n\n            # Async version:\n            async def agen(input: AsyncIterator[Any]) -> AsyncIterator[str]:\n                for token in [\"Have\", \" a\", \" nice\", \" day\"]:\n                    yield token\n\n            runnable = RunnableGenerator(agen)\n            await runnable.ainvoke(None)  # \"Have a nice day\"\n            [p async for p in runnable.astream(None)] # [\"Have\", \" a\", \" nice\", \" day\"]\n\n    RunnableGenerator makes it easy to implement custom behavior within a streaming\n    context. Below we show an example:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.runnables import RunnableGenerator, RunnableLambda\n            from langchain_openai import ChatOpenAI\n            from langchain_core.output_parsers import StrOutputParser\n\n\n            model = ChatOpenAI()\n            chant_chain = (\n                ChatPromptTemplate.from_template(\"Give me a 3 word chant about {topic}\")\n                | model\n                | StrOutputParser()\n            )\n\n            def character_generator(input: Iterator[str]) -> Iterator[str]:\n                for token in input:\n                    if \",\" in token or \".\" in token:\n                        yield \"\ud83d\udc4f\" + token\n                    else:\n                        yield token\n\n\n            runnable = chant_chain | character_generator\n            assert type(runnable.last) is RunnableGenerator\n            \"\".join(runnable.stream({\"topic\": \"waste\"})) # Reduce\ud83d\udc4f, Reuse\ud83d\udc4f, Recycle\ud83d\udc4f.\n\n            # Note that RunnableLambda can be used to delay streaming of one step in a\n            # sequence until the previous step is finished:\n            def reverse_generator(input: str) -> Iterator[str]:\n                # Yield characters of input in reverse order.\n                for character in input[::-1]:\n                    yield character\n\n            runnable = chant_chain | RunnableLambda(reverse_generator)\n            \"\".join(runnable.stream({\"topic\": \"waste\"}))  # \".elcycer ,esuer ,ecudeR\"\n    \"\"\"\n\n    def __init__(\n        self,\n        transform: Union[\n            Callable[[Iterator[Input]], Iterator[Output]],\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n        ],\n        atransform: Optional[\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]]\n        ] = None,\n        *,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Initialize a RunnableGenerator.\n\n        Args:\n            transform: The transform function.\n            atransform: The async transform function. Defaults to None.\n\n        Raises:\n            TypeError: If the transform is not a generator function.\n        \"\"\"\n        if atransform is not None:\n            self._atransform = atransform\n            func_for_name: Callable = atransform\n\n        if is_async_generator(transform):\n            self._atransform = transform  # type: ignore[assignment]\n            func_for_name = transform\n        elif inspect.isgeneratorfunction(transform):\n            self._transform = transform\n            func_for_name = transform\n        else:\n            msg = (\n                \"Expected a generator function type for `transform`.\"\n                f\"Instead got an unsupported type: {type(transform)}\"\n            )\n            raise TypeError(msg)\n\n        try:\n            self.name = name or func_for_name.__name__\n        except AttributeError:\n            self.name = \"RunnableGenerator\"\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        func = getattr(self, \"_transform\", None) or self._atransform\n        try:\n            params = inspect.signature(func).parameters\n            first_param = next(iter(params.values()), None)\n            if first_param and first_param.annotation != inspect.Parameter.empty:\n                return getattr(first_param.annotation, \"__args__\", (Any,))[0]\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable generator, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.InputType\n\n        func = getattr(self, \"_transform\", None) or self._atransform\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        func = getattr(self, \"_transform\", None) or self._atransform\n        try:\n            sig = inspect.signature(func)\n            return (\n                getattr(sig.return_annotation, \"__args__\", (Any,))[0]\n                if sig.return_annotation != inspect.Signature.empty\n                else Any\n            )\n        except ValueError:\n            return Any\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable generator, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.OutputType\n        func = getattr(self, \"_transform\", None) or self._atransform\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, RunnableGenerator):\n            if hasattr(self, \"_transform\") and hasattr(other, \"_transform\"):\n                return self._transform == other._transform\n            elif hasattr(self, \"_atransform\") and hasattr(other, \"_atransform\"):\n                return self._atransform == other._atransform\n            else:\n                return False\n        else:\n            return False\n\n    def __repr__(self) -> str:\n        return f\"RunnableGenerator({self.name})\"\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        if not hasattr(self, \"_transform\"):\n            msg = f\"{repr(self)} only supports async methods.\"\n            raise NotImplementedError(msg)\n        return self._transform_stream_with_config(\n            input,\n            self._transform,  # type: ignore[arg-type]\n            config,\n            **kwargs,  # type: ignore[arg-type]\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        final: Optional[Output] = None\n        for output in self.stream(input, config, **kwargs):\n            final = output if final is None else final + output  # type: ignore[operator]\n        return cast(Output, final)\n\n    def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        if not hasattr(self, \"_atransform\"):\n            msg = f\"{repr(self)} only supports sync methods.\"\n            raise NotImplementedError(msg)\n\n        return self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        )\n\n    def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        return self.atransform(input_aiter(), config, **kwargs)\n\n    async def ainvoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        final: Optional[Output] = None\n        async for output in self.astream(input, config, **kwargs):\n            final = output if final is None else final + output  # type: ignore[operator]\n        return cast(Output, final)\n\n\nclass RunnableLambda(Runnable[Input, Output]):\n    \"\"\"RunnableLambda converts a python callable into a Runnable.\n\n    Wrapping a callable in a RunnableLambda makes the callable usable\n    within either a sync or async context.\n\n    RunnableLambda can be composed as any other Runnable and provides\n    seamless integration with LangChain tracing.\n\n    ``RunnableLambda`` is best suited for code that does not need to support\n    streaming. If you need to support streaming (i.e., be able to operate\n    on chunks of inputs and yield chunks of outputs), use ``RunnableGenerator``\n    instead.\n\n    Note that if a ``RunnableLambda`` returns an instance of ``Runnable``, that\n    instance is invoked (or streamed) during execution.\n\n    Examples:\n\n        .. code-block:: python\n\n            # This is a RunnableLambda\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            runnable = RunnableLambda(add_one)\n\n            runnable.invoke(1) # returns 2\n            runnable.batch([1, 2, 3]) # returns [2, 3, 4]\n\n            # Async is supported by default by delegating to the sync implementation\n            await runnable.ainvoke(1) # returns 2\n            await runnable.abatch([1, 2, 3]) # returns [2, 3, 4]\n\n\n            # Alternatively, can provide both synd and sync implementations\n            async def add_one_async(x: int) -> int:\n                return x + 1\n\n            runnable = RunnableLambda(add_one, afunc=add_one_async)\n            runnable.invoke(1) # Uses add_one\n            await runnable.ainvoke(1) # Uses add_one_async\n    \"\"\"\n\n    def __init__(\n        self,\n        func: Union[\n            Union[\n                Callable[[Input], Output],\n                Callable[[Input], Iterator[Output]],\n                Callable[[Input, RunnableConfig], Output],\n                Callable[[Input, CallbackManagerForChainRun], Output],\n                Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\n            ],\n            Union[\n                Callable[[Input], Awaitable[Output]],\n                Callable[[Input], AsyncIterator[Output]],\n                Callable[[Input, RunnableConfig], Awaitable[Output]],\n                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n                Callable[\n                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                    Awaitable[Output],\n                ],\n            ],\n        ],\n        afunc: Optional[\n            Union[\n                Callable[[Input], Awaitable[Output]],\n                Callable[[Input], AsyncIterator[Output]],\n                Callable[[Input, RunnableConfig], Awaitable[Output]],\n                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n                Callable[\n                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                    Awaitable[Output],\n                ],\n            ]\n        ] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Create a RunnableLambda from a callable, and async callable or both.\n\n        Accepts both sync and async variants to allow providing efficient\n        implementations for sync and async execution.\n\n        Args:\n            func: Either sync or async callable\n            afunc: An async callable that takes an input and returns an output.\n                Defaults to None.\n            name: The name of the Runnable. Defaults to None.\n\n        Raises:\n            TypeError: If the func is not a callable type.\n            TypeError: If both func and afunc are provided.\n        \"\"\"\n        if afunc is not None:\n            self.afunc = afunc\n            func_for_name: Callable = afunc\n\n        if is_async_callable(func) or is_async_generator(func):\n            if afunc is not None:\n                msg = (\n                    \"Func was provided as a coroutine function, but afunc was \"\n                    \"also provided. If providing both, func should be a regular \"\n                    \"function to avoid ambiguity.\"\n                )\n                raise TypeError(msg)\n            self.afunc = func\n            func_for_name = func\n        elif callable(func):\n            self.func = cast(Callable[[Input], Output], func)\n            func_for_name = func\n        else:\n            msg = (\n                \"Expected a callable type for `func`.\"\n                f\"Instead got an unsupported type: {type(func)}\"\n            )\n            raise TypeError(msg)\n\n        try:\n            if name is not None:\n                self.name = name\n            elif func_for_name.__name__ != \"<lambda>\":\n                self.name = func_for_name.__name__\n        except AttributeError:\n            pass\n\n        self._repr: Optional[str] = None\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        \"\"\"The type of the input to this Runnable.\"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n        try:\n            params = inspect.signature(func).parameters\n            first_param = next(iter(params.values()), None)\n            if first_param and first_param.annotation != inspect.Parameter.empty:\n                return first_param.annotation\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"The pydantic schema for the input to this Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema for this Runnable.\n        \"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n\n        if isinstance(func, itemgetter):\n            # This is terrible, but afaict it's not possible to access _items\n            # on itemgetter objects, so we have to parse the repr\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\n            if all(\n                item[0] == \"'\" and item[-1] == \"'\" and len(item) > 2 for item in items\n            ):\n                fields = {item[1:-1]: (Any, ...) for item in items}\n                # It's a dict, lol\n                return create_model_v2(self.get_name(\"Input\"), field_definitions=fields)\n            else:\n                module = getattr(func, \"__module__\", None)\n                return create_model_v2(\n                    self.get_name(\"Input\"),\n                    root=list[Any],\n                    # To create the schema, we need to provide the module\n                    # where the underlying function is defined.\n                    # This allows pydantic to resolve type annotations appropriately.\n                    module_name=module,\n                )\n\n        if self.InputType != Any:\n            return super().get_input_schema(config)\n\n        if dict_keys := get_function_first_arg_dict_keys(func):\n            return create_model_v2(\n                self.get_name(\"Input\"),\n                field_definitions=dict.fromkeys(dict_keys, (Any, ...)),\n            )\n\n        return super().get_input_schema(config)\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        \"\"\"The type of the output of this Runnable as a type annotation.\n\n        Returns:\n            The type of the output of this Runnable.\n        \"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n        try:\n            sig = inspect.signature(func)\n            if sig.return_annotation != inspect.Signature.empty:\n                # unwrap iterator types\n                if getattr(sig.return_annotation, \"__origin__\", None) in (\n                    collections.abc.Iterator,\n                    collections.abc.AsyncIterator,\n                ):\n                    return getattr(sig.return_annotation, \"__args__\", (Any,))[0]\n                return sig.return_annotation\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable lambda, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.OutputType\n        func = getattr(self, \"func\", None) or self.afunc\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    @functools.cached_property\n    def deps(self) -> list[Runnable]:\n        \"\"\"The dependencies of this Runnable.\n\n        Returns:\n            The dependencies of this Runnable. If the function has nonlocal\n            variables that are Runnables, they are considered dependencies.\n        \"\"\"\n        if hasattr(self, \"func\"):\n            objects = get_function_nonlocals(self.func)\n        elif hasattr(self, \"afunc\"):\n            objects = get_function_nonlocals(self.afunc)\n        else:\n            objects = []\n\n        deps: list[Runnable] = []\n        for obj in objects:\n            if isinstance(obj, Runnable):\n                deps.append(obj)\n            elif isinstance(getattr(obj, \"__self__\", None), Runnable):\n                deps.append(obj.__self__)\n        return deps\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return get_unique_config_specs(\n            spec for dep in self.deps for spec in dep.config_specs\n        )\n\n    def get_graph(self, config: RunnableConfig | None = None) -> Graph:\n        if deps := self.deps:\n            graph = Graph()\n            input_node = graph.add_node(self.get_input_schema(config))\n            output_node = graph.add_node(self.get_output_schema(config))\n            for dep in deps:\n                dep_graph = dep.get_graph()\n                dep_graph.trim_first_node()\n                dep_graph.trim_last_node()\n                if not dep_graph:\n                    graph.add_edge(input_node, output_node)\n                else:\n                    dep_first_node, dep_last_node = graph.extend(dep_graph)\n                    if not dep_first_node:\n                        msg = f\"Runnable {dep} has no first node\"\n                        raise ValueError(msg)\n                    if not dep_last_node:\n                        msg = f\"Runnable {dep} has no last node\"\n                        raise ValueError(msg)\n                    graph.add_edge(input_node, dep_first_node)\n                    graph.add_edge(dep_last_node, output_node)\n        else:\n            graph = super().get_graph(config)\n\n        return graph\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, RunnableLambda):\n            if hasattr(self, \"func\") and hasattr(other, \"func\"):\n                return self.func == other.func\n            elif hasattr(self, \"afunc\") and hasattr(other, \"afunc\"):\n                return self.afunc == other.afunc\n            else:\n                return False\n        else:\n            return False\n\n    def __repr__(self) -> str:\n        \"\"\"A string representation of this Runnable.\"\"\"\n        if self._repr is None:\n            if hasattr(self, \"func\") and isinstance(self.func, itemgetter):\n                self._repr = f\"RunnableLambda({str(self.func)[len('operator.') :]})\"\n            elif hasattr(self, \"func\"):\n                self._repr = f\"RunnableLambda({get_lambda_source(self.func) or '...'})\"\n            elif hasattr(self, \"afunc\"):\n                self._repr = (\n                    f\"RunnableLambda(afunc={get_lambda_source(self.afunc) or '...'})\"\n                )\n            else:\n                self._repr = \"RunnableLambda(...)\"\n        return self._repr\n\n    def _invoke(\n        self,\n        input: Input,\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Output:\n        if inspect.isgeneratorfunction(self.func):\n            output: Optional[Output] = None\n            for chunk in call_func_with_variable_args(\n                cast(Callable[[Input], Iterator[Output]], self.func),\n                input,\n                config,\n                run_manager,\n                **kwargs,\n            ):\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk  # type: ignore[operator]\n                    except TypeError:\n                        output = chunk\n        else:\n            output = call_func_with_variable_args(\n                self.func, input, config, run_manager, **kwargs\n            )\n        # If the output is a Runnable, invoke it\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {input}.\"\n                )\n                raise RecursionError(msg)\n            output = output.invoke(\n                input,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            )\n        return cast(Output, output)\n\n    async def _ainvoke(\n        self,\n        input: Input,\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Output:\n        if hasattr(self, \"afunc\"):\n            afunc = self.afunc\n        else:\n            if inspect.isgeneratorfunction(self.func):\n\n                def func(\n                    input: Input,\n                    run_manager: AsyncCallbackManagerForChainRun,\n                    config: RunnableConfig,\n                    **kwargs: Any,\n                ) -> Output:\n                    output: Optional[Output] = None\n                    for chunk in call_func_with_variable_args(\n                        cast(Callable[[Input], Iterator[Output]], self.func),\n                        input,\n                        config,\n                        run_manager.get_sync(),\n                        **kwargs,\n                    ):\n                        if output is None:\n                            output = chunk\n                        else:\n                            try:\n                                output = output + chunk  # type: ignore[operator]\n                            except TypeError:\n                                output = chunk\n                    return cast(Output, output)\n\n            else:\n\n                def func(\n                    input: Input,\n                    run_manager: AsyncCallbackManagerForChainRun,\n                    config: RunnableConfig,\n                    **kwargs: Any,\n                ) -> Output:\n                    return call_func_with_variable_args(\n                        self.func, input, config, run_manager.get_sync(), **kwargs\n                    )\n\n            @wraps(func)\n            async def f(*args: Any, **kwargs: Any) -> Any:\n                return await run_in_executor(config, func, *args, **kwargs)\n\n            afunc = f\n\n        if is_async_generator(afunc):\n            output: Optional[Output] = None\n            async with aclosing(\n                cast(\n                    AsyncGenerator[Any, Any],\n                    acall_func_with_variable_args(\n                        cast(Callable, afunc),\n                        input,\n                        config,\n                        run_manager,\n                        **kwargs,\n                    ),\n                )\n            ) as stream:\n                async for chunk in cast(\n                    AsyncIterator[Output],\n                    stream,\n                ):\n                    if output is None:\n                        output = chunk\n                    else:\n                        try:\n                            output = output + chunk  # type: ignore[operator]\n                        except TypeError:\n                            output = chunk\n        else:\n            output = await acall_func_with_variable_args(\n                cast(Callable, afunc), input, config, run_manager, **kwargs\n            )\n        # If the output is a Runnable, invoke it\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {input}.\"\n                )\n                raise RecursionError(msg)\n            output = await output.ainvoke(\n                input,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            )\n        return cast(Output, output)\n\n    def _config(\n        self, config: Optional[RunnableConfig], callable: Callable[..., Any]\n    ) -> RunnableConfig:\n        return ensure_config(config)\n\n    def invoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Invoke this Runnable synchronously.\n\n        Args:\n            input: The input to this Runnable.\n            config: The config to use. Defaults to None.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            The output of this Runnable.\n\n        Raises:\n            TypeError: If the Runnable is a coroutine function.\n        \"\"\"\n        if hasattr(self, \"func\"):\n            return self._call_with_config(\n                self._invoke,\n                input,\n                self._config(config, self.func),\n                **kwargs,\n            )\n        else:\n            msg = (\n                \"Cannot invoke a coroutine function synchronously.\"\n                \"Use `ainvoke` instead.\"\n            )\n            raise TypeError(msg)\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Invoke this Runnable asynchronously.\n\n        Args:\n            input: The input to this Runnable.\n            config: The config to use. Defaults to None.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            The output of this Runnable.\n        \"\"\"\n        the_func = self.afunc if hasattr(self, \"afunc\") else self.func\n        return await self._acall_with_config(\n            self._ainvoke,\n            input,\n            self._config(config, the_func),\n            **kwargs,\n        )\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        final: Input\n        got_first_val = False\n        for ichunk in input:\n            # By definitions, RunnableLambdas consume all input before emitting output.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk.\n            # So we'll iterate until we get to the last chunk!\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if inspect.isgeneratorfunction(self.func):\n            output: Optional[Output] = None\n            for chunk in call_func_with_variable_args(\n                self.func, cast(Input, final), config, run_manager, **kwargs\n            ):\n                yield chunk\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk\n                    except TypeError:\n                        output = chunk\n        else:\n            output = call_func_with_variable_args(\n                self.func, cast(Input, final), config, run_manager, **kwargs\n            )\n\n        # If the output is a Runnable, use its stream output\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {final}.\"\n                )\n                raise RecursionError(msg)\n            for chunk in output.stream(\n                final,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            ):\n                yield chunk\n        elif not inspect.isgeneratorfunction(self.func):\n            # Otherwise, just yield it\n            yield cast(Output, output)\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        if hasattr(self, \"func\"):\n            yield from self._transform_stream_with_config(\n                input,\n                self._transform,\n                self._config(config, self.func),\n                **kwargs,\n            )\n        else:\n            msg = (\n                \"Cannot stream a coroutine function synchronously.\"\n                \"Use `astream` instead.\"\n            )\n            raise TypeError(msg)\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        final: Input\n        got_first_val = False\n        async for ichunk in input:\n            # By definitions, RunnableLambdas consume all input before emitting output.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk.\n            # So we'll iterate until we get to the last chunk!\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if hasattr(self, \"afunc\"):\n            afunc = self.afunc\n        else:\n            if inspect.isgeneratorfunction(self.func):\n                msg = (\n                    \"Cannot stream from a generator function asynchronously.\"\n                    \"Use .stream() instead.\"\n                )\n                raise TypeError(msg)\n\n            def func(\n                input: Input,\n                run_manager: AsyncCallbackManagerForChainRun,\n                config: RunnableConfig,\n                **kwargs: Any,\n            ) -> Output:\n                return call_func_with_variable_args(\n                    self.func, input, config, run_manager.get_sync(), **kwargs\n                )\n\n            @wraps(func)\n            async def f(*args: Any, **kwargs: Any) -> Any:\n                return await run_in_executor(config, func, *args, **kwargs)\n\n            afunc = f\n\n        if is_async_generator(afunc):\n            output: Optional[Output] = None\n            async for chunk in cast(\n                AsyncIterator[Output],\n                acall_func_with_variable_args(\n                    cast(Callable, afunc),\n                    cast(Input, final),\n                    config,\n                    run_manager,\n                    **kwargs,\n                ),\n            ):\n                yield chunk\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk  # type: ignore[operator]\n                    except TypeError:\n                        output = chunk\n        else:\n            output = await acall_func_with_variable_args(\n                cast(Callable, afunc), cast(Input, final), config, run_manager, **kwargs\n            )\n\n        # If the output is a Runnable, use its astream output\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {final}.\"\n                )\n                raise RecursionError(msg)\n            async for chunk in output.astream(\n                final,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            ):\n                yield chunk\n        elif not is_async_generator(afunc):\n            # Otherwise, just yield it\n            yield cast(Output, output)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for output in self._atransform_stream_with_config(\n            input,\n            self._atransform,\n            self._config(config, self.afunc if hasattr(self, \"afunc\") else self.func),\n            **kwargs,\n        ):\n            yield output\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\nclass RunnableEachBase(RunnableSerializable[list[Input], list[Output]]):\n    \"\"\"Runnable that delegates calls to another Runnable\n    with each element of the input sequence.\n\n    Use only if creating a new RunnableEach subclass with different __init__ args.\n\n    See documentation for RunnableEach for more details.\n    \"\"\"\n\n    bound: Runnable[Input, Output]\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        return list[self.bound.InputType]  # type: ignore[name-defined]\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=(\n                list[self.bound.get_input_schema(config)],  # type: ignore\n                None,\n            ),\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    @property\n    @override\n    def OutputType(self) -> type[list[Output]]:\n        return list[self.bound.OutputType]  # type: ignore[name-defined]\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        schema = self.bound.get_output_schema(config)\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=list[schema],  # type: ignore[valid-type]\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return self.bound.config_specs\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        return self.bound.get_graph(config)\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def _invoke(\n        self,\n        inputs: list[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> list[Output]:\n        configs = [\n            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs\n        ]\n        return self.bound.batch(inputs, configs, **kwargs)\n\n    def invoke(\n        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> list[Output]:\n        return self._call_with_config(self._invoke, input, config, **kwargs)\n\n    async def _ainvoke(\n        self,\n        inputs: list[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> list[Output]:\n        configs = [\n            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs\n        ]\n        return await self.bound.abatch(inputs, configs, **kwargs)\n\n    async def ainvoke(\n        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> list[Output]:\n        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)\n\n    async def astream_events(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[StreamEvent]:\n        for _ in range(1):\n            msg = \"RunnableEach does not support astream_events yet.\"\n            raise NotImplementedError(msg)\n            yield\n\n\nclass RunnableEach(RunnableEachBase[Input, Output]):\n    \"\"\"Runnable that delegates calls to another Runnable\n    with each element of the input sequence.\n\n    It allows you to call multiple inputs with the bounded Runnable.\n\n    RunnableEach makes it easy to run multiple inputs for the Runnable.\n    In the below example, we associate and run three inputs\n    with a Runnable:\n\n        .. code-block:: python\n\n            from langchain_core.runnables.base import RunnableEach\n            from langchain_openai import ChatOpenAI\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.output_parsers import StrOutputParser\n            prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about\n            {topic}\")\n            model = ChatOpenAI()\n            output_parser = StrOutputParser()\n            runnable = prompt | model | output_parser\n            runnable_each = RunnableEach(bound=runnable)\n            output = runnable_each.invoke([{'topic':'Computer Science'},\n                                        {'topic':'Art'},\n                                        {'topic':'Biology'}])\n            print(output)  # noqa: T201\n    \"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        name = name or self.name or f\"RunnableEach<{self.bound.get_name()}>\"\n        return super().get_name(suffix, name=name)\n\n    def bind(self, **kwargs: Any) -> RunnableEach[Input, Output]:\n        return RunnableEach(bound=self.bound.bind(**kwargs))\n\n    def with_config(\n        self, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> RunnableEach[Input, Output]:\n        return RunnableEach(bound=self.bound.with_config(config, **kwargs))\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> RunnableEach[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called before the Runnable starts running, with the Run object.\n                Defaults to None.\n            on_end: Called after the Runnable finishes running, with the Run object.\n                Defaults to None.\n            on_error: Called if the Runnable throws an error, with the Run object.\n                Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n        \"\"\"\n        return RunnableEach(\n            bound=self.bound.with_listeners(\n                on_start=on_start, on_end=on_end, on_error=on_error\n            )\n        )\n\n    def with_alisteners(\n        self,\n        *,\n        on_start: Optional[AsyncListener] = None,\n        on_end: Optional[AsyncListener] = None,\n        on_error: Optional[AsyncListener] = None,\n    ) -> RunnableEach[Input, Output]:\n        \"\"\"Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called asynchronously before the Runnable starts running,\n                      with the Run object. Defaults to None.\n            on_end: Called asynchronously after the Runnable finishes running,\n                    with the Run object. Defaults to None.\n            on_error: Called asynchronously if the Runnable throws an error,\n                    with the Run object. Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n        \"\"\"\n        return RunnableEach(\n            bound=self.bound.with_alisteners(\n                on_start=on_start, on_end=on_end, on_error=on_error\n            )\n        )\n\n\nclass RunnableBindingBase(RunnableSerializable[Input, Output]):\n    \"\"\"Runnable that delegates calls to another Runnable with a set of kwargs.\n\n    Use only if creating a new RunnableBinding subclass with different __init__ args.\n\n    See documentation for RunnableBinding for more details.\n    \"\"\"\n\n    bound: Runnable[Input, Output]\n    \"\"\"The underlying Runnable that this Runnable delegates to.\"\"\"\n\n    kwargs: Mapping[str, Any] = Field(default_factory=dict)\n    \"\"\"kwargs to pass to the underlying Runnable when running.\n\n    For example, when the Runnable binding is invoked the underlying\n    Runnable will be invoked with the same input but with these additional\n    kwargs.\n    \"\"\"\n\n    config: RunnableConfig = Field(default_factory=RunnableConfig)  # type: ignore\n    \"\"\"The config to bind to the underlying Runnable.\"\"\"\n\n    config_factories: list[Callable[[RunnableConfig], RunnableConfig]] = Field(\n        default_factory=list\n    )\n    \"\"\"The config factories to bind to the underlying Runnable.\"\"\"\n\n    # Union[Type[Input], BaseModel] + things like List[str]\n    custom_input_type: Optional[Any] = None\n    \"\"\"Override the input type of the underlying Runnable with a custom type.\n\n    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n    \"\"\"\n    # Union[Type[Output], BaseModel] + things like List[str]\n    custom_output_type: Optional[Any] = None\n    \"\"\"Override the output type of the underlying Runnable with a custom type.\n\n    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    def __init__(\n        self,\n        *,\n        bound: Runnable[Input, Output],\n        kwargs: Optional[Mapping[str, Any]] = None,\n        config: Optional[RunnableConfig] = None,\n        config_factories: Optional[\n            list[Callable[[RunnableConfig], RunnableConfig]]\n        ] = None,\n        custom_input_type: Optional[Union[type[Input], BaseModel]] = None,\n        custom_output_type: Optional[Union[type[Output], BaseModel]] = None,\n        **other_kwargs: Any,\n    ) -> None:\n        \"\"\"Create a RunnableBinding from a Runnable and kwargs.\n\n        Args:\n            bound: The underlying Runnable that this Runnable delegates calls to.\n            kwargs: optional kwargs to pass to the underlying Runnable, when running\n                    the underlying Runnable (e.g., via `invoke`, `batch`,\n                    `transform`, or `stream` or async variants)\n                    Defaults to None.\n            config: optional config to bind to the underlying Runnable.\n                    Defaults to None.\n            config_factories: optional list of config factories to apply to the\n                    config before binding to the underlying Runnable.\n                    Defaults to None.\n            custom_input_type: Specify to override the input type of the underlying\n                               Runnable with a custom type. Defaults to None.\n            custom_output_type: Specify to override the output type of the underlying\n                Runnable with a custom type. Defaults to None.\n            **other_kwargs: Unpacked into the base class.\n        \"\"\"\n        super().__init__(  # type: ignore[call-arg]\n            bound=bound,\n            kwargs=kwargs or {},\n            config=config or {},\n            config_factories=config_factories or [],\n            custom_input_type=custom_input_type,\n            custom_output_type=custom_output_type,\n            **other_kwargs,\n        )\n        # if we don't explicitly set config to the TypedDict here,\n        # the pydantic init above will strip out any of the \"extra\"\n        # fields even though total=False on the typed dict.\n        self.config = config or {}\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        return self.bound.get_name(suffix, name=name)\n\n    @property\n    @override\n    def InputType(self) -> type[Input]:\n        return (\n            cast(type[Input], self.custom_input_type)\n            if self.custom_input_type is not None\n            else self.bound.InputType\n        )\n\n    @property\n    @override\n    def OutputType(self) -> type[Output]:\n        return (\n            cast(type[Output], self.custom_output_type)\n            if self.custom_output_type is not None\n            else self.bound.OutputType\n        )\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        if self.custom_input_type is not None:\n            return super().get_input_schema(config)\n        return self.bound.get_input_schema(merge_configs(self.config, config))\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        if self.custom_output_type is not None:\n            return super().get_output_schema(config)\n        return self.bound.get_output_schema(merge_configs(self.config, config))\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return self.bound.config_specs\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        return self.bound.get_graph(self._merge_configs(config))\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:\n        config = merge_configs(self.config, *configs)\n        return merge_configs(config, *(f(config) for f in self.config_factories))\n\n    def invoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        return self.bound.invoke(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        return await self.bound.ainvoke(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if isinstance(config, list):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        return self.bound.batch(\n            inputs,\n            configs,\n            return_exceptions=return_exceptions,\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if isinstance(config, list):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        return await self.bound.abatch(\n            inputs,\n            configs,\n            return_exceptions=return_exceptions,\n            **{**self.kwargs, **kwargs},\n        )\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Output]]: ...\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...\n\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]:\n        if isinstance(config, Sequence):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        # lol mypy\n        if return_exceptions:\n            yield from self.bound.batch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            )\n        else:\n            yield from self.bound.batch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            )\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Output]]: ...\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...\n\n    async def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:\n        if isinstance(config, Sequence):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        if return_exceptions:\n            async for item in self.bound.abatch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            ):\n                yield item\n        else:\n            async for item in self.bound.abatch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            ):\n                yield item\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self.bound.stream(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for item in self.bound.astream(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        ):\n            yield item\n\n    async def astream_events(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[StreamEvent]:\n        async for item in self.bound.astream_events(\n            input, self._merge_configs(config), **{**self.kwargs, **kwargs}\n        ):\n            yield item\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        yield from self.bound.transform(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        async for item in self.bound.atransform(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        ):\n            yield item\n\n\nRunnableBindingBase.model_rebuild()\n\n\nclass RunnableBinding(RunnableBindingBase[Input, Output]):\n    \"\"\"Wrap a Runnable with additional functionality.\n\n    A RunnableBinding can be thought of as a \"runnable decorator\" that\n    preserves the essential features of Runnable; i.e., batching, streaming,\n    and async support, while adding additional functionality.\n\n    Any class that inherits from Runnable can be bound to a `RunnableBinding`.\n    Runnables expose a standard set of methods for creating `RunnableBindings`\n    or sub-classes of `RunnableBindings` (e.g., `RunnableRetry`,\n    `RunnableWithFallbacks`) that add additional functionality.\n\n    These methods include:\n\n    - ``bind``: Bind kwargs to pass to the underlying Runnable when running it.\n    - ``with_config``: Bind config to pass to the underlying Runnable when running it.\n    - ``with_listeners``:  Bind lifecycle listeners to the underlying Runnable.\n    - ``with_types``: Override the input and output types of the underlying Runnable.\n    - ``with_retry``: Bind a retry policy to the underlying Runnable.\n    - ``with_fallbacks``: Bind a fallback policy to the underlying Runnable.\n\n    Example:\n    `bind`: Bind kwargs to pass to the underlying Runnable when running it.\n\n        .. code-block:: python\n\n            # Create a Runnable binding that invokes the ChatModel with the\n            # additional kwarg `stop=['-']` when running it.\n            from langchain_community.chat_models import ChatOpenAI\n            model = ChatOpenAI()\n            model.invoke('Say \"Parrot-MAGIC\"', stop=['-']) # Should return `Parrot`\n            # Using it the easy way via `bind` method which returns a new\n            # RunnableBinding\n            runnable_binding = model.bind(stop=['-'])\n            runnable_binding.invoke('Say \"Parrot-MAGIC\"') # Should return `Parrot`\n\n        Can also be done by instantiating a RunnableBinding directly (not recommended):\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableBinding\n            runnable_binding = RunnableBinding(\n                bound=model,\n                kwargs={'stop': ['-']} # <-- Note the additional kwargs\n            )\n            runnable_binding.invoke('Say \"Parrot-MAGIC\"') # Should return `Parrot`\n    \"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:\n        \"\"\"Bind additional kwargs to a Runnable, returning a new Runnable.\n\n        Args:\n            **kwargs: The kwargs to bind to the Runnable.\n\n        Returns:\n            A new Runnable with the same type and config as the original,\n            but with the additional kwargs bound.\n        \"\"\"\n        return self.__class__(\n            bound=self.bound,\n            config=self.config,\n            kwargs={**self.kwargs, **kwargs},\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_config(\n        self,\n        config: Optional[RunnableConfig] = None,\n        # Sadly Unpack is not well supported by mypy so this will have to be untyped\n        **kwargs: Any,\n    ) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=cast(RunnableConfig, {**self.config, **(config or {}), **kwargs}),\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called before the Runnable starts running, with the Run object.\n                Defaults to None.\n            on_end: Called after the Runnable finishes running, with the Run object.\n                Defaults to None.\n            on_error: Called if the Runnable throws an error, with the Run object.\n                Defaults to None.\n\n        Returns:\n            The Runnable object contains information about the run, including its id,\n            type, input, output, error, start_time, end_time, and any tags or metadata\n            added to the run.\n        \"\"\"\n        from langchain_core.tracers.root_listeners import RootListenersTracer\n\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=self.config,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        RootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_types(\n        self,\n        input_type: Optional[Union[type[Input], BaseModel]] = None,\n        output_type: Optional[Union[type[Output], BaseModel]] = None,\n    ) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=self.config,\n            custom_input_type=(\n                input_type if input_type is not None else self.custom_input_type\n            ),\n            custom_output_type=(\n                output_type if output_type is not None else self.custom_output_type\n            ),\n        )\n\n    def with_retry(self, **kwargs: Any) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound.with_retry(**kwargs),\n            kwargs=self.kwargs,\n            config=self.config,\n        )\n\n    def __getattr__(self, name: str) -> Any:\n        attr = getattr(self.bound, name)\n\n        if callable(attr) and (\n            config_param := inspect.signature(attr).parameters.get(\"config\")\n        ):\n            if config_param.kind == inspect.Parameter.KEYWORD_ONLY:\n\n                @wraps(attr)\n                def wrapper(*args: Any, **kwargs: Any) -> Any:\n                    return attr(\n                        *args,\n                        config=merge_configs(self.config, kwargs.pop(\"config\", None)),\n                        **kwargs,\n                    )\n\n                return wrapper\n            elif config_param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                idx = list(inspect.signature(attr).parameters).index(\"config\")\n\n                @wraps(attr)\n                def wrapper(*args: Any, **kwargs: Any) -> Any:\n                    if len(args) >= idx + 1:\n                        argsl = list(args)\n                        argsl[idx] = merge_configs(self.config, argsl[idx])\n                        return attr(*argsl, **kwargs)\n                    else:\n                        return attr(\n                            *args,\n                            config=merge_configs(\n                                self.config, kwargs.pop(\"config\", None)\n                            ),\n                            **kwargs,\n                        )\n\n                return wrapper\n\n        return attr\n\n\nclass _RunnableCallableSync(Protocol[Input, Output]):\n    def __call__(self, __in: Input, *, config: RunnableConfig) -> Output: ...\n\n\nclass _RunnableCallableAsync(Protocol[Input, Output]):\n    def __call__(self, __in: Input, *, config: RunnableConfig) -> Awaitable[Output]: ...\n\n\nclass _RunnableCallableIterator(Protocol[Input, Output]):\n    def __call__(\n        self, __in: Iterator[Input], *, config: RunnableConfig\n    ) -> Iterator[Output]: ...\n\n\nclass _RunnableCallableAsyncIterator(Protocol[Input, Output]):\n    def __call__(\n        self, __in: AsyncIterator[Input], *, config: RunnableConfig\n    ) -> AsyncIterator[Output]: ...\n\n\nRunnableLike = Union[\n    Runnable[Input, Output],\n    Callable[[Input], Output],\n    Callable[[Input], Awaitable[Output]],\n    Callable[[Iterator[Input]], Iterator[Output]],\n    Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n    _RunnableCallableSync[Input, Output],\n    _RunnableCallableAsync[Input, Output],\n    _RunnableCallableIterator[Input, Output],\n    _RunnableCallableAsyncIterator[Input, Output],\n    Mapping[str, Any],\n]\n\n\ndef coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:\n    \"\"\"Coerce a Runnable-like object into a Runnable.\n\n    Args:\n        thing: A Runnable-like object.\n\n    Returns:\n        A Runnable.\n\n    Raises:\n        TypeError: If the object is not Runnable-like.\n    \"\"\"\n    if isinstance(thing, Runnable):\n        return thing\n    elif is_async_generator(thing) or inspect.isgeneratorfunction(thing):\n        return RunnableGenerator(thing)\n    elif callable(thing):\n        return RunnableLambda(cast(Callable[[Input], Output], thing))\n    elif isinstance(thing, dict):\n        return cast(Runnable[Input, Output], RunnableParallel(thing))\n    else:\n        msg = (\n            f\"Expected a Runnable, callable or dict.\"\n            f\"Instead got an unsupported type: {type(thing)}\"\n        )\n        raise TypeError(msg)\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Coroutine[Any, Any, Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Iterator[Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], AsyncIterator[Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Output],\n) -> Runnable[Input, Output]: ...\n\n\ndef chain(\n    func: Union[\n        Callable[[Input], Output],\n        Callable[[Input], Iterator[Output]],\n        Callable[[Input], Coroutine[Any, Any, Output]],\n        Callable[[Input], AsyncIterator[Output]],\n    ],\n) -> Runnable[Input, Output]:\n    \"\"\"Decorate a function to make it a Runnable.\n    Sets the name of the Runnable to the name of the function.\n    Any runnables called by the function will be traced as dependencies.\n\n    Args:\n        func: A callable.\n\n    Returns:\n        A Runnable.\n\n    Example:\n\n    .. code-block:: python\n\n        from langchain_core.runnables import chain\n        from langchain_core.prompts import PromptTemplate\n        from langchain_openai import OpenAI\n\n        @chain\n        def my_func(fields):\n            prompt = PromptTemplate(\"Hello, {name}!\")\n            llm = OpenAI()\n            formatted = prompt.invoke(**fields)\n\n            for chunk in llm.stream(formatted):\n                yield chunk\n    \"\"\"\n    return RunnableLambda(func)\n",
        "patch": "@@ -60,7 +60,6 @@\n     run_in_executor,\n )\n from langchain_core.runnables.graph import Graph\n-from langchain_core.runnables.schema import StreamEvent\n from langchain_core.runnables.utils import (\n     AddableDict,\n     AnyConfigurableField,\n@@ -94,6 +93,7 @@\n     from langchain_core.runnables.fallbacks import (\n         RunnableWithFallbacks as RunnableWithFallbacksT,\n     )\n+    from langchain_core.runnables.schema import StreamEvent\n     from langchain_core.tools import BaseTool\n     from langchain_core.tracers.log_stream import (\n         RunLog,"
      },
      {
        "filename": "libs/core/langchain_core/runnables/configurable.py",
        "content_before": "from __future__ import annotations\n\nimport enum\nimport threading\nfrom abc import abstractmethod\nfrom collections.abc import AsyncIterator, Iterator, Sequence\nfrom collections.abc import Mapping as Mapping\nfrom functools import wraps\nfrom typing import (\n    Any,\n    Callable,\n    Optional,\n    Union,\n    cast,\n)\nfrom weakref import WeakValueDictionary\n\nfrom pydantic import BaseModel, ConfigDict\nfrom typing_extensions import override\n\nfrom langchain_core.runnables.base import Runnable, RunnableSerializable\nfrom langchain_core.runnables.config import (\n    RunnableConfig,\n    ensure_config,\n    get_config_list,\n    get_executor_for_config,\n    merge_configs,\n)\nfrom langchain_core.runnables.graph import Graph\nfrom langchain_core.runnables.utils import (\n    AnyConfigurableField,\n    ConfigurableField,\n    ConfigurableFieldMultiOption,\n    ConfigurableFieldSingleOption,\n    ConfigurableFieldSpec,\n    Input,\n    Output,\n    gather_with_concurrency,\n    get_unique_config_specs,\n)\n\n\nclass DynamicRunnable(RunnableSerializable[Input, Output]):\n    \"\"\"Serializable Runnable that can be dynamically configured.\n\n    A DynamicRunnable should be initiated using the `configurable_fields` or\n    `configurable_alternatives` method of a Runnable.\n\n    Parameters:\n        default: The default Runnable to use.\n        config: The configuration to use.\n    \"\"\"\n\n    default: RunnableSerializable[Input, Output]\n\n    config: Optional[RunnableConfig] = None\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    @property\n    @override\n    def InputType(self) -> type[Input]:\n        return self.default.InputType\n\n    @property\n    @override\n    def OutputType(self) -> type[Output]:\n        return self.default.OutputType\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        runnable, config = self.prepare(config)\n        return runnable.get_input_schema(config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        runnable, config = self.prepare(config)\n        return runnable.get_output_schema(config)\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        runnable, config = self.prepare(config)\n        return runnable.get_graph(config)\n\n    def with_config(\n        self,\n        config: Optional[RunnableConfig] = None,\n        # Sadly Unpack is not well supported by mypy so this will have to be untyped\n        **kwargs: Any,\n    ) -> Runnable[Input, Output]:\n        return self.__class__(\n            **{**self.__dict__, \"config\": ensure_config(merge_configs(config, kwargs))}  # type: ignore[arg-type]\n        )\n\n    def prepare(\n        self, config: Optional[RunnableConfig] = None\n    ) -> tuple[Runnable[Input, Output], RunnableConfig]:\n        \"\"\"Prepare the Runnable for invocation.\n\n        Args:\n            config: The configuration to use. Defaults to None.\n\n        Returns:\n            Tuple[Runnable[Input, Output], RunnableConfig]: The prepared Runnable and\n            configuration.\n        \"\"\"\n        runnable: Runnable[Input, Output] = self\n        while isinstance(runnable, DynamicRunnable):\n            runnable, config = runnable._prepare(merge_configs(runnable.config, config))\n        return runnable, cast(RunnableConfig, config)\n\n    @abstractmethod\n    def _prepare(\n        self, config: Optional[RunnableConfig] = None\n    ) -> tuple[Runnable[Input, Output], RunnableConfig]: ...\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        runnable, config = self.prepare(config)\n        return runnable.invoke(input, config, **kwargs)\n\n    async def ainvoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        runnable, config = self.prepare(config)\n        return await runnable.ainvoke(input, config, **kwargs)\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        configs = get_config_list(config, len(inputs))\n        prepared = [self.prepare(c) for c in configs]\n\n        if all(p is self.default for p, _ in prepared):\n            return self.default.batch(\n                inputs,\n                [c for _, c in prepared],\n                return_exceptions=return_exceptions,\n                **kwargs,\n            )\n\n        if not inputs:\n            return []\n\n        def invoke(\n            prepared: tuple[Runnable[Input, Output], RunnableConfig],\n            input: Input,\n        ) -> Union[Output, Exception]:\n            bound, config = prepared\n            if return_exceptions:\n                try:\n                    return bound.invoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return bound.invoke(input, config, **kwargs)\n\n        # If there's only one input, don't bother with the executor\n        if len(inputs) == 1:\n            return cast(list[Output], [invoke(prepared[0], inputs[0])])\n\n        with get_executor_for_config(configs[0]) as executor:\n            return cast(list[Output], list(executor.map(invoke, prepared, inputs)))\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        configs = get_config_list(config, len(inputs))\n        prepared = [self.prepare(c) for c in configs]\n\n        if all(p is self.default for p, _ in prepared):\n            return await self.default.abatch(\n                inputs,\n                [c for _, c in prepared],\n                return_exceptions=return_exceptions,\n                **kwargs,\n            )\n\n        if not inputs:\n            return []\n\n        async def ainvoke(\n            prepared: tuple[Runnable[Input, Output], RunnableConfig],\n            input: Input,\n        ) -> Union[Output, Exception]:\n            bound, config = prepared\n            if return_exceptions:\n                try:\n                    return await bound.ainvoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return await bound.ainvoke(input, config, **kwargs)\n\n        coros = map(ainvoke, prepared, inputs)\n        return await gather_with_concurrency(configs[0].get(\"max_concurrency\"), *coros)\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        runnable, config = self.prepare(config)\n        return runnable.stream(input, config, **kwargs)\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        runnable, config = self.prepare(config)\n        async for chunk in runnable.astream(input, config, **kwargs):\n            yield chunk\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        runnable, config = self.prepare(config)\n        return runnable.transform(input, config, **kwargs)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        runnable, config = self.prepare(config)\n        async for chunk in runnable.atransform(input, config, **kwargs):\n            yield chunk\n\n    def __getattr__(self, name: str) -> Any:\n        attr = getattr(self.default, name)\n        if callable(attr):\n\n            @wraps(attr)\n            def wrapper(*args: Any, **kwargs: Any) -> Any:\n                for key, arg in kwargs.items():\n                    if key == \"config\" and (\n                        isinstance(arg, dict)\n                        and \"configurable\" in arg\n                        and isinstance(arg[\"configurable\"], dict)\n                    ):\n                        runnable, config = self.prepare(cast(RunnableConfig, arg))\n                        kwargs = {**kwargs, \"config\": config}\n                        return getattr(runnable, name)(*args, **kwargs)\n\n                for idx, arg in enumerate(args):\n                    if (\n                        isinstance(arg, dict)\n                        and \"configurable\" in arg\n                        and isinstance(arg[\"configurable\"], dict)\n                    ):\n                        runnable, config = self.prepare(cast(RunnableConfig, arg))\n                        argsl = list(args)\n                        argsl[idx] = config\n                        return getattr(runnable, name)(*argsl, **kwargs)\n\n                if self.config:\n                    runnable, config = self.prepare()\n                    return getattr(runnable, name)(*args, **kwargs)\n\n                return attr(*args, **kwargs)\n\n            return wrapper\n\n        else:\n            return attr\n\n\nclass RunnableConfigurableFields(DynamicRunnable[Input, Output]):\n    \"\"\"Runnable that can be dynamically configured.\n\n    A RunnableConfigurableFields should be initiated using the\n    `configurable_fields` method of a Runnable.\n\n    Parameters:\n        fields: The configurable fields to use.\n\n    Here is an example of using a RunnableConfigurableFields with LLMs:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import PromptTemplate\n            from langchain_core.runnables import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            model = ChatOpenAI(temperature=0).configurable_fields(\n                temperature=ConfigurableField(\n                    id=\"temperature\",\n                    name=\"LLM Temperature\",\n                    description=\"The temperature of the LLM\",\n                )\n            )\n            # This creates a RunnableConfigurableFields for a chat model.\n\n            # When invoking the created RunnableSequence, you can pass in the\n            # value for your ConfigurableField's id which in this case\n            # will be change in temperature\n\n            prompt = PromptTemplate.from_template(\"Pick a random number above {x}\")\n            chain = prompt | model\n\n            chain.invoke({\"x\": 0})\n            chain.invoke({\"x\": 0}, config={\"configurable\": {\"temperature\": 0.9}})\n\n\n    Here is an example of using a RunnableConfigurableFields with HubRunnables:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import PromptTemplate\n            from langchain_core.runnables import ConfigurableField\n            from langchain_openai import ChatOpenAI\n            from langchain.runnables.hub import HubRunnable\n\n            prompt = HubRunnable(\"rlm/rag-prompt\").configurable_fields(\n                owner_repo_commit=ConfigurableField(\n                    id=\"hub_commit\",\n                    name=\"Hub Commit\",\n                    description=\"The Hub commit to pull from\",\n                )\n            )\n\n            prompt.invoke({\"question\": \"foo\", \"context\": \"bar\"})\n\n            # Invoking prompt with `with_config` method\n\n            prompt.invoke(\n                {\"question\": \"foo\", \"context\": \"bar\"},\n                config={\"configurable\": {\"hub_commit\": \"rlm/rag-prompt-llama\"}},\n            )\n    \"\"\"\n\n    fields: dict[str, AnyConfigurableField]\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"Get the configuration specs for the RunnableConfigurableFields.\n\n        Returns:\n            List[ConfigurableFieldSpec]: The configuration specs.\n        \"\"\"\n        config_specs = []\n\n        for field_name, spec in self.fields.items():\n            if isinstance(spec, ConfigurableField):\n                config_specs.append(\n                    ConfigurableFieldSpec(\n                        id=spec.id,\n                        name=spec.name,\n                        description=spec.description\n                        or self.default.model_fields[field_name].description,\n                        annotation=spec.annotation\n                        or self.default.model_fields[field_name].annotation,\n                        default=getattr(self.default, field_name),\n                        is_shared=spec.is_shared,\n                    )\n                )\n            else:\n                config_specs.append(\n                    make_options_spec(\n                        spec, self.default.model_fields[field_name].description\n                    )\n                )\n\n        config_specs.extend(self.default.config_specs)\n\n        return get_unique_config_specs(config_specs)\n\n    def configurable_fields(\n        self, **kwargs: AnyConfigurableField\n    ) -> RunnableSerializable[Input, Output]:\n        \"\"\"Get a new RunnableConfigurableFields with the specified\n        configurable fields.\n        \"\"\"\n        return self.default.configurable_fields(**{**self.fields, **kwargs})\n\n    def _prepare(\n        self, config: Optional[RunnableConfig] = None\n    ) -> tuple[Runnable[Input, Output], RunnableConfig]:\n        config = ensure_config(config)\n        specs_by_id = {spec.id: (key, spec) for key, spec in self.fields.items()}\n        configurable_fields = {\n            specs_by_id[k][0]: v\n            for k, v in config.get(\"configurable\", {}).items()\n            if k in specs_by_id and isinstance(specs_by_id[k][1], ConfigurableField)\n        }\n        configurable_single_options = {\n            k: v.options[(config.get(\"configurable\", {}).get(v.id) or v.default)]\n            for k, v in self.fields.items()\n            if isinstance(v, ConfigurableFieldSingleOption)\n        }\n        configurable_multi_options = {\n            k: [\n                v.options[o]\n                for o in config.get(\"configurable\", {}).get(v.id, v.default)\n            ]\n            for k, v in self.fields.items()\n            if isinstance(v, ConfigurableFieldMultiOption)\n        }\n        configurable = {\n            **configurable_fields,\n            **configurable_single_options,\n            **configurable_multi_options,\n        }\n\n        if configurable:\n            init_params = {\n                k: v\n                for k, v in self.default.__dict__.items()\n                if k in self.default.model_fields\n            }\n            return (\n                self.default.__class__(**{**init_params, **configurable}),\n                config,\n            )\n        else:\n            return (self.default, config)\n\n\nRunnableConfigurableFields.model_rebuild()\n\n\n# Before Python 3.11 native StrEnum is not available\nclass StrEnum(str, enum.Enum):\n    \"\"\"String enum.\"\"\"\n\n\n_enums_for_spec: WeakValueDictionary[\n    Union[\n        ConfigurableFieldSingleOption, ConfigurableFieldMultiOption, ConfigurableField\n    ],\n    type[StrEnum],\n] = WeakValueDictionary()\n\n_enums_for_spec_lock = threading.Lock()\n\n\nclass RunnableConfigurableAlternatives(DynamicRunnable[Input, Output]):\n    \"\"\"Runnable that can be dynamically configured.\n\n    A RunnableConfigurableAlternatives should be initiated using the\n    `configurable_alternatives` method of a Runnable or can be\n    initiated directly as well.\n\n    Here is an example of using a RunnableConfigurableAlternatives that uses\n    alternative prompts to illustrate its functionality:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            # This creates a RunnableConfigurableAlternatives for Prompt Runnable\n            # with two alternatives.\n            prompt = PromptTemplate.from_template(\n                \"Tell me a joke about {topic}\"\n            ).configurable_alternatives(\n                ConfigurableField(id=\"prompt\"),\n                default_key=\"joke\",\n                poem=PromptTemplate.from_template(\"Write a short poem about {topic}\")\n            )\n\n            # When invoking the created RunnableSequence, you can pass in the\n            # value for your ConfigurableField's id which in this case will either be\n            # `joke` or `poem`.\n            chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n\n            # The `with_config` method brings in the desired Prompt Runnable in your\n            # Runnable Sequence.\n            chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})\n\n\n    Equivalently, you can initialize RunnableConfigurableAlternatives directly\n    and use in LCEL in the same way:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import ConfigurableField\n            from langchain_core.runnables.configurable import RunnableConfigurableAlternatives\n            from langchain_openai import ChatOpenAI\n\n            prompt = RunnableConfigurableAlternatives(\n                which=ConfigurableField(id='prompt'),\n                default=PromptTemplate.from_template(\"Tell me a joke about {topic}\"),\n                default_key='joke',\n                prefix_keys=False,\n                alternatives={\"poem\":PromptTemplate.from_template(\"Write a short poem about {topic}\")}\n            )\n            chain = prompt | ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n            chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"bears\"})\n\n    \"\"\"  # noqa: E501\n\n    which: ConfigurableField\n    \"\"\"The ConfigurableField to use to choose between alternatives.\"\"\"\n\n    alternatives: dict[\n        str,\n        Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],\n    ]\n    \"\"\"The alternatives to choose from.\"\"\"\n\n    default_key: str = \"default\"\n    \"\"\"The enum value to use for the default option. Defaults to \"default\".\"\"\"\n\n    prefix_keys: bool\n    \"\"\"Whether to prefix configurable fields of each alternative with a namespace\n    of the form <which.id>==<alternative_key>, eg. a key named \"temperature\" used by\n    the alternative named \"gpt3\" becomes \"model==gpt3/temperature\".\"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        with _enums_for_spec_lock:\n            if which_enum := _enums_for_spec.get(self.which):\n                pass\n            else:\n                which_enum = StrEnum(  # type: ignore[call-overload]\n                    self.which.name or self.which.id,\n                    (\n                        (v, v)\n                        for v in list(self.alternatives.keys()) + [self.default_key]\n                    ),\n                )\n                _enums_for_spec[self.which] = cast(type[StrEnum], which_enum)\n        return get_unique_config_specs(\n            # which alternative\n            [\n                ConfigurableFieldSpec(\n                    id=self.which.id,\n                    name=self.which.name,\n                    description=self.which.description,\n                    annotation=which_enum,\n                    default=self.default_key,\n                    is_shared=self.which.is_shared,\n                ),\n            ]\n            # config specs of the default option\n            + (\n                [\n                    prefix_config_spec(s, f\"{self.which.id}=={self.default_key}\")\n                    for s in self.default.config_specs\n                ]\n                if self.prefix_keys\n                else self.default.config_specs\n            )\n            # config specs of the alternatives\n            + [\n                (\n                    prefix_config_spec(s, f\"{self.which.id}=={alt_key}\")\n                    if self.prefix_keys\n                    else s\n                )\n                for alt_key, alt in self.alternatives.items()\n                if isinstance(alt, RunnableSerializable)\n                for s in alt.config_specs\n            ]\n        )\n\n    def configurable_fields(\n        self, **kwargs: AnyConfigurableField\n    ) -> RunnableSerializable[Input, Output]:\n        return self.__class__(\n            which=self.which,\n            default=self.default.configurable_fields(**kwargs),\n            alternatives=self.alternatives,\n            default_key=self.default_key,\n            prefix_keys=self.prefix_keys,\n        )\n\n    def _prepare(\n        self, config: Optional[RunnableConfig] = None\n    ) -> tuple[Runnable[Input, Output], RunnableConfig]:\n        config = ensure_config(config)\n        which = config.get(\"configurable\", {}).get(self.which.id, self.default_key)\n        # remap configurable keys for the chosen alternative\n        if self.prefix_keys:\n            config = cast(\n                RunnableConfig,\n                {\n                    **config,\n                    \"configurable\": {\n                        _strremoveprefix(k, f\"{self.which.id}=={which}/\"): v\n                        for k, v in config.get(\"configurable\", {}).items()\n                    },\n                },\n            )\n        # return the chosen alternative\n        if which == self.default_key:\n            return (self.default, config)\n        elif which in self.alternatives:\n            alt = self.alternatives[which]\n            if isinstance(alt, Runnable):\n                return (alt, config)\n            else:\n                return (alt(), config)\n        else:\n            msg = f\"Unknown alternative: {which}\"\n            raise ValueError(msg)\n\n\ndef _strremoveprefix(s: str, prefix: str) -> str:\n    \"\"\"str.removeprefix() is only available in Python 3.9+.\"\"\"\n    return s.replace(prefix, \"\", 1) if s.startswith(prefix) else s\n\n\ndef prefix_config_spec(\n    spec: ConfigurableFieldSpec, prefix: str\n) -> ConfigurableFieldSpec:\n    \"\"\"Prefix the id of a ConfigurableFieldSpec.\n\n    This is useful when a RunnableConfigurableAlternatives is used as a\n    ConfigurableField of another RunnableConfigurableAlternatives.\n\n    Args:\n        spec: The ConfigurableFieldSpec to prefix.\n        prefix: The prefix to add.\n\n    Returns:\n        ConfigurableFieldSpec: The prefixed ConfigurableFieldSpec.\n    \"\"\"\n    return (\n        ConfigurableFieldSpec(\n            id=f\"{prefix}/{spec.id}\",\n            name=spec.name,\n            description=spec.description,\n            annotation=spec.annotation,\n            default=spec.default,\n            is_shared=spec.is_shared,\n        )\n        if not spec.is_shared\n        else spec\n    )\n\n\ndef make_options_spec(\n    spec: Union[ConfigurableFieldSingleOption, ConfigurableFieldMultiOption],\n    description: Optional[str],\n) -> ConfigurableFieldSpec:\n    \"\"\"Make a ConfigurableFieldSpec for a ConfigurableFieldSingleOption or\n    ConfigurableFieldMultiOption.\n\n    Args:\n        spec: The ConfigurableFieldSingleOption or ConfigurableFieldMultiOption.\n        description: The description to use if the spec does not have one.\n\n    Returns:\n        The ConfigurableFieldSpec.\n    \"\"\"\n    with _enums_for_spec_lock:\n        if enum := _enums_for_spec.get(spec):\n            pass\n        else:\n            enum = StrEnum(  # type: ignore[call-overload]\n                spec.name or spec.id,\n                ((v, v) for v in list(spec.options.keys())),\n            )\n            _enums_for_spec[spec] = cast(type[StrEnum], enum)\n    if isinstance(spec, ConfigurableFieldSingleOption):\n        return ConfigurableFieldSpec(\n            id=spec.id,\n            name=spec.name,\n            description=spec.description or description,\n            annotation=enum,\n            default=spec.default,\n            is_shared=spec.is_shared,\n        )\n    else:\n        return ConfigurableFieldSpec(\n            id=spec.id,\n            name=spec.name,\n            description=spec.description or description,\n            annotation=Sequence[enum],  # type: ignore[valid-type]\n            default=spec.default,\n            is_shared=spec.is_shared,\n        )\n",
        "patch": "@@ -7,6 +7,7 @@\n from collections.abc import Mapping as Mapping\n from functools import wraps\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     Optional,\n@@ -26,7 +27,6 @@\n     get_executor_for_config,\n     merge_configs,\n )\n-from langchain_core.runnables.graph import Graph\n from langchain_core.runnables.utils import (\n     AnyConfigurableField,\n     ConfigurableField,\n@@ -39,6 +39,9 @@\n     get_unique_config_specs,\n )\n \n+if TYPE_CHECKING:\n+    from langchain_core.runnables.graph import Graph\n+\n \n class DynamicRunnable(RunnableSerializable[Input, Output]):\n     \"\"\"Serializable Runnable that can be dynamically configured."
      },
      {
        "filename": "libs/core/langchain_core/runnables/graph.py",
        "content_before": "from __future__ import annotations\n\nimport inspect\nfrom collections import defaultdict\nfrom collections.abc import Sequence\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    NamedTuple,\n    Optional,\n    Protocol,\n    TypedDict,\n    Union,\n    overload,\n)\nfrom uuid import UUID, uuid4\n\nfrom pydantic import BaseModel\n\nfrom langchain_core.utils.pydantic import _IgnoreUnserializable, is_basemodel_subclass\n\nif TYPE_CHECKING:\n    from langchain_core.runnables.base import Runnable as RunnableType\n\n\nclass Stringifiable(Protocol):\n    def __str__(self) -> str: ...\n\n\nclass LabelsDict(TypedDict):\n    \"\"\"Dictionary of labels for nodes and edges in a graph.\"\"\"\n\n    nodes: dict[str, str]\n    \"\"\"Labels for nodes.\"\"\"\n    edges: dict[str, str]\n    \"\"\"Labels for edges.\"\"\"\n\n\ndef is_uuid(value: str) -> bool:\n    \"\"\"Check if a string is a valid UUID.\n\n    Args:\n        value: The string to check.\n\n    Returns:\n        True if the string is a valid UUID, False otherwise.\n    \"\"\"\n    try:\n        UUID(value)\n    except ValueError:\n        return False\n    return True\n\n\nclass Edge(NamedTuple):\n    \"\"\"Edge in a graph.\n\n    Parameters:\n        source: The source node id.\n        target: The target node id.\n        data: Optional data associated with the edge. Defaults to None.\n        conditional: Whether the edge is conditional. Defaults to False.\n    \"\"\"\n\n    source: str\n    target: str\n    data: Optional[Stringifiable] = None\n    conditional: bool = False\n\n    def copy(\n        self, *, source: Optional[str] = None, target: Optional[str] = None\n    ) -> Edge:\n        \"\"\"Return a copy of the edge with optional new source and target nodes.\n\n        Args:\n            source: The new source node id. Defaults to None.\n            target: The new target node id. Defaults to None.\n\n        Returns:\n            A copy of the edge with the new source and target nodes.\n        \"\"\"\n        return Edge(\n            source=source or self.source,\n            target=target or self.target,\n            data=self.data,\n            conditional=self.conditional,\n        )\n\n\nclass Node(NamedTuple):\n    \"\"\"Node in a graph.\n\n    Parameters:\n        id: The unique identifier of the node.\n        name: The name of the node.\n        data: The data of the node.\n        metadata: Optional metadata for the node. Defaults to None.\n    \"\"\"\n\n    id: str\n    name: str\n    data: Union[type[BaseModel], RunnableType]\n    metadata: Optional[dict[str, Any]]\n\n    def copy(self, *, id: Optional[str] = None, name: Optional[str] = None) -> Node:\n        \"\"\"Return a copy of the node with optional new id and name.\n\n        Args:\n            id: The new node id. Defaults to None.\n            name: The new node name. Defaults to None.\n\n        Returns:\n            A copy of the node with the new id and name.\n        \"\"\"\n        return Node(\n            id=id or self.id,\n            name=name or self.name,\n            data=self.data,\n            metadata=self.metadata,\n        )\n\n\nclass Branch(NamedTuple):\n    \"\"\"Branch in a graph.\n\n    Parameters:\n        condition: A callable that returns a string representation of the condition.\n        ends: Optional dictionary of end node ids for the branches. Defaults\n            to None.\n    \"\"\"\n\n    condition: Callable[..., str]\n    ends: Optional[dict[str, str]]\n\n\nclass CurveStyle(Enum):\n    \"\"\"Enum for different curve styles supported by Mermaid.\"\"\"\n\n    BASIS = \"basis\"\n    BUMP_X = \"bumpX\"\n    BUMP_Y = \"bumpY\"\n    CARDINAL = \"cardinal\"\n    CATMULL_ROM = \"catmullRom\"\n    LINEAR = \"linear\"\n    MONOTONE_X = \"monotoneX\"\n    MONOTONE_Y = \"monotoneY\"\n    NATURAL = \"natural\"\n    STEP = \"step\"\n    STEP_AFTER = \"stepAfter\"\n    STEP_BEFORE = \"stepBefore\"\n\n\n@dataclass\nclass NodeStyles:\n    \"\"\"Schema for Hexadecimal color codes for different node types.\n\n    Parameters:\n        default: The default color code. Defaults to \"fill:#f2f0ff,line-height:1.2\".\n        first: The color code for the first node. Defaults to \"fill-opacity:0\".\n        last: The color code for the last node. Defaults to \"fill:#bfb6fc\".\n    \"\"\"\n\n    default: str = \"fill:#f2f0ff,line-height:1.2\"\n    first: str = \"fill-opacity:0\"\n    last: str = \"fill:#bfb6fc\"\n\n\nclass MermaidDrawMethod(Enum):\n    \"\"\"Enum for different draw methods supported by Mermaid.\"\"\"\n\n    PYPPETEER = \"pyppeteer\"  # Uses Pyppeteer to render the graph\n    API = \"api\"  # Uses Mermaid.INK API to render the graph\n\n\ndef node_data_str(id: str, data: Union[type[BaseModel], RunnableType]) -> str:\n    \"\"\"Convert the data of a node to a string.\n\n    Args:\n        id: The node id.\n        data: The node data.\n\n    Returns:\n        A string representation of the data.\n    \"\"\"\n    from langchain_core.runnables.base import Runnable\n\n    if not is_uuid(id):\n        return id\n    elif isinstance(data, Runnable):\n        data_str = data.get_name()\n    else:\n        data_str = data.__name__\n    return data_str if not data_str.startswith(\"Runnable\") else data_str[8:]\n\n\ndef node_data_json(\n    node: Node, *, with_schemas: bool = False\n) -> dict[str, Union[str, dict[str, Any]]]:\n    \"\"\"Convert the data of a node to a JSON-serializable format.\n\n    Args:\n        node: The node to convert.\n        with_schemas: Whether to include the schema of the data if\n            it is a Pydantic model. Defaults to False.\n\n    Returns:\n        A dictionary with the type of the data and the data itself.\n    \"\"\"\n    from langchain_core.load.serializable import to_json_not_implemented\n    from langchain_core.runnables.base import Runnable, RunnableSerializable\n\n    if isinstance(node.data, RunnableSerializable):\n        json: dict[str, Any] = {\n            \"type\": \"runnable\",\n            \"data\": {\n                \"id\": node.data.lc_id(),\n                \"name\": node_data_str(node.id, node.data),\n            },\n        }\n    elif isinstance(node.data, Runnable):\n        json = {\n            \"type\": \"runnable\",\n            \"data\": {\n                \"id\": to_json_not_implemented(node.data)[\"id\"],\n                \"name\": node_data_str(node.id, node.data),\n            },\n        }\n    elif inspect.isclass(node.data) and is_basemodel_subclass(node.data):\n        json = (\n            {\n                \"type\": \"schema\",\n                \"data\": node.data.model_json_schema(\n                    schema_generator=_IgnoreUnserializable\n                ),\n            }\n            if with_schemas\n            else {\n                \"type\": \"schema\",\n                \"data\": node_data_str(node.id, node.data),\n            }\n        )\n    else:\n        json = {\n            \"type\": \"unknown\",\n            \"data\": node_data_str(node.id, node.data),\n        }\n    if node.metadata is not None:\n        json[\"metadata\"] = node.metadata\n    return json\n\n\n@dataclass\nclass Graph:\n    \"\"\"Graph of nodes and edges.\n\n    Parameters:\n        nodes: Dictionary of nodes in the graph. Defaults to an empty dictionary.\n        edges: List of edges in the graph. Defaults to an empty list.\n    \"\"\"\n\n    nodes: dict[str, Node] = field(default_factory=dict)\n    edges: list[Edge] = field(default_factory=list)\n\n    def to_json(self, *, with_schemas: bool = False) -> dict[str, list[dict[str, Any]]]:\n        \"\"\"Convert the graph to a JSON-serializable format.\n\n        Args:\n            with_schemas: Whether to include the schemas of the nodes if they are\n                Pydantic models. Defaults to False.\n\n        Returns:\n            A dictionary with the nodes and edges of the graph.\n        \"\"\"\n        stable_node_ids = {\n            node.id: i if is_uuid(node.id) else node.id\n            for i, node in enumerate(self.nodes.values())\n        }\n        edges: list[dict[str, Any]] = []\n        for edge in self.edges:\n            edge_dict = {\n                \"source\": stable_node_ids[edge.source],\n                \"target\": stable_node_ids[edge.target],\n            }\n            if edge.data is not None:\n                edge_dict[\"data\"] = edge.data\n            if edge.conditional:\n                edge_dict[\"conditional\"] = True\n            edges.append(edge_dict)\n\n        return {\n            \"nodes\": [\n                {\n                    \"id\": stable_node_ids[node.id],\n                    **node_data_json(node, with_schemas=with_schemas),\n                }\n                for node in self.nodes.values()\n            ],\n            \"edges\": edges,\n        }\n\n    def __bool__(self) -> bool:\n        return bool(self.nodes)\n\n    def next_id(self) -> str:\n        \"\"\"Return a new unique node\n        identifier that can be used to add a node to the graph.\n        \"\"\"\n        return uuid4().hex\n\n    def add_node(\n        self,\n        data: Union[type[BaseModel], RunnableType],\n        id: Optional[str] = None,\n        *,\n        metadata: Optional[dict[str, Any]] = None,\n    ) -> Node:\n        \"\"\"Add a node to the graph and return it.\n\n        Args:\n            data: The data of the node.\n            id: The id of the node. Defaults to None.\n            metadata: Optional metadata for the node. Defaults to None.\n\n        Returns:\n            The node that was added to the graph.\n\n        Raises:\n            ValueError: If a node with the same id already exists.\n        \"\"\"\n        if id is not None and id in self.nodes:\n            msg = f\"Node with id {id} already exists\"\n            raise ValueError(msg)\n        id = id or self.next_id()\n        node = Node(id=id, data=data, metadata=metadata, name=node_data_str(id, data))\n        self.nodes[node.id] = node\n        return node\n\n    def remove_node(self, node: Node) -> None:\n        \"\"\"Remove a node from the graph and all edges connected to it.\n\n        Args:\n            node: The node to remove.\n        \"\"\"\n        self.nodes.pop(node.id)\n        self.edges = [\n            edge\n            for edge in self.edges\n            if edge.source != node.id and edge.target != node.id\n        ]\n\n    def add_edge(\n        self,\n        source: Node,\n        target: Node,\n        data: Optional[Stringifiable] = None,\n        conditional: bool = False,\n    ) -> Edge:\n        \"\"\"Add an edge to the graph and return it.\n\n        Args:\n            source: The source node of the edge.\n            target: The target node of the edge.\n            data: Optional data associated with the edge. Defaults to None.\n            conditional: Whether the edge is conditional. Defaults to False.\n\n        Returns:\n            The edge that was added to the graph.\n\n        Raises:\n            ValueError: If the source or target node is not in the graph.\n        \"\"\"\n        if source.id not in self.nodes:\n            msg = f\"Source node {source.id} not in graph\"\n            raise ValueError(msg)\n        if target.id not in self.nodes:\n            msg = f\"Target node {target.id} not in graph\"\n            raise ValueError(msg)\n        edge = Edge(\n            source=source.id, target=target.id, data=data, conditional=conditional\n        )\n        self.edges.append(edge)\n        return edge\n\n    def extend(\n        self, graph: Graph, *, prefix: str = \"\"\n    ) -> tuple[Optional[Node], Optional[Node]]:\n        \"\"\"Add all nodes and edges from another graph.\n        Note this doesn't check for duplicates, nor does it connect the graphs.\n\n        Args:\n            graph: The graph to add.\n            prefix: The prefix to add to the node ids. Defaults to \"\".\n\n        Returns:\n            A tuple of the first and last nodes of the subgraph.\n        \"\"\"\n        if all(is_uuid(node.id) for node in graph.nodes.values()):\n            prefix = \"\"\n\n        def prefixed(id: str) -> str:\n            return f\"{prefix}:{id}\" if prefix else id\n\n        # prefix each node\n        self.nodes.update(\n            {prefixed(k): v.copy(id=prefixed(k)) for k, v in graph.nodes.items()}\n        )\n        # prefix each edge's source and target\n        self.edges.extend(\n            [\n                edge.copy(source=prefixed(edge.source), target=prefixed(edge.target))\n                for edge in graph.edges\n            ]\n        )\n        # return (prefixed) first and last nodes of the subgraph\n        first, last = graph.first_node(), graph.last_node()\n        return (\n            first.copy(id=prefixed(first.id)) if first else None,\n            last.copy(id=prefixed(last.id)) if last else None,\n        )\n\n    def reid(self) -> Graph:\n        \"\"\"Return a new graph with all nodes re-identified,\n        using their unique, readable names where possible.\n        \"\"\"\n        node_name_to_ids = defaultdict(list)\n        for node in self.nodes.values():\n            node_name_to_ids[node.name].append(node.id)\n\n        unique_labels = {\n            node_id: node_name if len(node_ids) == 1 else f\"{node_name}_{i + 1}\"\n            for node_name, node_ids in node_name_to_ids.items()\n            for i, node_id in enumerate(node_ids)\n        }\n\n        def _get_node_id(node_id: str) -> str:\n            label = unique_labels[node_id]\n            if is_uuid(node_id):\n                return label\n            else:\n                return node_id\n\n        return Graph(\n            nodes={\n                _get_node_id(id): node.copy(id=_get_node_id(id))\n                for id, node in self.nodes.items()\n            },\n            edges=[\n                edge.copy(\n                    source=_get_node_id(edge.source),\n                    target=_get_node_id(edge.target),\n                )\n                for edge in self.edges\n            ],\n        )\n\n    def first_node(self) -> Optional[Node]:\n        \"\"\"Find the single node that is not a target of any edge.\n        If there is no such node, or there are multiple, return None.\n        When drawing the graph, this node would be the origin.\n        \"\"\"\n        return _first_node(self)\n\n    def last_node(self) -> Optional[Node]:\n        \"\"\"Find the single node that is not a source of any edge.\n        If there is no such node, or there are multiple, return None.\n        When drawing the graph, this node would be the destination.\n        \"\"\"\n        return _last_node(self)\n\n    def trim_first_node(self) -> None:\n        \"\"\"Remove the first node if it exists and has a single outgoing edge,\n        i.e., if removing it would not leave the graph without a \"first\" node.\n        \"\"\"\n        first_node = self.first_node()\n        if (\n            first_node\n            and _first_node(self, exclude=[first_node.id])\n            and len({e for e in self.edges if e.source == first_node.id}) == 1\n        ):\n            self.remove_node(first_node)\n\n    def trim_last_node(self) -> None:\n        \"\"\"Remove the last node if it exists and has a single incoming edge,\n        i.e., if removing it would not leave the graph without a \"last\" node.\n        \"\"\"\n        last_node = self.last_node()\n        if (\n            last_node\n            and _last_node(self, exclude=[last_node.id])\n            and len({e for e in self.edges if e.target == last_node.id}) == 1\n        ):\n            self.remove_node(last_node)\n\n    def draw_ascii(self) -> str:\n        \"\"\"Draw the graph as an ASCII art string.\"\"\"\n        from langchain_core.runnables.graph_ascii import draw_ascii\n\n        return draw_ascii(\n            {node.id: node.name for node in self.nodes.values()},\n            self.edges,\n        )\n\n    def print_ascii(self) -> None:\n        \"\"\"Print the graph as an ASCII art string.\"\"\"\n        print(self.draw_ascii())  # noqa: T201\n\n    @overload\n    def draw_png(\n        self,\n        output_file_path: str,\n        fontname: Optional[str] = None,\n        labels: Optional[LabelsDict] = None,\n    ) -> None: ...\n\n    @overload\n    def draw_png(\n        self,\n        output_file_path: None,\n        fontname: Optional[str] = None,\n        labels: Optional[LabelsDict] = None,\n    ) -> bytes: ...\n\n    def draw_png(\n        self,\n        output_file_path: Optional[str] = None,\n        fontname: Optional[str] = None,\n        labels: Optional[LabelsDict] = None,\n    ) -> Union[bytes, None]:\n        \"\"\"Draw the graph as a PNG image.\n\n        Args:\n            output_file_path: The path to save the image to. If None, the image\n                is not saved. Defaults to None.\n            fontname: The name of the font to use. Defaults to None.\n            labels: Optional labels for nodes and edges in the graph. Defaults to None.\n\n        Returns:\n            The PNG image as bytes if output_file_path is None, None otherwise.\n        \"\"\"\n        from langchain_core.runnables.graph_png import PngDrawer\n\n        default_node_labels = {node.id: node.name for node in self.nodes.values()}\n\n        return PngDrawer(\n            fontname,\n            LabelsDict(\n                nodes={\n                    **default_node_labels,\n                    **(labels[\"nodes\"] if labels is not None else {}),\n                },\n                edges=labels[\"edges\"] if labels is not None else {},\n            ),\n        ).draw(self, output_file_path)\n\n    def draw_mermaid(\n        self,\n        *,\n        with_styles: bool = True,\n        curve_style: CurveStyle = CurveStyle.LINEAR,\n        node_colors: Optional[NodeStyles] = None,\n        wrap_label_n_words: int = 9,\n    ) -> str:\n        \"\"\"Draw the graph as a Mermaid syntax string.\n\n        Args:\n            with_styles: Whether to include styles in the syntax. Defaults to True.\n            curve_style: The style of the edges. Defaults to CurveStyle.LINEAR.\n            node_colors: The colors of the nodes. Defaults to NodeStyles().\n            wrap_label_n_words: The number of words to wrap the node labels at.\n                Defaults to 9.\n\n        Returns:\n            The Mermaid syntax string.\n        \"\"\"\n        from langchain_core.runnables.graph_mermaid import draw_mermaid\n\n        graph = self.reid()\n        first_node = graph.first_node()\n        last_node = graph.last_node()\n\n        return draw_mermaid(\n            nodes=graph.nodes,\n            edges=graph.edges,\n            first_node=first_node.id if first_node else None,\n            last_node=last_node.id if last_node else None,\n            with_styles=with_styles,\n            curve_style=curve_style,\n            node_styles=node_colors,\n            wrap_label_n_words=wrap_label_n_words,\n        )\n\n    def draw_mermaid_png(\n        self,\n        *,\n        curve_style: CurveStyle = CurveStyle.LINEAR,\n        node_colors: Optional[NodeStyles] = None,\n        wrap_label_n_words: int = 9,\n        output_file_path: Optional[str] = None,\n        draw_method: MermaidDrawMethod = MermaidDrawMethod.API,\n        background_color: str = \"white\",\n        padding: int = 10,\n    ) -> bytes:\n        \"\"\"Draw the graph as a PNG image using Mermaid.\n\n        Args:\n            curve_style: The style of the edges. Defaults to CurveStyle.LINEAR.\n            node_colors: The colors of the nodes. Defaults to NodeStyles().\n            wrap_label_n_words: The number of words to wrap the node labels at.\n                Defaults to 9.\n            output_file_path: The path to save the image to. If None, the image\n                is not saved. Defaults to None.\n            draw_method: The method to use to draw the graph.\n                Defaults to MermaidDrawMethod.API.\n            background_color: The color of the background. Defaults to \"white\".\n            padding: The padding around the graph. Defaults to 10.\n\n        Returns:\n            The PNG image as bytes.\n        \"\"\"\n        from langchain_core.runnables.graph_mermaid import draw_mermaid_png\n\n        mermaid_syntax = self.draw_mermaid(\n            curve_style=curve_style,\n            node_colors=node_colors,\n            wrap_label_n_words=wrap_label_n_words,\n        )\n        return draw_mermaid_png(\n            mermaid_syntax=mermaid_syntax,\n            output_file_path=output_file_path,\n            draw_method=draw_method,\n            background_color=background_color,\n            padding=padding,\n        )\n\n\ndef _first_node(graph: Graph, exclude: Sequence[str] = ()) -> Optional[Node]:\n    \"\"\"Find the single node that is not a target of any edge.\n    Exclude nodes/sources with ids in the exclude list.\n    If there is no such node, or there are multiple, return None.\n    When drawing the graph, this node would be the origin.\n    \"\"\"\n    targets = {edge.target for edge in graph.edges if edge.source not in exclude}\n    found: list[Node] = []\n    for node in graph.nodes.values():\n        if node.id not in exclude and node.id not in targets:\n            found.append(node)\n    return found[0] if len(found) == 1 else None\n\n\ndef _last_node(graph: Graph, exclude: Sequence[str] = ()) -> Optional[Node]:\n    \"\"\"Find the single node that is not a source of any edge.\n    Exclude nodes/targets with ids in the exclude list.\n    If there is no such node, or there are multiple, return None.\n    When drawing the graph, this node would be the destination.\n    \"\"\"\n    sources = {edge.source for edge in graph.edges if edge.target not in exclude}\n    found: list[Node] = []\n    for node in graph.nodes.values():\n        if node.id not in exclude and node.id not in sources:\n            found.append(node)\n    return found[0] if len(found) == 1 else None\n",
        "patch": "@@ -2,7 +2,6 @@\n \n import inspect\n from collections import defaultdict\n-from collections.abc import Sequence\n from dataclasses import dataclass, field\n from enum import Enum\n from typing import (\n@@ -18,11 +17,13 @@\n )\n from uuid import UUID, uuid4\n \n-from pydantic import BaseModel\n-\n from langchain_core.utils.pydantic import _IgnoreUnserializable, is_basemodel_subclass\n \n if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n+    from pydantic import BaseModel\n+\n     from langchain_core.runnables.base import Runnable as RunnableType\n \n "
      },
      {
        "filename": "libs/core/langchain_core/runnables/passthrough.py",
        "content_before": "\"\"\"Implementation of the RunnablePassthrough.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport inspect\nimport threading\nfrom collections.abc import AsyncIterator, Awaitable, Iterator, Mapping\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom pydantic import BaseModel, RootModel\nfrom typing_extensions import override\n\nfrom langchain_core.runnables.base import (\n    Other,\n    Runnable,\n    RunnableParallel,\n    RunnableSerializable,\n)\nfrom langchain_core.runnables.config import (\n    RunnableConfig,\n    acall_func_with_variable_args,\n    call_func_with_variable_args,\n    ensure_config,\n    get_executor_for_config,\n    patch_config,\n)\nfrom langchain_core.runnables.graph import Graph\nfrom langchain_core.runnables.utils import (\n    AddableDict,\n    ConfigurableFieldSpec,\n)\nfrom langchain_core.utils.aiter import atee, py_anext\nfrom langchain_core.utils.iter import safetee\nfrom langchain_core.utils.pydantic import create_model_v2\n\nif TYPE_CHECKING:\n    from langchain_core.callbacks.manager import (\n        AsyncCallbackManagerForChainRun,\n        CallbackManagerForChainRun,\n    )\n\n\ndef identity(x: Other) -> Other:\n    \"\"\"Identity function.\n\n    Args:\n        x (Other): input.\n\n    Returns:\n        Other: output.\n    \"\"\"\n    return x\n\n\nasync def aidentity(x: Other) -> Other:\n    \"\"\"Async identity function.\n\n    Args:\n        x (Other): input.\n\n    Returns:\n        Other: output.\n    \"\"\"\n    return x\n\n\nclass RunnablePassthrough(RunnableSerializable[Other, Other]):\n    \"\"\"Runnable to passthrough inputs unchanged or with additional keys.\n\n    This Runnable behaves almost like the identity function, except that it\n    can be configured to add additional keys to the output, if the input is a\n    dict.\n\n    The examples below demonstrate this Runnable works using a few simple\n    chains. The chains rely on simple lambdas to make the examples easy to execute\n    and experiment with.\n\n    Parameters:\n        func (Callable[[Other], None], optional): Function to be called with the input.\n        afunc (Callable[[Other], Awaitable[None]], optional): Async function to\n            be called with the input.\n        input_type (Optional[Type[Other]], optional): Type of the input.\n        **kwargs (Any): Additional keyword arguments.\n\n    Examples:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import (\n                RunnableLambda,\n                RunnableParallel,\n                RunnablePassthrough,\n            )\n\n            runnable = RunnableParallel(\n                origin=RunnablePassthrough(),\n                modified=lambda x: x+1\n            )\n\n            runnable.invoke(1) # {'origin': 1, 'modified': 2}\n\n\n            def fake_llm(prompt: str) -> str: # Fake LLM for the example\n                return \"completion\"\n\n            chain = RunnableLambda(fake_llm) | {\n                'original': RunnablePassthrough(), # Original LLM output\n                'parsed': lambda text: text[::-1] # Parsing logic\n            }\n\n            chain.invoke('hello') # {'original': 'completion', 'parsed': 'noitelpmoc'}\n\n    In some cases, it may be useful to pass the input through while adding some\n    keys to the output. In this case, you can use the `assign` method:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnablePassthrough\n\n            def fake_llm(prompt: str) -> str: # Fake LLM for the example\n                return \"completion\"\n\n            runnable = {\n                'llm1':  fake_llm,\n                'llm2':  fake_llm,\n            } | RunnablePassthrough.assign(\n                total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n            )\n\n            runnable.invoke('hello')\n            # {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}\n    \"\"\"\n\n    input_type: Optional[type[Other]] = None\n\n    func: Optional[\n        Union[Callable[[Other], None], Callable[[Other, RunnableConfig], None]]\n    ] = None\n\n    afunc: Optional[\n        Union[\n            Callable[[Other], Awaitable[None]],\n            Callable[[Other, RunnableConfig], Awaitable[None]],\n        ]\n    ] = None\n\n    def __repr_args__(self) -> Any:\n        # Without this repr(self) raises a RecursionError\n        # See https://github.com/pydantic/pydantic/issues/7327\n        return []\n\n    def __init__(\n        self,\n        func: Optional[\n            Union[\n                Union[Callable[[Other], None], Callable[[Other, RunnableConfig], None]],\n                Union[\n                    Callable[[Other], Awaitable[None]],\n                    Callable[[Other, RunnableConfig], Awaitable[None]],\n                ],\n            ]\n        ] = None,\n        afunc: Optional[\n            Union[\n                Callable[[Other], Awaitable[None]],\n                Callable[[Other, RunnableConfig], Awaitable[None]],\n            ]\n        ] = None,\n        *,\n        input_type: Optional[type[Other]] = None,\n        **kwargs: Any,\n    ) -> None:\n        if inspect.iscoroutinefunction(func):\n            afunc = func\n            func = None\n\n        super().__init__(func=func, afunc=afunc, input_type=input_type, **kwargs)  # type: ignore[call-arg]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        return self.input_type or Any\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        return self.input_type or Any\n\n    @classmethod\n    def assign(\n        cls,\n        **kwargs: Union[\n            Runnable[dict[str, Any], Any],\n            Callable[[dict[str, Any]], Any],\n            Mapping[\n                str,\n                Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]],\n            ],\n        ],\n    ) -> RunnableAssign:\n        \"\"\"Merge the Dict input with the output produced by the mapping argument.\n\n        Args:\n            **kwargs: Runnable, Callable or a Mapping from keys to Runnables\n                or Callables.\n\n        Returns:\n            A Runnable that merges the Dict input with the output produced by the\n            mapping argument.\n        \"\"\"\n        return RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))\n\n    def invoke(\n        self, input: Other, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Other:\n        if self.func is not None:\n            call_func_with_variable_args(\n                self.func, input, ensure_config(config), **kwargs\n            )\n        return self._call_with_config(identity, input, config)\n\n    async def ainvoke(\n        self,\n        input: Other,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Other:\n        if self.afunc is not None:\n            await acall_func_with_variable_args(\n                self.afunc, input, ensure_config(config), **kwargs\n            )\n        elif self.func is not None:\n            call_func_with_variable_args(\n                self.func, input, ensure_config(config), **kwargs\n            )\n        return await self._acall_with_config(aidentity, input, config)\n\n    def transform(\n        self,\n        input: Iterator[Other],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Other]:\n        if self.func is None:\n            for chunk in self._transform_stream_with_config(input, identity, config):\n                yield chunk\n        else:\n            final: Other\n            got_first_chunk = False\n\n            for chunk in self._transform_stream_with_config(input, identity, config):\n                yield chunk\n\n                if not got_first_chunk:\n                    final = chunk\n                    got_first_chunk = True\n                else:\n                    try:\n                        final = final + chunk  # type: ignore[operator]\n                    except TypeError:\n                        final = chunk\n\n            if got_first_chunk:\n                call_func_with_variable_args(\n                    self.func, final, ensure_config(config), **kwargs\n                )\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Other],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Other]:\n        if self.afunc is None and self.func is None:\n            async for chunk in self._atransform_stream_with_config(\n                input, identity, config\n            ):\n                yield chunk\n        else:\n            got_first_chunk = False\n\n            async for chunk in self._atransform_stream_with_config(\n                input, identity, config\n            ):\n                yield chunk\n\n                # By definitions, a function will operate on the aggregated\n                # input. So we'll aggregate the input until we get to the last\n                # chunk.\n                # If the input is not addable, then we'll assume that we can\n                # only operate on the last chunk.\n                if not got_first_chunk:\n                    final = chunk\n                    got_first_chunk = True\n                else:\n                    try:\n                        final = final + chunk  # type: ignore[operator]\n                    except TypeError:\n                        final = chunk\n\n            if got_first_chunk:\n                config = ensure_config(config)\n                if self.afunc is not None:\n                    await acall_func_with_variable_args(\n                        self.afunc, final, config, **kwargs\n                    )\n                elif self.func is not None:\n                    call_func_with_variable_args(self.func, final, config, **kwargs)\n\n    def stream(\n        self,\n        input: Other,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Other]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    async def astream(\n        self,\n        input: Other,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Other]:\n        async def input_aiter() -> AsyncIterator[Other]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\n_graph_passthrough: RunnablePassthrough = RunnablePassthrough()\n\n\nclass RunnableAssign(RunnableSerializable[dict[str, Any], dict[str, Any]]):\n    \"\"\"Runnable that assigns key-value pairs to Dict[str, Any] inputs.\n\n    The `RunnableAssign` class takes input dictionaries and, through a\n    `RunnableParallel` instance, applies transformations, then combines\n    these with the original data, introducing new key-value pairs based\n    on the mapper's logic.\n\n    Parameters:\n        mapper (RunnableParallel[Dict[str, Any]]): A `RunnableParallel` instance\n            that will be used to transform the input dictionary.\n\n    Examples:\n        .. code-block:: python\n\n            # This is a RunnableAssign\n            from typing import Dict\n            from langchain_core.runnables.passthrough import (\n                RunnableAssign,\n                RunnableParallel,\n            )\n            from langchain_core.runnables.base import RunnableLambda\n\n            def add_ten(x: Dict[str, int]) -> Dict[str, int]:\n                return {\"added\": x[\"input\"] + 10}\n\n            mapper = RunnableParallel(\n                {\"add_step\": RunnableLambda(add_ten),}\n            )\n\n            runnable_assign = RunnableAssign(mapper)\n\n            # Synchronous example\n            runnable_assign.invoke({\"input\": 5})\n            # returns {'input': 5, 'add_step': {'added': 15}}\n\n            # Asynchronous example\n            await runnable_assign.ainvoke({\"input\": 5})\n            # returns {'input': 5, 'add_step': {'added': 15}}\n    \"\"\"\n\n    mapper: RunnableParallel\n\n    def __init__(self, mapper: RunnableParallel[dict[str, Any]], **kwargs: Any) -> None:\n        super().__init__(mapper=mapper, **kwargs)  # type: ignore[call-arg]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        name = (\n            name\n            or self.name\n            or f\"RunnableAssign<{','.join(self.mapper.steps__.keys())}>\"\n        )\n        return super().get_name(suffix, name=name)\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        map_input_schema = self.mapper.get_input_schema(config)\n        if not issubclass(map_input_schema, RootModel):\n            # ie. it's a dict\n            return map_input_schema\n\n        return super().get_input_schema(config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        map_input_schema = self.mapper.get_input_schema(config)\n        map_output_schema = self.mapper.get_output_schema(config)\n        if not issubclass(map_input_schema, RootModel) and not issubclass(\n            map_output_schema, RootModel\n        ):\n            fields = {}\n\n            for name, field_info in map_input_schema.model_fields.items():\n                fields[name] = (field_info.annotation, field_info.default)\n\n            for name, field_info in map_output_schema.model_fields.items():\n                fields[name] = (field_info.annotation, field_info.default)\n\n            return create_model_v2(  # type: ignore[call-overload]\n                \"RunnableAssignOutput\", field_definitions=fields\n            )\n        elif not issubclass(map_output_schema, RootModel):\n            # ie. only map output is a dict\n            # ie. input type is either unknown or inferred incorrectly\n            return map_output_schema\n\n        return super().get_output_schema(config)\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return self.mapper.config_specs\n\n    def get_graph(self, config: RunnableConfig | None = None) -> Graph:\n        # get graph from mapper\n        graph = self.mapper.get_graph(config)\n        # add passthrough node and edges\n        input_node = graph.first_node()\n        output_node = graph.last_node()\n        if input_node is not None and output_node is not None:\n            passthrough_node = graph.add_node(_graph_passthrough)\n            graph.add_edge(input_node, passthrough_node)\n            graph.add_edge(passthrough_node, output_node)\n        return graph\n\n    def _invoke(\n        self,\n        input: dict[str, Any],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> dict[str, Any]:\n        if not isinstance(input, dict):\n            msg = \"The input to RunnablePassthrough.assign() must be a dict.\"\n            raise ValueError(msg)  # noqa: TRY004\n\n        return {\n            **input,\n            **self.mapper.invoke(\n                input,\n                patch_config(config, callbacks=run_manager.get_child()),\n                **kwargs,\n            ),\n        }\n\n    def invoke(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> dict[str, Any]:\n        return self._call_with_config(self._invoke, input, config, **kwargs)\n\n    async def _ainvoke(\n        self,\n        input: dict[str, Any],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> dict[str, Any]:\n        if not isinstance(input, dict):\n            msg = \"The input to RunnablePassthrough.assign() must be a dict.\"\n            raise ValueError(msg)  # noqa: TRY004\n\n        return {\n            **input,\n            **await self.mapper.ainvoke(\n                input,\n                patch_config(config, callbacks=run_manager.get_child()),\n                **kwargs,\n            ),\n        }\n\n    async def ainvoke(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> dict[str, Any]:\n        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)\n\n    def _transform(\n        self,\n        input: Iterator[dict[str, Any]],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Iterator[dict[str, Any]]:\n        # collect mapper keys\n        mapper_keys = set(self.mapper.steps__.keys())\n        # create two streams, one for the map and one for the passthrough\n        for_passthrough, for_map = safetee(input, 2, lock=threading.Lock())\n\n        # create map output stream\n        map_output = self.mapper.transform(\n            for_map,\n            patch_config(\n                config,\n                callbacks=run_manager.get_child(),\n            ),\n            **kwargs,\n        )\n\n        # get executor to start map output stream in background\n        with get_executor_for_config(config) as executor:\n            # start map output stream\n            first_map_chunk_future = executor.submit(\n                next,\n                map_output,  # type: ignore\n                None,\n            )\n            # consume passthrough stream\n            for chunk in for_passthrough:\n                if not isinstance(chunk, dict):\n                    msg = \"The input to RunnablePassthrough.assign() must be a dict.\"\n                    raise ValueError(msg)  # noqa: TRY004\n                # remove mapper keys from passthrough chunk, to be overwritten by map\n                filtered = AddableDict(\n                    {k: v for k, v in chunk.items() if k not in mapper_keys}\n                )\n                if filtered:\n                    yield filtered\n            # yield map output\n            yield cast(dict[str, Any], first_map_chunk_future.result())\n            for chunk in map_output:\n                yield chunk\n\n    def transform(\n        self,\n        input: Iterator[dict[str, Any]],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any | None,\n    ) -> Iterator[dict[str, Any]]:\n        yield from self._transform_stream_with_config(\n            input, self._transform, config, **kwargs\n        )\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[dict[str, Any]],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        # collect mapper keys\n        mapper_keys = set(self.mapper.steps__.keys())\n        # create two streams, one for the map and one for the passthrough\n        for_passthrough, for_map = atee(input, 2, lock=asyncio.Lock())\n        # create map output stream\n        map_output = self.mapper.atransform(\n            for_map,\n            patch_config(\n                config,\n                callbacks=run_manager.get_child(),\n            ),\n            **kwargs,\n        )\n        # start map output stream\n        first_map_chunk_task: asyncio.Task = asyncio.create_task(\n            py_anext(map_output, None),  # type: ignore[arg-type]\n        )\n        # consume passthrough stream\n        async for chunk in for_passthrough:\n            if not isinstance(chunk, dict):\n                msg = \"The input to RunnablePassthrough.assign() must be a dict.\"\n                raise ValueError(msg)  # noqa: TRY004\n\n            # remove mapper keys from passthrough chunk, to be overwritten by map output\n            filtered = AddableDict(\n                {k: v for k, v in chunk.items() if k not in mapper_keys}\n            )\n            if filtered:\n                yield filtered\n        # yield map output\n        yield await first_map_chunk_task\n        async for chunk in map_output:\n            yield chunk\n\n    async def atransform(\n        self,\n        input: AsyncIterator[dict[str, Any]],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        async for chunk in self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        ):\n            yield chunk\n\n    def stream(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[dict[str, Any]]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    async def astream(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        async def input_aiter() -> AsyncIterator[dict[str, Any]]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\nclass RunnablePick(RunnableSerializable[dict[str, Any], dict[str, Any]]):\n    \"\"\"Runnable that picks keys from Dict[str, Any] inputs.\n\n    RunnablePick class represents a Runnable that selectively picks keys from a\n    dictionary input. It allows you to specify one or more keys to extract\n    from the input dictionary. It returns a new dictionary containing only\n    the selected keys.\n\n    Parameters:\n        keys (Union[str, List[str]]): A single key or a list of keys to pick from\n            the input dictionary.\n\n    Example :\n        .. code-block:: python\n\n            from langchain_core.runnables.passthrough import RunnablePick\n\n            input_data = {\n                'name': 'John',\n                'age': 30,\n                'city': 'New York',\n                'country': 'USA'\n            }\n\n            runnable = RunnablePick(keys=['name', 'age'])\n\n            output_data = runnable.invoke(input_data)\n\n            print(output_data)  # Output: {'name': 'John', 'age': 30}\n    \"\"\"\n\n    keys: Union[str, list[str]]\n\n    def __init__(self, keys: Union[str, list[str]], **kwargs: Any) -> None:\n        super().__init__(keys=keys, **kwargs)  # type: ignore[call-arg]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        name = (\n            name\n            or self.name\n            or f\"RunnablePick<{','.join([self.keys] if isinstance(self.keys, str) else self.keys)}>\"  # noqa: E501\n        )\n        return super().get_name(suffix, name=name)\n\n    def _pick(self, input: dict[str, Any]) -> Any:\n        if not isinstance(input, dict):\n            msg = \"The input to RunnablePassthrough.assign() must be a dict.\"\n            raise ValueError(msg)  # noqa: TRY004\n\n        if isinstance(self.keys, str):\n            return input.get(self.keys)\n        else:\n            picked = {k: input.get(k) for k in self.keys if k in input}\n            if picked:\n                return AddableDict(picked)\n            else:\n                return None\n\n    def _invoke(\n        self,\n        input: dict[str, Any],\n    ) -> dict[str, Any]:\n        return self._pick(input)\n\n    def invoke(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> dict[str, Any]:\n        return self._call_with_config(self._invoke, input, config, **kwargs)\n\n    async def _ainvoke(\n        self,\n        input: dict[str, Any],\n    ) -> dict[str, Any]:\n        return self._pick(input)\n\n    async def ainvoke(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> dict[str, Any]:\n        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)\n\n    def _transform(\n        self,\n        input: Iterator[dict[str, Any]],\n    ) -> Iterator[dict[str, Any]]:\n        for chunk in input:\n            picked = self._pick(chunk)\n            if picked is not None:\n                yield picked\n\n    def transform(\n        self,\n        input: Iterator[dict[str, Any]],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[dict[str, Any]]:\n        yield from self._transform_stream_with_config(\n            input, self._transform, config, **kwargs\n        )\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[dict[str, Any]],\n    ) -> AsyncIterator[dict[str, Any]]:\n        async for chunk in input:\n            picked = self._pick(chunk)\n            if picked is not None:\n                yield picked\n\n    async def atransform(\n        self,\n        input: AsyncIterator[dict[str, Any]],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        async for chunk in self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        ):\n            yield chunk\n\n    def stream(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[dict[str, Any]]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    async def astream(\n        self,\n        input: dict[str, Any],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        async def input_aiter() -> AsyncIterator[dict[str, Any]]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n",
        "patch": "@@ -5,7 +5,7 @@\n import asyncio\n import inspect\n import threading\n-from collections.abc import AsyncIterator, Awaitable, Iterator, Mapping\n+from collections.abc import Awaitable\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -32,7 +32,6 @@\n     get_executor_for_config,\n     patch_config,\n )\n-from langchain_core.runnables.graph import Graph\n from langchain_core.runnables.utils import (\n     AddableDict,\n     ConfigurableFieldSpec,\n@@ -42,10 +41,13 @@\n from langchain_core.utils.pydantic import create_model_v2\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Iterator, Mapping\n+\n     from langchain_core.callbacks.manager import (\n         AsyncCallbackManagerForChainRun,\n         CallbackManagerForChainRun,\n     )\n+    from langchain_core.runnables.graph import Graph\n \n \n def identity(x: Other) -> Other:"
      },
      {
        "filename": "libs/core/langchain_core/runnables/router.py",
        "content_before": "from __future__ import annotations\n\nfrom collections.abc import AsyncIterator, Iterator, Mapping\nfrom itertools import starmap\nfrom typing import (\n    Any,\n    Callable,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom pydantic import ConfigDict\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables.base import (\n    Input,\n    Output,\n    Runnable,\n    RunnableSerializable,\n    coerce_to_runnable,\n)\nfrom langchain_core.runnables.config import (\n    RunnableConfig,\n    get_config_list,\n    get_executor_for_config,\n)\nfrom langchain_core.runnables.utils import (\n    ConfigurableFieldSpec,\n    gather_with_concurrency,\n    get_unique_config_specs,\n)\n\n\nclass RouterInput(TypedDict):\n    \"\"\"Router input.\n\n    Attributes:\n        key: The key to route on.\n        input: The input to pass to the selected Runnable.\n    \"\"\"\n\n    key: str\n    input: Any\n\n\nclass RouterRunnable(RunnableSerializable[RouterInput, Output]):\n    \"\"\"Runnable that routes to a set of Runnables based on Input['key'].\n    Returns the output of the selected Runnable.\n\n    Parameters:\n        runnables: A mapping of keys to Runnables.\n\n    For example,\n\n    .. code-block:: python\n\n        from langchain_core.runnables.router import RouterRunnable\n        from langchain_core.runnables import RunnableLambda\n\n        add = RunnableLambda(func=lambda x: x + 1)\n        square = RunnableLambda(func=lambda x: x**2)\n\n        router = RouterRunnable(runnables={\"add\": add, \"square\": square})\n        router.invoke({\"key\": \"square\", \"input\": 3})\n    \"\"\"\n\n    runnables: Mapping[str, Runnable[Any, Output]]\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return get_unique_config_specs(\n            spec for step in self.runnables.values() for spec in step.config_specs\n        )\n\n    def __init__(\n        self,\n        runnables: Mapping[str, Union[Runnable[Any, Output], Callable[[Any], Output]]],\n    ) -> None:\n        super().__init__(  # type: ignore[call-arg]\n            runnables={key: coerce_to_runnable(r) for key, r in runnables.items()}\n        )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this class is serializable.\"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def invoke(\n        self, input: RouterInput, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        key = input[\"key\"]\n        actual_input = input[\"input\"]\n        if key not in self.runnables:\n            msg = f\"No runnable associated with key '{key}'\"\n            raise ValueError(msg)\n\n        runnable = self.runnables[key]\n        return runnable.invoke(actual_input, config)\n\n    async def ainvoke(\n        self,\n        input: RouterInput,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        key = input[\"key\"]\n        actual_input = input[\"input\"]\n        if key not in self.runnables:\n            msg = f\"No runnable associated with key '{key}'\"\n            raise ValueError(msg)\n\n        runnable = self.runnables[key]\n        return await runnable.ainvoke(actual_input, config)\n\n    def batch(\n        self,\n        inputs: list[RouterInput],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if not inputs:\n            return []\n\n        keys = [input[\"key\"] for input in inputs]\n        actual_inputs = [input[\"input\"] for input in inputs]\n        if any(key not in self.runnables for key in keys):\n            msg = \"One or more keys do not have a corresponding runnable\"\n            raise ValueError(msg)\n\n        def invoke(\n            runnable: Runnable, input: Input, config: RunnableConfig\n        ) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return runnable.invoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return runnable.invoke(input, config, **kwargs)\n\n        runnables = [self.runnables[key] for key in keys]\n        configs = get_config_list(config, len(inputs))\n        with get_executor_for_config(configs[0]) as executor:\n            return cast(\n                list[Output],\n                list(executor.map(invoke, runnables, actual_inputs, configs)),\n            )\n\n    async def abatch(\n        self,\n        inputs: list[RouterInput],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if not inputs:\n            return []\n\n        keys = [input[\"key\"] for input in inputs]\n        actual_inputs = [input[\"input\"] for input in inputs]\n        if any(key not in self.runnables for key in keys):\n            msg = \"One or more keys do not have a corresponding runnable\"\n            raise ValueError(msg)\n\n        async def ainvoke(\n            runnable: Runnable, input: Input, config: RunnableConfig\n        ) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return await runnable.ainvoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return await runnable.ainvoke(input, config, **kwargs)\n\n        runnables = [self.runnables[key] for key in keys]\n        configs = get_config_list(config, len(inputs))\n        return await gather_with_concurrency(\n            configs[0].get(\"max_concurrency\"),\n            *starmap(ainvoke, zip(runnables, actual_inputs, configs)),\n        )\n\n    def stream(\n        self,\n        input: RouterInput,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        key = input[\"key\"]\n        actual_input = input[\"input\"]\n        if key not in self.runnables:\n            msg = f\"No runnable associated with key '{key}'\"\n            raise ValueError(msg)\n\n        runnable = self.runnables[key]\n        yield from runnable.stream(actual_input, config)\n\n    async def astream(\n        self,\n        input: RouterInput,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        key = input[\"key\"]\n        actual_input = input[\"input\"]\n        if key not in self.runnables:\n            msg = f\"No runnable associated with key '{key}'\"\n            raise ValueError(msg)\n\n        runnable = self.runnables[key]\n        async for output in runnable.astream(actual_input, config):\n            yield output\n",
        "patch": "@@ -1,8 +1,9 @@\n from __future__ import annotations\n \n-from collections.abc import AsyncIterator, Iterator, Mapping\n+from collections.abc import Mapping\n from itertools import starmap\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     Optional,\n@@ -31,6 +32,9 @@\n     get_unique_config_specs,\n )\n \n+if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Iterator\n+\n \n class RouterInput(TypedDict):\n     \"\"\"Router input."
      },
      {
        "filename": "libs/core/langchain_core/runnables/schema.py",
        "content_before": "\"\"\"Module contains typedefs that are used with Runnables.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Sequence\nfrom typing import Any, Literal, Union\n\nfrom typing_extensions import NotRequired, TypedDict\n\n\nclass EventData(TypedDict, total=False):\n    \"\"\"Data associated with a streaming event.\"\"\"\n\n    input: Any\n    \"\"\"The input passed to the Runnable that generated the event.\n\n    Inputs will sometimes be available at the *START* of the Runnable, and\n    sometimes at the *END* of the Runnable.\n\n    If a Runnable is able to stream its inputs, then its input by definition\n    won't be known until the *END* of the Runnable when it has finished streaming\n    its inputs.\n    \"\"\"\n    output: Any\n    \"\"\"The output of the Runnable that generated the event.\n\n    Outputs will only be available at the *END* of the Runnable.\n\n    For most Runnables, this field can be inferred from the `chunk` field,\n    though there might be some exceptions for special cased Runnables (e.g., like\n    chat models), which may return more information.\n    \"\"\"\n    chunk: Any\n    \"\"\"A streaming chunk from the output that generated the event.\n\n    chunks support addition in general, and adding them up should result\n    in the output of the Runnable that generated the event.\n    \"\"\"\n\n\nclass BaseStreamEvent(TypedDict):\n    \"\"\"Streaming event.\n\n    Schema of a streaming event which is produced from the astream_events method.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            async def reverse(s: str) -> str:\n                return s[::-1]\n\n            chain = RunnableLambda(func=reverse)\n\n            events = [event async for event in chain.astream_events(\"hello\")]\n\n            # will produce the following events\n            # (where some fields have been omitted for brevity):\n            [\n                {\n                    \"data\": {\"input\": \"hello\"},\n                    \"event\": \"on_chain_start\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"chunk\": \"olleh\"},\n                    \"event\": \"on_chain_stream\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"output\": \"olleh\"},\n                    \"event\": \"on_chain_end\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n            ]\n    \"\"\"\n\n    event: str\n    \"\"\"Event names are of the format: on_[runnable_type]_(start|stream|end).\n\n    Runnable types are one of:\n\n    - **llm** - used by non chat models\n    - **chat_model** - used by chat models\n    - **prompt** --  e.g., ChatPromptTemplate\n    - **tool** -- from tools defined via @tool decorator or inheriting\n        from Tool/BaseTool\n    - **chain** - most Runnables are of this type\n\n    Further, the events are categorized as one of:\n\n    - **start** - when the Runnable starts\n    - **stream** - when the Runnable is streaming\n    - **end* - when the Runnable ends\n\n    start, stream and end are associated with slightly different `data` payload.\n\n    Please see the documentation for `EventData` for more details.\n    \"\"\"\n    run_id: str\n    \"\"\"An randomly generated ID to keep track of the execution of the given Runnable.\n\n    Each child Runnable that gets invoked as part of the execution of a parent Runnable\n    is assigned its own unique ID.\n    \"\"\"\n    tags: NotRequired[list[str]]\n    \"\"\"Tags associated with the Runnable that generated this event.\n\n    Tags are always inherited from parent Runnables.\n\n    Tags can either be bound to a Runnable using `.with_config({\"tags\":  [\"hello\"]})`\n    or passed at run time using `.astream_events(..., {\"tags\": [\"hello\"]})`.\n    \"\"\"\n    metadata: NotRequired[dict[str, Any]]\n    \"\"\"Metadata associated with the Runnable that generated this event.\n\n    Metadata can either be bound to a Runnable using\n\n        `.with_config({\"metadata\": { \"foo\": \"bar\" }})`\n\n    or passed at run time using\n\n        `.astream_events(..., {\"metadata\": {\"foo\": \"bar\"}})`.\n    \"\"\"\n\n    parent_ids: Sequence[str]\n    \"\"\"A list of the parent IDs associated with this event.\n\n    Root Events will have an empty list.\n\n    For example, if a Runnable A calls Runnable B, then the event generated by Runnable\n    B will have Runnable A's ID in the parent_ids field.\n\n    The order of the parent IDs is from the root parent to the immediate parent.\n\n    Only supported as of v2 of the astream events API. v1 will return an empty list.\n    \"\"\"\n\n\nclass StandardStreamEvent(BaseStreamEvent):\n    \"\"\"A standard stream event that follows LangChain convention for event data.\"\"\"\n\n    data: EventData\n    \"\"\"Event data.\n\n    The contents of the event data depend on the event type.\n    \"\"\"\n    name: str\n    \"\"\"The name of the Runnable that generated the event.\"\"\"\n\n\nclass CustomStreamEvent(BaseStreamEvent):\n    \"\"\"Custom stream event created by the user.\n\n    .. versionadded:: 0.2.15\n    \"\"\"\n\n    # Overwrite the event field to be more specific.\n    event: Literal[\"on_custom_event\"]  # type: ignore[misc]\n    \"\"\"The event type.\"\"\"\n    name: str\n    \"\"\"User defined name for the event.\"\"\"\n    data: Any\n    \"\"\"The data associated with the event. Free form and can be anything.\"\"\"\n\n\nStreamEvent = Union[StandardStreamEvent, CustomStreamEvent]\n",
        "patch": "@@ -2,11 +2,13 @@\n \n from __future__ import annotations\n \n-from collections.abc import Sequence\n-from typing import Any, Literal, Union\n+from typing import TYPE_CHECKING, Any, Literal, Union\n \n from typing_extensions import NotRequired, TypedDict\n \n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n \n class EventData(TypedDict, total=False):\n     \"\"\"Data associated with a streaming event.\"\"\""
      },
      {
        "filename": "libs/core/langchain_core/runnables/utils.py",
        "content_before": "\"\"\"Utility code for runnables.\"\"\"\n\nfrom __future__ import annotations\n\nimport ast\nimport asyncio\nimport inspect\nimport textwrap\nfrom collections.abc import (\n    AsyncIterable,\n    AsyncIterator,\n    Awaitable,\n    Coroutine,\n    Iterable,\n    Mapping,\n    Sequence,\n)\nfrom functools import lru_cache\nfrom inspect import signature\nfrom itertools import groupby\nfrom typing import (\n    Any,\n    Callable,\n    NamedTuple,\n    Optional,\n    Protocol,\n    TypeVar,\n    Union,\n)\n\nfrom typing_extensions import TypeGuard, override\n\nfrom langchain_core.runnables.schema import StreamEvent\n\n# Re-export create-model for backwards compatibility\nfrom langchain_core.utils.pydantic import create_model as create_model\n\nInput = TypeVar(\"Input\", contravariant=True)\n# Output type should implement __concat__, as eg str, list, dict do\nOutput = TypeVar(\"Output\", covariant=True)\n\n\nasync def gated_coro(semaphore: asyncio.Semaphore, coro: Coroutine) -> Any:\n    \"\"\"Run a coroutine with a semaphore.\n\n    Args:\n        semaphore: The semaphore to use.\n        coro: The coroutine to run.\n\n    Returns:\n        The result of the coroutine.\n    \"\"\"\n    async with semaphore:\n        return await coro\n\n\nasync def gather_with_concurrency(n: Union[int, None], *coros: Coroutine) -> list:\n    \"\"\"Gather coroutines with a limit on the number of concurrent coroutines.\n\n    Args:\n        n: The number of coroutines to run concurrently.\n        *coros: The coroutines to run.\n\n    Returns:\n        The results of the coroutines.\n    \"\"\"\n    if n is None:\n        return await asyncio.gather(*coros)\n\n    semaphore = asyncio.Semaphore(n)\n\n    return await asyncio.gather(*(gated_coro(semaphore, c) for c in coros))\n\n\ndef accepts_run_manager(callable: Callable[..., Any]) -> bool:\n    \"\"\"Check if a callable accepts a run_manager argument.\n\n    Args:\n        callable: The callable to check.\n\n    Returns:\n        bool: True if the callable accepts a run_manager argument, False otherwise.\n    \"\"\"\n    try:\n        return signature(callable).parameters.get(\"run_manager\") is not None\n    except ValueError:\n        return False\n\n\ndef accepts_config(callable: Callable[..., Any]) -> bool:\n    \"\"\"Check if a callable accepts a config argument.\n\n    Args:\n        callable: The callable to check.\n\n    Returns:\n        bool: True if the callable accepts a config argument, False otherwise.\n    \"\"\"\n    try:\n        return signature(callable).parameters.get(\"config\") is not None\n    except ValueError:\n        return False\n\n\ndef accepts_context(callable: Callable[..., Any]) -> bool:\n    \"\"\"Check if a callable accepts a context argument.\n\n    Args:\n        callable: The callable to check.\n\n    Returns:\n        bool: True if the callable accepts a context argument, False otherwise.\n    \"\"\"\n    try:\n        return signature(callable).parameters.get(\"context\") is not None\n    except ValueError:\n        return False\n\n\n@lru_cache(maxsize=1)\ndef asyncio_accepts_context() -> bool:\n    return accepts_context(asyncio.create_task)\n\n\nclass IsLocalDict(ast.NodeVisitor):\n    \"\"\"Check if a name is a local dict.\"\"\"\n\n    def __init__(self, name: str, keys: set[str]) -> None:\n        \"\"\"Initialize the visitor.\n\n        Args:\n            name: The name to check.\n            keys: The keys to populate.\n        \"\"\"\n        self.name = name\n        self.keys = keys\n\n    @override\n    def visit_Subscript(self, node: ast.Subscript) -> Any:\n        \"\"\"Visit a subscript node.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        if (\n            isinstance(node.ctx, ast.Load)\n            and isinstance(node.value, ast.Name)\n            and node.value.id == self.name\n            and isinstance(node.slice, ast.Constant)\n            and isinstance(node.slice.value, str)\n        ):\n            # we've found a subscript access on the name we're looking for\n            self.keys.add(node.slice.value)\n\n    @override\n    def visit_Call(self, node: ast.Call) -> Any:\n        \"\"\"Visit a call node.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        if (\n            isinstance(node.func, ast.Attribute)\n            and isinstance(node.func.value, ast.Name)\n            and node.func.value.id == self.name\n            and node.func.attr == \"get\"\n            and len(node.args) in (1, 2)\n            and isinstance(node.args[0], ast.Constant)\n            and isinstance(node.args[0].value, str)\n        ):\n            # we've found a .get() call on the name we're looking for\n            self.keys.add(node.args[0].value)\n\n\nclass IsFunctionArgDict(ast.NodeVisitor):\n    \"\"\"Check if the first argument of a function is a dict.\"\"\"\n\n    def __init__(self) -> None:\n        self.keys: set[str] = set()\n\n    @override\n    def visit_Lambda(self, node: ast.Lambda) -> Any:\n        \"\"\"Visit a lambda function.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        if not node.args.args:\n            return\n        input_arg_name = node.args.args[0].arg\n        IsLocalDict(input_arg_name, self.keys).visit(node.body)\n\n    @override\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:\n        \"\"\"Visit a function definition.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        if not node.args.args:\n            return\n        input_arg_name = node.args.args[0].arg\n        IsLocalDict(input_arg_name, self.keys).visit(node)\n\n    @override\n    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:\n        \"\"\"Visit an async function definition.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        if not node.args.args:\n            return\n        input_arg_name = node.args.args[0].arg\n        IsLocalDict(input_arg_name, self.keys).visit(node)\n\n\nclass NonLocals(ast.NodeVisitor):\n    \"\"\"Get nonlocal variables accessed.\"\"\"\n\n    def __init__(self) -> None:\n        self.loads: set[str] = set()\n        self.stores: set[str] = set()\n\n    @override\n    def visit_Name(self, node: ast.Name) -> Any:\n        \"\"\"Visit a name node.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        if isinstance(node.ctx, ast.Load):\n            self.loads.add(node.id)\n        elif isinstance(node.ctx, ast.Store):\n            self.stores.add(node.id)\n\n    @override\n    def visit_Attribute(self, node: ast.Attribute) -> Any:\n        \"\"\"Visit an attribute node.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        if isinstance(node.ctx, ast.Load):\n            parent = node.value\n            attr_expr = node.attr\n            while isinstance(parent, ast.Attribute):\n                attr_expr = parent.attr + \".\" + attr_expr\n                parent = parent.value\n            if isinstance(parent, ast.Name):\n                self.loads.add(parent.id + \".\" + attr_expr)\n                self.loads.discard(parent.id)\n            elif isinstance(parent, ast.Call):\n                if isinstance(parent.func, ast.Name):\n                    self.loads.add(parent.func.id)\n                else:\n                    parent = parent.func\n                    attr_expr = \"\"\n                    while isinstance(parent, ast.Attribute):\n                        if attr_expr:\n                            attr_expr = parent.attr + \".\" + attr_expr\n                        else:\n                            attr_expr = parent.attr\n                        parent = parent.value\n                    if isinstance(parent, ast.Name):\n                        self.loads.add(parent.id + \".\" + attr_expr)\n\n\nclass FunctionNonLocals(ast.NodeVisitor):\n    \"\"\"Get the nonlocal variables accessed of a function.\"\"\"\n\n    def __init__(self) -> None:\n        self.nonlocals: set[str] = set()\n\n    @override\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:\n        \"\"\"Visit a function definition.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        visitor = NonLocals()\n        visitor.visit(node)\n        self.nonlocals.update(visitor.loads - visitor.stores)\n\n    @override\n    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:\n        \"\"\"Visit an async function definition.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        visitor = NonLocals()\n        visitor.visit(node)\n        self.nonlocals.update(visitor.loads - visitor.stores)\n\n    @override\n    def visit_Lambda(self, node: ast.Lambda) -> Any:\n        \"\"\"Visit a lambda function.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        visitor = NonLocals()\n        visitor.visit(node)\n        self.nonlocals.update(visitor.loads - visitor.stores)\n\n\nclass GetLambdaSource(ast.NodeVisitor):\n    \"\"\"Get the source code of a lambda function.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the visitor.\"\"\"\n        self.source: Optional[str] = None\n        self.count = 0\n\n    @override\n    def visit_Lambda(self, node: ast.Lambda) -> Any:\n        \"\"\"Visit a lambda function.\n\n        Args:\n            node: The node to visit.\n\n        Returns:\n            Any: The result of the visit.\n        \"\"\"\n        self.count += 1\n        if hasattr(ast, \"unparse\"):\n            self.source = ast.unparse(node)\n\n\ndef get_function_first_arg_dict_keys(func: Callable) -> Optional[list[str]]:\n    \"\"\"Get the keys of the first argument of a function if it is a dict.\n\n    Args:\n        func: The function to check.\n\n    Returns:\n        Optional[List[str]]: The keys of the first argument if it is a dict,\n            None otherwise.\n    \"\"\"\n    try:\n        code = inspect.getsource(func)\n        tree = ast.parse(textwrap.dedent(code))\n        visitor = IsFunctionArgDict()\n        visitor.visit(tree)\n        return sorted(visitor.keys) if visitor.keys else None\n    except (SyntaxError, TypeError, OSError, SystemError):\n        return None\n\n\ndef get_lambda_source(func: Callable) -> Optional[str]:\n    \"\"\"Get the source code of a lambda function.\n\n    Args:\n        func: a Callable that can be a lambda function.\n\n    Returns:\n        str: the source code of the lambda function.\n    \"\"\"\n    try:\n        name = func.__name__ if func.__name__ != \"<lambda>\" else None\n    except AttributeError:\n        name = None\n    try:\n        code = inspect.getsource(func)\n        tree = ast.parse(textwrap.dedent(code))\n        visitor = GetLambdaSource()\n        visitor.visit(tree)\n    except (SyntaxError, TypeError, OSError, SystemError):\n        return name\n    return visitor.source if visitor.count == 1 else name\n\n\n@lru_cache(maxsize=256)\ndef get_function_nonlocals(func: Callable) -> list[Any]:\n    \"\"\"Get the nonlocal variables accessed by a function.\n\n    Args:\n        func: The function to check.\n\n    Returns:\n        List[Any]: The nonlocal variables accessed by the function.\n    \"\"\"\n    try:\n        code = inspect.getsource(func)\n        tree = ast.parse(textwrap.dedent(code))\n        visitor = FunctionNonLocals()\n        visitor.visit(tree)\n        values: list[Any] = []\n        closure = (\n            inspect.getclosurevars(func.__wrapped__)\n            if hasattr(func, \"__wrapped__\") and callable(func.__wrapped__)\n            else inspect.getclosurevars(func)\n        )\n        candidates = {**closure.globals, **closure.nonlocals}\n        for k, v in candidates.items():\n            if k in visitor.nonlocals:\n                values.append(v)\n            for kk in visitor.nonlocals:\n                if \".\" in kk and kk.startswith(k):\n                    vv = v\n                    for part in kk.split(\".\")[1:]:\n                        if vv is None:\n                            break\n                        else:\n                            try:\n                                vv = getattr(vv, part)\n                            except AttributeError:\n                                break\n                    else:\n                        values.append(vv)\n    except (SyntaxError, TypeError, OSError, SystemError):\n        return []\n\n    return values\n\n\ndef indent_lines_after_first(text: str, prefix: str) -> str:\n    \"\"\"Indent all lines of text after the first line.\n\n    Args:\n        text: The text to indent.\n        prefix: Used to determine the number of spaces to indent.\n\n    Returns:\n        str: The indented text.\n    \"\"\"\n    n_spaces = len(prefix)\n    spaces = \" \" * n_spaces\n    lines = text.splitlines()\n    return \"\\n\".join([lines[0]] + [spaces + line for line in lines[1:]])\n\n\nclass AddableDict(dict[str, Any]):\n    \"\"\"Dictionary that can be added to another dictionary.\"\"\"\n\n    def __add__(self, other: AddableDict) -> AddableDict:\n        chunk = AddableDict(self)\n        for key in other:\n            if key not in chunk or chunk[key] is None:\n                chunk[key] = other[key]\n            elif other[key] is not None:\n                try:\n                    added = chunk[key] + other[key]\n                except TypeError:\n                    added = other[key]\n                chunk[key] = added\n        return chunk\n\n    def __radd__(self, other: AddableDict) -> AddableDict:\n        chunk = AddableDict(other)\n        for key in self:\n            if key not in chunk or chunk[key] is None:\n                chunk[key] = self[key]\n            elif self[key] is not None:\n                try:\n                    added = chunk[key] + self[key]\n                except TypeError:\n                    added = self[key]\n                chunk[key] = added\n        return chunk\n\n\n_T_co = TypeVar(\"_T_co\", covariant=True)\n_T_contra = TypeVar(\"_T_contra\", contravariant=True)\n\n\nclass SupportsAdd(Protocol[_T_contra, _T_co]):\n    \"\"\"Protocol for objects that support addition.\"\"\"\n\n    def __add__(self, __x: _T_contra) -> _T_co: ...\n\n\nAddable = TypeVar(\"Addable\", bound=SupportsAdd[Any, Any])\n\n\ndef add(addables: Iterable[Addable]) -> Optional[Addable]:\n    \"\"\"Add a sequence of addable objects together.\n\n    Args:\n        addables: The addable objects to add.\n\n    Returns:\n        Optional[Addable]: The result of adding the addable objects.\n    \"\"\"\n    final: Optional[Addable] = None\n    for chunk in addables:\n        final = chunk if final is None else final + chunk\n    return final\n\n\nasync def aadd(addables: AsyncIterable[Addable]) -> Optional[Addable]:\n    \"\"\"Asynchronously add a sequence of addable objects together.\n\n    Args:\n        addables: The addable objects to add.\n\n    Returns:\n        Optional[Addable]: The result of adding the addable objects.\n    \"\"\"\n    final: Optional[Addable] = None\n    async for chunk in addables:\n        final = chunk if final is None else final + chunk\n    return final\n\n\nclass ConfigurableField(NamedTuple):\n    \"\"\"Field that can be configured by the user.\n\n    Parameters:\n        id: The unique identifier of the field.\n        name: The name of the field. Defaults to None.\n        description: The description of the field. Defaults to None.\n        annotation: The annotation of the field. Defaults to None.\n        is_shared: Whether the field is shared. Defaults to False.\n    \"\"\"\n\n    id: str\n\n    name: Optional[str] = None\n    description: Optional[str] = None\n    annotation: Optional[Any] = None\n    is_shared: bool = False\n\n    def __hash__(self) -> int:\n        return hash((self.id, self.annotation))\n\n\nclass ConfigurableFieldSingleOption(NamedTuple):\n    \"\"\"Field that can be configured by the user with a default value.\n\n    Parameters:\n        id: The unique identifier of the field.\n        options: The options for the field.\n        default: The default value for the field.\n        name: The name of the field. Defaults to None.\n        description: The description of the field. Defaults to None.\n        is_shared: Whether the field is shared. Defaults to False.\n    \"\"\"\n\n    id: str\n    options: Mapping[str, Any]\n    default: str\n\n    name: Optional[str] = None\n    description: Optional[str] = None\n    is_shared: bool = False\n\n    def __hash__(self) -> int:\n        return hash((self.id, tuple(self.options.keys()), self.default))\n\n\nclass ConfigurableFieldMultiOption(NamedTuple):\n    \"\"\"Field that can be configured by the user with multiple default values.\n\n    Parameters:\n        id: The unique identifier of the field.\n        options: The options for the field.\n        default: The default values for the field.\n        name: The name of the field. Defaults to None.\n        description: The description of the field. Defaults to None.\n        is_shared: Whether the field is shared. Defaults to False.\n    \"\"\"\n\n    id: str\n    options: Mapping[str, Any]\n    default: Sequence[str]\n\n    name: Optional[str] = None\n    description: Optional[str] = None\n    is_shared: bool = False\n\n    def __hash__(self) -> int:\n        return hash((self.id, tuple(self.options.keys()), tuple(self.default)))\n\n\nAnyConfigurableField = Union[\n    ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption\n]\n\n\nclass ConfigurableFieldSpec(NamedTuple):\n    \"\"\"Field that can be configured by the user. It is a specification of a field.\n\n    Parameters:\n        id: The unique identifier of the field.\n        annotation: The annotation of the field.\n        name: The name of the field. Defaults to None.\n        description: The description of the field. Defaults to None.\n        default: The default value for the field. Defaults to None.\n        is_shared: Whether the field is shared. Defaults to False.\n        dependencies: The dependencies of the field. Defaults to None.\n    \"\"\"\n\n    id: str\n    annotation: Any\n\n    name: Optional[str] = None\n    description: Optional[str] = None\n    default: Any = None\n    is_shared: bool = False\n    dependencies: Optional[list[str]] = None\n\n\ndef get_unique_config_specs(\n    specs: Iterable[ConfigurableFieldSpec],\n) -> list[ConfigurableFieldSpec]:\n    \"\"\"Get the unique config specs from a sequence of config specs.\n\n    Args:\n        specs: The config specs.\n\n    Returns:\n        List[ConfigurableFieldSpec]: The unique config specs.\n\n    Raises:\n        ValueError: If the runnable sequence contains conflicting config specs.\n    \"\"\"\n    grouped = groupby(\n        sorted(specs, key=lambda s: (s.id, *(s.dependencies or []))), lambda s: s.id\n    )\n    unique: list[ConfigurableFieldSpec] = []\n    for id, dupes in grouped:\n        first = next(dupes)\n        others = list(dupes)\n        if len(others) == 0 or all(o == first for o in others):\n            unique.append(first)\n        else:\n            msg = (\n                \"RunnableSequence contains conflicting config specs\"\n                f\"for {id}: {[first] + others}\"\n            )\n            raise ValueError(msg)\n    return unique\n\n\nclass _RootEventFilter:\n    def __init__(\n        self,\n        *,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n    ) -> None:\n        \"\"\"Utility to filter the root event in the astream_events implementation.\n\n        This is simply binding the arguments to the namespace to make save on\n        a bit of typing in the astream_events implementation.\n        \"\"\"\n        self.include_names = include_names\n        self.include_types = include_types\n        self.include_tags = include_tags\n        self.exclude_names = exclude_names\n        self.exclude_types = exclude_types\n        self.exclude_tags = exclude_tags\n\n    def include_event(self, event: StreamEvent, root_type: str) -> bool:\n        \"\"\"Determine whether to include an event.\"\"\"\n        if (\n            self.include_names is None\n            and self.include_types is None\n            and self.include_tags is None\n        ):\n            include = True\n        else:\n            include = False\n\n        event_tags = event.get(\"tags\") or []\n\n        if self.include_names is not None:\n            include = include or event[\"name\"] in self.include_names\n        if self.include_types is not None:\n            include = include or root_type in self.include_types\n        if self.include_tags is not None:\n            include = include or any(tag in self.include_tags for tag in event_tags)\n\n        if self.exclude_names is not None:\n            include = include and event[\"name\"] not in self.exclude_names\n        if self.exclude_types is not None:\n            include = include and root_type not in self.exclude_types\n        if self.exclude_tags is not None:\n            include = include and all(\n                tag not in self.exclude_tags for tag in event_tags\n            )\n\n        return include\n\n\ndef is_async_generator(\n    func: Any,\n) -> TypeGuard[Callable[..., AsyncIterator]]:\n    \"\"\"Check if a function is an async generator.\n\n    Args:\n        func: The function to check.\n\n    Returns:\n        TypeGuard[Callable[..., AsyncIterator]: True if the function is\n            an async generator, False otherwise.\n    \"\"\"\n    return (\n        inspect.isasyncgenfunction(func)\n        or hasattr(func, \"__call__\")  # noqa: B004\n        and inspect.isasyncgenfunction(func.__call__)\n    )\n\n\ndef is_async_callable(\n    func: Any,\n) -> TypeGuard[Callable[..., Awaitable]]:\n    \"\"\"Check if a function is async.\n\n    Args:\n        func: The function to check.\n\n    Returns:\n        TypeGuard[Callable[..., Awaitable]: True if the function is async,\n            False otherwise.\n    \"\"\"\n    return (\n        asyncio.iscoroutinefunction(func)\n        or hasattr(func, \"__call__\")  # noqa: B004\n        and asyncio.iscoroutinefunction(func.__call__)\n    )\n",
        "patch": "@@ -6,19 +6,11 @@\n import asyncio\n import inspect\n import textwrap\n-from collections.abc import (\n-    AsyncIterable,\n-    AsyncIterator,\n-    Awaitable,\n-    Coroutine,\n-    Iterable,\n-    Mapping,\n-    Sequence,\n-)\n from functools import lru_cache\n from inspect import signature\n from itertools import groupby\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     NamedTuple,\n@@ -30,11 +22,22 @@\n \n from typing_extensions import TypeGuard, override\n \n-from langchain_core.runnables.schema import StreamEvent\n-\n # Re-export create-model for backwards compatibility\n from langchain_core.utils.pydantic import create_model as create_model\n \n+if TYPE_CHECKING:\n+    from collections.abc import (\n+        AsyncIterable,\n+        AsyncIterator,\n+        Awaitable,\n+        Coroutine,\n+        Iterable,\n+        Mapping,\n+        Sequence,\n+    )\n+\n+    from langchain_core.runnables.schema import StreamEvent\n+\n Input = TypeVar(\"Input\", contravariant=True)\n # Output type should implement __concat__, as eg str, list, dict do\n Output = TypeVar(\"Output\", covariant=True)"
      },
      {
        "filename": "libs/core/langchain_core/structured_query.py",
        "content_before": "\"\"\"Internal representation of a structured query language.\"\"\"\n\nfrom __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom typing import Any, Optional, Union\n\nfrom pydantic import BaseModel\n\n\nclass Visitor(ABC):\n    \"\"\"Defines interface for IR translation using a visitor pattern.\"\"\"\n\n    allowed_comparators: Optional[Sequence[Comparator]] = None\n    \"\"\"Allowed comparators for the visitor.\"\"\"\n    allowed_operators: Optional[Sequence[Operator]] = None\n    \"\"\"Allowed operators for the visitor.\"\"\"\n\n    def _validate_func(self, func: Union[Operator, Comparator]) -> None:\n        if (\n            isinstance(func, Operator)\n            and self.allowed_operators is not None\n            and func not in self.allowed_operators\n        ):\n            msg = (\n                f\"Received disallowed operator {func}. Allowed \"\n                f\"comparators are {self.allowed_operators}\"\n            )\n            raise ValueError(msg)\n        if (\n            isinstance(func, Comparator)\n            and self.allowed_comparators is not None\n            and func not in self.allowed_comparators\n        ):\n            msg = (\n                f\"Received disallowed comparator {func}. Allowed \"\n                f\"comparators are {self.allowed_comparators}\"\n            )\n            raise ValueError(msg)\n\n    @abstractmethod\n    def visit_operation(self, operation: Operation) -> Any:\n        \"\"\"Translate an Operation.\n\n        Args:\n            operation: Operation to translate.\n        \"\"\"\n\n    @abstractmethod\n    def visit_comparison(self, comparison: Comparison) -> Any:\n        \"\"\"Translate a Comparison.\n\n        Args:\n            comparison: Comparison to translate.\n        \"\"\"\n\n    @abstractmethod\n    def visit_structured_query(self, structured_query: StructuredQuery) -> Any:\n        \"\"\"Translate a StructuredQuery.\n\n        Args:\n            structured_query: StructuredQuery to translate.\n        \"\"\"\n\n\ndef _to_snake_case(name: str) -> str:\n    \"\"\"Convert a name into snake_case.\"\"\"\n    snake_case = \"\"\n    for i, char in enumerate(name):\n        if char.isupper() and i != 0:\n            snake_case += \"_\" + char.lower()\n        else:\n            snake_case += char.lower()\n    return snake_case\n\n\nclass Expr(BaseModel):\n    \"\"\"Base class for all expressions.\"\"\"\n\n    def accept(self, visitor: Visitor) -> Any:\n        \"\"\"Accept a visitor.\n\n        Args:\n            visitor: visitor to accept.\n\n        Returns:\n            result of visiting.\n        \"\"\"\n        return getattr(visitor, f\"visit_{_to_snake_case(self.__class__.__name__)}\")(\n            self\n        )\n\n\nclass Operator(str, Enum):\n    \"\"\"Enumerator of the operations.\"\"\"\n\n    AND = \"and\"\n    OR = \"or\"\n    NOT = \"not\"\n\n\nclass Comparator(str, Enum):\n    \"\"\"Enumerator of the comparison operators.\"\"\"\n\n    EQ = \"eq\"\n    NE = \"ne\"\n    GT = \"gt\"\n    GTE = \"gte\"\n    LT = \"lt\"\n    LTE = \"lte\"\n    CONTAIN = \"contain\"\n    LIKE = \"like\"\n    IN = \"in\"\n    NIN = \"nin\"\n\n\nclass FilterDirective(Expr, ABC):\n    \"\"\"Filtering expression.\"\"\"\n\n\nclass Comparison(FilterDirective):\n    \"\"\"Comparison to a value.\n\n    Parameters:\n        comparator: The comparator to use.\n        attribute: The attribute to compare.\n        value: The value to compare to.\n    \"\"\"\n\n    comparator: Comparator\n    attribute: str\n    value: Any\n\n    def __init__(\n        self, comparator: Comparator, attribute: str, value: Any, **kwargs: Any\n    ) -> None:\n        # super exists from BaseModel\n        super().__init__(  # type: ignore[call-arg]\n            comparator=comparator, attribute=attribute, value=value, **kwargs\n        )\n\n\nclass Operation(FilterDirective):\n    \"\"\"Logical operation over other directives.\n\n    Parameters:\n        operator: The operator to use.\n        arguments: The arguments to the operator.\n    \"\"\"\n\n    operator: Operator\n    arguments: list[FilterDirective]\n\n    def __init__(\n        self, operator: Operator, arguments: list[FilterDirective], **kwargs: Any\n    ) -> None:\n        # super exists from BaseModel\n        super().__init__(  # type: ignore[call-arg]\n            operator=operator, arguments=arguments, **kwargs\n        )\n\n\nclass StructuredQuery(Expr):\n    \"\"\"Structured query.\"\"\"\n\n    query: str\n    \"\"\"Query string.\"\"\"\n    filter: Optional[FilterDirective]\n    \"\"\"Filtering expression.\"\"\"\n    limit: Optional[int]\n    \"\"\"Limit on the number of results.\"\"\"\n\n    def __init__(\n        self,\n        query: str,\n        filter: Optional[FilterDirective],\n        limit: Optional[int] = None,\n        **kwargs: Any,\n    ) -> None:\n        # super exists from BaseModel\n        super().__init__(  # type: ignore[call-arg]\n            query=query, filter=filter, limit=limit, **kwargs\n        )\n",
        "patch": "@@ -3,12 +3,14 @@\n from __future__ import annotations\n \n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n from enum import Enum\n-from typing import Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from pydantic import BaseModel\n \n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n \n class Visitor(ABC):\n     \"\"\"Defines interface for IR translation using a visitor pattern.\"\"\""
      },
      {
        "filename": "libs/core/langchain_core/tools/base.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport uuid\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom contextvars import copy_context\nfrom inspect import signature\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    PydanticDeprecationWarning,\n    SkipValidation,\n    ValidationError,\n    model_validator,\n    validate_arguments,\n)\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom pydantic.v1 import ValidationError as ValidationErrorV1\nfrom pydantic.v1 import validate_arguments as validate_arguments_v1\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManager,\n    BaseCallbackManager,\n    CallbackManager,\n    Callbacks,\n)\nfrom langchain_core.messages.tool import ToolCall, ToolMessage, ToolOutputMixin\nfrom langchain_core.runnables import (\n    RunnableConfig,\n    RunnableSerializable,\n    ensure_config,\n    patch_config,\n    run_in_executor,\n)\nfrom langchain_core.runnables.config import _set_config_context\nfrom langchain_core.runnables.utils import asyncio_accepts_context\nfrom langchain_core.utils.function_calling import (\n    _parse_google_docstring,\n    _py_38_safe_origin,\n)\nfrom langchain_core.utils.pydantic import (\n    TypeBaseModel,\n    _create_subset_model,\n    get_fields,\n    is_basemodel_subclass,\n    is_pydantic_v1_subclass,\n    is_pydantic_v2_subclass,\n)\n\nFILTERED_ARGS = (\"run_manager\", \"callbacks\")\n\n\nclass SchemaAnnotationError(TypeError):\n    \"\"\"Raised when 'args_schema' is missing or has an incorrect type annotation.\"\"\"\n\n\ndef _is_annotated_type(typ: type[Any]) -> bool:\n    return get_origin(typ) is Annotated\n\n\ndef _get_annotation_description(arg_type: type) -> str | None:\n    if _is_annotated_type(arg_type):\n        annotated_args = get_args(arg_type)\n        for annotation in annotated_args[1:]:\n            if isinstance(annotation, str):\n                return annotation\n    return None\n\n\ndef _get_filtered_args(\n    inferred_model: type[BaseModel],\n    func: Callable,\n    *,\n    filter_args: Sequence[str],\n    include_injected: bool = True,\n) -> dict:\n    \"\"\"Get the arguments from a function's signature.\"\"\"\n    schema = inferred_model.model_json_schema()[\"properties\"]\n    valid_keys = signature(func).parameters\n    return {\n        k: schema[k]\n        for i, (k, param) in enumerate(valid_keys.items())\n        if k not in filter_args\n        and (i > 0 or param.name not in (\"self\", \"cls\"))\n        and (include_injected or not _is_injected_arg_type(param.annotation))\n    }\n\n\ndef _parse_python_function_docstring(\n    function: Callable, annotations: dict, error_on_invalid_docstring: bool = False\n) -> tuple[str, dict]:\n    \"\"\"Parse the function and argument descriptions from the docstring of a function.\n\n    Assumes the function docstring follows Google Python style guide.\n    \"\"\"\n    docstring = inspect.getdoc(function)\n    return _parse_google_docstring(\n        docstring,\n        list(annotations),\n        error_on_invalid_docstring=error_on_invalid_docstring,\n    )\n\n\ndef _validate_docstring_args_against_annotations(\n    arg_descriptions: dict, annotations: dict\n) -> None:\n    \"\"\"Raise error if docstring arg is not in type annotations.\"\"\"\n    for docstring_arg in arg_descriptions:\n        if docstring_arg not in annotations:\n            msg = f\"Arg {docstring_arg} in docstring not found in function signature.\"\n            raise ValueError(msg)\n\n\ndef _infer_arg_descriptions(\n    fn: Callable,\n    *,\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = False,\n) -> tuple[str, dict]:\n    \"\"\"Infer argument descriptions from a function's docstring.\"\"\"\n    if hasattr(inspect, \"get_annotations\"):\n        # This is for python < 3.10\n        annotations = inspect.get_annotations(fn)  # type: ignore\n    else:\n        annotations = getattr(fn, \"__annotations__\", {})\n    if parse_docstring:\n        description, arg_descriptions = _parse_python_function_docstring(\n            fn, annotations, error_on_invalid_docstring=error_on_invalid_docstring\n        )\n    else:\n        description = inspect.getdoc(fn) or \"\"\n        arg_descriptions = {}\n    if parse_docstring:\n        _validate_docstring_args_against_annotations(arg_descriptions, annotations)\n    for arg, arg_type in annotations.items():\n        if arg in arg_descriptions:\n            continue\n        if desc := _get_annotation_description(arg_type):\n            arg_descriptions[arg] = desc\n    return description, arg_descriptions\n\n\ndef _is_pydantic_annotation(annotation: Any, pydantic_version: str = \"v2\") -> bool:\n    \"\"\"Determine if a type annotation is a Pydantic model.\"\"\"\n    base_model_class = BaseModelV1 if pydantic_version == \"v1\" else BaseModel\n    try:\n        return issubclass(annotation, base_model_class)\n    except TypeError:\n        return False\n\n\ndef _function_annotations_are_pydantic_v1(\n    signature: inspect.Signature, func: Callable\n) -> bool:\n    \"\"\"Determine if all Pydantic annotations in a function signature are from V1.\"\"\"\n    any_v1_annotations = any(\n        _is_pydantic_annotation(parameter.annotation, pydantic_version=\"v1\")\n        for parameter in signature.parameters.values()\n    )\n    any_v2_annotations = any(\n        _is_pydantic_annotation(parameter.annotation, pydantic_version=\"v2\")\n        for parameter in signature.parameters.values()\n    )\n    if any_v1_annotations and any_v2_annotations:\n        msg = (\n            f\"Function {func} contains a mix of Pydantic v1 and v2 annotations. \"\n            \"Only one version of Pydantic annotations per function is supported.\"\n        )\n        raise NotImplementedError(msg)\n    return any_v1_annotations and not any_v2_annotations\n\n\nclass _SchemaConfig:\n    \"\"\"Configuration for the pydantic model.\n\n    This is used to configure the pydantic model created from\n    a function's signature.\n\n    Parameters:\n        extra: Whether to allow extra fields in the model.\n        arbitrary_types_allowed: Whether to allow arbitrary types in the model.\n            Defaults to True.\n    \"\"\"\n\n    extra: str = \"forbid\"\n    arbitrary_types_allowed: bool = True\n\n\ndef create_schema_from_function(\n    model_name: str,\n    func: Callable,\n    *,\n    filter_args: Optional[Sequence[str]] = None,\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = False,\n    include_injected: bool = True,\n) -> type[BaseModel]:\n    \"\"\"Create a pydantic schema from a function's signature.\n\n    Args:\n        model_name: Name to assign to the generated pydantic schema.\n        func: Function to generate the schema from.\n        filter_args: Optional list of arguments to exclude from the schema.\n            Defaults to FILTERED_ARGS.\n        parse_docstring: Whether to parse the function's docstring for descriptions\n            for each argument. Defaults to False.\n        error_on_invalid_docstring: if ``parse_docstring`` is provided, configure\n            whether to raise ValueError on invalid Google Style docstrings.\n            Defaults to False.\n        include_injected: Whether to include injected arguments in the schema.\n            Defaults to True, since we want to include them in the schema\n            when *validating* tool inputs.\n\n    Returns:\n        A pydantic model with the same arguments as the function.\n    \"\"\"\n    sig = inspect.signature(func)\n\n    if _function_annotations_are_pydantic_v1(sig, func):\n        validated = validate_arguments_v1(func, config=_SchemaConfig)  # type: ignore\n    else:\n        # https://docs.pydantic.dev/latest/usage/validation_decorator/\n        with warnings.catch_warnings():\n            # We are using deprecated functionality here.\n            # This code should be re-written to simply construct a pydantic model\n            # using inspect.signature and create_model.\n            warnings.simplefilter(\"ignore\", category=PydanticDeprecationWarning)\n            validated = validate_arguments(func, config=_SchemaConfig)  # type: ignore\n\n    # Let's ignore `self` and `cls` arguments for class and instance methods\n    # If qualified name has a \".\", then it likely belongs in a class namespace\n    in_class = bool(func.__qualname__ and \".\" in func.__qualname__)\n\n    has_args = False\n    has_kwargs = False\n\n    for param in sig.parameters.values():\n        if param.kind == param.VAR_POSITIONAL:\n            has_args = True\n        elif param.kind == param.VAR_KEYWORD:\n            has_kwargs = True\n\n    inferred_model = validated.model  # type: ignore\n\n    if filter_args:\n        filter_args_ = filter_args\n    else:\n        # Handle classmethods and instance methods\n        existing_params: list[str] = list(sig.parameters.keys())\n        if existing_params and existing_params[0] in (\"self\", \"cls\") and in_class:\n            filter_args_ = [existing_params[0]] + list(FILTERED_ARGS)\n        else:\n            filter_args_ = list(FILTERED_ARGS)\n\n        for existing_param in existing_params:\n            if not include_injected and _is_injected_arg_type(\n                sig.parameters[existing_param].annotation\n            ):\n                filter_args_.append(existing_param)\n\n    description, arg_descriptions = _infer_arg_descriptions(\n        func,\n        parse_docstring=parse_docstring,\n        error_on_invalid_docstring=error_on_invalid_docstring,\n    )\n    # Pydantic adds placeholder virtual fields we need to strip\n    valid_properties = []\n    for field in get_fields(inferred_model):\n        if not has_args and field == \"args\":\n            continue\n        if not has_kwargs and field == \"kwargs\":\n            continue\n\n        if field == \"v__duplicate_kwargs\":  # Internal pydantic field\n            continue\n\n        if field not in filter_args_:\n            valid_properties.append(field)\n\n    return _create_subset_model(\n        model_name,\n        inferred_model,\n        list(valid_properties),\n        descriptions=arg_descriptions,\n        fn_description=description,\n    )\n\n\nclass ToolException(Exception):  # noqa: N818\n    \"\"\"Optional exception that tool throws when execution error occurs.\n\n    When this exception is thrown, the agent will not stop working,\n    but it will handle the exception according to the handle_tool_error\n    variable of the tool, and the processing result will be returned\n    to the agent as observation, and printed in red on the console.\n    \"\"\"\n\n\nArgsSchema = Union[TypeBaseModel, dict[str, Any]]\n\n\nclass BaseTool(RunnableSerializable[Union[str, dict, ToolCall], Any]):\n    \"\"\"Interface LangChain tools must implement.\"\"\"\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        \"\"\"Create the definition of the new tool class.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        args_schema_type = cls.__annotations__.get(\"args_schema\", None)\n\n        if args_schema_type is not None and args_schema_type == BaseModel:\n            # Throw errors for common mis-annotations.\n            # TODO: Use get_args / get_origin and fully\n            # specify valid annotations.\n            typehint_mandate = \"\"\"\nclass ChildTool(BaseTool):\n    ...\n    args_schema: Type[BaseModel] = SchemaClass\n    ...\"\"\"\n            name = cls.__name__\n            msg = (\n                f\"Tool definition for {name} must include valid type annotations\"\n                f\" for argument 'args_schema' to behave as expected.\\n\"\n                f\"Expected annotation of 'Type[BaseModel]'\"\n                f\" but got '{args_schema_type}'.\\n\"\n                f\"Expected class looks like:\\n\"\n                f\"{typehint_mandate}\"\n            )\n            raise SchemaAnnotationError(msg)\n\n    name: str\n    \"\"\"The unique name of the tool that clearly communicates its purpose.\"\"\"\n    description: str\n    \"\"\"Used to tell the model how/when/why to use the tool.\n\n    You can provide few-shot examples as a part of the description.\n    \"\"\"\n\n    args_schema: Annotated[Optional[ArgsSchema], SkipValidation()] = Field(\n        default=None, description=\"The tool schema.\"\n    )\n    \"\"\"Pydantic model class to validate and parse the tool's input arguments.\n\n    Args schema should be either:\n\n    - A subclass of pydantic.BaseModel.\n    or\n    - A subclass of pydantic.v1.BaseModel if accessing v1 namespace in pydantic 2\n    or\n    - a JSON schema dict\n    \"\"\"\n    return_direct: bool = False\n    \"\"\"Whether to return the tool's output directly.\n\n    Setting this to True means\n    that after the tool is called, the AgentExecutor will stop looping.\n    \"\"\"\n    verbose: bool = False\n    \"\"\"Whether to log the tool's progress.\"\"\"\n\n    callbacks: Callbacks = Field(default=None, exclude=True)\n    \"\"\"Callbacks to be called during tool execution.\"\"\"\n\n    callback_manager: Optional[BaseCallbackManager] = deprecated(\n        name=\"callback_manager\", since=\"0.1.7\", removal=\"1.0\", alternative=\"callbacks\"\n    )(\n        Field(\n            default=None,\n            exclude=True,\n            description=\"Callback manager to add to the run trace.\",\n        )\n    )\n    tags: Optional[list[str]] = None\n    \"\"\"Optional list of tags associated with the tool. Defaults to None.\n    These tags will be associated with each call to this tool,\n    and passed as arguments to the handlers defined in `callbacks`.\n    You can use these to eg identify a specific instance of a tool with its use case.\n    \"\"\"\n    metadata: Optional[dict[str, Any]] = None\n    \"\"\"Optional metadata associated with the tool. Defaults to None.\n    This metadata will be associated with each call to this tool,\n    and passed as arguments to the handlers defined in `callbacks`.\n    You can use these to eg identify a specific instance of a tool with its use case.\n    \"\"\"\n\n    handle_tool_error: Optional[Union[bool, str, Callable[[ToolException], str]]] = (\n        False\n    )\n    \"\"\"Handle the content of the ToolException thrown.\"\"\"\n\n    handle_validation_error: Optional[\n        Union[bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]]\n    ] = False\n    \"\"\"Handle the content of the ValidationError thrown.\"\"\"\n\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\"\n    \"\"\"The tool response format. Defaults to 'content'.\n\n    If \"content\" then the output of the tool is interpreted as the contents of a\n    ToolMessage. If \"content_and_artifact\" then the output is expected to be a\n    two-tuple corresponding to the (content, artifact) of a ToolMessage.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Initialize the tool.\"\"\"\n        if (\n            \"args_schema\" in kwargs\n            and kwargs[\"args_schema\"] is not None\n            and not is_basemodel_subclass(kwargs[\"args_schema\"])\n            and not isinstance(kwargs[\"args_schema\"], dict)\n        ):\n            msg = (\n                \"args_schema must be a subclass of pydantic BaseModel or \"\n                f\"a JSON schema dict. Got: {kwargs['args_schema']}.\"\n            )\n            raise TypeError(msg)\n        super().__init__(**kwargs)\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    def is_single_input(self) -> bool:\n        \"\"\"Whether the tool only accepts a single input.\"\"\"\n        keys = {k for k in self.args if k != \"kwargs\"}\n        return len(keys) == 1\n\n    @property\n    def args(self) -> dict:\n        if isinstance(self.args_schema, dict):\n            json_schema = self.args_schema\n        else:\n            input_schema = self.get_input_schema()\n            json_schema = input_schema.model_json_schema()\n        return json_schema[\"properties\"]\n\n    @property\n    def tool_call_schema(self) -> ArgsSchema:\n        if isinstance(self.args_schema, dict):\n            if self.description:\n                return {\n                    **self.args_schema,\n                    \"description\": self.description,\n                }\n\n            return self.args_schema\n\n        full_schema = self.get_input_schema()\n        fields = []\n        for name, type_ in get_all_basemodel_annotations(full_schema).items():\n            if not _is_injected_arg_type(type_):\n                fields.append(name)\n        return _create_subset_model(\n            self.name, full_schema, fields, fn_description=self.description\n        )\n\n    # --- Runnable ---\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"The tool's input schema.\n\n        Args:\n            config: The configuration for the tool.\n\n        Returns:\n            The input schema for the tool.\n        \"\"\"\n        if self.args_schema is not None:\n            if isinstance(self.args_schema, dict):\n                return super().get_input_schema(config)\n            return self.args_schema\n        else:\n            return create_schema_from_function(self.name, self._run)\n\n    def invoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n        return self.run(tool_input, **kwargs)\n\n    async def ainvoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n        return await self.arun(tool_input, **kwargs)\n\n    # --- Tool ---\n\n    def _parse_input(\n        self, tool_input: Union[str, dict], tool_call_id: Optional[str]\n    ) -> Union[str, dict[str, Any]]:\n        \"\"\"Convert tool input to a pydantic model.\n\n        Args:\n            tool_input: The input to the tool.\n        \"\"\"\n        input_args = self.args_schema\n        if isinstance(tool_input, str):\n            if input_args is not None:\n                if isinstance(input_args, dict):\n                    msg = (\n                        \"String tool inputs are not allowed when \"\n                        \"using tools with JSON schema args_schema.\"\n                    )\n                    raise ValueError(msg)\n                key_ = next(iter(get_fields(input_args).keys()))\n                if hasattr(input_args, \"model_validate\"):\n                    input_args.model_validate({key_: tool_input})\n                else:\n                    input_args.parse_obj({key_: tool_input})\n            return tool_input\n        else:\n            if input_args is not None:\n                if isinstance(input_args, dict):\n                    return tool_input\n                elif issubclass(input_args, BaseModel):\n                    for k, v in get_all_basemodel_annotations(input_args).items():\n                        if (\n                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)\n                            and k not in tool_input\n                        ):\n                            if tool_call_id is None:\n                                msg = (\n                                    \"When tool includes an InjectedToolCallId \"\n                                    \"argument, tool must always be invoked with a full \"\n                                    \"model ToolCall of the form: {'args': {...}, \"\n                                    \"'name': '...', 'type': 'tool_call', \"\n                                    \"'tool_call_id': '...'}\"\n                                )\n                                raise ValueError(msg)\n                            tool_input[k] = tool_call_id\n                    result = input_args.model_validate(tool_input)\n                    result_dict = result.model_dump()\n                elif issubclass(input_args, BaseModelV1):\n                    for k, v in get_all_basemodel_annotations(input_args).items():\n                        if (\n                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)\n                            and k not in tool_input\n                        ):\n                            if tool_call_id is None:\n                                msg = (\n                                    \"When tool includes an InjectedToolCallId \"\n                                    \"argument, tool must always be invoked with a full \"\n                                    \"model ToolCall of the form: {'args': {...}, \"\n                                    \"'name': '...', 'type': 'tool_call', \"\n                                    \"'tool_call_id': '...'}\"\n                                )\n                                raise ValueError(msg)\n                            tool_input[k] = tool_call_id\n                    result = input_args.parse_obj(tool_input)\n                    result_dict = result.dict()\n                else:\n                    msg = (\n                        \"args_schema must be a Pydantic BaseModel, \"\n                        f\"got {self.args_schema}\"\n                    )\n                    raise NotImplementedError(msg)\n                return {\n                    k: getattr(result, k)\n                    for k, v in result_dict.items()\n                    if k in tool_input\n                }\n            return tool_input\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def raise_deprecation(cls, values: dict) -> Any:\n        \"\"\"Raise deprecation warning if callback_manager is used.\n\n        Args:\n            values: The values to validate.\n\n        Returns:\n            The validated values.\n        \"\"\"\n        if values.get(\"callback_manager\") is not None:\n            warnings.warn(\n                \"callback_manager is deprecated. Please use callbacks instead.\",\n                DeprecationWarning,\n                stacklevel=6,\n            )\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\n        return values\n\n    @abstractmethod\n    def _run(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Use the tool.\n\n        Add run_manager: Optional[CallbackManagerForToolRun] = None\n        to child implementations to enable tracing.\n        \"\"\"\n\n    async def _arun(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Use the tool asynchronously.\n\n        Add run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n        to child implementations to enable tracing.\n        \"\"\"\n        if kwargs.get(\"run_manager\") and signature(self._run).parameters.get(\n            \"run_manager\"\n        ):\n            kwargs[\"run_manager\"] = kwargs[\"run_manager\"].get_sync()\n        return await run_in_executor(None, self._run, *args, **kwargs)\n\n    def _to_args_and_kwargs(\n        self, tool_input: Union[str, dict], tool_call_id: Optional[str]\n    ) -> tuple[tuple, dict]:\n        if (\n            self.args_schema is not None\n            and isinstance(self.args_schema, type)\n            and is_basemodel_subclass(self.args_schema)\n            and not get_fields(self.args_schema)\n        ):\n            # StructuredTool with no args\n            return (), {}\n        tool_input = self._parse_input(tool_input, tool_call_id)\n        # For backwards compatibility, if run_input is a string,\n        # pass as a positional argument.\n        if isinstance(tool_input, str):\n            return (tool_input,), {}\n        else:\n            return (), tool_input\n\n    def run(\n        self,\n        tool_input: Union[str, dict[str, Any]],\n        verbose: Optional[bool] = None,\n        start_color: Optional[str] = \"green\",\n        color: Optional[str] = \"green\",\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        config: Optional[RunnableConfig] = None,\n        tool_call_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the tool.\n\n        Args:\n            tool_input: The input to the tool.\n            verbose: Whether to log the tool's progress. Defaults to None.\n            start_color: The color to use when starting the tool. Defaults to 'green'.\n            color: The color to use when ending the tool. Defaults to 'green'.\n            callbacks: Callbacks to be called during tool execution. Defaults to None.\n            tags: Optional list of tags associated with the tool. Defaults to None.\n            metadata: Optional metadata associated with the tool. Defaults to None.\n            run_name: The name of the run. Defaults to None.\n            run_id: The id of the run. Defaults to None.\n            config: The configuration for the tool. Defaults to None.\n            tool_call_id: The id of the tool call. Defaults to None.\n            kwargs: Keyword arguments to be passed to tool callbacks\n\n        Returns:\n            The output of the tool.\n\n        Raises:\n            ToolException: If an error occurs during tool execution.\n        \"\"\"\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose or bool(verbose),\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n\n        run_manager = callback_manager.on_tool_start(\n            {\"name\": self.name, \"description\": self.description},\n            tool_input if isinstance(tool_input, str) else str(tool_input),\n            color=start_color,\n            name=run_name,\n            run_id=run_id,\n            # Inputs by definition should always be dicts.\n            # For now, it's unclear whether this assumption is ever violated,\n            # but if it is we will send a `None` value to the callback instead\n            # TODO: will need to address issue via a patch.\n            inputs=tool_input if isinstance(tool_input, dict) else None,\n            **kwargs,\n        )\n\n        content = None\n        artifact = None\n        status = \"success\"\n        error_to_raise: Union[Exception, KeyboardInterrupt, None] = None\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)\n            if signature(self._run).parameters.get(\"run_manager\"):\n                tool_kwargs = tool_kwargs | {\"run_manager\": run_manager}\n            if config_param := _get_runnable_config_param(self._run):\n                tool_kwargs = tool_kwargs | {config_param: config}\n            response = context.run(self._run, *tool_args, **tool_kwargs)\n            if self.response_format == \"content_and_artifact\":\n                if not isinstance(response, tuple) or len(response) != 2:\n                    msg = (\n                        \"Since response_format='content_and_artifact' \"\n                        \"a two-tuple of the message content and raw tool output is \"\n                        f\"expected. Instead generated response of type: \"\n                        f\"{type(response)}.\"\n                    )\n                    error_to_raise = ValueError(msg)\n                else:\n                    content, artifact = response\n            else:\n                content = response\n        except (ValidationError, ValidationErrorV1) as e:\n            if not self.handle_validation_error:\n                error_to_raise = e\n            else:\n                content = _handle_validation_error(e, flag=self.handle_validation_error)\n                status = \"error\"\n        except ToolException as e:\n            if not self.handle_tool_error:\n                error_to_raise = e\n            else:\n                content = _handle_tool_error(e, flag=self.handle_tool_error)\n                status = \"error\"\n        except (Exception, KeyboardInterrupt) as e:\n            error_to_raise = e\n\n        if error_to_raise:\n            run_manager.on_tool_error(error_to_raise)\n            raise error_to_raise\n        output = _format_output(content, artifact, tool_call_id, self.name, status)\n        run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n        return output\n\n    async def arun(\n        self,\n        tool_input: Union[str, dict],\n        verbose: Optional[bool] = None,\n        start_color: Optional[str] = \"green\",\n        color: Optional[str] = \"green\",\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        config: Optional[RunnableConfig] = None,\n        tool_call_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the tool asynchronously.\n\n        Args:\n            tool_input: The input to the tool.\n            verbose: Whether to log the tool's progress. Defaults to None.\n            start_color: The color to use when starting the tool. Defaults to 'green'.\n            color: The color to use when ending the tool. Defaults to 'green'.\n            callbacks: Callbacks to be called during tool execution. Defaults to None.\n            tags: Optional list of tags associated with the tool. Defaults to None.\n            metadata: Optional metadata associated with the tool. Defaults to None.\n            run_name: The name of the run. Defaults to None.\n            run_id: The id of the run. Defaults to None.\n            config: The configuration for the tool. Defaults to None.\n            tool_call_id: The id of the tool call. Defaults to None.\n            kwargs: Keyword arguments to be passed to tool callbacks\n\n        Returns:\n            The output of the tool.\n\n        Raises:\n            ToolException: If an error occurs during tool execution.\n        \"\"\"\n        callback_manager = AsyncCallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose or bool(verbose),\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n        run_manager = await callback_manager.on_tool_start(\n            {\"name\": self.name, \"description\": self.description},\n            tool_input if isinstance(tool_input, str) else str(tool_input),\n            color=start_color,\n            name=run_name,\n            run_id=run_id,\n            # Inputs by definition should always be dicts.\n            # For now, it's unclear whether this assumption is ever violated,\n            # but if it is we will send a `None` value to the callback instead\n            # TODO: will need to address issue via a patch.\n            inputs=tool_input if isinstance(tool_input, dict) else None,\n            **kwargs,\n        )\n        content = None\n        artifact = None\n        status = \"success\"\n        error_to_raise: Optional[Union[Exception, KeyboardInterrupt]] = None\n        try:\n            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            func_to_check = (\n                self._run if self.__class__._arun is BaseTool._arun else self._arun\n            )\n            if signature(func_to_check).parameters.get(\"run_manager\"):\n                tool_kwargs[\"run_manager\"] = run_manager\n            if config_param := _get_runnable_config_param(func_to_check):\n                tool_kwargs[config_param] = config\n\n            coro = context.run(self._arun, *tool_args, **tool_kwargs)\n            if asyncio_accepts_context():\n                response = await asyncio.create_task(coro, context=context)  # type: ignore\n            else:\n                response = await coro\n            if self.response_format == \"content_and_artifact\":\n                if not isinstance(response, tuple) or len(response) != 2:\n                    msg = (\n                        \"Since response_format='content_and_artifact' \"\n                        \"a two-tuple of the message content and raw tool output is \"\n                        f\"expected. Instead generated response of type: \"\n                        f\"{type(response)}.\"\n                    )\n                    error_to_raise = ValueError(msg)\n                else:\n                    content, artifact = response\n            else:\n                content = response\n        except ValidationError as e:\n            if not self.handle_validation_error:\n                error_to_raise = e\n            else:\n                content = _handle_validation_error(e, flag=self.handle_validation_error)\n                status = \"error\"\n        except ToolException as e:\n            if not self.handle_tool_error:\n                error_to_raise = e\n            else:\n                content = _handle_tool_error(e, flag=self.handle_tool_error)\n                status = \"error\"\n        except (Exception, KeyboardInterrupt) as e:\n            error_to_raise = e\n\n        if error_to_raise:\n            await run_manager.on_tool_error(error_to_raise)\n            raise error_to_raise\n\n        output = _format_output(content, artifact, tool_call_id, self.name, status)\n        await run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n        return output\n\n    @deprecated(\"0.1.47\", alternative=\"invoke\", removal=\"1.0\")\n    def __call__(self, tool_input: str, callbacks: Callbacks = None) -> str:\n        \"\"\"Make tool callable.\"\"\"\n        return self.run(tool_input, callbacks=callbacks)\n\n\ndef _is_tool_call(x: Any) -> bool:\n    return isinstance(x, dict) and x.get(\"type\") == \"tool_call\"\n\n\ndef _handle_validation_error(\n    e: Union[ValidationError, ValidationErrorV1],\n    *,\n    flag: Union[\n        Literal[True], str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> str:\n    if isinstance(flag, bool):\n        content = \"Tool input validation error\"\n    elif isinstance(flag, str):\n        content = flag\n    elif callable(flag):\n        content = flag(e)\n    else:\n        msg = (\n            f\"Got unexpected type of `handle_validation_error`. Expected bool, \"\n            f\"str or callable. Received: {flag}\"\n        )\n        raise ValueError(msg)  # noqa: TRY004\n    return content\n\n\ndef _handle_tool_error(\n    e: ToolException,\n    *,\n    flag: Optional[Union[Literal[True], str, Callable[[ToolException], str]]],\n) -> str:\n    if isinstance(flag, bool):\n        content = e.args[0] if e.args else \"Tool execution error\"\n    elif isinstance(flag, str):\n        content = flag\n    elif callable(flag):\n        content = flag(e)\n    else:\n        msg = (\n            f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\n            f\"or callable. Received: {flag}\"\n        )\n        raise ValueError(msg)  # noqa: TRY004\n    return content\n\n\ndef _prep_run_args(\n    input: Union[str, dict, ToolCall],\n    config: Optional[RunnableConfig],\n    **kwargs: Any,\n) -> tuple[Union[str, dict], dict]:\n    config = ensure_config(config)\n    if _is_tool_call(input):\n        tool_call_id: Optional[str] = cast(ToolCall, input)[\"id\"]\n        tool_input: Union[str, dict] = cast(ToolCall, input)[\"args\"].copy()\n    else:\n        tool_call_id = None\n        tool_input = cast(Union[str, dict], input)\n    return (\n        tool_input,\n        dict(\n            callbacks=config.get(\"callbacks\"),\n            tags=config.get(\"tags\"),\n            metadata=config.get(\"metadata\"),\n            run_name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            config=config,\n            tool_call_id=tool_call_id,\n            **kwargs,\n        ),\n    )\n\n\ndef _format_output(\n    content: Any,\n    artifact: Any,\n    tool_call_id: Optional[str],\n    name: str,\n    status: str,\n) -> Union[ToolOutputMixin, Any]:\n    if isinstance(content, ToolOutputMixin) or tool_call_id is None:\n        return content\n    if not _is_message_content_type(content):\n        content = _stringify(content)\n    return ToolMessage(\n        content,\n        artifact=artifact,\n        tool_call_id=tool_call_id,\n        name=name,\n        status=status,\n    )\n\n\ndef _is_message_content_type(obj: Any) -> bool:\n    \"\"\"Check for OpenAI or Anthropic format tool message content.\"\"\"\n    return (\n        isinstance(obj, str)\n        or isinstance(obj, list)\n        and all(_is_message_content_block(e) for e in obj)\n    )\n\n\ndef _is_message_content_block(obj: Any) -> bool:\n    \"\"\"Check for OpenAI or Anthropic format tool message content blocks.\"\"\"\n    if isinstance(obj, str):\n        return True\n    elif isinstance(obj, dict):\n        return obj.get(\"type\", None) in (\"text\", \"image_url\", \"image\", \"json\")\n    else:\n        return False\n\n\ndef _stringify(content: Any) -> str:\n    try:\n        return json.dumps(content, ensure_ascii=False)\n    except Exception:\n        return str(content)\n\n\ndef _get_type_hints(func: Callable) -> Optional[dict[str, type]]:\n    if isinstance(func, functools.partial):\n        func = func.func\n    try:\n        return get_type_hints(func)\n    except Exception:\n        return None\n\n\ndef _get_runnable_config_param(func: Callable) -> Optional[str]:\n    type_hints = _get_type_hints(func)\n    if not type_hints:\n        return None\n    for name, type_ in type_hints.items():\n        if type_ is RunnableConfig:\n            return name\n    return None\n\n\nclass InjectedToolArg:\n    \"\"\"Annotation for a Tool arg that is **not** meant to be generated by a model.\"\"\"\n\n\nclass InjectedToolCallId(InjectedToolArg):\n    r'''Annotation for injecting the tool_call_id.\n\n    Example:\n        ..code-block:: python\n\n            from typing_extensions import Annotated\n\n            from langchain_core.messages import ToolMessage\n            from langchain_core.tools import tool, InjectedToolCallID\n\n            @tool\n            def foo(x: int, tool_call_id: Annotated[str, InjectedToolCallID]) -> ToolMessage:\n                \"\"\"Return x.\"\"\"\n                return ToolMessage(str(x), artifact=x, name=\"foo\", tool_call_id=tool_call_id)\n    '''  # noqa: E501\n\n\ndef _is_injected_arg_type(\n    type_: type, injected_type: Optional[type[InjectedToolArg]] = None\n) -> bool:\n    injected_type = injected_type or InjectedToolArg\n    return any(\n        isinstance(arg, injected_type)\n        or (isinstance(arg, type) and issubclass(arg, injected_type))\n        for arg in get_args(type_)[1:]\n    )\n\n\ndef get_all_basemodel_annotations(\n    cls: Union[TypeBaseModel, Any], *, default_to_bound: bool = True\n) -> dict[str, type]:\n    # cls has no subscript: cls = FooBar\n    if isinstance(cls, type):\n        annotations: dict[str, type] = {}\n        for name, param in inspect.signature(cls).parameters.items():\n            # Exclude hidden init args added by pydantic Config. For example if\n            # BaseModel(extra=\"allow\") then \"extra_data\" will part of init sig.\n            if (\n                fields := getattr(cls, \"model_fields\", {})  # pydantic v2+\n                or getattr(cls, \"__fields__\", {})  # pydantic v1\n            ) and name not in fields:\n                continue\n            annotations[name] = param.annotation\n        orig_bases: tuple = getattr(cls, \"__orig_bases__\", ())\n    # cls has subscript: cls = FooBar[int]\n    else:\n        annotations = get_all_basemodel_annotations(\n            get_origin(cls), default_to_bound=False\n        )\n        orig_bases = (cls,)\n\n    # Pydantic v2 automatically resolves inherited generics, Pydantic v1 does not.\n    if not (isinstance(cls, type) and is_pydantic_v2_subclass(cls)):\n        # if cls = FooBar inherits from Baz[str], orig_bases will contain Baz[str]\n        # if cls = FooBar inherits from Baz, orig_bases will contain Baz\n        # if cls = FooBar[int], orig_bases will contain FooBar[int]\n        for parent in orig_bases:\n            # if class = FooBar inherits from Baz, parent = Baz\n            if isinstance(parent, type) and is_pydantic_v1_subclass(parent):\n                annotations.update(\n                    get_all_basemodel_annotations(parent, default_to_bound=False)\n                )\n                continue\n\n            parent_origin = get_origin(parent)\n\n            # if class = FooBar inherits from non-pydantic class\n            if not parent_origin:\n                continue\n\n            # if class = FooBar inherits from Baz[str]:\n            # parent = Baz[str],\n            # parent_origin = Baz,\n            # generic_type_vars = (type vars in Baz)\n            # generic_map = {type var in Baz: str}\n            generic_type_vars: tuple = getattr(parent_origin, \"__parameters__\", ())\n            generic_map = dict(zip(generic_type_vars, get_args(parent)))\n            for field in getattr(parent_origin, \"__annotations__\", {}):\n                annotations[field] = _replace_type_vars(\n                    annotations[field], generic_map, default_to_bound\n                )\n\n    return {\n        k: _replace_type_vars(v, default_to_bound=default_to_bound)\n        for k, v in annotations.items()\n    }\n\n\ndef _replace_type_vars(\n    type_: type,\n    generic_map: Optional[dict[TypeVar, type]] = None,\n    default_to_bound: bool = True,\n) -> type:\n    generic_map = generic_map or {}\n    if isinstance(type_, TypeVar):\n        if type_ in generic_map:\n            return generic_map[type_]\n        elif default_to_bound:\n            return type_.__bound__ or Any\n        else:\n            return type_\n    elif (origin := get_origin(type_)) and (args := get_args(type_)):\n        new_args = tuple(\n            _replace_type_vars(arg, generic_map, default_to_bound) for arg in args\n        )\n        return _py_38_safe_origin(origin)[new_args]  # type: ignore[index]\n    else:\n        return type_\n\n\nclass BaseToolkit(BaseModel, ABC):\n    \"\"\"Base Toolkit representing a collection of related tools.\"\"\"\n\n    @abstractmethod\n    def get_tools(self) -> list[BaseTool]:\n        \"\"\"Get the tools in the toolkit.\"\"\"\n",
        "patch": "@@ -4,13 +4,12 @@\n import functools\n import inspect\n import json\n-import uuid\n import warnings\n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n from contextvars import copy_context\n from inspect import signature\n from typing import (\n+    TYPE_CHECKING,\n     Annotated,\n     Any,\n     Callable,\n@@ -68,6 +67,10 @@\n     is_pydantic_v2_subclass,\n )\n \n+if TYPE_CHECKING:\n+    import uuid\n+    from collections.abc import Sequence\n+\n FILTERED_ARGS = (\"run_manager\", \"callbacks\")\n \n "
      },
      {
        "filename": "libs/core/langchain_core/tools/retriever.py",
        "content_before": "from __future__ import annotations\n\nfrom functools import partial\nfrom typing import Literal, Optional, Union\n\nfrom pydantic import BaseModel, Field\n\nfrom langchain_core.callbacks import Callbacks\nfrom langchain_core.documents import Document\nfrom langchain_core.prompts import (\n    BasePromptTemplate,\n    PromptTemplate,\n    aformat_document,\n    format_document,\n)\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.tools.simple import Tool\n\n\nclass RetrieverInput(BaseModel):\n    \"\"\"Input to the retriever.\"\"\"\n\n    query: str = Field(description=\"query to look up in retriever\")\n\n\ndef _get_relevant_documents(\n    query: str,\n    retriever: BaseRetriever,\n    document_prompt: BasePromptTemplate,\n    document_separator: str,\n    callbacks: Callbacks = None,\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n) -> Union[str, tuple[str, list[Document]]]:\n    docs = retriever.invoke(query, config={\"callbacks\": callbacks})\n    content = document_separator.join(\n        format_document(doc, document_prompt) for doc in docs\n    )\n    if response_format == \"content_and_artifact\":\n        return (content, docs)\n\n    return content\n\n\nasync def _aget_relevant_documents(\n    query: str,\n    retriever: BaseRetriever,\n    document_prompt: BasePromptTemplate,\n    document_separator: str,\n    callbacks: Callbacks = None,\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n) -> Union[str, tuple[str, list[Document]]]:\n    docs = await retriever.ainvoke(query, config={\"callbacks\": callbacks})\n    content = document_separator.join(\n        [await aformat_document(doc, document_prompt) for doc in docs]\n    )\n\n    if response_format == \"content_and_artifact\":\n        return (content, docs)\n\n    return content\n\n\ndef create_retriever_tool(\n    retriever: BaseRetriever,\n    name: str,\n    description: str,\n    *,\n    document_prompt: Optional[BasePromptTemplate] = None,\n    document_separator: str = \"\\n\\n\",\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n) -> Tool:\n    \"\"\"Create a tool to do retrieval of documents.\n\n    Args:\n        retriever: The retriever to use for the retrieval\n        name: The name for the tool. This will be passed to the language model,\n            so should be unique and somewhat descriptive.\n        description: The description for the tool. This will be passed to the language\n            model, so should be descriptive.\n        document_prompt: The prompt to use for the document. Defaults to None.\n        document_separator: The separator to use between documents. Defaults to \"\\n\\n\".\n        response_format: The tool response format. If \"content\" then the output of\n                the tool is interpreted as the contents of a ToolMessage. If\n                \"content_and_artifact\" then the output is expected to be a two-tuple\n                corresponding to the (content, artifact) of a ToolMessage (artifact\n                being a list of documents in this case). Defaults to \"content\".\n\n    Returns:\n        Tool class to pass to an agent.\n    \"\"\"\n    document_prompt = document_prompt or PromptTemplate.from_template(\"{page_content}\")\n    func = partial(\n        _get_relevant_documents,\n        retriever=retriever,\n        document_prompt=document_prompt,\n        document_separator=document_separator,\n        response_format=response_format,\n    )\n    afunc = partial(\n        _aget_relevant_documents,\n        retriever=retriever,\n        document_prompt=document_prompt,\n        document_separator=document_separator,\n        response_format=response_format,\n    )\n    return Tool(\n        name=name,\n        description=description,\n        func=func,\n        coroutine=afunc,\n        args_schema=RetrieverInput,\n        response_format=response_format,\n    )\n",
        "patch": "@@ -1,21 +1,23 @@\n from __future__ import annotations\n \n from functools import partial\n-from typing import Literal, Optional, Union\n+from typing import TYPE_CHECKING, Literal, Optional, Union\n \n from pydantic import BaseModel, Field\n \n-from langchain_core.callbacks import Callbacks\n-from langchain_core.documents import Document\n from langchain_core.prompts import (\n     BasePromptTemplate,\n     PromptTemplate,\n     aformat_document,\n     format_document,\n )\n-from langchain_core.retrievers import BaseRetriever\n from langchain_core.tools.simple import Tool\n \n+if TYPE_CHECKING:\n+    from langchain_core.callbacks import Callbacks\n+    from langchain_core.documents import Document\n+    from langchain_core.retrievers import BaseRetriever\n+\n \n class RetrieverInput(BaseModel):\n     \"\"\"Input to the retriever.\"\"\""
      },
      {
        "filename": "libs/core/langchain_core/tools/simple.py",
        "content_before": "from __future__ import annotations\n\nfrom collections.abc import Awaitable\nfrom inspect import signature\nfrom typing import (\n    Any,\n    Callable,\n    Optional,\n    Union,\n)\n\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.messages import ToolCall\nfrom langchain_core.runnables import RunnableConfig, run_in_executor\nfrom langchain_core.tools.base import (\n    ArgsSchema,\n    BaseTool,\n    ToolException,\n    _get_runnable_config_param,\n)\n\n\nclass Tool(BaseTool):\n    \"\"\"Tool that takes in function or coroutine directly.\"\"\"\n\n    description: str = \"\"\n    func: Optional[Callable[..., str]]\n    \"\"\"The function to run when the tool is called.\"\"\"\n    coroutine: Optional[Callable[..., Awaitable[str]]] = None\n    \"\"\"The asynchronous version of the function.\"\"\"\n\n    # --- Runnable ---\n\n    async def ainvoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        if not self.coroutine:\n            # If the tool does not implement async, fall back to default implementation\n            return await run_in_executor(config, self.invoke, input, config, **kwargs)\n\n        return await super().ainvoke(input, config, **kwargs)\n\n    # --- Tool ---\n\n    @property\n    def args(self) -> dict:\n        \"\"\"The tool's input arguments.\n\n        Returns:\n            The input arguments for the tool.\n        \"\"\"\n        if self.args_schema is not None:\n            if isinstance(self.args_schema, dict):\n                json_schema = self.args_schema\n            else:\n                json_schema = self.args_schema.model_json_schema()\n            return json_schema[\"properties\"]\n        # For backwards compatibility, if the function signature is ambiguous,\n        # assume it takes a single string input.\n        return {\"tool_input\": {\"type\": \"string\"}}\n\n    def _to_args_and_kwargs(\n        self, tool_input: Union[str, dict], tool_call_id: Optional[str]\n    ) -> tuple[tuple, dict]:\n        \"\"\"Convert tool input to pydantic model.\"\"\"\n        args, kwargs = super()._to_args_and_kwargs(tool_input, tool_call_id)\n        # For backwards compatibility. The tool must be run with a single input\n        all_args = list(args) + list(kwargs.values())\n        if len(all_args) != 1:\n            msg = (\n                f\"\"\"Too many arguments to single-input tool {self.name}.\n                Consider using StructuredTool instead.\"\"\"\n                f\" Args: {all_args}\"\n            )\n            raise ToolException(msg)\n        return tuple(all_args), {}\n\n    def _run(\n        self,\n        *args: Any,\n        config: RunnableConfig,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Use the tool.\"\"\"\n        if self.func:\n            if run_manager and signature(self.func).parameters.get(\"callbacks\"):\n                kwargs[\"callbacks\"] = run_manager.get_child()\n            if config_param := _get_runnable_config_param(self.func):\n                kwargs[config_param] = config\n            return self.func(*args, **kwargs)\n        msg = \"Tool does not support sync invocation.\"\n        raise NotImplementedError(msg)\n\n    async def _arun(\n        self,\n        *args: Any,\n        config: RunnableConfig,\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Use the tool asynchronously.\"\"\"\n        if self.coroutine:\n            if run_manager and signature(self.coroutine).parameters.get(\"callbacks\"):\n                kwargs[\"callbacks\"] = run_manager.get_child()\n            if config_param := _get_runnable_config_param(self.coroutine):\n                kwargs[config_param] = config\n            return await self.coroutine(*args, **kwargs)\n\n        # NOTE: this code is unreachable since _arun is only called if coroutine is not\n        # None.\n        return await super()._arun(\n            *args, config=config, run_manager=run_manager, **kwargs\n        )\n\n    # TODO: this is for backwards compatibility, remove in future\n    def __init__(\n        self, name: str, func: Optional[Callable], description: str, **kwargs: Any\n    ) -> None:\n        \"\"\"Initialize tool.\"\"\"\n        super().__init__(  # type: ignore[call-arg]\n            name=name, func=func, description=description, **kwargs\n        )\n\n    @classmethod\n    def from_function(\n        cls,\n        func: Optional[Callable],\n        name: str,  # We keep these required to support backwards compatibility\n        description: str,\n        return_direct: bool = False,\n        args_schema: Optional[ArgsSchema] = None,\n        coroutine: Optional[\n            Callable[..., Awaitable[Any]]\n        ] = None,  # This is last for compatibility, but should be after func\n        **kwargs: Any,\n    ) -> Tool:\n        \"\"\"Initialize tool from a function.\n\n        Args:\n            func: The function to create the tool from.\n            name: The name of the tool.\n            description: The description of the tool.\n            return_direct: Whether to return the output directly. Defaults to False.\n            args_schema: The schema of the tool's input arguments. Defaults to None.\n            coroutine: The asynchronous version of the function. Defaults to None.\n            kwargs: Additional arguments to pass to the tool.\n\n        Returns:\n            The tool.\n\n        Raises:\n            ValueError: If the function is not provided.\n        \"\"\"\n        if func is None and coroutine is None:\n            msg = \"Function and/or coroutine must be provided\"\n            raise ValueError(msg)\n        return cls(\n            name=name,\n            func=func,\n            coroutine=coroutine,\n            description=description,\n            return_direct=return_direct,\n            args_schema=args_schema,\n            **kwargs,\n        )\n\n\nTool.model_rebuild()\n",
        "patch": "@@ -3,6 +3,7 @@\n from collections.abc import Awaitable\n from inspect import signature\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     Optional,\n@@ -13,7 +14,6 @@\n     AsyncCallbackManagerForToolRun,\n     CallbackManagerForToolRun,\n )\n-from langchain_core.messages import ToolCall\n from langchain_core.runnables import RunnableConfig, run_in_executor\n from langchain_core.tools.base import (\n     ArgsSchema,\n@@ -22,6 +22,9 @@\n     _get_runnable_config_param,\n )\n \n+if TYPE_CHECKING:\n+    from langchain_core.messages import ToolCall\n+\n \n class Tool(BaseTool):\n     \"\"\"Tool that takes in function or coroutine directly.\"\"\""
      },
      {
        "filename": "libs/core/langchain_core/tools/structured.py",
        "content_before": "from __future__ import annotations\n\nimport textwrap\nfrom collections.abc import Awaitable\nfrom inspect import signature\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    Union,\n)\n\nfrom pydantic import Field, SkipValidation\n\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.messages import ToolCall\nfrom langchain_core.runnables import RunnableConfig, run_in_executor\nfrom langchain_core.tools.base import (\n    FILTERED_ARGS,\n    ArgsSchema,\n    BaseTool,\n    _get_runnable_config_param,\n    create_schema_from_function,\n)\nfrom langchain_core.utils.pydantic import is_basemodel_subclass\n\n\nclass StructuredTool(BaseTool):\n    \"\"\"Tool that can operate on any number of inputs.\"\"\"\n\n    description: str = \"\"\n    args_schema: Annotated[ArgsSchema, SkipValidation()] = Field(\n        ..., description=\"The tool schema.\"\n    )\n    \"\"\"The input arguments' schema.\"\"\"\n    func: Optional[Callable[..., Any]] = None\n    \"\"\"The function to run when the tool is called.\"\"\"\n    coroutine: Optional[Callable[..., Awaitable[Any]]] = None\n    \"\"\"The asynchronous version of the function.\"\"\"\n\n    # --- Runnable ---\n\n    # TODO: Is this needed?\n    async def ainvoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        if not self.coroutine:\n            # If the tool does not implement async, fall back to default implementation\n            return await run_in_executor(config, self.invoke, input, config, **kwargs)\n\n        return await super().ainvoke(input, config, **kwargs)\n\n    # --- Tool ---\n\n    @property\n    def args(self) -> dict:\n        \"\"\"The tool's input arguments.\"\"\"\n        if isinstance(self.args_schema, dict):\n            json_schema = self.args_schema\n        else:\n            input_schema = self.get_input_schema()\n            json_schema = input_schema.model_json_schema()\n        return json_schema[\"properties\"]\n\n    def _run(\n        self,\n        *args: Any,\n        config: RunnableConfig,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Use the tool.\"\"\"\n        if self.func:\n            if run_manager and signature(self.func).parameters.get(\"callbacks\"):\n                kwargs[\"callbacks\"] = run_manager.get_child()\n            if config_param := _get_runnable_config_param(self.func):\n                kwargs[config_param] = config\n            return self.func(*args, **kwargs)\n        msg = \"StructuredTool does not support sync invocation.\"\n        raise NotImplementedError(msg)\n\n    async def _arun(\n        self,\n        *args: Any,\n        config: RunnableConfig,\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Use the tool asynchronously.\"\"\"\n        if self.coroutine:\n            if run_manager and signature(self.coroutine).parameters.get(\"callbacks\"):\n                kwargs[\"callbacks\"] = run_manager.get_child()\n            if config_param := _get_runnable_config_param(self.coroutine):\n                kwargs[config_param] = config\n            return await self.coroutine(*args, **kwargs)\n\n        # If self.coroutine is None, then this will delegate to the default\n        # implementation which is expected to delegate to _run on a separate thread.\n        return await super()._arun(\n            *args, config=config, run_manager=run_manager, **kwargs\n        )\n\n    @classmethod\n    def from_function(\n        cls,\n        func: Optional[Callable] = None,\n        coroutine: Optional[Callable[..., Awaitable[Any]]] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        args_schema: Optional[ArgsSchema] = None,\n        infer_schema: bool = True,\n        *,\n        response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n        parse_docstring: bool = False,\n        error_on_invalid_docstring: bool = False,\n        **kwargs: Any,\n    ) -> StructuredTool:\n        \"\"\"Create tool from a given function.\n\n        A classmethod that helps to create a tool from a function.\n\n        Args:\n            func: The function from which to create a tool.\n            coroutine: The async function from which to create a tool.\n            name: The name of the tool. Defaults to the function name.\n            description: The description of the tool.\n                Defaults to the function docstring.\n            return_direct: Whether to return the result directly or as a callback.\n                Defaults to False.\n            args_schema: The schema of the tool's input arguments. Defaults to None.\n            infer_schema: Whether to infer the schema from the function's signature.\n                Defaults to True.\n            response_format: The tool response format. If \"content\" then the output of\n                the tool is interpreted as the contents of a ToolMessage. If\n                \"content_and_artifact\" then the output is expected to be a two-tuple\n                corresponding to the (content, artifact) of a ToolMessage.\n                Defaults to \"content\".\n            parse_docstring: if ``infer_schema`` and ``parse_docstring``, will attempt\n                to parse parameter descriptions from Google Style function docstrings.\n                Defaults to False.\n            error_on_invalid_docstring: if ``parse_docstring`` is provided, configure\n                whether to raise ValueError on invalid Google Style docstrings.\n                Defaults to False.\n            kwargs: Additional arguments to pass to the tool\n\n        Returns:\n            The tool.\n\n        Raises:\n            ValueError: If the function is not provided.\n\n        Examples:\n\n            .. code-block:: python\n\n                def add(a: int, b: int) -> int:\n                    \\\"\\\"\\\"Add two numbers\\\"\\\"\\\"\n                    return a + b\n                tool = StructuredTool.from_function(add)\n                tool.run(1, 2) # 3\n        \"\"\"\n        if func is not None:\n            source_function = func\n        elif coroutine is not None:\n            source_function = coroutine\n        else:\n            msg = \"Function and/or coroutine must be provided\"\n            raise ValueError(msg)\n        name = name or source_function.__name__\n        if args_schema is None and infer_schema:\n            # schema name is appended within function\n            args_schema = create_schema_from_function(\n                name,\n                source_function,\n                parse_docstring=parse_docstring,\n                error_on_invalid_docstring=error_on_invalid_docstring,\n                filter_args=_filter_schema_args(source_function),\n            )\n        description_ = description\n        if description is None and not parse_docstring:\n            description_ = source_function.__doc__ or None\n        if description_ is None and args_schema:\n            if isinstance(args_schema, type) and is_basemodel_subclass(args_schema):\n                description_ = args_schema.__doc__ or None\n            elif isinstance(args_schema, dict):\n                description_ = args_schema.get(\"description\")\n            else:\n                msg = (\n                    \"Invalid args_schema: expected BaseModel or dict, \"\n                    f\"got {args_schema}\"\n                )\n                raise TypeError(msg)\n        if description_ is None:\n            msg = \"Function must have a docstring if description not provided.\"\n            raise ValueError(msg)\n        if description is None:\n            # Only apply if using the function's docstring\n            description_ = textwrap.dedent(description_).strip()\n\n        # Description example:\n        # search_api(query: str) - Searches the API for the query.\n        description_ = f\"{description_.strip()}\"\n        return cls(\n            name=name,\n            func=func,\n            coroutine=coroutine,\n            args_schema=args_schema,  # type: ignore[arg-type]\n            description=description_,\n            return_direct=return_direct,\n            response_format=response_format,\n            **kwargs,\n        )\n\n\ndef _filter_schema_args(func: Callable) -> list[str]:\n    filter_args = list(FILTERED_ARGS)\n    if config_param := _get_runnable_config_param(func):\n        filter_args.append(config_param)\n    # filter_args.extend(_get_non_model_params(type_hints))\n    return filter_args\n",
        "patch": "@@ -4,6 +4,7 @@\n from collections.abc import Awaitable\n from inspect import signature\n from typing import (\n+    TYPE_CHECKING,\n     Annotated,\n     Any,\n     Callable,\n@@ -18,7 +19,6 @@\n     AsyncCallbackManagerForToolRun,\n     CallbackManagerForToolRun,\n )\n-from langchain_core.messages import ToolCall\n from langchain_core.runnables import RunnableConfig, run_in_executor\n from langchain_core.tools.base import (\n     FILTERED_ARGS,\n@@ -29,6 +29,9 @@\n )\n from langchain_core.utils.pydantic import is_basemodel_subclass\n \n+if TYPE_CHECKING:\n+    from langchain_core.messages import ToolCall\n+\n \n class StructuredTool(BaseTool):\n     \"\"\"Tool that can operate on any number of inputs.\"\"\""
      },
      {
        "filename": "libs/core/langchain_core/tracers/base.py",
        "content_before": "\"\"\"Base interfaces for tracing runs.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Optional,\n    Union,\n)\nfrom uuid import UUID\n\nfrom tenacity import RetryCallState\n\nfrom langchain_core.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\nfrom langchain_core.exceptions import TracerException  # noqa\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\nfrom langchain_core.tracers.core import _TracerCore\nfrom langchain_core.tracers.schemas import Run\n\nif TYPE_CHECKING:\n    from langchain_core.documents import Document\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseTracer(_TracerCore, BaseCallbackHandler, ABC):\n    \"\"\"Base interface for tracers.\"\"\"\n\n    @abstractmethod\n    def _persist_run(self, run: Run) -> None:\n        \"\"\"Persist a run.\"\"\"\n\n    def _start_trace(self, run: Run) -> None:\n        \"\"\"Start a trace for a run.\"\"\"\n        super()._start_trace(run)\n        self._on_run_create(run)\n\n    def _end_trace(self, run: Run) -> None:\n        \"\"\"End a trace for a run.\"\"\"\n        if not run.parent_run_id:\n            self._persist_run(run)\n        self.run_map.pop(str(run.id))\n        self._on_run_update(run)\n\n    def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Start a trace for an LLM run.\n\n        Args:\n            serialized: The serialized model.\n            messages: The messages to start the chat with.\n            run_id: The run ID.\n            tags: The tags for the run. Defaults to None.\n            parent_run_id: The parent run ID. Defaults to None.\n            metadata: The metadata for the run. Defaults to None.\n            name: The name of the run.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        chat_model_run = self._create_chat_model_run(\n            serialized=serialized,\n            messages=messages,\n            run_id=run_id,\n            parent_run_id=parent_run_id,\n            tags=tags,\n            metadata=metadata,\n            name=name,\n            **kwargs,\n        )\n        self._start_trace(chat_model_run)\n        self._on_chat_model_start(chat_model_run)\n        return chat_model_run\n\n    def on_llm_start(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Start a trace for an LLM run.\n\n        Args:\n            serialized: The serialized model.\n            prompts: The prompts to start the LLM with.\n            run_id: The run ID.\n            tags: The tags for the run. Defaults to None.\n            parent_run_id: The parent run ID. Defaults to None.\n            metadata: The metadata for the run. Defaults to None.\n            name: The name of the run.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        llm_run = self._create_llm_run(\n            serialized=serialized,\n            prompts=prompts,\n            run_id=run_id,\n            parent_run_id=parent_run_id,\n            tags=tags,\n            metadata=metadata,\n            name=name,\n            **kwargs,\n        )\n        self._start_trace(llm_run)\n        self._on_llm_start(llm_run)\n        return llm_run\n\n    def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\n\n        Args:\n            token: The token.\n            chunk: The chunk. Defaults to None.\n            run_id: The run ID.\n            parent_run_id: The parent run ID. Defaults to None.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        # \"chat_model\" is only used for the experimental new streaming_events format.\n        # This change should not affect any existing tracers.\n        llm_run = self._llm_run_with_token_event(\n            token=token,\n            run_id=run_id,\n            chunk=chunk,\n            parent_run_id=parent_run_id,\n            **kwargs,\n        )\n        self._on_llm_new_token(llm_run, token, chunk)\n        return llm_run\n\n    def on_retry(\n        self,\n        retry_state: RetryCallState,\n        *,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Run on retry.\n\n        Args:\n            retry_state: The retry state.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        llm_run = self._llm_run_with_retry_event(\n            retry_state=retry_state,\n            run_id=run_id,\n        )\n        return llm_run\n\n    def on_llm_end(self, response: LLMResult, *, run_id: UUID, **kwargs: Any) -> Run:\n        \"\"\"End a trace for an LLM run.\n\n        Args:\n            response: The response.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        # \"chat_model\" is only used for the experimental new streaming_events format.\n        # This change should not affect any existing tracers.\n        llm_run = self._complete_llm_run(\n            response=response,\n            run_id=run_id,\n        )\n        self._end_trace(llm_run)\n        self._on_llm_end(llm_run)\n        return llm_run\n\n    def on_llm_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Handle an error for an LLM run.\n\n        Args:\n            error: The error.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        # \"chat_model\" is only used for the experimental new streaming_events format.\n        # This change should not affect any existing tracers.\n        llm_run = self._errored_llm_run(\n            error=error,\n            run_id=run_id,\n        )\n        self._end_trace(llm_run)\n        self._on_llm_error(llm_run)\n        return llm_run\n\n    def on_chain_start(\n        self,\n        serialized: dict[str, Any],\n        inputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_type: Optional[str] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Start a trace for a chain run.\n\n        Args:\n            serialized: The serialized chain.\n            inputs: The inputs for the chain.\n            run_id: The run ID.\n            tags: The tags for the run. Defaults to None.\n            parent_run_id: The parent run ID. Defaults to None.\n            metadata: The metadata for the run. Defaults to None.\n            run_type: The type of the run. Defaults to None.\n            name: The name of the run.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        chain_run = self._create_chain_run(\n            serialized=serialized,\n            inputs=inputs,\n            run_id=run_id,\n            tags=tags,\n            parent_run_id=parent_run_id,\n            metadata=metadata,\n            run_type=run_type,\n            name=name,\n            **kwargs,\n        )\n        self._start_trace(chain_run)\n        self._on_chain_start(chain_run)\n        return chain_run\n\n    def on_chain_end(\n        self,\n        outputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"End a trace for a chain run.\n\n        Args:\n            outputs: The outputs for the chain.\n            run_id: The run ID.\n            inputs: The inputs for the chain. Defaults to None.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        chain_run = self._complete_chain_run(\n            outputs=outputs,\n            run_id=run_id,\n            inputs=inputs,\n            **kwargs,\n        )\n        self._end_trace(chain_run)\n        self._on_chain_end(chain_run)\n        return chain_run\n\n    def on_chain_error(\n        self,\n        error: BaseException,\n        *,\n        inputs: Optional[dict[str, Any]] = None,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Handle an error for a chain run.\n\n        Args:\n            error: The error.\n            inputs: The inputs for the chain. Defaults to None.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        chain_run = self._errored_chain_run(\n            error=error,\n            run_id=run_id,\n            inputs=inputs,\n            **kwargs,\n        )\n        self._end_trace(chain_run)\n        self._on_chain_error(chain_run)\n        return chain_run\n\n    def on_tool_start(\n        self,\n        serialized: dict[str, Any],\n        input_str: str,\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Start a trace for a tool run.\n\n        Args:\n            serialized: The serialized tool.\n            input_str: The input string.\n            run_id: The run ID.\n            tags: The tags for the run. Defaults to None.\n            parent_run_id: The parent run ID. Defaults to None.\n            metadata: The metadata for the run. Defaults to None.\n            name: The name of the run.\n            inputs: The inputs for the tool.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        tool_run = self._create_tool_run(\n            serialized=serialized,\n            input_str=input_str,\n            run_id=run_id,\n            tags=tags,\n            parent_run_id=parent_run_id,\n            metadata=metadata,\n            name=name,\n            inputs=inputs,\n            **kwargs,\n        )\n        self._start_trace(tool_run)\n        self._on_tool_start(tool_run)\n        return tool_run\n\n    def on_tool_end(self, output: Any, *, run_id: UUID, **kwargs: Any) -> Run:\n        \"\"\"End a trace for a tool run.\n\n        Args:\n            output: The output for the tool.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        tool_run = self._complete_tool_run(\n            output=output,\n            run_id=run_id,\n            **kwargs,\n        )\n        self._end_trace(tool_run)\n        self._on_tool_end(tool_run)\n        return tool_run\n\n    def on_tool_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Handle an error for a tool run.\n\n        Args:\n            error: The error.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        tool_run = self._errored_tool_run(\n            error=error,\n            run_id=run_id,\n        )\n        self._end_trace(tool_run)\n        self._on_tool_error(tool_run)\n        return tool_run\n\n    def on_retriever_start(\n        self,\n        serialized: dict[str, Any],\n        query: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Run when the Retriever starts running.\n\n        Args:\n            serialized: The serialized retriever.\n            query: The query.\n            run_id: The run ID.\n            parent_run_id: The parent run ID. Defaults to None.\n            tags: The tags for the run. Defaults to None.\n            metadata: The metadata for the run. Defaults to None.\n            name: The name of the run.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        retrieval_run = self._create_retrieval_run(\n            serialized=serialized,\n            query=query,\n            run_id=run_id,\n            parent_run_id=parent_run_id,\n            tags=tags,\n            metadata=metadata,\n            name=name,\n            **kwargs,\n        )\n        self._start_trace(retrieval_run)\n        self._on_retriever_start(retrieval_run)\n        return retrieval_run\n\n    def on_retriever_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Run when Retriever errors.\n\n        Args:\n            error: The error.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        retrieval_run = self._errored_retrieval_run(\n            error=error,\n            run_id=run_id,\n            **kwargs,\n        )\n        self._end_trace(retrieval_run)\n        self._on_retriever_error(retrieval_run)\n        return retrieval_run\n\n    def on_retriever_end(\n        self, documents: Sequence[Document], *, run_id: UUID, **kwargs: Any\n    ) -> Run:\n        \"\"\"Run when the Retriever ends running.\n\n        Args:\n            documents: The documents.\n            run_id: The run ID.\n            kwargs: Additional arguments.\n\n        Returns:\n            The run.\n        \"\"\"\n        retrieval_run = self._complete_retrieval_run(\n            documents=documents,\n            run_id=run_id,\n            **kwargs,\n        )\n        self._end_trace(retrieval_run)\n        self._on_retriever_end(retrieval_run)\n        return retrieval_run\n\n    def __deepcopy__(self, memo: dict) -> BaseTracer:\n        \"\"\"Deepcopy the tracer.\"\"\"\n        return self\n\n    def __copy__(self) -> BaseTracer:\n        \"\"\"Copy the tracer.\"\"\"\n        return self\n\n\nclass AsyncBaseTracer(_TracerCore, AsyncCallbackHandler, ABC):\n    \"\"\"Async Base interface for tracers.\"\"\"\n\n    @abstractmethod\n    async def _persist_run(self, run: Run) -> None:\n        \"\"\"Persist a run.\"\"\"\n\n    async def _start_trace(self, run: Run) -> None:\n        \"\"\"Start a trace for a run.\n\n        Starting a trace will run concurrently with each _on_[run_type]_start method.\n        No _on_[run_type]_start callback should depend on operations in _start_trace.\n        \"\"\"\n        super()._start_trace(run)\n        await self._on_run_create(run)\n\n    async def _end_trace(self, run: Run) -> None:\n        \"\"\"End a trace for a run.\n\n        Ending a trace will run concurrently with each _on_[run_type]_end method.\n        No _on_[run_type]_end callback should depend on operations in _end_trace.\n        \"\"\"\n        if not run.parent_run_id:\n            await self._persist_run(run)\n        self.run_map.pop(str(run.id))\n        await self._on_run_update(run)\n\n    async def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Any:\n        chat_model_run = self._create_chat_model_run(\n            serialized=serialized,\n            messages=messages,\n            run_id=run_id,\n            parent_run_id=parent_run_id,\n            tags=tags,\n            metadata=metadata,\n            name=name,\n            **kwargs,\n        )\n        tasks = [\n            self._start_trace(chat_model_run),\n            self._on_chat_model_start(chat_model_run),\n        ]\n        await asyncio.gather(*tasks)\n        return chat_model_run\n\n    async def on_llm_start(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        llm_run = self._create_llm_run(\n            serialized=serialized,\n            prompts=prompts,\n            run_id=run_id,\n            parent_run_id=parent_run_id,\n            tags=tags,\n            metadata=metadata,\n            **kwargs,\n        )\n        tasks = [self._start_trace(llm_run), self._on_llm_start(llm_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> None:\n        llm_run = self._llm_run_with_token_event(\n            token=token,\n            run_id=run_id,\n            chunk=chunk,\n            parent_run_id=parent_run_id,\n            **kwargs,\n        )\n        await self._on_llm_new_token(llm_run, token, chunk)\n\n    async def on_retry(\n        self,\n        retry_state: RetryCallState,\n        *,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> None:\n        self._llm_run_with_retry_event(\n            retry_state=retry_state,\n            run_id=run_id,\n        )\n\n    async def on_llm_end(\n        self,\n        response: LLMResult,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        llm_run = self._complete_llm_run(\n            response=response,\n            run_id=run_id,\n        )\n        tasks = [self._on_llm_end(llm_run), self._end_trace(llm_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_llm_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        llm_run = self._errored_llm_run(\n            error=error,\n            run_id=run_id,\n        )\n        tasks = [self._on_llm_error(llm_run), self._end_trace(llm_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_chain_start(\n        self,\n        serialized: dict[str, Any],\n        inputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_type: Optional[str] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        chain_run = self._create_chain_run(\n            serialized=serialized,\n            inputs=inputs,\n            run_id=run_id,\n            tags=tags,\n            parent_run_id=parent_run_id,\n            metadata=metadata,\n            run_type=run_type,\n            name=name,\n            **kwargs,\n        )\n        tasks = [self._start_trace(chain_run), self._on_chain_start(chain_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_chain_end(\n        self,\n        outputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        chain_run = self._complete_chain_run(\n            outputs=outputs,\n            run_id=run_id,\n            inputs=inputs,\n            **kwargs,\n        )\n        tasks = [self._end_trace(chain_run), self._on_chain_end(chain_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_chain_error(\n        self,\n        error: BaseException,\n        *,\n        inputs: Optional[dict[str, Any]] = None,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> None:\n        chain_run = self._errored_chain_run(\n            error=error,\n            inputs=inputs,\n            run_id=run_id,\n            **kwargs,\n        )\n        tasks = [self._end_trace(chain_run), self._on_chain_error(chain_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_tool_start(\n        self,\n        serialized: dict[str, Any],\n        input_str: str,\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        tool_run = self._create_tool_run(\n            serialized=serialized,\n            input_str=input_str,\n            run_id=run_id,\n            tags=tags,\n            parent_run_id=parent_run_id,\n            metadata=metadata,\n            inputs=inputs,\n            **kwargs,\n        )\n        tasks = [self._start_trace(tool_run), self._on_tool_start(tool_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_tool_end(\n        self,\n        output: Any,\n        *,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> None:\n        tool_run = self._complete_tool_run(\n            output=output,\n            run_id=run_id,\n            **kwargs,\n        )\n        tasks = [self._end_trace(tool_run), self._on_tool_end(tool_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_tool_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        tool_run = self._errored_tool_run(\n            error=error,\n            run_id=run_id,\n        )\n        tasks = [self._end_trace(tool_run), self._on_tool_error(tool_run)]\n        await asyncio.gather(*tasks)\n\n    async def on_retriever_start(\n        self,\n        serialized: dict[str, Any],\n        query: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        retriever_run = self._create_retrieval_run(\n            serialized=serialized,\n            query=query,\n            run_id=run_id,\n            parent_run_id=parent_run_id,\n            tags=tags,\n            metadata=metadata,\n            name=name,\n        )\n        tasks = [\n            self._start_trace(retriever_run),\n            self._on_retriever_start(retriever_run),\n        ]\n        await asyncio.gather(*tasks)\n\n    async def on_retriever_error(\n        self,\n        error: BaseException,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        retrieval_run = self._errored_retrieval_run(\n            error=error,\n            run_id=run_id,\n            **kwargs,\n        )\n        tasks = [\n            self._end_trace(retrieval_run),\n            self._on_retriever_error(retrieval_run),\n        ]\n        await asyncio.gather(*tasks)\n\n    async def on_retriever_end(\n        self,\n        documents: Sequence[Document],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        retrieval_run = self._complete_retrieval_run(\n            documents=documents,\n            run_id=run_id,\n            **kwargs,\n        )\n        tasks = [self._end_trace(retrieval_run), self._on_retriever_end(retrieval_run)]\n        await asyncio.gather(*tasks)\n\n    async def _on_run_create(self, run: Run) -> None:\n        \"\"\"Process a run upon creation.\"\"\"\n\n    async def _on_run_update(self, run: Run) -> None:\n        \"\"\"Process a run upon update.\"\"\"\n\n    async def _on_llm_start(self, run: Run) -> None:\n        \"\"\"Process the LLM Run upon start.\"\"\"\n\n    async def _on_llm_end(self, run: Run) -> None:\n        \"\"\"Process the LLM Run.\"\"\"\n\n    async def _on_llm_error(self, run: Run) -> None:\n        \"\"\"Process the LLM Run upon error.\"\"\"\n\n    async def _on_llm_new_token(\n        self,\n        run: Run,\n        token: str,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],\n    ) -> None:\n        \"\"\"Process new LLM token.\"\"\"\n\n    async def _on_chain_start(self, run: Run) -> None:\n        \"\"\"Process the Chain Run upon start.\"\"\"\n\n    async def _on_chain_end(self, run: Run) -> None:\n        \"\"\"Process the Chain Run.\"\"\"\n\n    async def _on_chain_error(self, run: Run) -> None:\n        \"\"\"Process the Chain Run upon error.\"\"\"\n\n    async def _on_tool_start(self, run: Run) -> None:\n        \"\"\"Process the Tool Run upon start.\"\"\"\n\n    async def _on_tool_end(self, run: Run) -> None:\n        \"\"\"Process the Tool Run.\"\"\"\n\n    async def _on_tool_error(self, run: Run) -> None:\n        \"\"\"Process the Tool Run upon error.\"\"\"\n\n    async def _on_chat_model_start(self, run: Run) -> None:\n        \"\"\"Process the Chat Model Run upon start.\"\"\"\n\n    async def _on_retriever_start(self, run: Run) -> None:\n        \"\"\"Process the Retriever Run upon start.\"\"\"\n\n    async def _on_retriever_end(self, run: Run) -> None:\n        \"\"\"Process the Retriever Run.\"\"\"\n\n    async def _on_retriever_error(self, run: Run) -> None:\n        \"\"\"Process the Retriever Run upon error.\"\"\"\n",
        "patch": "@@ -5,26 +5,27 @@\n import asyncio\n import logging\n from abc import ABC, abstractmethod\n-from collections.abc import Sequence\n from typing import (\n     TYPE_CHECKING,\n     Any,\n     Optional,\n     Union,\n )\n-from uuid import UUID\n-\n-from tenacity import RetryCallState\n \n from langchain_core.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n from langchain_core.exceptions import TracerException  # noqa\n-from langchain_core.messages import BaseMessage\n-from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n from langchain_core.tracers.core import _TracerCore\n-from langchain_core.tracers.schemas import Run\n \n if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+    from uuid import UUID\n+\n+    from tenacity import RetryCallState\n+\n     from langchain_core.documents import Document\n+    from langchain_core.messages import BaseMessage\n+    from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult\n+    from langchain_core.tracers.schemas import Run\n \n logger = logging.getLogger(__name__)\n "
      },
      {
        "filename": "libs/core/langchain_core/tracers/context.py",
        "content_before": "from __future__ import annotations\n\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import UUID\n\nfrom langsmith import run_helpers as ls_rh\nfrom langsmith import utils as ls_utils\n\nfrom langchain_core.tracers.langchain import LangChainTracer\nfrom langchain_core.tracers.run_collector import RunCollectorCallbackHandler\nfrom langchain_core.tracers.schemas import TracerSessionV1\n\nif TYPE_CHECKING:\n    from langsmith import Client as LangSmithClient\n\n    from langchain_core.callbacks.base import BaseCallbackHandler, Callbacks\n    from langchain_core.callbacks.manager import AsyncCallbackManager, CallbackManager\n\n# for backwards partial compatibility if this is imported by users but unused\ntracing_callback_var: Any = None\ntracing_v2_callback_var: ContextVar[Optional[LangChainTracer]] = ContextVar(\n    \"tracing_callback_v2\", default=None\n)\nrun_collector_var: ContextVar[Optional[RunCollectorCallbackHandler]] = ContextVar(\n    \"run_collector\", default=None\n)\n\n\n@contextmanager\ndef tracing_enabled(\n    session_name: str = \"default\",\n) -> Generator[TracerSessionV1, None, None]:\n    \"\"\"Throw an error because this has been replaced by tracing_v2_enabled.\"\"\"\n    msg = (\n        \"tracing_enabled is no longer supported. Please use tracing_enabled_v2 instead.\"\n    )\n    raise RuntimeError(msg)\n\n\n@contextmanager\ndef tracing_v2_enabled(\n    project_name: Optional[str] = None,\n    *,\n    example_id: Optional[Union[str, UUID]] = None,\n    tags: Optional[list[str]] = None,\n    client: Optional[LangSmithClient] = None,\n) -> Generator[LangChainTracer, None, None]:\n    \"\"\"Instruct LangChain to log all runs in context to LangSmith.\n\n    Args:\n        project_name (str, optional): The name of the project.\n            Defaults to \"default\".\n        example_id (str or UUID, optional): The ID of the example.\n            Defaults to None.\n        tags (List[str], optional): The tags to add to the run.\n            Defaults to None.\n        client (LangSmithClient, optional): The client of the langsmith.\n            Defaults to None.\n\n    Yields:\n        LangChainTracer: The LangChain tracer.\n\n    Example:\n        >>> with tracing_v2_enabled():\n        ...     # LangChain code will automatically be traced\n\n        You can use this to fetch the LangSmith run URL:\n\n        >>> with tracing_v2_enabled() as cb:\n        ...     chain.invoke(\"foo\")\n        ...     run_url = cb.get_run_url()\n    \"\"\"\n    if isinstance(example_id, str):\n        example_id = UUID(example_id)\n    cb = LangChainTracer(\n        example_id=example_id,\n        project_name=project_name,\n        tags=tags,\n        client=client,\n    )\n    try:\n        tracing_v2_callback_var.set(cb)\n        yield cb\n    finally:\n        tracing_v2_callback_var.set(None)\n\n\n@contextmanager\ndef collect_runs() -> Generator[RunCollectorCallbackHandler, None, None]:\n    \"\"\"Collect all run traces in context.\n\n    Yields:\n        run_collector.RunCollectorCallbackHandler: The run collector callback handler.\n\n    Example:\n        >>> with collect_runs() as runs_cb:\n                chain.invoke(\"foo\")\n                run_id = runs_cb.traced_runs[0].id\n    \"\"\"\n    cb = RunCollectorCallbackHandler()\n    run_collector_var.set(cb)\n    yield cb\n    run_collector_var.set(None)\n\n\ndef _get_trace_callbacks(\n    project_name: Optional[str] = None,\n    example_id: Optional[Union[str, UUID]] = None,\n    callback_manager: Optional[Union[CallbackManager, AsyncCallbackManager]] = None,\n) -> Callbacks:\n    if _tracing_v2_is_enabled():\n        project_name_ = project_name or _get_tracer_project()\n        tracer = tracing_v2_callback_var.get() or LangChainTracer(\n            project_name=project_name_,\n            example_id=example_id,\n        )\n        if callback_manager is None:\n            from langchain_core.callbacks.base import Callbacks\n\n            cb = cast(Callbacks, [tracer])\n        else:\n            if not any(\n                isinstance(handler, LangChainTracer)\n                for handler in callback_manager.handlers\n            ):\n                callback_manager.add_handler(tracer, True)\n                # If it already has a LangChainTracer, we don't need to add another one.\n                # this would likely mess up the trace hierarchy.\n            cb = callback_manager\n    else:\n        cb = None\n    return cb\n\n\ndef _tracing_v2_is_enabled() -> Union[bool, Literal[\"local\"]]:\n    if tracing_v2_callback_var.get() is not None:\n        return True\n    return ls_utils.tracing_is_enabled()\n\n\ndef _get_tracer_project() -> str:\n    tracing_context = ls_rh.get_tracing_context()\n    run_tree = tracing_context[\"parent\"]\n    if run_tree is None and tracing_context[\"project_name\"] is not None:\n        return tracing_context[\"project_name\"]\n    return getattr(\n        run_tree,\n        \"session_name\",\n        getattr(\n            # Note, if people are trying to nest @traceable functions and the\n            # tracing_v2_enabled context manager, this will likely mess up the\n            # tree structure.\n            tracing_v2_callback_var.get(),\n            \"project\",\n            # Have to set this to a string even though it always will return\n            # a string because `get_tracer_project` technically can return\n            # None, but only when a specific argument is supplied.\n            # Therefore, this just tricks the mypy type checker\n            str(ls_utils.get_tracer_project()),\n        ),\n    )\n\n\n_configure_hooks: list[\n    tuple[\n        ContextVar[Optional[BaseCallbackHandler]],\n        bool,\n        Optional[type[BaseCallbackHandler]],\n        Optional[str],\n    ]\n] = []\n\n\ndef register_configure_hook(\n    context_var: ContextVar[Optional[Any]],\n    inheritable: bool,\n    handle_class: Optional[type[BaseCallbackHandler]] = None,\n    env_var: Optional[str] = None,\n) -> None:\n    \"\"\"Register a configure hook.\n\n    Args:\n        context_var (ContextVar[Optional[Any]]): The context variable.\n        inheritable (bool): Whether the context variable is inheritable.\n        handle_class (Optional[Type[BaseCallbackHandler]], optional):\n          The callback handler class. Defaults to None.\n        env_var (Optional[str], optional): The environment variable. Defaults to None.\n\n    Raises:\n        ValueError: If env_var is set, handle_class must also be set\n          to a non-None value.\n    \"\"\"\n    if env_var is not None and handle_class is None:\n        msg = \"If env_var is set, handle_class must also be set to a non-None value.\"\n        raise ValueError(msg)\n    from langchain_core.callbacks.base import BaseCallbackHandler\n\n    _configure_hooks.append(\n        (\n            # the typings of ContextVar do not have the generic arg set as covariant\n            # so we have to cast it\n            cast(ContextVar[Optional[BaseCallbackHandler]], context_var),\n            inheritable,\n            handle_class,\n            env_var,\n        )\n    )\n\n\nregister_configure_hook(run_collector_var, False)\n",
        "patch": "@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-from collections.abc import Generator\n from contextlib import contextmanager\n from contextvars import ContextVar\n from typing import (\n@@ -18,13 +17,15 @@\n \n from langchain_core.tracers.langchain import LangChainTracer\n from langchain_core.tracers.run_collector import RunCollectorCallbackHandler\n-from langchain_core.tracers.schemas import TracerSessionV1\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator\n+\n     from langsmith import Client as LangSmithClient\n \n     from langchain_core.callbacks.base import BaseCallbackHandler, Callbacks\n     from langchain_core.callbacks.manager import AsyncCallbackManager, CallbackManager\n+    from langchain_core.tracers.schemas import TracerSessionV1\n \n # for backwards partial compatibility if this is imported by users but unused\n tracing_callback_var: Any = None"
      },
      {
        "filename": "libs/core/langchain_core/tracers/core.py",
        "content_before": "\"\"\"Utilities for the root listener.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport sys\nimport traceback\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Coroutine, Sequence\nfrom datetime import datetime, timezone\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Literal,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import UUID\n\nfrom tenacity import RetryCallState\n\nfrom langchain_core.exceptions import TracerException\nfrom langchain_core.load import dumpd\nfrom langchain_core.messages import BaseMessage\nfrom langchain_core.outputs import (\n    ChatGeneration,\n    ChatGenerationChunk,\n    GenerationChunk,\n    LLMResult,\n)\nfrom langchain_core.tracers.schemas import Run\n\nif TYPE_CHECKING:\n    from langchain_core.documents import Document\n\nlogger = logging.getLogger(__name__)\n\nSCHEMA_FORMAT_TYPE = Literal[\"original\", \"streaming_events\"]\n\n\nclass _TracerCore(ABC):\n    \"\"\"Abstract base class for tracers.\n\n    This class provides common methods, and reusable methods for tracers.\n    \"\"\"\n\n    log_missing_parent: bool = True\n\n    def __init__(\n        self,\n        *,\n        _schema_format: Literal[\n            \"original\", \"streaming_events\", \"original+chat\"\n        ] = \"original\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize the tracer.\n\n        Args:\n            _schema_format: Primarily changes how the inputs and outputs are\n                handled. For internal use only. This API will change.\n\n                - 'original' is the format used by all current tracers.\n                  This format is slightly inconsistent with respect to inputs\n                  and outputs.\n                - 'streaming_events' is used for supporting streaming events,\n                  for internal usage. It will likely change in the future, or\n                  be deprecated entirely in favor of a dedicated async tracer\n                  for streaming events.\n                - 'original+chat' is a format that is the same as 'original'\n                  except it does NOT raise an attribute error on_chat_model_start\n            kwargs: Additional keyword arguments that will be passed to\n                the superclass.\n        \"\"\"\n        super().__init__(**kwargs)\n        self._schema_format = _schema_format  # For internal use only API will change.\n        self.run_map: dict[str, Run] = {}\n        \"\"\"Map of run ID to run. Cleared on run end.\"\"\"\n        self.order_map: dict[UUID, tuple[UUID, str]] = {}\n        \"\"\"Map of run ID to (trace_id, dotted_order). Cleared when tracer GCed.\"\"\"\n\n    @abstractmethod\n    def _persist_run(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Persist a run.\"\"\"\n\n    @staticmethod\n    def _add_child_run(\n        parent_run: Run,\n        child_run: Run,\n    ) -> None:\n        \"\"\"Add child run to a chain run or tool run.\"\"\"\n        parent_run.child_runs.append(child_run)\n\n    @staticmethod\n    def _get_stacktrace(error: BaseException) -> str:\n        \"\"\"Get the stacktrace of the parent error.\"\"\"\n        msg = repr(error)\n        try:\n            if sys.version_info < (3, 10):\n                tb = traceback.format_exception(\n                    error.__class__, error, error.__traceback__\n                )\n            else:\n                tb = traceback.format_exception(error)\n            return (msg + \"\\n\\n\".join(tb)).strip()\n        except:  # noqa: E722\n            return msg\n\n    def _start_trace(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:  # type: ignore[return]\n        current_dotted_order = run.start_time.strftime(\"%Y%m%dT%H%M%S%fZ\") + str(run.id)\n        if run.parent_run_id:\n            if parent := self.order_map.get(run.parent_run_id):\n                run.trace_id, run.dotted_order = parent\n                run.dotted_order += \".\" + current_dotted_order\n                if parent_run := self.run_map.get(str(run.parent_run_id)):\n                    self._add_child_run(parent_run, run)\n            else:\n                if self.log_missing_parent:\n                    logger.debug(\n                        f\"Parent run {run.parent_run_id} not found for run {run.id}.\"\n                        \" Treating as a root run.\"\n                    )\n                run.parent_run_id = None\n                run.trace_id = run.id\n                run.dotted_order = current_dotted_order\n        else:\n            run.trace_id = run.id\n            run.dotted_order = current_dotted_order\n        self.order_map[run.id] = (run.trace_id, run.dotted_order)\n        self.run_map[str(run.id)] = run\n\n    def _get_run(\n        self, run_id: UUID, run_type: Union[str, set[str], None] = None\n    ) -> Run:\n        try:\n            run = self.run_map[str(run_id)]\n        except KeyError as exc:\n            msg = f\"No indexed run ID {run_id}.\"\n            raise TracerException(msg) from exc\n\n        if isinstance(run_type, str):\n            run_types: Union[set[str], None] = {run_type}\n        else:\n            run_types = run_type\n        if run_types is not None and run.run_type not in run_types:\n            msg = (\n                f\"Found {run.run_type} run at ID {run_id}, \"\n                f\"but expected {run_types} run.\"\n            )\n            raise TracerException(msg)\n        return run\n\n    def _create_chat_model_run(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Create a chat model run.\"\"\"\n        if self._schema_format not in (\"streaming_events\", \"original+chat\"):\n            # Please keep this un-implemented for backwards compatibility.\n            # When it's unimplemented old tracers that use the \"original\" format\n            # fallback on the on_llm_start method implementation if they\n            # find that the on_chat_model_start method is not implemented.\n            # This can eventually be cleaned up by writing a \"modern\" tracer\n            # that has all the updated schema changes corresponding to\n            # the \"streaming_events\" format.\n            msg = (\n                f\"Chat model tracing is not supported in \"\n                f\"for {self._schema_format} format.\"\n            )\n            raise NotImplementedError(msg)\n        start_time = datetime.now(timezone.utc)\n        if metadata:\n            kwargs.update({\"metadata\": metadata})\n        return Run(\n            id=run_id,\n            parent_run_id=parent_run_id,\n            serialized=serialized,\n            inputs={\"messages\": [[dumpd(msg) for msg in batch] for batch in messages]},\n            extra=kwargs,\n            events=[{\"name\": \"start\", \"time\": start_time}],\n            start_time=start_time,\n            # WARNING: This is valid ONLY for streaming_events.\n            # run_type=\"llm\" is what's used by virtually all tracers.\n            # Changing this to \"chat_model\" may break triggering on_llm_start\n            run_type=\"chat_model\",\n            tags=tags,\n            name=name,  # type: ignore[arg-type]\n        )\n\n    def _create_llm_run(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Create a llm run.\"\"\"\n        start_time = datetime.now(timezone.utc)\n        if metadata:\n            kwargs.update({\"metadata\": metadata})\n        return Run(\n            id=run_id,\n            parent_run_id=parent_run_id,\n            serialized=serialized,\n            # TODO: Figure out how to expose kwargs here\n            inputs={\"prompts\": prompts},\n            extra=kwargs,\n            events=[{\"name\": \"start\", \"time\": start_time}],\n            start_time=start_time,\n            run_type=\"llm\",\n            tags=tags or [],\n            name=name,  # type: ignore[arg-type]\n        )\n\n    def _llm_run_with_token_event(\n        self,\n        token: str,\n        run_id: UUID,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Append token event to LLM run and return the run.\"\"\"\n        llm_run = self._get_run(run_id, run_type={\"llm\", \"chat_model\"})\n        event_kwargs: dict[str, Any] = {\"token\": token}\n        if chunk:\n            event_kwargs[\"chunk\"] = chunk\n        llm_run.events.append(\n            {\n                \"name\": \"new_token\",\n                \"time\": datetime.now(timezone.utc),\n                \"kwargs\": event_kwargs,\n            },\n        )\n        return llm_run\n\n    def _llm_run_with_retry_event(\n        self,\n        retry_state: RetryCallState,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        llm_run = self._get_run(run_id)\n        retry_d: dict[str, Any] = {\n            \"slept\": retry_state.idle_for,\n            \"attempt\": retry_state.attempt_number,\n        }\n        if retry_state.outcome is None:\n            retry_d[\"outcome\"] = \"N/A\"\n        elif retry_state.outcome.failed:\n            retry_d[\"outcome\"] = \"failed\"\n            exception = retry_state.outcome.exception()\n            retry_d[\"exception\"] = str(exception)\n            retry_d[\"exception_type\"] = exception.__class__.__name__\n        else:\n            retry_d[\"outcome\"] = \"success\"\n            retry_d[\"result\"] = str(retry_state.outcome.result())\n        llm_run.events.append(\n            {\n                \"name\": \"retry\",\n                \"time\": datetime.now(timezone.utc),\n                \"kwargs\": retry_d,\n            },\n        )\n        return llm_run\n\n    def _complete_llm_run(self, response: LLMResult, run_id: UUID) -> Run:\n        llm_run = self._get_run(run_id, run_type={\"llm\", \"chat_model\"})\n        llm_run.outputs = response.model_dump()\n        for i, generations in enumerate(response.generations):\n            for j, generation in enumerate(generations):\n                output_generation = llm_run.outputs[\"generations\"][i][j]\n                if \"message\" in output_generation:\n                    output_generation[\"message\"] = dumpd(\n                        cast(ChatGeneration, generation).message\n                    )\n        llm_run.end_time = datetime.now(timezone.utc)\n        llm_run.events.append({\"name\": \"end\", \"time\": llm_run.end_time})\n\n        return llm_run\n\n    def _errored_llm_run(self, error: BaseException, run_id: UUID) -> Run:\n        llm_run = self._get_run(run_id, run_type={\"llm\", \"chat_model\"})\n        llm_run.error = self._get_stacktrace(error)\n        llm_run.end_time = datetime.now(timezone.utc)\n        llm_run.events.append({\"name\": \"error\", \"time\": llm_run.end_time})\n\n        return llm_run\n\n    def _create_chain_run(\n        self,\n        serialized: dict[str, Any],\n        inputs: dict[str, Any],\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_type: Optional[str] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Create a chain Run.\"\"\"\n        start_time = datetime.now(timezone.utc)\n        if metadata:\n            kwargs.update({\"metadata\": metadata})\n        return Run(\n            id=run_id,\n            parent_run_id=parent_run_id,\n            serialized=serialized,\n            inputs=self._get_chain_inputs(inputs),\n            extra=kwargs,\n            events=[{\"name\": \"start\", \"time\": start_time}],\n            start_time=start_time,\n            child_runs=[],\n            run_type=run_type or \"chain\",\n            name=name,  # type: ignore[arg-type]\n            tags=tags or [],\n        )\n\n    def _get_chain_inputs(self, inputs: Any) -> Any:\n        \"\"\"Get the inputs for a chain run.\"\"\"\n        if self._schema_format in (\"original\", \"original+chat\"):\n            return inputs if isinstance(inputs, dict) else {\"input\": inputs}\n        elif self._schema_format == \"streaming_events\":\n            return {\n                \"input\": inputs,\n            }\n        else:\n            msg = f\"Invalid format: {self._schema_format}\"\n            raise ValueError(msg)\n\n    def _get_chain_outputs(self, outputs: Any) -> Any:\n        \"\"\"Get the outputs for a chain run.\"\"\"\n        if self._schema_format in (\"original\", \"original+chat\"):\n            return outputs if isinstance(outputs, dict) else {\"output\": outputs}\n        elif self._schema_format == \"streaming_events\":\n            return {\n                \"output\": outputs,\n            }\n        else:\n            msg = f\"Invalid format: {self._schema_format}\"\n            raise ValueError(msg)\n\n    def _complete_chain_run(\n        self,\n        outputs: dict[str, Any],\n        run_id: UUID,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Update a chain run with outputs and end time.\"\"\"\n        chain_run = self._get_run(run_id)\n        chain_run.outputs = self._get_chain_outputs(outputs)\n        chain_run.end_time = datetime.now(timezone.utc)\n        chain_run.events.append({\"name\": \"end\", \"time\": chain_run.end_time})\n        if inputs is not None:\n            chain_run.inputs = self._get_chain_inputs(inputs)\n        return chain_run\n\n    def _errored_chain_run(\n        self,\n        error: BaseException,\n        inputs: Optional[dict[str, Any]],\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        chain_run = self._get_run(run_id)\n        chain_run.error = self._get_stacktrace(error)\n        chain_run.end_time = datetime.now(timezone.utc)\n        chain_run.events.append({\"name\": \"error\", \"time\": chain_run.end_time})\n        if inputs is not None:\n            chain_run.inputs = self._get_chain_inputs(inputs)\n        return chain_run\n\n    def _create_tool_run(\n        self,\n        serialized: dict[str, Any],\n        input_str: str,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Create a tool run.\"\"\"\n        start_time = datetime.now(timezone.utc)\n        if metadata:\n            kwargs.update({\"metadata\": metadata})\n\n        if self._schema_format in (\"original\", \"original+chat\"):\n            inputs = {\"input\": input_str}\n        elif self._schema_format == \"streaming_events\":\n            inputs = {\"input\": inputs}\n        else:\n            msg = f\"Invalid format: {self._schema_format}\"\n            raise AssertionError(msg)\n\n        return Run(\n            id=run_id,\n            parent_run_id=parent_run_id,\n            serialized=serialized,\n            # Wrapping in dict since Run requires a dict object.\n            inputs=inputs,\n            extra=kwargs,\n            events=[{\"name\": \"start\", \"time\": start_time}],\n            start_time=start_time,\n            child_runs=[],\n            run_type=\"tool\",\n            tags=tags or [],\n            name=name,  # type: ignore[arg-type]\n        )\n\n    def _complete_tool_run(\n        self,\n        output: dict[str, Any],\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Update a tool run with outputs and end time.\"\"\"\n        tool_run = self._get_run(run_id, run_type=\"tool\")\n        tool_run.outputs = {\"output\": output}\n        tool_run.end_time = datetime.now(timezone.utc)\n        tool_run.events.append({\"name\": \"end\", \"time\": tool_run.end_time})\n        return tool_run\n\n    def _errored_tool_run(\n        self,\n        error: BaseException,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Update a tool run with error and end time.\"\"\"\n        tool_run = self._get_run(run_id, run_type=\"tool\")\n        tool_run.error = self._get_stacktrace(error)\n        tool_run.end_time = datetime.now(timezone.utc)\n        tool_run.events.append({\"name\": \"error\", \"time\": tool_run.end_time})\n        return tool_run\n\n    def _create_retrieval_run(\n        self,\n        serialized: dict[str, Any],\n        query: str,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Create a retrieval run.\"\"\"\n        start_time = datetime.now(timezone.utc)\n        if metadata:\n            kwargs.update({\"metadata\": metadata})\n        return Run(\n            id=run_id,\n            name=name or \"Retriever\",\n            parent_run_id=parent_run_id,\n            serialized=serialized,\n            inputs={\"query\": query},\n            extra=kwargs,\n            events=[{\"name\": \"start\", \"time\": start_time}],\n            start_time=start_time,\n            tags=tags,\n            child_runs=[],\n            run_type=\"retriever\",\n        )\n\n    def _complete_retrieval_run(\n        self,\n        documents: Sequence[Document],\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Update a retrieval run with outputs and end time.\"\"\"\n        retrieval_run = self._get_run(run_id, run_type=\"retriever\")\n        retrieval_run.outputs = {\"documents\": documents}\n        retrieval_run.end_time = datetime.now(timezone.utc)\n        retrieval_run.events.append({\"name\": \"end\", \"time\": retrieval_run.end_time})\n        return retrieval_run\n\n    def _errored_retrieval_run(\n        self,\n        error: BaseException,\n        run_id: UUID,\n        **kwargs: Any,\n    ) -> Run:\n        retrieval_run = self._get_run(run_id, run_type=\"retriever\")\n        retrieval_run.error = self._get_stacktrace(error)\n        retrieval_run.end_time = datetime.now(timezone.utc)\n        retrieval_run.events.append({\"name\": \"error\", \"time\": retrieval_run.end_time})\n        return retrieval_run\n\n    def __deepcopy__(self, memo: dict) -> _TracerCore:\n        \"\"\"Deepcopy the tracer.\"\"\"\n        return self\n\n    def __copy__(self) -> _TracerCore:\n        \"\"\"Copy the tracer.\"\"\"\n        return self\n\n    def _end_trace(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"End a trace for a run.\"\"\"\n        return None\n\n    def _on_run_create(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process a run upon creation.\"\"\"\n        return None\n\n    def _on_run_update(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process a run upon update.\"\"\"\n        return None\n\n    def _on_llm_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the LLM Run upon start.\"\"\"\n        return None\n\n    def _on_llm_new_token(\n        self,\n        run: Run,\n        token: str,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],\n    ) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process new LLM token.\"\"\"\n        return None\n\n    def _on_llm_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the LLM Run.\"\"\"\n        return None\n\n    def _on_llm_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the LLM Run upon error.\"\"\"\n        return None\n\n    def _on_chain_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Chain Run upon start.\"\"\"\n        return None\n\n    def _on_chain_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Chain Run.\"\"\"\n        return None\n\n    def _on_chain_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Chain Run upon error.\"\"\"\n        return None\n\n    def _on_tool_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Tool Run upon start.\"\"\"\n        return None\n\n    def _on_tool_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Tool Run.\"\"\"\n        return None\n\n    def _on_tool_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Tool Run upon error.\"\"\"\n        return None\n\n    def _on_chat_model_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Chat Model Run upon start.\"\"\"\n        return None\n\n    def _on_retriever_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Retriever Run upon start.\"\"\"\n        return None\n\n    def _on_retriever_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Retriever Run.\"\"\"\n        return None\n\n    def _on_retriever_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:\n        \"\"\"Process the Retriever Run upon error.\"\"\"\n        return None\n",
        "patch": "@@ -6,7 +6,6 @@\n import sys\n import traceback\n from abc import ABC, abstractmethod\n-from collections.abc import Coroutine, Sequence\n from datetime import datetime, timezone\n from typing import (\n     TYPE_CHECKING,\n@@ -16,13 +15,9 @@\n     Union,\n     cast,\n )\n-from uuid import UUID\n-\n-from tenacity import RetryCallState\n \n from langchain_core.exceptions import TracerException\n from langchain_core.load import dumpd\n-from langchain_core.messages import BaseMessage\n from langchain_core.outputs import (\n     ChatGeneration,\n     ChatGenerationChunk,\n@@ -32,7 +27,13 @@\n from langchain_core.tracers.schemas import Run\n \n if TYPE_CHECKING:\n+    from collections.abc import Coroutine, Sequence\n+    from uuid import UUID\n+\n+    from tenacity import RetryCallState\n+\n     from langchain_core.documents import Document\n+    from langchain_core.messages import BaseMessage\n \n logger = logging.getLogger(__name__)\n "
      },
      {
        "filename": "libs/core/langchain_core/tracers/evaluation.py",
        "content_before": "\"\"\"A tracer that runs evaluators over completed runs.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport threading\nimport weakref\nfrom collections.abc import Sequence\nfrom concurrent.futures import Future, ThreadPoolExecutor, wait\nfrom typing import Any, Optional, Union, cast\nfrom uuid import UUID\n\nimport langsmith\nfrom langsmith.evaluation.evaluator import EvaluationResult, EvaluationResults\n\nfrom langchain_core.tracers import langchain as langchain_tracer\nfrom langchain_core.tracers.base import BaseTracer\nfrom langchain_core.tracers.context import tracing_v2_enabled\nfrom langchain_core.tracers.langchain import _get_executor\nfrom langchain_core.tracers.schemas import Run\n\nlogger = logging.getLogger(__name__)\n\n_TRACERS: weakref.WeakSet[EvaluatorCallbackHandler] = weakref.WeakSet()\n\n\ndef wait_for_all_evaluators() -> None:\n    \"\"\"Wait for all tracers to finish.\"\"\"\n    global _TRACERS\n    for tracer in list(_TRACERS):\n        if tracer is not None:\n            tracer.wait_for_futures()\n\n\nclass EvaluatorCallbackHandler(BaseTracer):\n    \"\"\"Tracer that runs a run evaluator whenever a run is persisted.\n\n    Args:\n        evaluators : Sequence[RunEvaluator]\n            The run evaluators to apply to all top level runs.\n        client : LangSmith Client, optional\n            The LangSmith client instance to use for evaluating the runs.\n            If not specified, a new instance will be created.\n        example_id : Union[UUID, str], optional\n            The example ID to be associated with the runs.\n        project_name : str, optional\n            The LangSmith project name to be organize eval chain runs under.\n\n    Attributes:\n        example_id : Union[UUID, None]\n            The example ID associated with the runs.\n        client : Client\n            The LangSmith client instance used for evaluating the runs.\n        evaluators : Sequence[RunEvaluator]\n            The sequence of run evaluators to be executed.\n        executor : ThreadPoolExecutor\n            The thread pool executor used for running the evaluators.\n        futures : Set[Future]\n            The set of futures representing the running evaluators.\n        skip_unfinished : bool\n            Whether to skip runs that are not finished or raised\n            an error.\n        project_name : Optional[str]\n            The LangSmith project name to be organize eval chain runs under.\n    \"\"\"\n\n    name: str = \"evaluator_callback_handler\"\n\n    def __init__(\n        self,\n        evaluators: Sequence[langsmith.RunEvaluator],\n        client: Optional[langsmith.Client] = None,\n        example_id: Optional[Union[UUID, str]] = None,\n        skip_unfinished: bool = True,\n        project_name: Optional[str] = \"evaluators\",\n        max_concurrency: Optional[int] = None,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(**kwargs)\n        self.example_id = (\n            UUID(example_id) if isinstance(example_id, str) else example_id\n        )\n        self.client = client or langchain_tracer.get_client()\n        self.evaluators = evaluators\n        if max_concurrency is None:\n            self.executor: Optional[ThreadPoolExecutor] = _get_executor()\n        elif max_concurrency > 0:\n            self.executor = ThreadPoolExecutor(max_workers=max_concurrency)\n            weakref.finalize(\n                self,\n                lambda: cast(ThreadPoolExecutor, self.executor).shutdown(wait=True),\n            )\n        else:\n            self.executor = None\n        self.futures: weakref.WeakSet[Future] = weakref.WeakSet()\n        self.skip_unfinished = skip_unfinished\n        self.project_name = project_name\n        self.logged_eval_results: dict[tuple[str, str], list[EvaluationResult]] = {}\n        self.lock = threading.Lock()\n        global _TRACERS\n        _TRACERS.add(self)\n\n    def _evaluate_in_project(self, run: Run, evaluator: langsmith.RunEvaluator) -> None:\n        \"\"\"Evaluate the run in the project.\n\n        Args:\n        ----------\n        run : Run\n            The run to be evaluated.\n        evaluator : RunEvaluator\n            The evaluator to use for evaluating the run.\n\n        \"\"\"\n        try:\n            if self.project_name is None:\n                eval_result = self.client.evaluate_run(run, evaluator)\n                eval_results = [eval_result]\n            with tracing_v2_enabled(\n                project_name=self.project_name, tags=[\"eval\"], client=self.client\n            ) as cb:\n                reference_example = (\n                    self.client.read_example(run.reference_example_id)\n                    if run.reference_example_id\n                    else None\n                )\n                evaluation_result = evaluator.evaluate_run(\n                    # This is subclass, but getting errors for some reason\n                    run,  # type: ignore\n                    example=reference_example,\n                )\n                eval_results = self._log_evaluation_feedback(\n                    evaluation_result,\n                    run,\n                    source_run_id=cb.latest_run.id if cb.latest_run else None,\n                )\n        except Exception as e:\n            logger.error(\n                f\"Error evaluating run {run.id} with \"\n                f\"{evaluator.__class__.__name__}: {repr(e)}\",\n                exc_info=True,\n            )\n            raise\n        example_id = str(run.reference_example_id)\n        with self.lock:\n            for res in eval_results:\n                run_id = str(getattr(res, \"target_run_id\", run.id))\n                self.logged_eval_results.setdefault((run_id, example_id), []).append(\n                    res\n                )\n\n    def _select_eval_results(\n        self,\n        results: Union[EvaluationResult, EvaluationResults],\n    ) -> list[EvaluationResult]:\n        if isinstance(results, EvaluationResult):\n            results_ = [results]\n        elif isinstance(results, dict) and \"results\" in results:\n            results_ = cast(list[EvaluationResult], results[\"results\"])\n        else:\n            msg = (\n                f\"Invalid evaluation result type {type(results)}.\"\n                \" Expected EvaluationResult or EvaluationResults.\"\n            )\n            raise TypeError(msg)\n        return results_\n\n    def _log_evaluation_feedback(\n        self,\n        evaluator_response: Union[EvaluationResult, EvaluationResults],\n        run: Run,\n        source_run_id: Optional[UUID] = None,\n    ) -> list[EvaluationResult]:\n        results = self._select_eval_results(evaluator_response)\n        for res in results:\n            source_info_: dict[str, Any] = {}\n            if res.evaluator_info:\n                source_info_ = {**res.evaluator_info, **source_info_}\n            run_id_ = getattr(res, \"target_run_id\", None)\n            if run_id_ is None:\n                run_id_ = run.id\n            self.client.create_feedback(\n                run_id_,\n                res.key,\n                score=res.score,\n                value=res.value,\n                comment=res.comment,\n                correction=res.correction,\n                source_info=source_info_,\n                source_run_id=res.source_run_id or source_run_id,\n                feedback_source_type=langsmith.schemas.FeedbackSourceType.MODEL,\n            )\n        return results\n\n    def _persist_run(self, run: Run) -> None:\n        \"\"\"Run the evaluator on the run.\n\n        Args:\n        ----------\n        run : Run\n            The run to be evaluated.\n\n        \"\"\"\n        if self.skip_unfinished and not run.outputs:\n            logger.debug(f\"Skipping unfinished run {run.id}\")\n            return\n        run_ = run.copy()\n        run_.reference_example_id = self.example_id\n        for evaluator in self.evaluators:\n            if self.executor is None:\n                self._evaluate_in_project(run_, evaluator)\n            else:\n                self.futures.add(\n                    self.executor.submit(self._evaluate_in_project, run_, evaluator)\n                )\n\n    def wait_for_futures(self) -> None:\n        \"\"\"Wait for all futures to complete.\"\"\"\n        wait(self.futures)\n",
        "patch": "@@ -5,9 +5,8 @@\n import logging\n import threading\n import weakref\n-from collections.abc import Sequence\n from concurrent.futures import Future, ThreadPoolExecutor, wait\n-from typing import Any, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n from uuid import UUID\n \n import langsmith\n@@ -17,7 +16,11 @@\n from langchain_core.tracers.base import BaseTracer\n from langchain_core.tracers.context import tracing_v2_enabled\n from langchain_core.tracers.langchain import _get_executor\n-from langchain_core.tracers.schemas import Run\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n+    from langchain_core.tracers.schemas import Run\n \n logger = logging.getLogger(__name__)\n "
      },
      {
        "filename": "libs/core/langchain_core/tracers/event_stream.py",
        "content_before": "\"\"\"Internal tracer to power the event stream API.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport logging\nfrom collections.abc import AsyncIterator, Iterator, Sequence\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom uuid import UUID, uuid4\n\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom langchain_core.callbacks.base import AsyncCallbackHandler\nfrom langchain_core.messages import AIMessageChunk, BaseMessage, BaseMessageChunk\nfrom langchain_core.outputs import (\n    ChatGenerationChunk,\n    GenerationChunk,\n    LLMResult,\n)\nfrom langchain_core.runnables.schema import (\n    CustomStreamEvent,\n    EventData,\n    StandardStreamEvent,\n    StreamEvent,\n)\nfrom langchain_core.runnables.utils import (\n    Input,\n    Output,\n    _RootEventFilter,\n)\nfrom langchain_core.tracers._streaming import _StreamingCallbackHandler\nfrom langchain_core.tracers.log_stream import LogEntry\nfrom langchain_core.tracers.memory_stream import _MemoryStream\nfrom langchain_core.utils.aiter import aclosing, py_anext\n\nif TYPE_CHECKING:\n    from langchain_core.documents import Document\n    from langchain_core.runnables import Runnable, RunnableConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass RunInfo(TypedDict):\n    \"\"\"Information about a run.\n\n    This is used to keep track of the metadata associated with a run.\n\n    Parameters:\n        name: The name of the run.\n        tags: The tags associated with the run.\n        metadata: The metadata associated with the run.\n        run_type: The type of the run.\n        inputs: The inputs to the run.\n        parent_run_id: The ID of the parent run.\n    \"\"\"\n\n    name: str\n    tags: list[str]\n    metadata: dict[str, Any]\n    run_type: str\n    inputs: NotRequired[Any]\n    parent_run_id: Optional[UUID]\n\n\ndef _assign_name(name: Optional[str], serialized: Optional[dict[str, Any]]) -> str:\n    \"\"\"Assign a name to a run.\"\"\"\n    if name is not None:\n        return name\n    if serialized is not None:\n        if \"name\" in serialized:\n            return serialized[\"name\"]\n        elif \"id\" in serialized:\n            return serialized[\"id\"][-1]\n    return \"Unnamed\"\n\n\nT = TypeVar(\"T\")\n\n\nclass _AstreamEventsCallbackHandler(AsyncCallbackHandler, _StreamingCallbackHandler):\n    \"\"\"An implementation of an async callback handler for astream events.\"\"\"\n\n    def __init__(\n        self,\n        *args: Any,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize the tracer.\"\"\"\n        super().__init__(*args, **kwargs)\n        # Map of run ID to run info.\n        # the entry corresponding to a given run id is cleaned\n        # up when each corresponding run ends.\n        self.run_map: dict[UUID, RunInfo] = {}\n        # The callback event that corresponds to the end of a parent run\n        # may be invoked BEFORE the callback event that corresponds to the end\n        # of a child run, which results in clean up of run_map.\n        # So we keep track of the mapping between children and parent run IDs\n        # in a separate container. This container is GCed when the tracer is GCed.\n        self.parent_map: dict[UUID, Optional[UUID]] = {}\n\n        self.is_tapped: dict[UUID, Any] = {}\n\n        # Filter which events will be sent over the queue.\n        self.root_event_filter = _RootEventFilter(\n            include_names=include_names,\n            include_types=include_types,\n            include_tags=include_tags,\n            exclude_names=exclude_names,\n            exclude_types=exclude_types,\n            exclude_tags=exclude_tags,\n        )\n\n        loop = asyncio.get_event_loop()\n        memory_stream = _MemoryStream[StreamEvent](loop)\n        self.send_stream = memory_stream.get_send_stream()\n        self.receive_stream = memory_stream.get_receive_stream()\n\n    def _get_parent_ids(self, run_id: UUID) -> list[str]:\n        \"\"\"Get the parent IDs of a run (non-recursively) cast to strings.\"\"\"\n        parent_ids = []\n\n        while parent_id := self.parent_map.get(run_id):\n            str_parent_id = str(parent_id)\n            if str_parent_id in parent_ids:\n                msg = (\n                    f\"Parent ID {parent_id} is already in the parent_ids list. \"\n                    f\"This should never happen.\"\n                )\n                raise AssertionError(msg)\n            parent_ids.append(str_parent_id)\n            run_id = parent_id\n\n        # Return the parent IDs in reverse order, so that the first\n        # parent ID is the root and the last ID is the immediate parent.\n        return parent_ids[::-1]\n\n    def _send(self, event: StreamEvent, event_type: str) -> None:\n        \"\"\"Send an event to the stream.\"\"\"\n        if self.root_event_filter.include_event(event, event_type):\n            self.send_stream.send_nowait(event)\n\n    def __aiter__(self) -> AsyncIterator[Any]:\n        \"\"\"Iterate over the receive stream.\"\"\"\n        return self.receive_stream.__aiter__()\n\n    async def tap_output_aiter(\n        self, run_id: UUID, output: AsyncIterator[T]\n    ) -> AsyncIterator[T]:\n        \"\"\"Tap the output aiter.\n\n        This method is used to tap the output of a Runnable that produces\n        an async iterator. It is used to generate stream events for the\n        output of the Runnable.\n\n        Args:\n            run_id: The ID of the run.\n            output: The output of the Runnable.\n\n        Yields:\n            T: The output of the Runnable.\n        \"\"\"\n        sentinel = object()\n        # atomic check and set\n        tap = self.is_tapped.setdefault(run_id, sentinel)\n        # wait for first chunk\n        first = await py_anext(output, default=sentinel)\n        if first is sentinel:\n            return\n        # get run info\n        run_info = self.run_map.get(run_id)\n        if run_info is None:\n            # run has finished, don't issue any stream events\n            yield cast(T, first)\n            return\n        if tap is sentinel:\n            # if we are the first to tap, issue stream events\n            event: StandardStreamEvent = {\n                \"event\": f\"on_{run_info['run_type']}_stream\",\n                \"run_id\": str(run_id),\n                \"name\": run_info[\"name\"],\n                \"tags\": run_info[\"tags\"],\n                \"metadata\": run_info[\"metadata\"],\n                \"data\": {},\n                \"parent_ids\": self._get_parent_ids(run_id),\n            }\n            self._send({**event, \"data\": {\"chunk\": first}}, run_info[\"run_type\"])\n            yield cast(T, first)\n            # consume the rest of the output\n            async for chunk in output:\n                self._send(\n                    {**event, \"data\": {\"chunk\": chunk}},\n                    run_info[\"run_type\"],\n                )\n                yield chunk\n        else:\n            # otherwise just pass through\n            yield cast(T, first)\n            # consume the rest of the output\n            async for chunk in output:\n                yield chunk\n\n    def tap_output_iter(self, run_id: UUID, output: Iterator[T]) -> Iterator[T]:\n        \"\"\"Tap the output aiter.\n\n        Args:\n            run_id: The ID of the run.\n            output: The output of the Runnable.\n\n        Yields:\n            T: The output of the Runnable.\n        \"\"\"\n        sentinel = object()\n        # atomic check and set\n        tap = self.is_tapped.setdefault(run_id, sentinel)\n        # wait for first chunk\n        first = next(output, sentinel)\n        if first is sentinel:\n            return\n        # get run info\n        run_info = self.run_map.get(run_id)\n        if run_info is None:\n            # run has finished, don't issue any stream events\n            yield cast(T, first)\n            return\n        if tap is sentinel:\n            # if we are the first to tap, issue stream events\n            event: StandardStreamEvent = {\n                \"event\": f\"on_{run_info['run_type']}_stream\",\n                \"run_id\": str(run_id),\n                \"name\": run_info[\"name\"],\n                \"tags\": run_info[\"tags\"],\n                \"metadata\": run_info[\"metadata\"],\n                \"data\": {},\n                \"parent_ids\": self._get_parent_ids(run_id),\n            }\n            self._send({**event, \"data\": {\"chunk\": first}}, run_info[\"run_type\"])\n            yield cast(T, first)\n            # consume the rest of the output\n            for chunk in output:\n                self._send(\n                    {**event, \"data\": {\"chunk\": chunk}},\n                    run_info[\"run_type\"],\n                )\n                yield chunk\n        else:\n            # otherwise just pass through\n            yield cast(T, first)\n            # consume the rest of the output\n            for chunk in output:\n                yield chunk\n\n    def _write_run_start_info(\n        self,\n        run_id: UUID,\n        *,\n        tags: Optional[list[str]],\n        metadata: Optional[dict[str, Any]],\n        parent_run_id: Optional[UUID],\n        name_: str,\n        run_type: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Update the run info.\"\"\"\n        info: RunInfo = {\n            \"tags\": tags or [],\n            \"metadata\": metadata or {},\n            \"name\": name_,\n            \"run_type\": run_type,\n            \"parent_run_id\": parent_run_id,\n        }\n\n        if \"inputs\" in kwargs:\n            # Handle inputs in a special case to allow inputs to be an\n            # optionally provided and distinguish between missing value\n            # vs. None value.\n            info[\"inputs\"] = kwargs[\"inputs\"]\n\n        self.run_map[run_id] = info\n        self.parent_map[run_id] = parent_run_id\n\n    async def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Start a trace for an LLM run.\"\"\"\n        name_ = _assign_name(name, serialized)\n        run_type = \"chat_model\"\n\n        self._write_run_start_info(\n            run_id,\n            tags=tags,\n            metadata=metadata,\n            parent_run_id=parent_run_id,\n            name_=name_,\n            run_type=run_type,\n            inputs={\"messages\": messages},\n        )\n\n        self._send(\n            {\n                \"event\": \"on_chat_model_start\",\n                \"data\": {\n                    \"input\": {\"messages\": messages},\n                },\n                \"name\": name_,\n                \"tags\": tags or [],\n                \"run_id\": str(run_id),\n                \"metadata\": metadata or {},\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_type,\n        )\n\n    async def on_llm_start(\n        self,\n        serialized: dict[str, Any],\n        prompts: list[str],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Start a trace for an LLM run.\"\"\"\n        name_ = _assign_name(name, serialized)\n        run_type = \"llm\"\n\n        self._write_run_start_info(\n            run_id,\n            tags=tags,\n            metadata=metadata,\n            parent_run_id=parent_run_id,\n            name_=name_,\n            run_type=run_type,\n            inputs={\"prompts\": prompts},\n        )\n\n        self._send(\n            {\n                \"event\": \"on_llm_start\",\n                \"data\": {\n                    \"input\": {\n                        \"prompts\": prompts,\n                    }\n                },\n                \"name\": name_,\n                \"tags\": tags or [],\n                \"run_id\": str(run_id),\n                \"metadata\": metadata or {},\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_type,\n        )\n\n    async def on_custom_event(\n        self,\n        name: str,\n        data: Any,\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Generate a custom astream event.\"\"\"\n        event = CustomStreamEvent(\n            event=\"on_custom_event\",\n            run_id=str(run_id),\n            name=name,\n            tags=tags or [],\n            metadata=metadata or {},\n            data=data,\n            parent_ids=self._get_parent_ids(run_id),\n        )\n        self._send(event, name)\n\n    async def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n        run_info = self.run_map.get(run_id)\n        chunk_: Union[GenerationChunk, BaseMessageChunk]\n\n        if run_info is None:\n            msg = f\"Run ID {run_id} not found in run map.\"\n            raise AssertionError(msg)\n        if self.is_tapped.get(run_id):\n            return\n        if run_info[\"run_type\"] == \"chat_model\":\n            event = \"on_chat_model_stream\"\n\n            if chunk is None:\n                chunk_ = AIMessageChunk(content=token)\n            else:\n                chunk_ = cast(ChatGenerationChunk, chunk).message\n\n        elif run_info[\"run_type\"] == \"llm\":\n            event = \"on_llm_stream\"\n            if chunk is None:\n                chunk_ = GenerationChunk(text=token)\n            else:\n                chunk_ = cast(GenerationChunk, chunk)\n        else:\n            msg = f\"Unexpected run type: {run_info['run_type']}\"\n            raise ValueError(msg)\n\n        self._send(\n            {\n                \"event\": event,\n                \"data\": {\n                    \"chunk\": chunk_,\n                },\n                \"run_id\": str(run_id),\n                \"name\": run_info[\"name\"],\n                \"tags\": run_info[\"tags\"],\n                \"metadata\": run_info[\"metadata\"],\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_info[\"run_type\"],\n        )\n\n    async def on_llm_end(\n        self, response: LLMResult, *, run_id: UUID, **kwargs: Any\n    ) -> None:\n        \"\"\"End a trace for an LLM run.\"\"\"\n        run_info = self.run_map.pop(run_id)\n        inputs_ = run_info[\"inputs\"]\n\n        generations: Union[list[list[GenerationChunk]], list[list[ChatGenerationChunk]]]\n        output: Union[dict, BaseMessage] = {}\n\n        if run_info[\"run_type\"] == \"chat_model\":\n            generations = cast(list[list[ChatGenerationChunk]], response.generations)\n            for gen in generations:\n                if output != {}:\n                    break\n                for chunk in gen:\n                    output = chunk.message\n                    break\n\n            event = \"on_chat_model_end\"\n        elif run_info[\"run_type\"] == \"llm\":\n            generations = cast(list[list[GenerationChunk]], response.generations)\n            output = {\n                \"generations\": [\n                    [\n                        {\n                            \"text\": chunk.text,\n                            \"generation_info\": chunk.generation_info,\n                            \"type\": chunk.type,\n                        }\n                        for chunk in gen\n                    ]\n                    for gen in generations\n                ],\n                \"llm_output\": response.llm_output,\n            }\n            event = \"on_llm_end\"\n        else:\n            msg = f\"Unexpected run type: {run_info['run_type']}\"\n            raise ValueError(msg)\n\n        self._send(\n            {\n                \"event\": event,\n                \"data\": {\"output\": output, \"input\": inputs_},\n                \"run_id\": str(run_id),\n                \"name\": run_info[\"name\"],\n                \"tags\": run_info[\"tags\"],\n                \"metadata\": run_info[\"metadata\"],\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_info[\"run_type\"],\n        )\n\n    async def on_chain_start(\n        self,\n        serialized: dict[str, Any],\n        inputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_type: Optional[str] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Start a trace for a chain run.\"\"\"\n        name_ = _assign_name(name, serialized)\n        run_type_ = run_type or \"chain\"\n\n        data: EventData = {}\n\n        # Work-around Runnable core code not sending input in some\n        # cases.\n        if inputs != {\"input\": \"\"}:\n            data[\"input\"] = inputs\n            kwargs[\"inputs\"] = inputs\n\n        self._write_run_start_info(\n            run_id,\n            tags=tags,\n            metadata=metadata,\n            parent_run_id=parent_run_id,\n            name_=name_,\n            run_type=run_type_,\n            **kwargs,\n        )\n\n        self._send(\n            {\n                \"event\": f\"on_{run_type_}_start\",\n                \"data\": data,\n                \"name\": name_,\n                \"tags\": tags or [],\n                \"run_id\": str(run_id),\n                \"metadata\": metadata or {},\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_type_,\n        )\n\n    async def on_chain_end(\n        self,\n        outputs: dict[str, Any],\n        *,\n        run_id: UUID,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"End a trace for a chain run.\"\"\"\n        run_info = self.run_map.pop(run_id)\n        run_type = run_info[\"run_type\"]\n\n        event = f\"on_{run_type}_end\"\n\n        inputs = inputs or run_info.get(\"inputs\") or {}\n\n        data: EventData = {\n            \"output\": outputs,\n            \"input\": inputs,\n        }\n\n        self._send(\n            {\n                \"event\": event,\n                \"data\": data,\n                \"run_id\": str(run_id),\n                \"name\": run_info[\"name\"],\n                \"tags\": run_info[\"tags\"],\n                \"metadata\": run_info[\"metadata\"],\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_type,\n        )\n\n    async def on_tool_start(\n        self,\n        serialized: dict[str, Any],\n        input_str: str,\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        inputs: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Start a trace for a tool run.\"\"\"\n        name_ = _assign_name(name, serialized)\n\n        self._write_run_start_info(\n            run_id,\n            tags=tags,\n            metadata=metadata,\n            parent_run_id=parent_run_id,\n            name_=name_,\n            run_type=\"tool\",\n            inputs=inputs,\n        )\n\n        self._send(\n            {\n                \"event\": \"on_tool_start\",\n                \"data\": {\n                    \"input\": inputs or {},\n                },\n                \"name\": name_,\n                \"tags\": tags or [],\n                \"run_id\": str(run_id),\n                \"metadata\": metadata or {},\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            \"tool\",\n        )\n\n    async def on_tool_end(self, output: Any, *, run_id: UUID, **kwargs: Any) -> None:\n        \"\"\"End a trace for a tool run.\"\"\"\n        run_info = self.run_map.pop(run_id)\n        if \"inputs\" not in run_info:\n            msg = (\n                f\"Run ID {run_id} is a tool call and is expected to have \"\n                f\"inputs associated with it.\"\n            )\n            raise AssertionError(msg)\n        inputs = run_info[\"inputs\"]\n\n        self._send(\n            {\n                \"event\": \"on_tool_end\",\n                \"data\": {\n                    \"output\": output,\n                    \"input\": inputs,\n                },\n                \"run_id\": str(run_id),\n                \"name\": run_info[\"name\"],\n                \"tags\": run_info[\"tags\"],\n                \"metadata\": run_info[\"metadata\"],\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            \"tool\",\n        )\n\n    async def on_retriever_start(\n        self,\n        serialized: dict[str, Any],\n        query: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when Retriever starts running.\"\"\"\n        name_ = _assign_name(name, serialized)\n        run_type = \"retriever\"\n\n        self._write_run_start_info(\n            run_id,\n            tags=tags,\n            metadata=metadata,\n            parent_run_id=parent_run_id,\n            name_=name_,\n            run_type=run_type,\n            inputs={\"query\": query},\n        )\n\n        self._send(\n            {\n                \"event\": \"on_retriever_start\",\n                \"data\": {\n                    \"input\": {\n                        \"query\": query,\n                    }\n                },\n                \"name\": name_,\n                \"tags\": tags or [],\n                \"run_id\": str(run_id),\n                \"metadata\": metadata or {},\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_type,\n        )\n\n    async def on_retriever_end(\n        self, documents: Sequence[Document], *, run_id: UUID, **kwargs: Any\n    ) -> None:\n        \"\"\"Run when Retriever ends running.\"\"\"\n        run_info = self.run_map.pop(run_id)\n\n        self._send(\n            {\n                \"event\": \"on_retriever_end\",\n                \"data\": {\n                    \"output\": documents,\n                    \"input\": run_info[\"inputs\"],\n                },\n                \"run_id\": str(run_id),\n                \"name\": run_info[\"name\"],\n                \"tags\": run_info[\"tags\"],\n                \"metadata\": run_info[\"metadata\"],\n                \"parent_ids\": self._get_parent_ids(run_id),\n            },\n            run_info[\"run_type\"],\n        )\n\n    def __deepcopy__(self, memo: dict) -> _AstreamEventsCallbackHandler:\n        \"\"\"Deepcopy the tracer.\"\"\"\n        return self\n\n    def __copy__(self) -> _AstreamEventsCallbackHandler:\n        \"\"\"Copy the tracer.\"\"\"\n        return self\n\n\nasync def _astream_events_implementation_v1(\n    runnable: Runnable[Input, Output],\n    input: Any,\n    config: Optional[RunnableConfig] = None,\n    *,\n    include_names: Optional[Sequence[str]] = None,\n    include_types: Optional[Sequence[str]] = None,\n    include_tags: Optional[Sequence[str]] = None,\n    exclude_names: Optional[Sequence[str]] = None,\n    exclude_types: Optional[Sequence[str]] = None,\n    exclude_tags: Optional[Sequence[str]] = None,\n    **kwargs: Any,\n) -> AsyncIterator[StandardStreamEvent]:\n    from langchain_core.runnables import ensure_config\n    from langchain_core.runnables.utils import _RootEventFilter\n    from langchain_core.tracers.log_stream import (\n        LogStreamCallbackHandler,\n        RunLog,\n        _astream_log_implementation,\n    )\n\n    stream = LogStreamCallbackHandler(\n        auto_close=False,\n        include_names=include_names,\n        include_types=include_types,\n        include_tags=include_tags,\n        exclude_names=exclude_names,\n        exclude_types=exclude_types,\n        exclude_tags=exclude_tags,\n        _schema_format=\"streaming_events\",\n    )\n\n    run_log = RunLog(state=None)  # type: ignore[arg-type]\n    encountered_start_event = False\n\n    _root_event_filter = _RootEventFilter(\n        include_names=include_names,\n        include_types=include_types,\n        include_tags=include_tags,\n        exclude_names=exclude_names,\n        exclude_types=exclude_types,\n        exclude_tags=exclude_tags,\n    )\n\n    config = ensure_config(config)\n    root_tags = config.get(\"tags\", [])\n    root_metadata = config.get(\"metadata\", {})\n    root_name = config.get(\"run_name\", runnable.get_name())\n\n    # Ignoring mypy complaint about too many different union combinations\n    # This arises because many of the argument types are unions\n    async for log in _astream_log_implementation(  # type: ignore[misc]\n        runnable,\n        input,\n        config=config,\n        stream=stream,\n        diff=True,\n        with_streamed_output_list=True,\n        **kwargs,\n    ):\n        run_log = run_log + log\n\n        if not encountered_start_event:\n            # Yield the start event for the root runnable.\n            encountered_start_event = True\n            state = run_log.state.copy()\n\n            event = StandardStreamEvent(\n                event=f\"on_{state['type']}_start\",\n                run_id=state[\"id\"],\n                name=root_name,\n                tags=root_tags,\n                metadata=root_metadata,\n                data={\n                    \"input\": input,\n                },\n                parent_ids=[],  # Not supported in v1\n            )\n\n            if _root_event_filter.include_event(event, state[\"type\"]):\n                yield event\n\n        paths = {\n            op[\"path\"].split(\"/\")[2]\n            for op in log.ops\n            if op[\"path\"].startswith(\"/logs/\")\n        }\n        # Elements in a set should be iterated in the same order\n        # as they were inserted in modern python versions.\n        for path in paths:\n            data: EventData = {}\n            log_entry: LogEntry = run_log.state[\"logs\"][path]\n            if log_entry[\"end_time\"] is None:\n                event_type = \"stream\" if log_entry[\"streamed_output\"] else \"start\"\n            else:\n                event_type = \"end\"\n\n            if event_type == \"start\":\n                # Include the inputs with the start event if they are available.\n                # Usually they will NOT be available for components that operate\n                # on streams, since those components stream the input and\n                # don't know its final value until the end of the stream.\n                inputs = log_entry[\"inputs\"]\n                if inputs is not None:\n                    data[\"input\"] = inputs\n\n            if event_type == \"end\":\n                inputs = log_entry[\"inputs\"]\n                if inputs is not None:\n                    data[\"input\"] = inputs\n\n                # None is a VALID output for an end event\n                data[\"output\"] = log_entry[\"final_output\"]\n\n            if event_type == \"stream\":\n                num_chunks = len(log_entry[\"streamed_output\"])\n                if num_chunks != 1:\n                    msg = (\n                        f\"Expected exactly one chunk of streamed output, \"\n                        f\"got {num_chunks} instead. This is impossible. \"\n                        f\"Encountered in: {log_entry['name']}\"\n                    )\n                    raise AssertionError(msg)\n\n                data = {\"chunk\": log_entry[\"streamed_output\"][0]}\n                # Clean up the stream, we don't need it anymore.\n                # And this avoids duplicates as well!\n                log_entry[\"streamed_output\"] = []\n\n            yield StandardStreamEvent(\n                event=f\"on_{log_entry['type']}_{event_type}\",\n                name=log_entry[\"name\"],\n                run_id=log_entry[\"id\"],\n                tags=log_entry[\"tags\"],\n                metadata=log_entry[\"metadata\"],\n                data=data,\n                parent_ids=[],  # Not supported in v1\n            )\n\n        # Finally, we take care of the streaming output from the root chain\n        # if there is any.\n        state = run_log.state\n        if state[\"streamed_output\"]:\n            num_chunks = len(state[\"streamed_output\"])\n            if num_chunks != 1:\n                msg = (\n                    f\"Expected exactly one chunk of streamed output, \"\n                    f\"got {num_chunks} instead. This is impossible. \"\n                    f\"Encountered in: {state['name']}\"\n                )\n                raise AssertionError(msg)\n\n            data = {\"chunk\": state[\"streamed_output\"][0]}\n            # Clean up the stream, we don't need it anymore.\n            state[\"streamed_output\"] = []\n\n            event = StandardStreamEvent(\n                event=f\"on_{state['type']}_stream\",\n                run_id=state[\"id\"],\n                tags=root_tags,\n                metadata=root_metadata,\n                name=root_name,\n                data=data,\n                parent_ids=[],  # Not supported in v1\n            )\n            if _root_event_filter.include_event(event, state[\"type\"]):\n                yield event\n\n    state = run_log.state\n\n    # Finally yield the end event for the root runnable.\n    event = StandardStreamEvent(\n        event=f\"on_{state['type']}_end\",\n        name=root_name,\n        run_id=state[\"id\"],\n        tags=root_tags,\n        metadata=root_metadata,\n        data={\n            \"output\": state[\"final_output\"],\n        },\n        parent_ids=[],  # Not supported in v1\n    )\n    if _root_event_filter.include_event(event, state[\"type\"]):\n        yield event\n\n\nasync def _astream_events_implementation_v2(\n    runnable: Runnable[Input, Output],\n    input: Any,\n    config: Optional[RunnableConfig] = None,\n    *,\n    include_names: Optional[Sequence[str]] = None,\n    include_types: Optional[Sequence[str]] = None,\n    include_tags: Optional[Sequence[str]] = None,\n    exclude_names: Optional[Sequence[str]] = None,\n    exclude_types: Optional[Sequence[str]] = None,\n    exclude_tags: Optional[Sequence[str]] = None,\n    **kwargs: Any,\n) -> AsyncIterator[StandardStreamEvent]:\n    \"\"\"Implementation of the astream events API for V2 runnables.\"\"\"\n    from langchain_core.callbacks.base import BaseCallbackManager\n    from langchain_core.runnables import ensure_config\n\n    event_streamer = _AstreamEventsCallbackHandler(\n        include_names=include_names,\n        include_types=include_types,\n        include_tags=include_tags,\n        exclude_names=exclude_names,\n        exclude_types=exclude_types,\n        exclude_tags=exclude_tags,\n    )\n\n    # Assign the stream handler to the config\n    config = ensure_config(config)\n    run_id = cast(UUID, config.setdefault(\"run_id\", uuid4()))\n    callbacks = config.get(\"callbacks\")\n    if callbacks is None:\n        config[\"callbacks\"] = [event_streamer]\n    elif isinstance(callbacks, list):\n        config[\"callbacks\"] = callbacks + [event_streamer]\n    elif isinstance(callbacks, BaseCallbackManager):\n        callbacks = callbacks.copy()\n        callbacks.add_handler(event_streamer, inherit=True)\n        config[\"callbacks\"] = callbacks\n    else:\n        msg = (\n            f\"Unexpected type for callbacks: {callbacks}.\"\n            \"Expected None, list or AsyncCallbackManager.\"\n        )\n        raise ValueError(msg)\n\n    # Call the runnable in streaming mode,\n    # add each chunk to the output stream\n    async def consume_astream() -> None:\n        try:\n            # if astream also calls tap_output_aiter this will be a no-op\n            async with aclosing(runnable.astream(input, config, **kwargs)) as stream:\n                async for _ in event_streamer.tap_output_aiter(run_id, stream):\n                    # All the content will be picked up\n                    pass\n        finally:\n            await event_streamer.send_stream.aclose()\n\n    # Start the runnable in a task, so we can start consuming output\n    task = asyncio.create_task(consume_astream())\n\n    first_event_sent = False\n    first_event_run_id = None\n\n    try:\n        async for event in event_streamer:\n            if not first_event_sent:\n                first_event_sent = True\n                # This is a work-around an issue where the inputs into the\n                # chain are not available until the entire input is consumed.\n                # As a temporary solution, we'll modify the input to be the input\n                # that was passed into the chain.\n                event[\"data\"][\"input\"] = input\n                first_event_run_id = event[\"run_id\"]\n                yield event\n                continue\n\n            # If it's the end event corresponding to the root runnable\n            # we dont include the input in the event since it's guaranteed\n            # to be included in the first event.\n            if (\n                event[\"run_id\"] == first_event_run_id\n                and event[\"event\"].endswith(\"_end\")\n                and \"input\" in event[\"data\"]\n            ):\n                del event[\"data\"][\"input\"]\n\n            yield event\n    except asyncio.CancelledError as exc:\n        # Cancel the task if it's still running\n        task.cancel(exc.args[0] if exc.args else None)\n        raise\n    finally:\n        # Cancel the task if it's still running\n        task.cancel()\n        # Await it anyway, to run any cleanup code, and propagate any exceptions\n        with contextlib.suppress(asyncio.CancelledError):\n            await task\n",
        "patch": "@@ -5,7 +5,6 @@\n import asyncio\n import contextlib\n import logging\n-from collections.abc import AsyncIterator, Iterator, Sequence\n from typing import (\n     TYPE_CHECKING,\n     Any,\n@@ -37,13 +36,15 @@\n     _RootEventFilter,\n )\n from langchain_core.tracers._streaming import _StreamingCallbackHandler\n-from langchain_core.tracers.log_stream import LogEntry\n from langchain_core.tracers.memory_stream import _MemoryStream\n from langchain_core.utils.aiter import aclosing, py_anext\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Iterator, Sequence\n+\n     from langchain_core.documents import Document\n     from langchain_core.runnables import Runnable, RunnableConfig\n+    from langchain_core.tracers.log_stream import LogEntry\n \n logger = logging.getLogger(__name__)\n "
      },
      {
        "filename": "libs/core/langchain_core/tracers/langchain.py",
        "content_before": "\"\"\"A Tracer implementation that records to LangChain endpoint.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport warnings\nfrom concurrent.futures import ThreadPoolExecutor\nfrom datetime import datetime, timezone\nfrom typing import TYPE_CHECKING, Any, Optional, Union\nfrom uuid import UUID\n\nfrom langsmith import Client\nfrom langsmith import run_trees as rt\nfrom langsmith import utils as ls_utils\nfrom pydantic import PydanticDeprecationWarning\nfrom tenacity import (\n    Retrying,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential_jitter,\n)\n\nfrom langchain_core.env import get_runtime_environment\nfrom langchain_core.load import dumpd\nfrom langchain_core.outputs import ChatGenerationChunk, GenerationChunk\nfrom langchain_core.tracers.base import BaseTracer\nfrom langchain_core.tracers.schemas import Run\n\nif TYPE_CHECKING:\n    from langchain_core.messages import BaseMessage\n\nlogger = logging.getLogger(__name__)\n_LOGGED = set()\n_EXECUTOR: Optional[ThreadPoolExecutor] = None\n\n\ndef log_error_once(method: str, exception: Exception) -> None:\n    \"\"\"Log an error once.\n\n    Args:\n        method: The method that raised the exception.\n        exception: The exception that was raised.\n    \"\"\"\n    global _LOGGED\n    if (method, type(exception)) in _LOGGED:\n        return\n    _LOGGED.add((method, type(exception)))\n    logger.error(exception)\n\n\ndef wait_for_all_tracers() -> None:\n    \"\"\"Wait for all tracers to finish.\"\"\"\n    if rt._CLIENT is not None and rt._CLIENT.tracing_queue is not None:\n        rt._CLIENT.tracing_queue.join()\n\n\ndef get_client() -> Client:\n    \"\"\"Get the client.\"\"\"\n    return rt.get_cached_client()\n\n\ndef _get_executor() -> ThreadPoolExecutor:\n    \"\"\"Get the executor.\"\"\"\n    global _EXECUTOR\n    if _EXECUTOR is None:\n        _EXECUTOR = ThreadPoolExecutor()\n    return _EXECUTOR\n\n\ndef _run_to_dict(run: Run, exclude_inputs: bool = False) -> dict:\n    # TODO: Update once langsmith moves to Pydantic V2 and we can swap run.dict for\n    # run.model_dump\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=PydanticDeprecationWarning)\n\n        res = {\n            **run.dict(exclude={\"child_runs\", \"inputs\", \"outputs\"}),\n            \"outputs\": run.outputs,\n        }\n        if not exclude_inputs:\n            res[\"inputs\"] = run.inputs\n    return res\n\n\nclass LangChainTracer(BaseTracer):\n    \"\"\"Implementation of the SharedTracer that POSTS to the LangChain endpoint.\"\"\"\n\n    run_inline = True\n\n    def __init__(\n        self,\n        example_id: Optional[Union[UUID, str]] = None,\n        project_name: Optional[str] = None,\n        client: Optional[Client] = None,\n        tags: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Initialize the LangChain tracer.\n\n        Args:\n            example_id: The example ID.\n            project_name: The project name. Defaults to the tracer project.\n            client: The client. Defaults to the global client.\n            tags: The tags. Defaults to an empty list.\n            kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.example_id = (\n            UUID(example_id) if isinstance(example_id, str) else example_id\n        )\n        self.project_name = project_name or ls_utils.get_tracer_project()\n        self.client = client or get_client()\n        self.tags = tags or []\n        self.latest_run: Optional[Run] = None\n\n    def _start_trace(self, run: Run) -> None:\n        if self.project_name:\n            run.session_name = self.project_name\n        if self.tags is not None:\n            if run.tags:\n                run.tags = sorted(set(run.tags + self.tags))\n            else:\n                run.tags = self.tags.copy()\n\n        super()._start_trace(run)\n        if run._client is None:\n            run._client = self.client  # type: ignore\n\n    def on_chat_model_start(\n        self,\n        serialized: dict[str, Any],\n        messages: list[list[BaseMessage]],\n        *,\n        run_id: UUID,\n        tags: Optional[list[str]] = None,\n        parent_run_id: Optional[UUID] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        name: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Start a trace for an LLM run.\n\n        Args:\n            serialized: The serialized model.\n            messages: The messages.\n            run_id: The run ID.\n            tags: The tags. Defaults to None.\n            parent_run_id: The parent run ID. Defaults to None.\n            metadata: The metadata. Defaults to None.\n            name: The name. Defaults to None.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            Run: The run.\n        \"\"\"\n        start_time = datetime.now(timezone.utc)\n        if metadata:\n            kwargs.update({\"metadata\": metadata})\n        chat_model_run = Run(\n            id=run_id,\n            parent_run_id=parent_run_id,\n            serialized=serialized,\n            inputs={\"messages\": [[dumpd(msg) for msg in batch] for batch in messages]},\n            extra=kwargs,\n            events=[{\"name\": \"start\", \"time\": start_time}],\n            start_time=start_time,\n            run_type=\"llm\",\n            tags=tags,\n            name=name,  # type: ignore[arg-type]\n        )\n        self._start_trace(chat_model_run)\n        self._on_chat_model_start(chat_model_run)\n        return chat_model_run\n\n    def _persist_run(self, run: Run) -> None:\n        self.latest_run = run\n\n    def get_run_url(self) -> str:\n        \"\"\"Get the LangSmith root run URL.\n\n        Returns:\n            str: The LangSmith root run URL.\n\n        Raises:\n            ValueError: If no traced run is found.\n            ValueError: If the run URL cannot be found.\n        \"\"\"\n        if not self.latest_run:\n            msg = \"No traced run found.\"\n            raise ValueError(msg)\n        # If this is the first run in a project, the project may not yet be created.\n        # This method is only really useful for debugging flows, so we will assume\n        # there is some tolerace for latency.\n        for attempt in Retrying(\n            stop=stop_after_attempt(5),\n            wait=wait_exponential_jitter(),\n            retry=retry_if_exception_type(ls_utils.LangSmithError),\n        ):\n            with attempt:\n                return self.client.get_run_url(\n                    run=self.latest_run, project_name=self.project_name\n                )\n        msg = \"Failed to get run URL.\"\n        raise ValueError(msg)\n\n    def _get_tags(self, run: Run) -> list[str]:\n        \"\"\"Get combined tags for a run.\"\"\"\n        tags = set(run.tags or [])\n        tags.update(self.tags or [])\n        return list(tags)\n\n    def _persist_run_single(self, run: Run) -> None:\n        \"\"\"Persist a run.\"\"\"\n        try:\n            run_dict = _run_to_dict(run)\n            run_dict[\"tags\"] = self._get_tags(run)\n            extra = run_dict.get(\"extra\", {})\n            extra[\"runtime\"] = get_runtime_environment()\n            run_dict[\"extra\"] = extra\n            inputs_is_truthy = bool(run_dict.get(\"inputs\"))\n            run.extra[\"inputs_is_truthy\"] = inputs_is_truthy\n            self.client.create_run(**run_dict, project_name=self.project_name)\n        except Exception as e:\n            # Errors are swallowed by the thread executor so we need to log them here\n            log_error_once(\"post\", e)\n            raise\n\n    def _update_run_single(self, run: Run) -> None:\n        \"\"\"Update a run.\"\"\"\n        try:\n            exclude_inputs = run.extra.get(\"inputs_is_truthy\", False)\n            run_dict = _run_to_dict(run, exclude_inputs=exclude_inputs)\n            run_dict[\"tags\"] = self._get_tags(run)\n            self.client.update_run(run.id, **run_dict)\n        except Exception as e:\n            # Errors are swallowed by the thread executor so we need to log them here\n            log_error_once(\"patch\", e)\n            raise\n\n    def _on_llm_start(self, run: Run) -> None:\n        \"\"\"Persist an LLM run.\"\"\"\n        if run.parent_run_id is None:\n            run.reference_example_id = self.example_id\n        self._persist_run_single(run)\n\n    def _llm_run_with_token_event(\n        self,\n        token: str,\n        run_id: UUID,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Run:\n        \"\"\"Append token event to LLM run and return the run.\"\"\"\n        return super()._llm_run_with_token_event(\n            # Drop the chunk; we don't need to save it\n            token,\n            run_id,\n            chunk=None,\n            parent_run_id=parent_run_id,\n            **kwargs,\n        )\n\n    def _on_chat_model_start(self, run: Run) -> None:\n        \"\"\"Persist an LLM run.\"\"\"\n        if run.parent_run_id is None:\n            run.reference_example_id = self.example_id\n        self._persist_run_single(run)\n\n    def _on_llm_end(self, run: Run) -> None:\n        \"\"\"Process the LLM Run.\"\"\"\n        self._update_run_single(run)\n\n    def _on_llm_error(self, run: Run) -> None:\n        \"\"\"Process the LLM Run upon error.\"\"\"\n        self._update_run_single(run)\n\n    def _on_chain_start(self, run: Run) -> None:\n        \"\"\"Process the Chain Run upon start.\"\"\"\n        if run.parent_run_id is None:\n            run.reference_example_id = self.example_id\n        self._persist_run_single(run)\n\n    def _on_chain_end(self, run: Run) -> None:\n        \"\"\"Process the Chain Run.\"\"\"\n        self._update_run_single(run)\n\n    def _on_chain_error(self, run: Run) -> None:\n        \"\"\"Process the Chain Run upon error.\"\"\"\n        self._update_run_single(run)\n\n    def _on_tool_start(self, run: Run) -> None:\n        \"\"\"Process the Tool Run upon start.\"\"\"\n        if run.parent_run_id is None:\n            run.reference_example_id = self.example_id\n        self._persist_run_single(run)\n\n    def _on_tool_end(self, run: Run) -> None:\n        \"\"\"Process the Tool Run.\"\"\"\n        self._update_run_single(run)\n\n    def _on_tool_error(self, run: Run) -> None:\n        \"\"\"Process the Tool Run upon error.\"\"\"\n        self._update_run_single(run)\n\n    def _on_retriever_start(self, run: Run) -> None:\n        \"\"\"Process the Retriever Run upon start.\"\"\"\n        if run.parent_run_id is None:\n            run.reference_example_id = self.example_id\n        self._persist_run_single(run)\n\n    def _on_retriever_end(self, run: Run) -> None:\n        \"\"\"Process the Retriever Run.\"\"\"\n        self._update_run_single(run)\n\n    def _on_retriever_error(self, run: Run) -> None:\n        \"\"\"Process the Retriever Run upon error.\"\"\"\n        self._update_run_single(run)\n\n    def wait_for_futures(self) -> None:\n        \"\"\"Wait for the given futures to complete.\"\"\"\n        if self.client is not None and self.client.tracing_queue is not None:\n            self.client.tracing_queue.join()\n",
        "patch": "@@ -22,12 +22,12 @@\n \n from langchain_core.env import get_runtime_environment\n from langchain_core.load import dumpd\n-from langchain_core.outputs import ChatGenerationChunk, GenerationChunk\n from langchain_core.tracers.base import BaseTracer\n from langchain_core.tracers.schemas import Run\n \n if TYPE_CHECKING:\n     from langchain_core.messages import BaseMessage\n+    from langchain_core.outputs import ChatGenerationChunk, GenerationChunk\n \n logger = logging.getLogger(__name__)\n _LOGGED = set()"
      },
      {
        "filename": "libs/core/langchain_core/tracers/log_stream.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport contextlib\nimport copy\nimport threading\nfrom collections import defaultdict\nfrom collections.abc import AsyncIterator, Iterator, Sequence\nfrom typing import (\n    Any,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n    overload,\n)\nfrom uuid import UUID\n\nimport jsonpatch  # type: ignore[import]\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom langchain_core.load import dumps\nfrom langchain_core.load.load import load\nfrom langchain_core.outputs import ChatGenerationChunk, GenerationChunk\nfrom langchain_core.runnables import Runnable, RunnableConfig, ensure_config\nfrom langchain_core.runnables.utils import Input, Output\nfrom langchain_core.tracers._streaming import _StreamingCallbackHandler\nfrom langchain_core.tracers.base import BaseTracer\nfrom langchain_core.tracers.memory_stream import _MemoryStream\nfrom langchain_core.tracers.schemas import Run\n\n\nclass LogEntry(TypedDict):\n    \"\"\"A single entry in the run log.\"\"\"\n\n    id: str\n    \"\"\"ID of the sub-run.\"\"\"\n    name: str\n    \"\"\"Name of the object being run.\"\"\"\n    type: str\n    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"\n    tags: list[str]\n    \"\"\"List of tags for the run.\"\"\"\n    metadata: dict[str, Any]\n    \"\"\"Key-value pairs of metadata for the run.\"\"\"\n    start_time: str\n    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"\n\n    streamed_output_str: list[str]\n    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"\n    streamed_output: list[Any]\n    \"\"\"List of output chunks streamed by this run, if available.\"\"\"\n    inputs: NotRequired[Optional[Any]]\n    \"\"\"Inputs to this run. Not available currently via astream_log.\"\"\"\n    final_output: Optional[Any]\n    \"\"\"Final output of this run.\n\n    Only available after the run has finished successfully.\"\"\"\n    end_time: Optional[str]\n    \"\"\"ISO-8601 timestamp of when the run ended.\n    Only available after the run has finished.\"\"\"\n\n\nclass RunState(TypedDict):\n    \"\"\"State of the run.\"\"\"\n\n    id: str\n    \"\"\"ID of the run.\"\"\"\n    streamed_output: list[Any]\n    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"\n    final_output: Optional[Any]\n    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.\n    Updated throughout the run when supported by the Runnable.\"\"\"\n\n    name: str\n    \"\"\"Name of the object being run.\"\"\"\n    type: str\n    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"\n\n    # Do we want tags/metadata on the root run? Client kinda knows it in most situations\n    # tags: List[str]\n\n    logs: dict[str, LogEntry]\n    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will\n    contain only the runs that matched the filters.\"\"\"\n\n\nclass RunLogPatch:\n    \"\"\"Patch to the run log.\"\"\"\n\n    ops: list[dict[str, Any]]\n    \"\"\"List of jsonpatch operations, which describe how to create the run state\n    from an empty dict. This is the minimal representation of the log, designed to\n    be serialized as JSON and sent over the wire to reconstruct the log on the other\n    side. Reconstruction of the state can be done with any jsonpatch-compliant library,\n    see https://jsonpatch.com for more information.\"\"\"\n\n    def __init__(self, *ops: dict[str, Any]) -> None:\n        self.ops = list(ops)\n\n    def __add__(self, other: Union[RunLogPatch, Any]) -> RunLog:\n        if type(other) is RunLogPatch:\n            ops = self.ops + other.ops\n            state = jsonpatch.apply_patch(None, copy.deepcopy(ops))\n            return RunLog(*ops, state=state)\n\n        msg = f\"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'\"\n        raise TypeError(msg)\n\n    def __repr__(self) -> str:\n        from pprint import pformat\n\n        # 1:-1 to get rid of the [] around the list\n        return f\"RunLogPatch({pformat(self.ops)[1:-1]})\"\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, RunLogPatch) and self.ops == other.ops\n\n\nclass RunLog(RunLogPatch):\n    \"\"\"Run log.\"\"\"\n\n    state: RunState\n    \"\"\"Current state of the log, obtained from applying all ops in sequence.\"\"\"\n\n    def __init__(self, *ops: dict[str, Any], state: RunState) -> None:\n        super().__init__(*ops)\n        self.state = state\n\n    def __add__(self, other: Union[RunLogPatch, Any]) -> RunLog:\n        if type(other) is RunLogPatch:\n            ops = self.ops + other.ops\n            state = jsonpatch.apply_patch(self.state, other.ops)\n            return RunLog(*ops, state=state)\n\n        msg = f\"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'\"\n        raise TypeError(msg)\n\n    def __repr__(self) -> str:\n        from pprint import pformat\n\n        return f\"RunLog({pformat(self.state)})\"\n\n    def __eq__(self, other: object) -> bool:\n        # First compare that the state is the same\n        if not isinstance(other, RunLog):\n            return False\n        if self.state != other.state:\n            return False\n        # Then compare that the ops are the same\n        return super().__eq__(other)\n\n\nT = TypeVar(\"T\")\n\n\nclass LogStreamCallbackHandler(BaseTracer, _StreamingCallbackHandler):\n    \"\"\"Tracer that streams run logs to a stream.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        auto_close: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        # Schema format is for internal use only.\n        _schema_format: Literal[\"original\", \"streaming_events\"] = \"streaming_events\",\n    ) -> None:\n        \"\"\"A tracer that streams run logs to a stream.\n\n        Args:\n            auto_close: Whether to close the stream when the root run finishes.\n            include_names: Only include runs from Runnables with matching names.\n            include_types: Only include runs from Runnables with matching types.\n            include_tags: Only include runs from Runnables with matching tags.\n            exclude_names: Exclude runs from Runnables with matching names.\n            exclude_types: Exclude runs from Runnables with matching types.\n            exclude_tags: Exclude runs from Runnables with matching tags.\n            _schema_format: Primarily changes how the inputs and outputs are\n                handled.\n                **For internal use only. This API will change.**\n                - 'original' is the format used by all current tracers.\n                  This format is slightly inconsistent with respect to inputs\n                  and outputs.\n                - 'streaming_events' is used for supporting streaming events,\n                  for internal usage. It will likely change in the future, or\n                  be deprecated entirely in favor of a dedicated async tracer\n                  for streaming events.\n\n        Raises:\n            ValueError: If an invalid schema format is provided (internal use only).\n        \"\"\"\n        if _schema_format not in {\"original\", \"streaming_events\"}:\n            msg = (\n                f\"Invalid schema format: {_schema_format}. \"\n                f\"Expected one of 'original', 'streaming_events'.\"\n            )\n            raise ValueError(msg)\n        super().__init__(_schema_format=_schema_format)\n\n        self.auto_close = auto_close\n        self.include_names = include_names\n        self.include_types = include_types\n        self.include_tags = include_tags\n        self.exclude_names = exclude_names\n        self.exclude_types = exclude_types\n        self.exclude_tags = exclude_tags\n\n        loop = asyncio.get_event_loop()\n        memory_stream = _MemoryStream[RunLogPatch](loop)\n        self.lock = threading.Lock()\n        self.send_stream = memory_stream.get_send_stream()\n        self.receive_stream = memory_stream.get_receive_stream()\n        self._key_map_by_run_id: dict[UUID, str] = {}\n        self._counter_map_by_name: dict[str, int] = defaultdict(int)\n        self.root_id: Optional[UUID] = None\n\n    def __aiter__(self) -> AsyncIterator[RunLogPatch]:\n        return self.receive_stream.__aiter__()\n\n    def send(self, *ops: dict[str, Any]) -> bool:\n        \"\"\"Send a patch to the stream, return False if the stream is closed.\n\n        Args:\n            *ops: The operations to send to the stream.\n\n        Returns:\n            bool: True if the patch was sent successfully, False if the stream\n                is closed.\n        \"\"\"\n        # We will likely want to wrap this in try / except at some point\n        # to handle exceptions that might arise at run time.\n        # For now we'll let the exception bubble up, and always return\n        # True on the happy path.\n        self.send_stream.send_nowait(RunLogPatch(*ops))\n        return True\n\n    async def tap_output_aiter(\n        self, run_id: UUID, output: AsyncIterator[T]\n    ) -> AsyncIterator[T]:\n        \"\"\"Tap an output async iterator to stream its values to the log.\n\n        Args:\n            run_id: The ID of the run.\n            output: The output async iterator.\n\n        Yields:\n            T: The output value.\n        \"\"\"\n        async for chunk in output:\n            # root run is handled in .astream_log()\n            # if we can't find the run silently ignore\n            # eg. because this run wasn't included in the log\n            if (\n                run_id != self.root_id\n                and (key := self._key_map_by_run_id.get(run_id))\n                and (\n                    not self.send(\n                        {\n                            \"op\": \"add\",\n                            \"path\": f\"/logs/{key}/streamed_output/-\",\n                            \"value\": chunk,\n                        }\n                    )\n                )\n            ):\n                break\n\n            yield chunk\n\n    def tap_output_iter(self, run_id: UUID, output: Iterator[T]) -> Iterator[T]:\n        \"\"\"Tap an output async iterator to stream its values to the log.\n\n        Args:\n            run_id: The ID of the run.\n            output: The output iterator.\n\n        Yields:\n            T: The output value.\n        \"\"\"\n        for chunk in output:\n            # root run is handled in .astream_log()\n            # if we can't find the run silently ignore\n            # eg. because this run wasn't included in the log\n            if (\n                run_id != self.root_id\n                and (key := self._key_map_by_run_id.get(run_id))\n                and (\n                    not self.send(\n                        {\n                            \"op\": \"add\",\n                            \"path\": f\"/logs/{key}/streamed_output/-\",\n                            \"value\": chunk,\n                        }\n                    )\n                )\n            ):\n                break\n\n            yield chunk\n\n    def include_run(self, run: Run) -> bool:\n        \"\"\"Check if a Run should be included in the log.\n\n        Args:\n            run: The Run to check.\n\n        Returns:\n            bool: True if the run should be included, False otherwise.\n        \"\"\"\n        if run.id == self.root_id:\n            return False\n\n        run_tags = run.tags or []\n\n        if (\n            self.include_names is None\n            and self.include_types is None\n            and self.include_tags is None\n        ):\n            include = True\n        else:\n            include = False\n\n        if self.include_names is not None:\n            include = include or run.name in self.include_names\n        if self.include_types is not None:\n            include = include or run.run_type in self.include_types\n        if self.include_tags is not None:\n            include = include or any(tag in self.include_tags for tag in run_tags)\n\n        if self.exclude_names is not None:\n            include = include and run.name not in self.exclude_names\n        if self.exclude_types is not None:\n            include = include and run.run_type not in self.exclude_types\n        if self.exclude_tags is not None:\n            include = include and all(tag not in self.exclude_tags for tag in run_tags)\n\n        return include\n\n    def _persist_run(self, run: Run) -> None:\n        # This is a legacy method only called once for an entire run tree\n        # therefore not useful here\n        pass\n\n    def _on_run_create(self, run: Run) -> None:\n        \"\"\"Start a run.\"\"\"\n        if self.root_id is None:\n            self.root_id = run.id\n            if not self.send(\n                {\n                    \"op\": \"replace\",\n                    \"path\": \"\",\n                    \"value\": RunState(\n                        id=str(run.id),\n                        streamed_output=[],\n                        final_output=None,\n                        logs={},\n                        name=run.name,\n                        type=run.run_type,\n                    ),\n                }\n            ):\n                return\n\n        if not self.include_run(run):\n            return\n\n        # Determine previous index, increment by 1\n        with self.lock:\n            self._counter_map_by_name[run.name] += 1\n            count = self._counter_map_by_name[run.name]\n            self._key_map_by_run_id[run.id] = (\n                run.name if count == 1 else f\"{run.name}:{count}\"\n            )\n\n        entry = LogEntry(\n            id=str(run.id),\n            name=run.name,\n            type=run.run_type,\n            tags=run.tags or [],\n            metadata=(run.extra or {}).get(\"metadata\", {}),\n            start_time=run.start_time.isoformat(timespec=\"milliseconds\"),\n            streamed_output=[],\n            streamed_output_str=[],\n            final_output=None,\n            end_time=None,\n        )\n\n        if self._schema_format == \"streaming_events\":\n            # If using streaming events let's add inputs as well\n            entry[\"inputs\"] = _get_standardized_inputs(run, self._schema_format)\n\n        # Add the run to the stream\n        self.send(\n            {\n                \"op\": \"add\",\n                \"path\": f\"/logs/{self._key_map_by_run_id[run.id]}\",\n                \"value\": entry,\n            }\n        )\n\n    def _on_run_update(self, run: Run) -> None:\n        \"\"\"Finish a run.\"\"\"\n        try:\n            index = self._key_map_by_run_id.get(run.id)\n\n            if index is None:\n                return\n\n            ops = []\n\n            if self._schema_format == \"streaming_events\":\n                ops.append(\n                    {\n                        \"op\": \"replace\",\n                        \"path\": f\"/logs/{index}/inputs\",\n                        \"value\": _get_standardized_inputs(run, self._schema_format),\n                    }\n                )\n\n            ops.extend(\n                [\n                    # Replace 'inputs' with final inputs\n                    # This is needed because in many cases the inputs are not\n                    # known until after the run is finished and the entire\n                    # input stream has been processed by the runnable.\n                    {\n                        \"op\": \"add\",\n                        \"path\": f\"/logs/{index}/final_output\",\n                        # to undo the dumpd done by some runnables / tracer / etc\n                        \"value\": _get_standardized_outputs(run, self._schema_format),\n                    },\n                    {\n                        \"op\": \"add\",\n                        \"path\": f\"/logs/{index}/end_time\",\n                        \"value\": run.end_time.isoformat(timespec=\"milliseconds\")\n                        if run.end_time is not None\n                        else None,\n                    },\n                ]\n            )\n\n            self.send(*ops)\n        finally:\n            if run.id == self.root_id and self.auto_close:\n                self.send_stream.close()\n\n    def _on_llm_new_token(\n        self,\n        run: Run,\n        token: str,\n        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],\n    ) -> None:\n        \"\"\"Process new LLM token.\"\"\"\n        index = self._key_map_by_run_id.get(run.id)\n\n        if index is None:\n            return\n\n        self.send(\n            {\n                \"op\": \"add\",\n                \"path\": f\"/logs/{index}/streamed_output_str/-\",\n                \"value\": token,\n            },\n            {\n                \"op\": \"add\",\n                \"path\": f\"/logs/{index}/streamed_output/-\",\n                \"value\": chunk.message\n                if isinstance(chunk, ChatGenerationChunk)\n                else token,\n            },\n        )\n\n\ndef _get_standardized_inputs(\n    run: Run, schema_format: Literal[\"original\", \"streaming_events\"]\n) -> Optional[dict[str, Any]]:\n    \"\"\"Extract standardized inputs from a run.\n\n    Standardizes the inputs based on the type of the runnable used.\n\n    Args:\n        run: Run object\n        schema_format: The schema format to use.\n\n    Returns:\n        Valid inputs are only dict. By conventions, inputs always represented\n        invocation using named arguments.\n        None means that the input is not yet known!\n    \"\"\"\n    if schema_format == \"original\":\n        msg = (\n            \"Do not assign inputs with original schema drop the key for now.\"\n            \"When inputs are added to astream_log they should be added with \"\n            \"standardized schema for streaming events.\"\n        )\n        raise NotImplementedError(msg)\n\n    inputs = load(run.inputs)\n\n    if run.run_type in {\"retriever\", \"llm\", \"chat_model\"}:\n        return inputs\n\n    # new style chains\n    # These nest an additional 'input' key inside the 'inputs' to make sure\n    # the input is always a dict. We need to unpack and user the inner value.\n    inputs = inputs[\"input\"]\n    # We should try to fix this in Runnables and callbacks/tracers\n    # Runnables should be using a None type here not a placeholder\n    # dict.\n    if inputs == {\"input\": \"\"}:  # Workaround for Runnables not using None\n        # The input is not known, so we don't assign data['input']\n        return None\n    return inputs\n\n\ndef _get_standardized_outputs(\n    run: Run, schema_format: Literal[\"original\", \"streaming_events\", \"original+chat\"]\n) -> Optional[Any]:\n    \"\"\"Extract standardized output from a run.\n\n    Standardizes the outputs based on the type of the runnable used.\n\n    Args:\n        log: The log entry.\n        schema_format: The schema format to use.\n\n    Returns:\n        An output if returned, otherwise a None\n    \"\"\"\n    outputs = load(run.outputs)\n    if schema_format == \"original\":\n        if run.run_type == \"prompt\" and \"output\" in outputs:\n            # These were previously dumped before the tracer.\n            # Now we needn't do anything to them.\n            return outputs[\"output\"]\n        # Return the old schema, without standardizing anything\n        return outputs\n\n    if run.run_type in {\"retriever\", \"llm\", \"chat_model\"}:\n        return outputs\n\n    if isinstance(outputs, dict):\n        return outputs.get(\"output\", None)\n\n    return None\n\n\n@overload\ndef _astream_log_implementation(\n    runnable: Runnable[Input, Output],\n    input: Any,\n    config: Optional[RunnableConfig] = None,\n    *,\n    stream: LogStreamCallbackHandler,\n    diff: Literal[True] = True,\n    with_streamed_output_list: bool = True,\n    **kwargs: Any,\n) -> AsyncIterator[RunLogPatch]: ...\n\n\n@overload\ndef _astream_log_implementation(\n    runnable: Runnable[Input, Output],\n    input: Any,\n    config: Optional[RunnableConfig] = None,\n    *,\n    stream: LogStreamCallbackHandler,\n    diff: Literal[False],\n    with_streamed_output_list: bool = True,\n    **kwargs: Any,\n) -> AsyncIterator[RunLog]: ...\n\n\nasync def _astream_log_implementation(\n    runnable: Runnable[Input, Output],\n    input: Any,\n    config: Optional[RunnableConfig] = None,\n    *,\n    stream: LogStreamCallbackHandler,\n    diff: bool = True,\n    with_streamed_output_list: bool = True,\n    **kwargs: Any,\n) -> Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]:\n    \"\"\"Implementation of astream_log for a given runnable.\n\n    The implementation has been factored out (at least temporarily) as both\n    astream_log and astream_events relies on it.\n    \"\"\"\n    import jsonpatch  # type: ignore[import]\n\n    from langchain_core.callbacks.base import BaseCallbackManager\n    from langchain_core.tracers.log_stream import (\n        RunLog,\n        RunLogPatch,\n    )\n\n    # Assign the stream handler to the config\n    config = ensure_config(config)\n    callbacks = config.get(\"callbacks\")\n    if callbacks is None:\n        config[\"callbacks\"] = [stream]\n    elif isinstance(callbacks, list):\n        config[\"callbacks\"] = callbacks + [stream]\n    elif isinstance(callbacks, BaseCallbackManager):\n        callbacks = callbacks.copy()\n        callbacks.add_handler(stream, inherit=True)\n        config[\"callbacks\"] = callbacks\n    else:\n        msg = (\n            f\"Unexpected type for callbacks: {callbacks}.\"\n            \"Expected None, list or AsyncCallbackManager.\"\n        )\n        raise ValueError(msg)\n\n    # Call the runnable in streaming mode,\n    # add each chunk to the output stream\n    async def consume_astream() -> None:\n        try:\n            prev_final_output: Optional[Output] = None\n            final_output: Optional[Output] = None\n\n            async for chunk in runnable.astream(input, config, **kwargs):\n                prev_final_output = final_output\n                if final_output is None:\n                    final_output = chunk\n                else:\n                    try:\n                        final_output = final_output + chunk  # type: ignore\n                    except TypeError:\n                        prev_final_output = None\n                        final_output = chunk\n                patches: list[dict[str, Any]] = []\n                if with_streamed_output_list:\n                    patches.append(\n                        {\n                            \"op\": \"add\",\n                            \"path\": \"/streamed_output/-\",\n                            # chunk cannot be shared between\n                            # streamed_output and final_output\n                            # otherwise jsonpatch.apply will\n                            # modify both\n                            \"value\": copy.deepcopy(chunk),\n                        }\n                    )\n                for op in jsonpatch.JsonPatch.from_diff(\n                    prev_final_output, final_output, dumps=dumps\n                ):\n                    patches.append({**op, \"path\": f\"/final_output{op['path']}\"})\n                await stream.send_stream.send(RunLogPatch(*patches))\n        finally:\n            await stream.send_stream.aclose()\n\n    # Start the runnable in a task, so we can start consuming output\n    task = asyncio.create_task(consume_astream())\n    try:\n        # Yield each chunk from the output stream\n        if diff:\n            async for log in stream:\n                yield log\n        else:\n            state = RunLog(state=None)  # type: ignore[arg-type]\n            async for log in stream:\n                state = state + log\n                yield state\n    finally:\n        # Wait for the runnable to finish, if not cancelled (eg. by break)\n        with contextlib.suppress(asyncio.CancelledError):\n            await task\n",
        "patch": "@@ -5,16 +5,15 @@\n import copy\n import threading\n from collections import defaultdict\n-from collections.abc import AsyncIterator, Iterator, Sequence\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Literal,\n     Optional,\n     TypeVar,\n     Union,\n     overload,\n )\n-from uuid import UUID\n \n import jsonpatch  # type: ignore[import]\n from typing_extensions import NotRequired, TypedDict\n@@ -23,11 +22,16 @@\n from langchain_core.load.load import load\n from langchain_core.outputs import ChatGenerationChunk, GenerationChunk\n from langchain_core.runnables import Runnable, RunnableConfig, ensure_config\n-from langchain_core.runnables.utils import Input, Output\n from langchain_core.tracers._streaming import _StreamingCallbackHandler\n from langchain_core.tracers.base import BaseTracer\n from langchain_core.tracers.memory_stream import _MemoryStream\n-from langchain_core.tracers.schemas import Run\n+\n+if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Iterator, Sequence\n+    from uuid import UUID\n+\n+    from langchain_core.runnables.utils import Input, Output\n+    from langchain_core.tracers.schemas import Run\n \n \n class LogEntry(TypedDict):"
      },
      {
        "filename": "libs/core/langchain_core/tracers/root_listeners.py",
        "content_before": "from collections.abc import Awaitable\nfrom typing import Callable, Optional, Union\nfrom uuid import UUID\n\nfrom langchain_core.runnables.config import (\n    RunnableConfig,\n    acall_func_with_variable_args,\n    call_func_with_variable_args,\n)\nfrom langchain_core.tracers.base import AsyncBaseTracer, BaseTracer\nfrom langchain_core.tracers.schemas import Run\n\nListener = Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\nAsyncListener = Union[\n    Callable[[Run], Awaitable[None]], Callable[[Run, RunnableConfig], Awaitable[None]]\n]\n\n\nclass RootListenersTracer(BaseTracer):\n    \"\"\"Tracer that calls listeners on run start, end, and error.\n\n    Parameters:\n        log_missing_parent: Whether to log a warning if the parent is missing.\n            Default is False.\n        config: The runnable config.\n        on_start: The listener to call on run start.\n        on_end: The listener to call on run end.\n        on_error: The listener to call on run error.\n    \"\"\"\n\n    log_missing_parent = False\n\n    def __init__(\n        self,\n        *,\n        config: RunnableConfig,\n        on_start: Optional[Listener],\n        on_end: Optional[Listener],\n        on_error: Optional[Listener],\n    ) -> None:\n        \"\"\"Initialize the tracer.\n\n        Args:\n            config: The runnable config.\n            on_start: The listener to call on run start.\n            on_end: The listener to call on run end.\n            on_error: The listener to call on run error\n        \"\"\"\n        super().__init__(_schema_format=\"original+chat\")\n\n        self.config = config\n        self._arg_on_start = on_start\n        self._arg_on_end = on_end\n        self._arg_on_error = on_error\n        self.root_id: Optional[UUID] = None\n\n    def _persist_run(self, run: Run) -> None:\n        # This is a legacy method only called once for an entire run tree\n        # therefore not useful here\n        pass\n\n    def _on_run_create(self, run: Run) -> None:\n        if self.root_id is not None:\n            return\n\n        self.root_id = run.id\n\n        if self._arg_on_start is not None:\n            call_func_with_variable_args(self._arg_on_start, run, self.config)\n\n    def _on_run_update(self, run: Run) -> None:\n        if run.id != self.root_id:\n            return\n\n        if run.error is None:\n            if self._arg_on_end is not None:\n                call_func_with_variable_args(self._arg_on_end, run, self.config)\n        else:\n            if self._arg_on_error is not None:\n                call_func_with_variable_args(self._arg_on_error, run, self.config)\n\n\nclass AsyncRootListenersTracer(AsyncBaseTracer):\n    \"\"\"Async Tracer that calls listeners on run start, end, and error.\n\n    Parameters:\n        log_missing_parent: Whether to log a warning if the parent is missing.\n            Default is False.\n        config: The runnable config.\n        on_start: The listener to call on run start.\n        on_end: The listener to call on run end.\n        on_error: The listener to call on run error.\n    \"\"\"\n\n    log_missing_parent = False\n\n    def __init__(\n        self,\n        *,\n        config: RunnableConfig,\n        on_start: Optional[AsyncListener],\n        on_end: Optional[AsyncListener],\n        on_error: Optional[AsyncListener],\n    ) -> None:\n        \"\"\"Initialize the tracer.\n\n        Args:\n            config: The runnable config.\n            on_start: The listener to call on run start.\n            on_end: The listener to call on run end.\n            on_error: The listener to call on run error\n        \"\"\"\n        super().__init__(_schema_format=\"original+chat\")\n\n        self.config = config\n        self._arg_on_start = on_start\n        self._arg_on_end = on_end\n        self._arg_on_error = on_error\n        self.root_id: Optional[UUID] = None\n\n    async def _persist_run(self, run: Run) -> None:\n        # This is a legacy method only called once for an entire run tree\n        # therefore not useful here\n        pass\n\n    async def _on_run_create(self, run: Run) -> None:\n        if self.root_id is not None:\n            return\n\n        self.root_id = run.id\n\n        if self._arg_on_start is not None:\n            await acall_func_with_variable_args(self._arg_on_start, run, self.config)\n\n    async def _on_run_update(self, run: Run) -> None:\n        if run.id != self.root_id:\n            return\n\n        if run.error is None:\n            if self._arg_on_end is not None:\n                await acall_func_with_variable_args(self._arg_on_end, run, self.config)\n        else:\n            if self._arg_on_error is not None:\n                await acall_func_with_variable_args(\n                    self._arg_on_error, run, self.config\n                )\n",
        "patch": "@@ -1,6 +1,5 @@\n from collections.abc import Awaitable\n-from typing import Callable, Optional, Union\n-from uuid import UUID\n+from typing import TYPE_CHECKING, Callable, Optional, Union\n \n from langchain_core.runnables.config import (\n     RunnableConfig,\n@@ -10,6 +9,9 @@\n from langchain_core.tracers.base import AsyncBaseTracer, BaseTracer\n from langchain_core.tracers.schemas import Run\n \n+if TYPE_CHECKING:\n+    from uuid import UUID\n+\n Listener = Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n AsyncListener = Union[\n     Callable[[Run], Awaitable[None]], Callable[[Run, RunnableConfig], Awaitable[None]]"
      },
      {
        "filename": "libs/core/langchain_core/utils/json_schema.py",
        "content_before": "from __future__ import annotations\n\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import Any, Optional\n\n\ndef _retrieve_ref(path: str, schema: dict) -> dict:\n    components = path.split(\"/\")\n    if components[0] != \"#\":\n        msg = (\n            \"ref paths are expected to be URI fragments, meaning they should start \"\n            \"with #.\"\n        )\n        raise ValueError(msg)\n    out = schema\n    for component in components[1:]:\n        if component in out:\n            out = out[component]\n        elif component.isdigit() and int(component) in out:\n            out = out[int(component)]\n        else:\n            msg = f\"Reference '{path}' not found.\"\n            raise KeyError(msg)\n    return deepcopy(out)\n\n\ndef _dereference_refs_helper(\n    obj: Any,\n    full_schema: dict[str, Any],\n    skip_keys: Sequence[str],\n    processed_refs: Optional[set[str]] = None,\n) -> Any:\n    if processed_refs is None:\n        processed_refs = set()\n\n    if isinstance(obj, dict):\n        obj_out = {}\n        for k, v in obj.items():\n            if k in skip_keys:\n                obj_out[k] = v\n            elif k == \"$ref\":\n                if v in processed_refs:\n                    continue\n                processed_refs.add(v)\n                ref = _retrieve_ref(v, full_schema)\n                full_ref = _dereference_refs_helper(\n                    ref, full_schema, skip_keys, processed_refs\n                )\n                processed_refs.remove(v)\n                return full_ref\n            elif isinstance(v, (list, dict)):\n                obj_out[k] = _dereference_refs_helper(\n                    v, full_schema, skip_keys, processed_refs\n                )\n            else:\n                obj_out[k] = v\n        return obj_out\n    elif isinstance(obj, list):\n        return [\n            _dereference_refs_helper(el, full_schema, skip_keys, processed_refs)\n            for el in obj\n        ]\n    else:\n        return obj\n\n\ndef _infer_skip_keys(\n    obj: Any, full_schema: dict, processed_refs: Optional[set[str]] = None\n) -> list[str]:\n    if processed_refs is None:\n        processed_refs = set()\n\n    keys = []\n    if isinstance(obj, dict):\n        for k, v in obj.items():\n            if k == \"$ref\":\n                if v in processed_refs:\n                    continue\n                processed_refs.add(v)\n                ref = _retrieve_ref(v, full_schema)\n                keys.append(v.split(\"/\")[1])\n                keys += _infer_skip_keys(ref, full_schema, processed_refs)\n            elif isinstance(v, (list, dict)):\n                keys += _infer_skip_keys(v, full_schema, processed_refs)\n    elif isinstance(obj, list):\n        for el in obj:\n            keys += _infer_skip_keys(el, full_schema, processed_refs)\n    return keys\n\n\ndef dereference_refs(\n    schema_obj: dict,\n    *,\n    full_schema: Optional[dict] = None,\n    skip_keys: Optional[Sequence[str]] = None,\n) -> dict:\n    \"\"\"Try to substitute $refs in JSON Schema.\n\n    Args:\n        schema_obj: The schema object to dereference.\n        full_schema: The full schema object. Defaults to None.\n        skip_keys: The keys to skip. Defaults to None.\n\n    Returns:\n        The dereferenced schema object.\n    \"\"\"\n    full_schema = full_schema or schema_obj\n    skip_keys = (\n        skip_keys\n        if skip_keys is not None\n        else _infer_skip_keys(schema_obj, full_schema)\n    )\n    return _dereference_refs_helper(schema_obj, full_schema, skip_keys)\n",
        "patch": "@@ -1,8 +1,10 @@\n from __future__ import annotations\n \n-from collections.abc import Sequence\n from copy import deepcopy\n-from typing import Any, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n \n \n def _retrieve_ref(path: str, schema: dict) -> dict:"
      },
      {
        "filename": "libs/core/langchain_core/utils/mustache.py",
        "content_before": "\"\"\"Adapted from https://github.com/noahmorrison/chevron\nMIT License.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom collections.abc import Iterator, Mapping, Sequence\nfrom types import MappingProxyType\nfrom typing import (\n    Any,\n    Literal,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom typing_extensions import TypeAlias\n\nlogger = logging.getLogger(__name__)\n\n\nScopes: TypeAlias = list[Union[Literal[False, 0], Mapping[str, Any]]]\n\n\n# Globals\n_CURRENT_LINE = 1\n_LAST_TAG_LINE = None\n\n\nclass ChevronError(SyntaxError):\n    \"\"\"Custom exception for Chevron errors.\"\"\"\n\n\n#\n# Helper functions\n#\n\n\ndef grab_literal(template: str, l_del: str) -> tuple[str, str]:\n    \"\"\"Parse a literal from the template.\n\n    Args:\n        template: The template to parse.\n        l_del: The left delimiter.\n\n    Returns:\n        Tuple[str, str]: The literal and the template.\n    \"\"\"\n    global _CURRENT_LINE\n\n    try:\n        # Look for the next tag and move the template to it\n        literal, template = template.split(l_del, 1)\n        _CURRENT_LINE += literal.count(\"\\n\")\n\n    # There are no more tags in the template?\n    except ValueError:\n        # Then the rest of the template is a literal\n        return (template, \"\")\n\n    return (literal, template)\n\n\ndef l_sa_check(template: str, literal: str, is_standalone: bool) -> bool:\n    \"\"\"Do a preliminary check to see if a tag could be a standalone.\n\n    Args:\n        template: The template. (Not used.)\n        literal: The literal.\n        is_standalone: Whether the tag is standalone.\n\n    Returns:\n        bool: Whether the tag could be a standalone.\n    \"\"\"\n    # If there is a newline, or the previous tag was a standalone\n    if literal.find(\"\\n\") != -1 or is_standalone:\n        padding = literal.split(\"\\n\")[-1]\n\n        # If all the characters since the last newline are spaces\n        # Then the next tag could be a standalone\n        # Otherwise it can't be\n        return padding.isspace() or padding == \"\"\n    else:\n        return False\n\n\ndef r_sa_check(template: str, tag_type: str, is_standalone: bool) -> bool:\n    \"\"\"Do a final check to see if a tag could be a standalone.\n\n    Args:\n        template: The template.\n        tag_type: The type of the tag.\n        is_standalone: Whether the tag is standalone.\n\n    Returns:\n        bool: Whether the tag could be a standalone.\n    \"\"\"\n    # Check right side if we might be a standalone\n    if is_standalone and tag_type not in [\"variable\", \"no escape\"]:\n        on_newline = template.split(\"\\n\", 1)\n\n        # If the stuff to the right of us are spaces we're a standalone\n        return on_newline[0].isspace() or not on_newline[0]\n\n    # If we're a tag can't be a standalone\n    else:\n        return False\n\n\ndef parse_tag(template: str, l_del: str, r_del: str) -> tuple[tuple[str, str], str]:\n    \"\"\"Parse a tag from a template.\n\n    Args:\n        template: The template.\n        l_del: The left delimiter.\n        r_del: The right delimiter.\n\n    Returns:\n        Tuple[Tuple[str, str], str]: The tag and the template.\n\n    Raises:\n        ChevronError: If the tag is unclosed.\n        ChevronError: If the set delimiter tag is unclosed.\n    \"\"\"\n    global _CURRENT_LINE, _LAST_TAG_LINE\n\n    tag_types = {\n        \"!\": \"comment\",\n        \"#\": \"section\",\n        \"^\": \"inverted section\",\n        \"/\": \"end\",\n        \">\": \"partial\",\n        \"=\": \"set delimiter?\",\n        \"{\": \"no escape?\",\n        \"&\": \"no escape\",\n    }\n\n    # Get the tag\n    try:\n        tag, template = template.split(r_del, 1)\n    except ValueError as e:\n        msg = f\"unclosed tag at line {_CURRENT_LINE}\"\n        raise ChevronError(msg) from e\n\n    # Find the type meaning of the first character\n    tag_type = tag_types.get(tag[0], \"variable\")\n\n    # If the type is not a variable\n    if tag_type != \"variable\":\n        # Then that first character is not needed\n        tag = tag[1:]\n\n    # If we might be a set delimiter tag\n    if tag_type == \"set delimiter?\":\n        # Double check to make sure we are\n        if tag.endswith(\"=\"):\n            tag_type = \"set delimiter\"\n            # Remove the equal sign\n            tag = tag[:-1]\n\n        # Otherwise we should complain\n        else:\n            msg = f\"unclosed set delimiter tag\\nat line {_CURRENT_LINE}\"\n            raise ChevronError(msg)\n\n    elif (\n        # If we might be a no html escape tag\n        tag_type == \"no escape?\"\n        # And we have a third curly brace\n        # (And are using curly braces as delimiters)\n        and l_del == \"{{\"\n        and r_del == \"}}\"\n        and template.startswith(\"}\")\n    ):\n        # Then we are a no html escape tag\n        template = template[1:]\n        tag_type = \"no escape\"\n\n    # Strip the whitespace off the key and return\n    return ((tag_type, tag.strip()), template)\n\n\n#\n# The main tokenizing function\n#\n\n\ndef tokenize(\n    template: str, def_ldel: str = \"{{\", def_rdel: str = \"}}\"\n) -> Iterator[tuple[str, str]]:\n    \"\"\"Tokenize a mustache template.\n\n    Tokenizes a mustache template in a generator fashion,\n    using file-like objects. It also accepts a string containing\n    the template.\n\n    Args:\n        template: a file-like object, or a string of a mustache template\n        def_ldel: The default left delimiter\n            (\"{{\" by default, as in spec compliant mustache)\n        def_rdel: The default right delimiter\n            (\"}}\" by default, as in spec compliant mustache)\n\n    Returns:\n        A generator of mustache tags in the form of a tuple (tag_type, tag_key)\n            Where tag_type is one of:\n             * literal\n             * section\n             * inverted section\n             * end\n             * partial\n             * no escape\n            And tag_key is either the key or in the case of a literal tag,\n            the literal itself.\n    \"\"\"\n    global _CURRENT_LINE, _LAST_TAG_LINE\n    _CURRENT_LINE = 1\n    _LAST_TAG_LINE = None\n\n    is_standalone = True\n    open_sections = []\n    l_del = def_ldel\n    r_del = def_rdel\n\n    while template:\n        literal, template = grab_literal(template, l_del)\n\n        # If the template is completed\n        if not template:\n            # Then yield the literal and leave\n            yield (\"literal\", literal)\n            break\n\n        # Do the first check to see if we could be a standalone\n        is_standalone = l_sa_check(template, literal, is_standalone)\n\n        # Parse the tag\n        tag, template = parse_tag(template, l_del, r_del)\n        tag_type, tag_key = tag\n\n        # Special tag logic\n\n        # If we are a set delimiter tag\n        if tag_type == \"set delimiter\":\n            # Then get and set the delimiters\n            dels = tag_key.strip().split(\" \")\n            l_del, r_del = dels[0], dels[-1]\n\n        # If we are a section tag\n        elif tag_type in [\"section\", \"inverted section\"]:\n            # Then open a new section\n            open_sections.append(tag_key)\n            _LAST_TAG_LINE = _CURRENT_LINE\n\n        # If we are an end tag\n        elif tag_type == \"end\":\n            # Then check to see if the last opened section\n            # is the same as us\n            try:\n                last_section = open_sections.pop()\n            except IndexError as e:\n                msg = (\n                    f'Trying to close tag \"{tag_key}\"\\n'\n                    \"Looks like it was not opened.\\n\"\n                    f\"line {_CURRENT_LINE + 1}\"\n                )\n                raise ChevronError(msg) from e\n            if tag_key != last_section:\n                # Otherwise we need to complain\n                msg = (\n                    f'Trying to close tag \"{tag_key}\"\\n'\n                    f'last open tag is \"{last_section}\"\\n'\n                    f\"line {_CURRENT_LINE + 1}\"\n                )\n                raise ChevronError(msg)\n\n        # Do the second check to see if we're a standalone\n        is_standalone = r_sa_check(template, tag_type, is_standalone)\n\n        # Which if we are\n        if is_standalone:\n            # Remove the stuff before the newline\n            template = template.split(\"\\n\", 1)[-1]\n\n            # Partials need to keep the spaces on their left\n            if tag_type != \"partial\":\n                # But other tags don't\n                literal = literal.rstrip(\" \")\n\n        # Start yielding\n        # Ignore literals that are empty\n        if literal != \"\":\n            yield (\"literal\", literal)\n\n        # Ignore comments and set delimiters\n        if tag_type not in [\"comment\", \"set delimiter?\"]:\n            yield (tag_type, tag_key)\n\n    # If there are any open sections when we're done\n    if open_sections:\n        # Then we need to complain\n        msg = (\n            \"Unexpected EOF\\n\"\n            f'the tag \"{open_sections[-1]}\" was never closed\\n'\n            f\"was opened at line {_LAST_TAG_LINE}\"\n        )\n        raise ChevronError(msg)\n\n\n#\n# Helper functions\n#\n\n\ndef _html_escape(string: str) -> str:\n    \"\"\"HTML escape all of these \" & < >.\"\"\"\n    html_codes = {\n        '\"': \"&quot;\",\n        \"<\": \"&lt;\",\n        \">\": \"&gt;\",\n    }\n\n    # & must be handled first\n    string = string.replace(\"&\", \"&amp;\")\n    for char in html_codes:\n        string = string.replace(char, html_codes[char])\n    return string\n\n\ndef _get_key(\n    key: str,\n    scopes: Scopes,\n    warn: bool,\n    keep: bool,\n    def_ldel: str,\n    def_rdel: str,\n) -> Any:\n    \"\"\"Get a key from the current scope.\"\"\"\n    # If the key is a dot\n    if key == \".\":\n        # Then just return the current scope\n        return scopes[0]\n\n    # Loop through the scopes\n    for scope in scopes:\n        try:\n            # Return an empty string if falsy, with two exceptions\n            # 0 should return 0, and False should return False\n            if scope in (0, False):\n                return scope\n\n            # For every dot separated key\n            for child in key.split(\".\"):\n                # Return an empty string if falsy, with two exceptions\n                # 0 should return 0, and False should return False\n                if scope in (0, False):\n                    return scope\n                # Move into the scope\n                try:\n                    # Try subscripting (Normal dictionaries)\n                    scope = cast(dict[str, Any], scope)[child]\n                except (TypeError, AttributeError):\n                    try:\n                        scope = getattr(scope, child)\n                    except (TypeError, AttributeError):\n                        # Try as a list\n                        scope = scope[int(child)]  # type: ignore\n\n            try:\n                # This allows for custom falsy data types\n                # https://github.com/noahmorrison/chevron/issues/35\n                if scope._CHEVRON_return_scope_when_falsy:  # type: ignore\n                    return scope\n            except AttributeError:\n                if scope in (0, False):\n                    return scope\n                return scope or \"\"\n        except (AttributeError, KeyError, IndexError, ValueError):\n            # We couldn't find the key in the current scope\n            # We'll try again on the next pass\n            pass\n\n    # We couldn't find the key in any of the scopes\n\n    if warn:\n        logger.warn(f\"Could not find key '{key}'\")\n\n    if keep:\n        return f\"{def_ldel} {key} {def_rdel}\"\n\n    return \"\"\n\n\ndef _get_partial(name: str, partials_dict: Mapping[str, str]) -> str:\n    \"\"\"Load a partial.\"\"\"\n    try:\n        # Maybe the partial is in the dictionary\n        return partials_dict[name]\n    except KeyError:\n        return \"\"\n\n\n#\n# The main rendering function\n#\ng_token_cache: dict[str, list[tuple[str, str]]] = {}\n\nEMPTY_DICT: MappingProxyType[str, str] = MappingProxyType({})\n\n\ndef render(\n    template: Union[str, list[tuple[str, str]]] = \"\",\n    data: Mapping[str, Any] = EMPTY_DICT,\n    partials_dict: Mapping[str, str] = EMPTY_DICT,\n    padding: str = \"\",\n    def_ldel: str = \"{{\",\n    def_rdel: str = \"}}\",\n    scopes: Optional[Scopes] = None,\n    warn: bool = False,\n    keep: bool = False,\n) -> str:\n    \"\"\"Render a mustache template.\n\n    Renders a mustache template with a data scope and inline partial capability.\n\n    Args:\n        template: A file-like object or a string containing the template.\n        data: A python dictionary with your data scope.\n        partials_path: The path to where your partials are stored.\n             If set to None, then partials won't be loaded from the file system\n             (defaults to '.').\n        partials_ext: The extension that you want the parser to look for\n            (defaults to 'mustache').\n        partials_dict: A python dictionary which will be search for partials\n             before the filesystem is. {'include': 'foo'} is the same\n             as a file called include.mustache\n             (defaults to {}).\n        padding: This is for padding partials, and shouldn't be used\n            (but can be if you really want to).\n        def_ldel: The default left delimiter\n             (\"{{\" by default, as in spec compliant mustache).\n        def_rdel: The default right delimiter\n             (\"}}\" by default, as in spec compliant mustache).\n        scopes: The list of scopes that get_key will look through.\n        warn: Log a warning when a template substitution isn't found in the data\n        keep: Keep unreplaced tags when a substitution isn't found in the data.\n\n    Returns:\n        A string containing the rendered template.\n    \"\"\"\n    # If the template is a sequence but not derived from a string\n    if isinstance(template, Sequence) and not isinstance(template, str):\n        # Then we don't need to tokenize it\n        # But it does need to be a generator\n        tokens: Iterator[tuple[str, str]] = (token for token in template)\n    else:\n        if template in g_token_cache:\n            tokens = (token for token in g_token_cache[template])\n        else:\n            # Otherwise make a generator\n            tokens = tokenize(template, def_ldel, def_rdel)\n\n    output = \"\"\n\n    if scopes is None:\n        scopes = [data]\n\n    # Run through the tokens\n    for tag, key in tokens:\n        # Set the current scope\n        current_scope = scopes[0]\n\n        # If we're an end tag\n        if tag == \"end\":\n            # Pop out of the latest scope\n            del scopes[0]\n\n        # If the current scope is falsy and not the only scope\n        elif not current_scope and len(scopes) != 1:\n            if tag in [\"section\", \"inverted section\"]:\n                # Set the most recent scope to a falsy value\n                scopes.insert(0, False)\n\n        # If we're a literal tag\n        elif tag == \"literal\":\n            # Add padding to the key and add it to the output\n            output += key.replace(\"\\n\", \"\\n\" + padding)\n\n        # If we're a variable tag\n        elif tag == \"variable\":\n            # Add the html escaped key to the output\n            thing = _get_key(\n                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel\n            )\n            if thing is True and key == \".\":\n                # if we've coerced into a boolean by accident\n                # (inverted tags do this)\n                # then get the un-coerced object (next in the stack)\n                thing = scopes[1]\n            if not isinstance(thing, str):\n                thing = str(thing)\n            output += _html_escape(thing)\n\n        # If we're a no html escape tag\n        elif tag == \"no escape\":\n            # Just lookup the key and add it\n            thing = _get_key(\n                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel\n            )\n            if not isinstance(thing, str):\n                thing = str(thing)\n            output += thing\n\n        # If we're a section tag\n        elif tag == \"section\":\n            # Get the sections scope\n            scope = _get_key(\n                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel\n            )\n\n            # If the scope is a callable (as described in\n            # https://mustache.github.io/mustache.5.html)\n            if callable(scope):\n                # Generate template text from tags\n                text = \"\"\n                tags: list[tuple[str, str]] = []\n                for token in tokens:\n                    if token == (\"end\", key):\n                        break\n\n                    tags.append(token)\n                    tag_type, tag_key = token\n                    if tag_type == \"literal\":\n                        text += tag_key\n                    elif tag_type == \"no escape\":\n                        text += f\"{def_ldel}& {tag_key} {def_rdel}\"\n                    else:\n                        text += \"{}{} {}{}\".format(\n                            def_ldel,\n                            {\n                                \"comment\": \"!\",\n                                \"section\": \"#\",\n                                \"inverted section\": \"^\",\n                                \"end\": \"/\",\n                                \"partial\": \">\",\n                                \"set delimiter\": \"=\",\n                                \"no escape\": \"&\",\n                                \"variable\": \"\",\n                            }[tag_type],\n                            tag_key,\n                            def_rdel,\n                        )\n\n                g_token_cache[text] = tags\n\n                rend = scope(\n                    text,\n                    lambda template, data=None: render(\n                        template,\n                        data={},\n                        partials_dict=partials_dict,\n                        padding=padding,\n                        def_ldel=def_ldel,\n                        def_rdel=def_rdel,\n                        scopes=data and [data] + scopes or scopes,\n                        warn=warn,\n                        keep=keep,\n                    ),\n                )\n\n                output += rend\n\n            # If the scope is a sequence, an iterator or generator but not\n            # derived from a string\n            elif isinstance(scope, (Sequence, Iterator)) and not isinstance(scope, str):\n                # Then we need to do some looping\n\n                # Gather up all the tags inside the section\n                # (And don't be tricked by nested end tags with the same key)\n                # TODO: This feels like it still has edge cases, no?\n                tags = []\n                tags_with_same_key = 0\n                for token in tokens:\n                    if token == (\"section\", key):\n                        tags_with_same_key += 1\n                    if token == (\"end\", key):\n                        tags_with_same_key -= 1\n                        if tags_with_same_key < 0:\n                            break\n                    tags.append(token)\n\n                # For every item in the scope\n                for thing in scope:\n                    # Append it as the most recent scope and render\n                    new_scope = [thing] + scopes\n                    rend = render(\n                        template=tags,\n                        scopes=new_scope,\n                        padding=padding,\n                        partials_dict=partials_dict,\n                        def_ldel=def_ldel,\n                        def_rdel=def_rdel,\n                        warn=warn,\n                        keep=keep,\n                    )\n\n                    output += rend\n\n            else:\n                # Otherwise we're just a scope section\n                scopes.insert(0, scope)\n\n        # If we're an inverted section\n        elif tag == \"inverted section\":\n            # Add the flipped scope to the scopes\n            scope = _get_key(\n                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel\n            )\n            scopes.insert(0, cast(Literal[False], not scope))\n\n        # If we're a partial\n        elif tag == \"partial\":\n            # Load the partial\n            partial = _get_partial(key, partials_dict)\n\n            # Find what to pad the partial with\n            left = output.rpartition(\"\\n\")[2]\n            part_padding = padding\n            if left.isspace():\n                part_padding += left\n\n            # Render the partial\n            part_out = render(\n                template=partial,\n                partials_dict=partials_dict,\n                def_ldel=def_ldel,\n                def_rdel=def_rdel,\n                padding=part_padding,\n                scopes=scopes,\n                warn=warn,\n                keep=keep,\n            )\n\n            # If the partial was indented\n            if left.isspace():\n                # then remove the spaces from the end\n                part_out = part_out.rstrip(\" \\t\")\n\n            # Add the partials output to the output\n            output += part_out\n\n    return output\n",
        "patch": "@@ -8,14 +8,16 @@\n from collections.abc import Iterator, Mapping, Sequence\n from types import MappingProxyType\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Literal,\n     Optional,\n     Union,\n     cast,\n )\n \n-from typing_extensions import TypeAlias\n+if TYPE_CHECKING:\n+    from typing_extensions import TypeAlias\n \n logger = logging.getLogger(__name__)\n "
      },
      {
        "filename": "libs/core/langchain_core/utils/pydantic.py",
        "content_before": "\"\"\"Utilities for pydantic.\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nimport textwrap\nimport warnings\nfrom contextlib import nullcontext\nfrom functools import lru_cache, wraps\nfrom types import GenericAlias\nfrom typing import (\n    Any,\n    Callable,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n    overload,\n)\n\nimport pydantic\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    PydanticDeprecationWarning,\n    RootModel,\n    root_validator,\n)\nfrom pydantic import (\n    create_model as _create_model_base,\n)\nfrom pydantic.json_schema import (\n    DEFAULT_REF_TEMPLATE,\n    GenerateJsonSchema,\n    JsonSchemaMode,\n    JsonSchemaValue,\n)\nfrom pydantic_core import core_schema\n\n\ndef get_pydantic_major_version() -> int:\n    \"\"\"Get the major version of Pydantic.\"\"\"\n    try:\n        import pydantic\n\n        return int(pydantic.__version__.split(\".\")[0])\n    except ImportError:\n        return 0\n\n\ndef _get_pydantic_minor_version() -> int:\n    \"\"\"Get the minor version of Pydantic.\"\"\"\n    try:\n        import pydantic\n\n        return int(pydantic.__version__.split(\".\")[1])\n    except ImportError:\n        return 0\n\n\nPYDANTIC_MAJOR_VERSION = get_pydantic_major_version()\nPYDANTIC_MINOR_VERSION = _get_pydantic_minor_version()\n\n\nif PYDANTIC_MAJOR_VERSION == 1:\n    from pydantic.fields import FieldInfo as FieldInfoV1\n\n    PydanticBaseModel = pydantic.BaseModel\n    TypeBaseModel = type[BaseModel]\nelif PYDANTIC_MAJOR_VERSION == 2:\n    from pydantic.v1.fields import FieldInfo as FieldInfoV1  # type: ignore[assignment]\n\n    # Union type needs to be last assignment to PydanticBaseModel to make mypy happy.\n    PydanticBaseModel = Union[BaseModel, pydantic.BaseModel]  # type: ignore\n    TypeBaseModel = Union[type[BaseModel], type[pydantic.BaseModel]]  # type: ignore\nelse:\n    msg = f\"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}\"\n    raise ValueError(msg)\n\n\nTBaseModel = TypeVar(\"TBaseModel\", bound=PydanticBaseModel)\n\n\ndef is_pydantic_v1_subclass(cls: type) -> bool:\n    \"\"\"Check if the installed Pydantic version is 1.x-like.\"\"\"\n    if PYDANTIC_MAJOR_VERSION == 1:\n        return True\n    elif PYDANTIC_MAJOR_VERSION == 2:\n        from pydantic.v1 import BaseModel as BaseModelV1\n\n        if issubclass(cls, BaseModelV1):\n            return True\n    return False\n\n\ndef is_pydantic_v2_subclass(cls: type) -> bool:\n    \"\"\"Check if the installed Pydantic version is 1.x-like.\"\"\"\n    from pydantic import BaseModel\n\n    return PYDANTIC_MAJOR_VERSION == 2 and issubclass(cls, BaseModel)\n\n\ndef is_basemodel_subclass(cls: type) -> bool:\n    \"\"\"Check if the given class is a subclass of Pydantic BaseModel.\n\n    Check if the given class is a subclass of any of the following:\n\n    * pydantic.BaseModel in Pydantic 1.x\n    * pydantic.BaseModel in Pydantic 2.x\n    * pydantic.v1.BaseModel in Pydantic 2.x\n    \"\"\"\n    # Before we can use issubclass on the cls we need to check if it is a class\n    if not inspect.isclass(cls) or isinstance(cls, GenericAlias):\n        return False\n\n    if PYDANTIC_MAJOR_VERSION == 1:\n        from pydantic import BaseModel as BaseModelV1Proper\n\n        if issubclass(cls, BaseModelV1Proper):\n            return True\n    elif PYDANTIC_MAJOR_VERSION == 2:\n        from pydantic import BaseModel as BaseModelV2\n        from pydantic.v1 import BaseModel as BaseModelV1\n\n        if issubclass(cls, BaseModelV2):\n            return True\n\n        if issubclass(cls, BaseModelV1):\n            return True\n    else:\n        msg = f\"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}\"\n        raise ValueError(msg)\n    return False\n\n\ndef is_basemodel_instance(obj: Any) -> bool:\n    \"\"\"Check if the given class is an instance of Pydantic BaseModel.\n\n    Check if the given class is an instance of any of the following:\n\n    * pydantic.BaseModel in Pydantic 1.x\n    * pydantic.BaseModel in Pydantic 2.x\n    * pydantic.v1.BaseModel in Pydantic 2.x\n    \"\"\"\n    if PYDANTIC_MAJOR_VERSION == 1:\n        from pydantic import BaseModel as BaseModelV1Proper\n\n        if isinstance(obj, BaseModelV1Proper):\n            return True\n    elif PYDANTIC_MAJOR_VERSION == 2:\n        from pydantic import BaseModel as BaseModelV2\n        from pydantic.v1 import BaseModel as BaseModelV1\n\n        if isinstance(obj, BaseModelV2):\n            return True\n\n        if isinstance(obj, BaseModelV1):\n            return True\n    else:\n        msg = f\"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}\"\n        raise ValueError(msg)\n    return False\n\n\n# How to type hint this?\ndef pre_init(func: Callable) -> Any:\n    \"\"\"Decorator to run a function before model initialization.\n\n    Args:\n        func (Callable): The function to run before model initialization.\n\n    Returns:\n        Any: The decorated function.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(action=\"ignore\", category=PydanticDeprecationWarning)\n\n        @root_validator(pre=True)\n        @wraps(func)\n        def wrapper(cls: type[BaseModel], values: dict[str, Any]) -> dict[str, Any]:\n            \"\"\"Decorator to run a function before model initialization.\n\n            Args:\n                cls (Type[BaseModel]): The model class.\n                values (Dict[str, Any]): The values to initialize the model with.\n\n            Returns:\n                Dict[str, Any]: The values to initialize the model with.\n            \"\"\"\n            # Insert default values\n            fields = cls.model_fields\n            for name, field_info in fields.items():\n                # Check if allow_population_by_field_name is enabled\n                # If yes, then set the field name to the alias\n                if (\n                    hasattr(cls, \"Config\")\n                    and hasattr(cls.Config, \"allow_population_by_field_name\")\n                    and cls.Config.allow_population_by_field_name\n                    and field_info.alias in values\n                ):\n                    values[name] = values.pop(field_info.alias)\n                if (\n                    hasattr(cls, \"model_config\")\n                    and cls.model_config.get(\"populate_by_name\")\n                    and field_info.alias in values\n                ):\n                    values[name] = values.pop(field_info.alias)\n\n                if (\n                    name not in values or values[name] is None\n                ) and not field_info.is_required():\n                    if field_info.default_factory is not None:\n                        values[name] = field_info.default_factory()  # type: ignore\n                    else:\n                        values[name] = field_info.default\n\n            # Call the decorated function\n            return func(cls, values)\n\n    return wrapper\n\n\nclass _IgnoreUnserializable(GenerateJsonSchema):\n    \"\"\"A JSON schema generator that ignores unknown types.\n\n    https://docs.pydantic.dev/latest/concepts/json_schema/#customizing-the-json-schema-generation-process\n    \"\"\"\n\n    def handle_invalid_for_json_schema(\n        self, schema: core_schema.CoreSchema, error_info: str\n    ) -> JsonSchemaValue:\n        return {}\n\n\ndef _create_subset_model_v1(\n    name: str,\n    model: type[BaseModel],\n    field_names: list,\n    *,\n    descriptions: Optional[dict] = None,\n    fn_description: Optional[str] = None,\n) -> type[BaseModel]:\n    \"\"\"Create a pydantic model with only a subset of model's fields.\"\"\"\n    if PYDANTIC_MAJOR_VERSION == 1:\n        from pydantic import create_model\n    elif PYDANTIC_MAJOR_VERSION == 2:\n        from pydantic.v1 import create_model  # type: ignore\n    else:\n        msg = f\"Unsupported pydantic version: {PYDANTIC_MAJOR_VERSION}\"\n        raise NotImplementedError(msg)\n\n    fields = {}\n\n    for field_name in field_names:\n        # Using pydantic v1 so can access __fields__ as a dict.\n        field = model.__fields__[field_name]  # type: ignore\n        t = (\n            # this isn't perfect but should work for most functions\n            field.outer_type_\n            if field.required and not field.allow_none\n            else Optional[field.outer_type_]\n        )\n        if descriptions and field_name in descriptions:\n            field.field_info.description = descriptions[field_name]\n        fields[field_name] = (t, field.field_info)\n\n    rtn = create_model(name, **fields)  # type: ignore\n    rtn.__doc__ = textwrap.dedent(fn_description or model.__doc__ or \"\")\n    return rtn\n\n\ndef _create_subset_model_v2(\n    name: str,\n    model: type[pydantic.BaseModel],\n    field_names: list[str],\n    *,\n    descriptions: Optional[dict] = None,\n    fn_description: Optional[str] = None,\n) -> type[pydantic.BaseModel]:\n    \"\"\"Create a pydantic model with a subset of the model fields.\"\"\"\n    from pydantic import create_model\n    from pydantic.fields import FieldInfo\n\n    descriptions_ = descriptions or {}\n    fields = {}\n    for field_name in field_names:\n        field = model.model_fields[field_name]  # type: ignore\n        description = descriptions_.get(field_name, field.description)\n        field_info = FieldInfo(description=description, default=field.default)\n        if field.metadata:\n            field_info.metadata = field.metadata\n        fields[field_name] = (field.annotation, field_info)\n\n    rtn = create_model(  # type: ignore\n        name, **fields, __config__=ConfigDict(arbitrary_types_allowed=True)\n    )\n\n    # TODO(0.3): Determine if there is a more \"pydantic\" way to preserve annotations.\n    # This is done to preserve __annotations__ when working with pydantic 2.x\n    # and using the Annotated type with TypedDict.\n    # Comment out the following line, to trigger the relevant test case.\n    selected_annotations = [\n        (name, annotation)\n        for name, annotation in model.__annotations__.items()\n        if name in field_names\n    ]\n\n    rtn.__annotations__ = dict(selected_annotations)\n    rtn.__doc__ = textwrap.dedent(fn_description or model.__doc__ or \"\")\n    return rtn\n\n\n# Private functionality to create a subset model that's compatible across\n# different versions of pydantic.\n# Handles pydantic versions 1.x and 2.x. including v1 of pydantic in 2.x.\n# However, can't find a way to type hint this.\ndef _create_subset_model(\n    name: str,\n    model: TypeBaseModel,\n    field_names: list[str],\n    *,\n    descriptions: Optional[dict] = None,\n    fn_description: Optional[str] = None,\n) -> type[BaseModel]:\n    \"\"\"Create subset model using the same pydantic version as the input model.\"\"\"\n    if PYDANTIC_MAJOR_VERSION == 1:\n        return _create_subset_model_v1(\n            name,\n            model,\n            field_names,\n            descriptions=descriptions,\n            fn_description=fn_description,\n        )\n    elif PYDANTIC_MAJOR_VERSION == 2:\n        from pydantic.v1 import BaseModel as BaseModelV1\n\n        if issubclass(model, BaseModelV1):\n            return _create_subset_model_v1(\n                name,\n                model,\n                field_names,\n                descriptions=descriptions,\n                fn_description=fn_description,\n            )\n        else:\n            return _create_subset_model_v2(\n                name,\n                model,\n                field_names,\n                descriptions=descriptions,\n                fn_description=fn_description,\n            )\n    else:\n        msg = f\"Unsupported pydantic version: {PYDANTIC_MAJOR_VERSION}\"\n        raise NotImplementedError(msg)\n\n\nif PYDANTIC_MAJOR_VERSION == 2:\n    from pydantic import BaseModel as BaseModelV2\n    from pydantic.fields import FieldInfo as FieldInfoV2\n    from pydantic.v1 import BaseModel as BaseModelV1\n\n    @overload\n    def get_fields(model: type[BaseModelV2]) -> dict[str, FieldInfoV2]: ...\n\n    @overload\n    def get_fields(model: BaseModelV2) -> dict[str, FieldInfoV2]: ...\n\n    @overload\n    def get_fields(model: type[BaseModelV1]) -> dict[str, FieldInfoV1]: ...\n\n    @overload\n    def get_fields(model: BaseModelV1) -> dict[str, FieldInfoV1]: ...\n\n    def get_fields(\n        model: Union[\n            BaseModelV2,\n            BaseModelV1,\n            type[BaseModelV2],\n            type[BaseModelV1],\n        ],\n    ) -> Union[dict[str, FieldInfoV2], dict[str, FieldInfoV1]]:\n        \"\"\"Get the field names of a Pydantic model.\"\"\"\n        if hasattr(model, \"model_fields\"):\n            return model.model_fields  # type: ignore\n\n        elif hasattr(model, \"__fields__\"):\n            return model.__fields__  # type: ignore\n        else:\n            msg = f\"Expected a Pydantic model. Got {type(model)}\"\n            raise TypeError(msg)\n\nelif PYDANTIC_MAJOR_VERSION == 1:\n    from pydantic import BaseModel as BaseModelV1_\n\n    def get_fields(  # type: ignore[no-redef]\n        model: Union[type[BaseModelV1_], BaseModelV1_],\n    ) -> dict[str, FieldInfoV1]:\n        \"\"\"Get the field names of a Pydantic model.\"\"\"\n        return model.__fields__  # type: ignore\n\nelse:\n    msg = f\"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}\"\n    raise ValueError(msg)\n\n_SchemaConfig = ConfigDict(\n    arbitrary_types_allowed=True, frozen=True, protected_namespaces=()\n)\n\nNO_DEFAULT = object()\n\n\ndef _create_root_model(\n    name: str,\n    type_: Any,\n    module_name: Optional[str] = None,\n    default_: object = NO_DEFAULT,\n) -> type[BaseModel]:\n    \"\"\"Create a base class.\"\"\"\n\n    def schema(\n        cls: type[BaseModel],\n        by_alias: bool = True,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n    ) -> dict[str, Any]:\n        # Complains about schema not being defined in superclass\n        schema_ = super(cls, cls).schema(  # type: ignore[misc]\n            by_alias=by_alias, ref_template=ref_template\n        )\n        schema_[\"title\"] = name\n        return schema_\n\n    def model_json_schema(\n        cls: type[BaseModel],\n        by_alias: bool = True,\n        ref_template: str = DEFAULT_REF_TEMPLATE,\n        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,\n        mode: JsonSchemaMode = \"validation\",\n    ) -> dict[str, Any]:\n        # Complains about model_json_schema not being defined in superclass\n        schema_ = super(cls, cls).model_json_schema(  # type: ignore[misc]\n            by_alias=by_alias,\n            ref_template=ref_template,\n            schema_generator=schema_generator,\n            mode=mode,\n        )\n        schema_[\"title\"] = name\n        return schema_\n\n    base_class_attributes = {\n        \"__annotations__\": {\"root\": type_},\n        \"model_config\": ConfigDict(arbitrary_types_allowed=True),\n        \"schema\": classmethod(schema),\n        \"model_json_schema\": classmethod(model_json_schema),\n        \"__module__\": module_name or \"langchain_core.runnables.utils\",\n    }\n\n    if default_ is not NO_DEFAULT:\n        base_class_attributes[\"root\"] = default_\n    with warnings.catch_warnings():\n        try:\n            if (\n                isinstance(type_, type)\n                and not isinstance(type_, GenericAlias)\n                and issubclass(type_, BaseModelV1)\n            ):\n                warnings.filterwarnings(\n                    action=\"ignore\", category=PydanticDeprecationWarning\n                )\n        except TypeError:\n            pass\n        custom_root_type = type(name, (RootModel,), base_class_attributes)\n    return cast(type[BaseModel], custom_root_type)\n\n\n@lru_cache(maxsize=256)\ndef _create_root_model_cached(\n    model_name: str,\n    type_: Any,\n    *,\n    module_name: Optional[str] = None,\n    default_: object = NO_DEFAULT,\n) -> type[BaseModel]:\n    return _create_root_model(\n        model_name, type_, default_=default_, module_name=module_name\n    )\n\n\n@lru_cache(maxsize=256)\ndef _create_model_cached(\n    __model_name: str,\n    **field_definitions: Any,\n) -> type[BaseModel]:\n    return _create_model_base(\n        __model_name,\n        __config__=_SchemaConfig,\n        **_remap_field_definitions(field_definitions),\n    )\n\n\ndef create_model(\n    __model_name: str,\n    __module_name: Optional[str] = None,\n    **field_definitions: Any,\n) -> type[BaseModel]:\n    \"\"\"Create a pydantic model with the given field definitions.\n\n    Please use create_model_v2 instead of this function.\n\n    Args:\n        __model_name: The name of the model.\n        __module_name: The name of the module where the model is defined.\n            This is used by Pydantic to resolve any forward references.\n        **field_definitions: The field definitions for the model.\n\n    Returns:\n        Type[BaseModel]: The created model.\n    \"\"\"\n    kwargs = {}\n    if \"__root__\" in field_definitions:\n        kwargs[\"root\"] = field_definitions.pop(\"__root__\")\n\n    return create_model_v2(\n        __model_name,\n        module_name=__module_name,\n        field_definitions=field_definitions,\n        **kwargs,\n    )\n\n\n# Reserved names should capture all the `public` names / methods that are\n# used by BaseModel internally. This will keep the reserved names up-to-date.\n# For reference, the reserved names are:\n# \"construct\", \"copy\", \"dict\", \"from_orm\", \"json\", \"parse_file\", \"parse_obj\",\n# \"parse_raw\", \"schema\", \"schema_json\", \"update_forward_refs\", \"validate\",\n# \"model_computed_fields\", \"model_config\", \"model_construct\", \"model_copy\",\n# \"model_dump\", \"model_dump_json\", \"model_extra\", \"model_fields\",\n# \"model_fields_set\", \"model_json_schema\", \"model_parametrized_name\",\n# \"model_post_init\", \"model_rebuild\", \"model_validate\", \"model_validate_json\",\n# \"model_validate_strings\"\n_RESERVED_NAMES = {key for key in dir(BaseModel) if not key.startswith(\"_\")}\n\n\ndef _remap_field_definitions(field_definitions: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"This remaps fields to avoid colliding with internal pydantic fields.\"\"\"\n    from pydantic import Field\n    from pydantic.fields import FieldInfo\n\n    remapped = {}\n    for key, value in field_definitions.items():\n        if key.startswith(\"_\") or key in _RESERVED_NAMES:\n            # Let's add a prefix to avoid colliding with internal pydantic fields\n            if isinstance(value, FieldInfo):\n                msg = (\n                    f\"Remapping for fields starting with '_' or fields with a name \"\n                    f\"matching a reserved name {_RESERVED_NAMES} is not supported if \"\n                    f\" the field is a pydantic Field instance. Got {key}.\"\n                )\n                raise NotImplementedError(msg)\n            type_, default_ = value\n            remapped[f\"private_{key}\"] = (\n                type_,\n                Field(\n                    default=default_,\n                    alias=key,\n                    serialization_alias=key,\n                    title=key.lstrip(\"_\").replace(\"_\", \" \").title(),\n                ),\n            )\n        else:\n            remapped[key] = value\n    return remapped\n\n\ndef create_model_v2(\n    model_name: str,\n    *,\n    module_name: Optional[str] = None,\n    field_definitions: Optional[dict[str, Any]] = None,\n    root: Optional[Any] = None,\n) -> type[BaseModel]:\n    \"\"\"Create a pydantic model with the given field definitions.\n\n    Attention:\n        Please do not use outside of langchain packages. This API\n        is subject to change at any time.\n\n    Args:\n        model_name: The name of the model.\n        module_name: The name of the module where the model is defined.\n            This is used by Pydantic to resolve any forward references.\n        field_definitions: The field definitions for the model.\n        root: Type for a root model (RootModel)\n\n    Returns:\n        Type[BaseModel]: The created model.\n    \"\"\"\n    field_definitions = cast(dict[str, Any], field_definitions or {})  # type: ignore[no-redef]\n\n    if root:\n        if field_definitions:\n            msg = (\n                \"When specifying __root__ no other \"\n                f\"fields should be provided. Got {field_definitions}\"\n            )\n            raise NotImplementedError(msg)\n\n        if isinstance(root, tuple):\n            kwargs = {\"type_\": root[0], \"default_\": root[1]}\n        else:\n            kwargs = {\"type_\": root}\n\n        try:\n            named_root_model = _create_root_model_cached(\n                model_name, module_name=module_name, **kwargs\n            )\n        except TypeError:\n            # something in the arguments into _create_root_model_cached is not hashable\n            named_root_model = _create_root_model(\n                model_name,\n                module_name=module_name,\n                **kwargs,\n            )\n        return named_root_model\n\n    # No root, just field definitions\n    names = set(field_definitions.keys())\n\n    capture_warnings = False\n\n    for name in names:\n        # Also if any non-reserved name is used (e.g., model_id or model_name)\n        if name.startswith(\"model\"):\n            capture_warnings = True\n\n    with warnings.catch_warnings() if capture_warnings else nullcontext():  # type: ignore[attr-defined]\n        if capture_warnings:\n            warnings.filterwarnings(action=\"ignore\")\n        try:\n            return _create_model_cached(model_name, **field_definitions)\n        except TypeError:\n            # something in field definitions is not hashable\n            return _create_model_base(\n                model_name,\n                __config__=_SchemaConfig,\n                **_remap_field_definitions(field_definitions),\n            )\n",
        "patch": "@@ -9,6 +9,7 @@\n from functools import lru_cache, wraps\n from types import GenericAlias\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     Callable,\n     Optional,\n@@ -29,13 +30,16 @@\n from pydantic import (\n     create_model as _create_model_base,\n )\n+from pydantic.fields import FieldInfo as FieldInfoV2\n from pydantic.json_schema import (\n     DEFAULT_REF_TEMPLATE,\n     GenerateJsonSchema,\n     JsonSchemaMode,\n     JsonSchemaValue,\n )\n-from pydantic_core import core_schema\n+\n+if TYPE_CHECKING:\n+    from pydantic_core import core_schema\n \n \n def get_pydantic_major_version() -> int:\n@@ -71,8 +75,8 @@ def _get_pydantic_minor_version() -> int:\n     from pydantic.v1.fields import FieldInfo as FieldInfoV1  # type: ignore[assignment]\n \n     # Union type needs to be last assignment to PydanticBaseModel to make mypy happy.\n-    PydanticBaseModel = Union[BaseModel, pydantic.BaseModel]  # type: ignore\n-    TypeBaseModel = Union[type[BaseModel], type[pydantic.BaseModel]]  # type: ignore\n+    PydanticBaseModel = Union[BaseModel, pydantic.BaseModel]  # type: ignore[assignment,misc]\n+    TypeBaseModel = Union[type[BaseModel], type[pydantic.BaseModel]]  # type: ignore[misc]\n else:\n     msg = f\"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}\"\n     raise ValueError(msg)\n@@ -357,7 +361,6 @@ def _create_subset_model(\n \n if PYDANTIC_MAJOR_VERSION == 2:\n     from pydantic import BaseModel as BaseModelV2\n-    from pydantic.fields import FieldInfo as FieldInfoV2\n     from pydantic.v1 import BaseModel as BaseModelV1\n \n     @overload"
      },
      {
        "filename": "libs/core/langchain_core/vectorstores/base.py",
        "content_before": "\"\"\"**Vector store** stores embedded data and performs vector search.\n\nOne of the most common ways to store and search over unstructured data is to\nembed it and store the resulting embedding vectors, and then query the store\nand retrieve the data that are 'most similar' to the embedded query.\n\n**Class hierarchy:**\n\n.. code-block::\n\n    VectorStore --> <name>  # Examples: Annoy, FAISS, Milvus\n\n    BaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever\n\n**Main helpers:**\n\n.. code-block::\n\n    Embeddings, Document\n\"\"\"  # noqa: E501\n\nfrom __future__ import annotations\n\nimport logging\nimport math\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Collection, Iterable, Iterator, Sequence\nfrom itertools import cycle\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Optional,\n    TypeVar,\n)\n\nfrom pydantic import ConfigDict, Field, model_validator\n\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.retrievers import BaseRetriever, LangSmithRetrieverParams\nfrom langchain_core.runnables.config import run_in_executor\n\nif TYPE_CHECKING:\n    from langchain_core.callbacks.manager import (\n        AsyncCallbackManagerForRetrieverRun,\n        CallbackManagerForRetrieverRun,\n    )\n    from langchain_core.documents import Document\n\nlogger = logging.getLogger(__name__)\n\nVST = TypeVar(\"VST\", bound=\"VectorStore\")\n\n\nclass VectorStore(ABC):\n    \"\"\"Interface for vector store.\"\"\"\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[list[dict]] = None,\n        *,\n        ids: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> list[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n\n        Args:\n            texts: Iterable of strings to add to the vectorstore.\n            metadatas: Optional list of metadatas associated with the texts.\n            ids: Optional list of IDs associated with the texts.\n            **kwargs: vectorstore specific parameters.\n                One of the kwargs should be `ids` which is a list of ids\n                associated with the texts.\n\n        Returns:\n            List of ids from adding the texts into the vectorstore.\n\n        Raises:\n            ValueError: If the number of metadatas does not match the number of texts.\n            ValueError: If the number of ids does not match the number of texts.\n        \"\"\"\n        if type(self).add_documents != VectorStore.add_documents:\n            # Import document in local scope to avoid circular imports\n            from langchain_core.documents import Document\n\n            # This condition is triggered if the subclass has provided\n            # an implementation of the upsert method.\n            # The existing add_texts\n            texts_: Sequence[str] = (\n                texts if isinstance(texts, (list, tuple)) else list(texts)\n            )\n            if metadatas and len(metadatas) != len(texts_):\n                msg = (\n                    \"The number of metadatas must match the number of texts.\"\n                    f\"Got {len(metadatas)} metadatas and {len(texts_)} texts.\"\n                )\n                raise ValueError(msg)\n            metadatas_ = iter(metadatas) if metadatas else cycle([{}])\n            ids_: Iterator[Optional[str]] = iter(ids) if ids else cycle([None])\n            docs = [\n                Document(id=id_, page_content=text, metadata=metadata_)\n                for text, metadata_, id_ in zip(texts, metadatas_, ids_)\n            ]\n            if ids is not None:\n                # For backward compatibility\n                kwargs[\"ids\"] = ids\n\n            return self.add_documents(docs, **kwargs)\n        msg = f\"`add_texts` has not been implemented for {self.__class__.__name__} \"\n        raise NotImplementedError(msg)\n\n    @property\n    def embeddings(self) -> Optional[Embeddings]:\n        \"\"\"Access the query embedding object if available.\"\"\"\n        logger.debug(\n            f\"The embeddings property has not been \"\n            f\"implemented for {self.__class__.__name__}\"\n        )\n        return None\n\n    def delete(self, ids: Optional[list[str]] = None, **kwargs: Any) -> Optional[bool]:\n        \"\"\"Delete by vector ID or other criteria.\n\n        Args:\n            ids: List of ids to delete. If None, delete all. Default is None.\n            **kwargs: Other keyword arguments that subclasses might use.\n\n        Returns:\n            Optional[bool]: True if deletion is successful,\n            False otherwise, None if not implemented.\n        \"\"\"\n        msg = \"delete method must be implemented by subclass.\"\n        raise NotImplementedError(msg)\n\n    def get_by_ids(self, ids: Sequence[str], /) -> list[Document]:\n        \"\"\"Get documents by their IDs.\n\n        The returned documents are expected to have the ID field set to the ID of the\n        document in the vector store.\n\n        Fewer documents may be returned than requested if some IDs are not found or\n        if there are duplicated IDs.\n\n        Users should not assume that the order of the returned documents matches\n        the order of the input IDs. Instead, users should rely on the ID field of the\n        returned documents.\n\n        This method should **NOT** raise exceptions if no documents are found for\n        some IDs.\n\n        Args:\n            ids: List of ids to retrieve.\n\n        Returns:\n            List of Documents.\n\n        .. versionadded:: 0.2.11\n        \"\"\"\n        msg = f\"{self.__class__.__name__} does not yet support get_by_ids.\"\n        raise NotImplementedError(msg)\n\n    # Implementations should override this method to provide an async native version.\n    async def aget_by_ids(self, ids: Sequence[str], /) -> list[Document]:\n        \"\"\"Async get documents by their IDs.\n\n        The returned documents are expected to have the ID field set to the ID of the\n        document in the vector store.\n\n        Fewer documents may be returned than requested if some IDs are not found or\n        if there are duplicated IDs.\n\n        Users should not assume that the order of the returned documents matches\n        the order of the input IDs. Instead, users should rely on the ID field of the\n        returned documents.\n\n        This method should **NOT** raise exceptions if no documents are found for\n        some IDs.\n\n        Args:\n            ids: List of ids to retrieve.\n\n        Returns:\n            List of Documents.\n\n        .. versionadded:: 0.2.11\n        \"\"\"\n        return await run_in_executor(None, self.get_by_ids, ids)\n\n    async def adelete(\n        self, ids: Optional[list[str]] = None, **kwargs: Any\n    ) -> Optional[bool]:\n        \"\"\"Async delete by vector ID or other criteria.\n\n        Args:\n            ids: List of ids to delete. If None, delete all. Default is None.\n            **kwargs: Other keyword arguments that subclasses might use.\n\n        Returns:\n            Optional[bool]: True if deletion is successful,\n            False otherwise, None if not implemented.\n        \"\"\"\n        return await run_in_executor(None, self.delete, ids, **kwargs)\n\n    async def aadd_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[list[dict]] = None,\n        *,\n        ids: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> list[str]:\n        \"\"\"Async run more texts through the embeddings and add to the vectorstore.\n\n        Args:\n            texts: Iterable of strings to add to the vectorstore.\n            metadatas: Optional list of metadatas associated with the texts.\n                Default is None.\n            ids: Optional list\n            **kwargs: vectorstore specific parameters.\n\n        Returns:\n            List of ids from adding the texts into the vectorstore.\n\n        Raises:\n            ValueError: If the number of metadatas does not match the number of texts.\n            ValueError: If the number of ids does not match the number of texts.\n        \"\"\"\n        if ids is not None:\n            # For backward compatibility\n            kwargs[\"ids\"] = ids\n        if type(self).aadd_documents != VectorStore.aadd_documents:\n            # Import document in local scope to avoid circular imports\n            from langchain_core.documents import Document\n\n            # This condition is triggered if the subclass has provided\n            # an implementation of the upsert method.\n            # The existing add_texts\n            texts_: Sequence[str] = (\n                texts if isinstance(texts, (list, tuple)) else list(texts)\n            )\n            if metadatas and len(metadatas) != len(texts_):\n                msg = (\n                    \"The number of metadatas must match the number of texts.\"\n                    f\"Got {len(metadatas)} metadatas and {len(texts_)} texts.\"\n                )\n                raise ValueError(msg)\n            metadatas_ = iter(metadatas) if metadatas else cycle([{}])\n            ids_: Iterator[Optional[str]] = iter(ids) if ids else cycle([None])\n\n            docs = [\n                Document(id=id_, page_content=text, metadata=metadata_)\n                for text, metadata_, id_ in zip(texts, metadatas_, ids_)\n            ]\n            return await self.aadd_documents(docs, **kwargs)\n        return await run_in_executor(None, self.add_texts, texts, metadatas, **kwargs)\n\n    def add_documents(self, documents: list[Document], **kwargs: Any) -> list[str]:\n        \"\"\"Add or update documents in the vectorstore.\n\n        Args:\n            documents: Documents to add to the vectorstore.\n            kwargs: Additional keyword arguments.\n                if kwargs contains ids and documents contain ids,\n                the ids in the kwargs will receive precedence.\n\n        Returns:\n            List of IDs of the added texts.\n\n        Raises:\n            ValueError: If the number of ids does not match the number of documents.\n        \"\"\"\n        if type(self).add_texts != VectorStore.add_texts:\n            if \"ids\" not in kwargs:\n                ids = [doc.id for doc in documents]\n\n                # If there's at least one valid ID, we'll assume that IDs\n                # should be used.\n                if any(ids):\n                    kwargs[\"ids\"] = ids\n\n            texts = [doc.page_content for doc in documents]\n            metadatas = [doc.metadata for doc in documents]\n            return self.add_texts(texts, metadatas, **kwargs)\n        msg = (\n            f\"`add_documents` and `add_texts` has not been implemented \"\n            f\"for {self.__class__.__name__} \"\n        )\n        raise NotImplementedError(msg)\n\n    async def aadd_documents(\n        self, documents: list[Document], **kwargs: Any\n    ) -> list[str]:\n        \"\"\"Async run more documents through the embeddings and add to\n        the vectorstore.\n\n        Args:\n            documents: Documents to add to the vectorstore.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            List of IDs of the added texts.\n\n        Raises:\n            ValueError: If the number of IDs does not match the number of documents.\n        \"\"\"\n        # If the async method has been overridden, we'll use that.\n        if type(self).aadd_texts != VectorStore.aadd_texts:\n            if \"ids\" not in kwargs:\n                ids = [doc.id for doc in documents]\n\n                # If there's at least one valid ID, we'll assume that IDs\n                # should be used.\n                if any(ids):\n                    kwargs[\"ids\"] = ids\n\n            texts = [doc.page_content for doc in documents]\n            metadatas = [doc.metadata for doc in documents]\n            return await self.aadd_texts(texts, metadatas, **kwargs)\n\n        return await run_in_executor(None, self.add_documents, documents, **kwargs)\n\n    def search(self, query: str, search_type: str, **kwargs: Any) -> list[Document]:\n        \"\"\"Return docs most similar to query using a specified search type.\n\n        Args:\n            query: Input text\n            search_type: Type of search to perform. Can be \"similarity\",\n                \"mmr\", or \"similarity_score_threshold\".\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents most similar to the query.\n\n        Raises:\n            ValueError: If search_type is not one of \"similarity\",\n                \"mmr\", or \"similarity_score_threshold\".\n        \"\"\"\n        if search_type == \"similarity\":\n            return self.similarity_search(query, **kwargs)\n        elif search_type == \"similarity_score_threshold\":\n            docs_and_similarities = self.similarity_search_with_relevance_scores(\n                query, **kwargs\n            )\n            return [doc for doc, _ in docs_and_similarities]\n        elif search_type == \"mmr\":\n            return self.max_marginal_relevance_search(query, **kwargs)\n        else:\n            msg = (\n                f\"search_type of {search_type} not allowed. Expected \"\n                \"search_type to be 'similarity', 'similarity_score_threshold'\"\n                \" or 'mmr'.\"\n            )\n            raise ValueError(msg)\n\n    async def asearch(\n        self, query: str, search_type: str, **kwargs: Any\n    ) -> list[Document]:\n        \"\"\"Async return docs most similar to query using a specified search type.\n\n        Args:\n            query: Input text.\n            search_type: Type of search to perform. Can be \"similarity\",\n                \"mmr\", or \"similarity_score_threshold\".\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents most similar to the query.\n\n        Raises:\n            ValueError: If search_type is not one of \"similarity\",\n                \"mmr\", or \"similarity_score_threshold\".\n        \"\"\"\n        if search_type == \"similarity\":\n            return await self.asimilarity_search(query, **kwargs)\n        elif search_type == \"similarity_score_threshold\":\n            docs_and_similarities = await self.asimilarity_search_with_relevance_scores(\n                query, **kwargs\n            )\n            return [doc for doc, _ in docs_and_similarities]\n        elif search_type == \"mmr\":\n            return await self.amax_marginal_relevance_search(query, **kwargs)\n        else:\n            msg = (\n                f\"search_type of {search_type} not allowed. Expected \"\n                \"search_type to be 'similarity', 'similarity_score_threshold' or 'mmr'.\"\n            )\n            raise ValueError(msg)\n\n    @abstractmethod\n    def similarity_search(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        \"\"\"Return docs most similar to query.\n\n        Args:\n            query: Input text.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents most similar to the query.\n        \"\"\"\n\n    @staticmethod\n    def _euclidean_relevance_score_fn(distance: float) -> float:\n        \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n        # The 'correct' relevance function\n        # may differ depending on a few things, including:\n        # - the distance / similarity metric used by the VectorStore\n        # - the scale of your embeddings (OpenAI's are unit normed. Many\n        #  others are not!)\n        # - embedding dimensionality\n        # - etc.\n        # This function converts the Euclidean norm of normalized embeddings\n        # (0 is most similar, sqrt(2) most dissimilar)\n        # to a similarity function (0 to 1)\n        return 1.0 - distance / math.sqrt(2)\n\n    @staticmethod\n    def _cosine_relevance_score_fn(distance: float) -> float:\n        \"\"\"Normalize the distance to a score on a scale [0, 1].\"\"\"\n        return 1.0 - distance\n\n    @staticmethod\n    def _max_inner_product_relevance_score_fn(distance: float) -> float:\n        \"\"\"Normalize the distance to a score on a scale [0, 1].\"\"\"\n        if distance > 0:\n            return 1.0 - distance\n\n        return -1.0 * distance\n\n    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n        \"\"\"The 'correct' relevance function\n        may differ depending on a few things, including:\n        - the distance / similarity metric used by the VectorStore\n        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)\n        - embedding dimensionality\n        - etc.\n\n        Vectorstores should define their own selection-based method of relevance.\n        \"\"\"\n        raise NotImplementedError\n\n    def similarity_search_with_score(\n        self, *args: Any, **kwargs: Any\n    ) -> list[tuple[Document, float]]:\n        \"\"\"Run similarity search with distance.\n\n        Args:\n            *args: Arguments to pass to the search method.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Tuples of (doc, similarity_score).\n        \"\"\"\n        raise NotImplementedError\n\n    async def asimilarity_search_with_score(\n        self, *args: Any, **kwargs: Any\n    ) -> list[tuple[Document, float]]:\n        \"\"\"Async run similarity search with distance.\n\n        Args:\n            *args: Arguments to pass to the search method.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Tuples of (doc, similarity_score).\n        \"\"\"\n        # This is a temporary workaround to make the similarity search\n        # asynchronous. The proper solution is to make the similarity search\n        # asynchronous in the vector store implementations.\n        return await run_in_executor(\n            None, self.similarity_search_with_score, *args, **kwargs\n        )\n\n    def _similarity_search_with_relevance_scores(\n        self,\n        query: str,\n        k: int = 4,\n        **kwargs: Any,\n    ) -> list[tuple[Document, float]]:\n        \"\"\"Default similarity search with relevance scores. Modify if necessary\n        in subclass.\n        Return docs and relevance scores in the range [0, 1].\n\n        0 is dissimilar, 1 is most similar.\n\n        Args:\n            query: Input text.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: kwargs to be passed to similarity search. Should include:\n                score_threshold: Optional, a floating point value between 0 to 1 to\n                    filter the resulting set of retrieved docs\n\n        Returns:\n            List of Tuples of (doc, similarity_score)\n        \"\"\"\n        relevance_score_fn = self._select_relevance_score_fn()\n        docs_and_scores = self.similarity_search_with_score(query, k, **kwargs)\n        return [(doc, relevance_score_fn(score)) for doc, score in docs_and_scores]\n\n    async def _asimilarity_search_with_relevance_scores(\n        self,\n        query: str,\n        k: int = 4,\n        **kwargs: Any,\n    ) -> list[tuple[Document, float]]:\n        \"\"\"Default similarity search with relevance scores. Modify if necessary\n        in subclass.\n        Return docs and relevance scores in the range [0, 1].\n\n        0 is dissimilar, 1 is most similar.\n\n        Args:\n            query: Input text.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: kwargs to be passed to similarity search. Should include:\n                score_threshold: Optional, a floating point value between 0 to 1 to\n                    filter the resulting set of retrieved docs\n\n        Returns:\n            List of Tuples of (doc, similarity_score)\n        \"\"\"\n        relevance_score_fn = self._select_relevance_score_fn()\n        docs_and_scores = await self.asimilarity_search_with_score(query, k, **kwargs)\n        return [(doc, relevance_score_fn(score)) for doc, score in docs_and_scores]\n\n    def similarity_search_with_relevance_scores(\n        self,\n        query: str,\n        k: int = 4,\n        **kwargs: Any,\n    ) -> list[tuple[Document, float]]:\n        \"\"\"Return docs and relevance scores in the range [0, 1].\n\n        0 is dissimilar, 1 is most similar.\n\n        Args:\n            query: Input text.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: kwargs to be passed to similarity search. Should include:\n                score_threshold: Optional, a floating point value between 0 to 1 to\n                    filter the resulting set of retrieved docs.\n\n        Returns:\n            List of Tuples of (doc, similarity_score).\n        \"\"\"\n        score_threshold = kwargs.pop(\"score_threshold\", None)\n\n        docs_and_similarities = self._similarity_search_with_relevance_scores(\n            query, k=k, **kwargs\n        )\n        if any(\n            similarity < 0.0 or similarity > 1.0\n            for _, similarity in docs_and_similarities\n        ):\n            warnings.warn(\n                \"Relevance scores must be between\"\n                f\" 0 and 1, got {docs_and_similarities}\",\n                stacklevel=2,\n            )\n\n        if score_threshold is not None:\n            docs_and_similarities = [\n                (doc, similarity)\n                for doc, similarity in docs_and_similarities\n                if similarity >= score_threshold\n            ]\n            if len(docs_and_similarities) == 0:\n                logger.warning(\n                    \"No relevant docs were retrieved using the relevance score\"\n                    f\" threshold {score_threshold}\"\n                )\n        return docs_and_similarities\n\n    async def asimilarity_search_with_relevance_scores(\n        self,\n        query: str,\n        k: int = 4,\n        **kwargs: Any,\n    ) -> list[tuple[Document, float]]:\n        \"\"\"Async return docs and relevance scores in the range [0, 1].\n\n        0 is dissimilar, 1 is most similar.\n\n        Args:\n            query: Input text.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: kwargs to be passed to similarity search. Should include:\n                score_threshold: Optional, a floating point value between 0 to 1 to\n                    filter the resulting set of retrieved docs\n\n        Returns:\n            List of Tuples of (doc, similarity_score)\n        \"\"\"\n        score_threshold = kwargs.pop(\"score_threshold\", None)\n\n        docs_and_similarities = await self._asimilarity_search_with_relevance_scores(\n            query, k=k, **kwargs\n        )\n        if any(\n            similarity < 0.0 or similarity > 1.0\n            for _, similarity in docs_and_similarities\n        ):\n            warnings.warn(\n                \"Relevance scores must be between\"\n                f\" 0 and 1, got {docs_and_similarities}\",\n                stacklevel=2,\n            )\n\n        if score_threshold is not None:\n            docs_and_similarities = [\n                (doc, similarity)\n                for doc, similarity in docs_and_similarities\n                if similarity >= score_threshold\n            ]\n            if len(docs_and_similarities) == 0:\n                logger.warning(\n                    \"No relevant docs were retrieved using the relevance score\"\n                    f\" threshold {score_threshold}\"\n                )\n        return docs_and_similarities\n\n    async def asimilarity_search(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        \"\"\"Async return docs most similar to query.\n\n        Args:\n            query: Input text.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents most similar to the query.\n        \"\"\"\n        # This is a temporary workaround to make the similarity search\n        # asynchronous. The proper solution is to make the similarity search\n        # asynchronous in the vector store implementations.\n        return await run_in_executor(None, self.similarity_search, query, k=k, **kwargs)\n\n    def similarity_search_by_vector(\n        self, embedding: list[float], k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        \"\"\"Return docs most similar to embedding vector.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents most similar to the query vector.\n        \"\"\"\n        raise NotImplementedError\n\n    async def asimilarity_search_by_vector(\n        self, embedding: list[float], k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        \"\"\"Async return docs most similar to embedding vector.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents most similar to the query vector.\n        \"\"\"\n        # This is a temporary workaround to make the similarity search\n        # asynchronous. The proper solution is to make the similarity search\n        # asynchronous in the vector store implementations.\n        return await run_in_executor(\n            None, self.similarity_search_by_vector, embedding, k=k, **kwargs\n        )\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -> list[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n                Default is 20.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                of diversity among the results with 0 corresponding\n                to maximum diversity and 1 to minimum diversity.\n                Defaults to 0.5.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        raise NotImplementedError\n\n    async def amax_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -> list[Document]:\n        \"\"\"Async return docs selected using the maximal marginal relevance.\n\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            query: Text to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n                Default is 20.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                of diversity among the results with 0 corresponding\n                to maximum diversity and 1 to minimum diversity.\n                Defaults to 0.5.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        # This is a temporary workaround to make the similarity search\n        # asynchronous. The proper solution is to make the similarity search\n        # asynchronous in the vector store implementations.\n        return await run_in_executor(\n            None,\n            self.max_marginal_relevance_search,\n            query,\n            k=k,\n            fetch_k=fetch_k,\n            lambda_mult=lambda_mult,\n            **kwargs,\n        )\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: list[float],\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -> list[Document]:\n        \"\"\"Return docs selected using the maximal marginal relevance.\n\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n                Default is 20.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                of diversity among the results with 0 corresponding\n                to maximum diversity and 1 to minimum diversity.\n                Defaults to 0.5.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        raise NotImplementedError\n\n    async def amax_marginal_relevance_search_by_vector(\n        self,\n        embedding: list[float],\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -> list[Document]:\n        \"\"\"Async return docs selected using the maximal marginal relevance.\n\n        Maximal marginal relevance optimizes for similarity to query AND diversity\n        among selected documents.\n\n        Args:\n            embedding: Embedding to look up documents similar to.\n            k: Number of Documents to return. Defaults to 4.\n            fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n                Default is 20.\n            lambda_mult: Number between 0 and 1 that determines the degree\n                of diversity among the results with 0 corresponding\n                to maximum diversity and 1 to minimum diversity.\n                Defaults to 0.5.\n            **kwargs: Arguments to pass to the search method.\n\n        Returns:\n            List of Documents selected by maximal marginal relevance.\n        \"\"\"\n        return await run_in_executor(\n            None,\n            self.max_marginal_relevance_search_by_vector,\n            embedding,\n            k=k,\n            fetch_k=fetch_k,\n            lambda_mult=lambda_mult,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_documents(\n        cls: type[VST],\n        documents: list[Document],\n        embedding: Embeddings,\n        **kwargs: Any,\n    ) -> VST:\n        \"\"\"Return VectorStore initialized from documents and embeddings.\n\n        Args:\n            documents: List of Documents to add to the vectorstore.\n            embedding: Embedding function to use.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            VectorStore: VectorStore initialized from documents and embeddings.\n        \"\"\"\n        texts = [d.page_content for d in documents]\n        metadatas = [d.metadata for d in documents]\n\n        if \"ids\" not in kwargs:\n            ids = [doc.id for doc in documents]\n\n            # If there's at least one valid ID, we'll assume that IDs\n            # should be used.\n            if any(ids):\n                kwargs[\"ids\"] = ids\n\n        return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)\n\n    @classmethod\n    async def afrom_documents(\n        cls: type[VST],\n        documents: list[Document],\n        embedding: Embeddings,\n        **kwargs: Any,\n    ) -> VST:\n        \"\"\"Async return VectorStore initialized from documents and embeddings.\n\n        Args:\n            documents: List of Documents to add to the vectorstore.\n            embedding: Embedding function to use.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            VectorStore: VectorStore initialized from documents and embeddings.\n        \"\"\"\n        texts = [d.page_content for d in documents]\n        metadatas = [d.metadata for d in documents]\n\n        if \"ids\" not in kwargs:\n            ids = [doc.id for doc in documents]\n\n            # If there's at least one valid ID, we'll assume that IDs\n            # should be used.\n            if any(ids):\n                kwargs[\"ids\"] = ids\n\n        return await cls.afrom_texts(texts, embedding, metadatas=metadatas, **kwargs)\n\n    @classmethod\n    @abstractmethod\n    def from_texts(\n        cls: type[VST],\n        texts: list[str],\n        embedding: Embeddings,\n        metadatas: Optional[list[dict]] = None,\n        *,\n        ids: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> VST:\n        \"\"\"Return VectorStore initialized from texts and embeddings.\n\n        Args:\n            texts: Texts to add to the vectorstore.\n            embedding: Embedding function to use.\n            metadatas: Optional list of metadatas associated with the texts.\n                Default is None.\n            ids: Optional list of IDs associated with the texts.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            VectorStore: VectorStore initialized from texts and embeddings.\n        \"\"\"\n\n    @classmethod\n    async def afrom_texts(\n        cls: type[VST],\n        texts: list[str],\n        embedding: Embeddings,\n        metadatas: Optional[list[dict]] = None,\n        *,\n        ids: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> VST:\n        \"\"\"Async return VectorStore initialized from texts and embeddings.\n\n        Args:\n            texts: Texts to add to the vectorstore.\n            embedding: Embedding function to use.\n            metadatas: Optional list of metadatas associated with the texts.\n                Default is None.\n            ids: Optional list of IDs associated with the texts.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            VectorStore: VectorStore initialized from texts and embeddings.\n        \"\"\"\n        if ids is not None:\n            kwargs[\"ids\"] = ids\n        return await run_in_executor(\n            None, cls.from_texts, texts, embedding, metadatas, **kwargs\n        )\n\n    def _get_retriever_tags(self) -> list[str]:\n        \"\"\"Get tags for retriever.\"\"\"\n        tags = [self.__class__.__name__]\n        if self.embeddings:\n            tags.append(self.embeddings.__class__.__name__)\n        return tags\n\n    def as_retriever(self, **kwargs: Any) -> VectorStoreRetriever:\n        \"\"\"Return VectorStoreRetriever initialized from this VectorStore.\n\n        Args:\n            **kwargs: Keyword arguments to pass to the search function.\n                Can include:\n                search_type (Optional[str]): Defines the type of search that\n                    the Retriever should perform.\n                    Can be \"similarity\" (default), \"mmr\", or\n                    \"similarity_score_threshold\".\n                search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n                    search function. Can include things like:\n                        k: Amount of documents to return (Default: 4)\n                        score_threshold: Minimum relevance threshold\n                            for similarity_score_threshold\n                        fetch_k: Amount of documents to pass to MMR algorithm\n                            (Default: 20)\n                        lambda_mult: Diversity of results returned by MMR;\n                            1 for minimum diversity and 0 for maximum. (Default: 0.5)\n                        filter: Filter by document metadata\n\n        Returns:\n            VectorStoreRetriever: Retriever class for VectorStore.\n\n        Examples:\n\n        .. code-block:: python\n\n            # Retrieve more documents with higher diversity\n            # Useful if your dataset has many similar documents\n            docsearch.as_retriever(\n                search_type=\"mmr\",\n                search_kwargs={'k': 6, 'lambda_mult': 0.25}\n            )\n\n            # Fetch more documents for the MMR algorithm to consider\n            # But only return the top 5\n            docsearch.as_retriever(\n                search_type=\"mmr\",\n                search_kwargs={'k': 5, 'fetch_k': 50}\n            )\n\n            # Only retrieve documents that have a relevance score\n            # Above a certain threshold\n            docsearch.as_retriever(\n                search_type=\"similarity_score_threshold\",\n                search_kwargs={'score_threshold': 0.8}\n            )\n\n            # Only get the single most similar document from the dataset\n            docsearch.as_retriever(search_kwargs={'k': 1})\n\n            # Use a filter to only retrieve documents from a specific paper\n            docsearch.as_retriever(\n                search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n            )\n        \"\"\"\n        tags = kwargs.pop(\"tags\", None) or [] + self._get_retriever_tags()\n        return VectorStoreRetriever(vectorstore=self, tags=tags, **kwargs)\n\n\nclass VectorStoreRetriever(BaseRetriever):\n    \"\"\"Base Retriever class for VectorStore.\"\"\"\n\n    vectorstore: VectorStore\n    \"\"\"VectorStore to use for retrieval.\"\"\"\n    search_type: str = \"similarity\"\n    \"\"\"Type of search to perform. Defaults to \"similarity\".\"\"\"\n    search_kwargs: dict = Field(default_factory=dict)\n    \"\"\"Keyword arguments to pass to the search function.\"\"\"\n    allowed_search_types: ClassVar[Collection[str]] = (\n        \"similarity\",\n        \"similarity_score_threshold\",\n        \"mmr\",\n    )\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_search_type(cls, values: dict) -> Any:\n        \"\"\"Validate search type.\n\n        Args:\n            values: Values to validate.\n\n        Returns:\n            Values: Validated values.\n\n        Raises:\n            ValueError: If search_type is not one of the allowed search types.\n            ValueError: If score_threshold is not specified with a float value(0~1)\n        \"\"\"\n        search_type = values.get(\"search_type\", \"similarity\")\n        if search_type not in cls.allowed_search_types:\n            msg = (\n                f\"search_type of {search_type} not allowed. Valid values are: \"\n                f\"{cls.allowed_search_types}\"\n            )\n            raise ValueError(msg)\n        if search_type == \"similarity_score_threshold\":\n            score_threshold = values.get(\"search_kwargs\", {}).get(\"score_threshold\")\n            if (score_threshold is None) or (not isinstance(score_threshold, float)):\n                msg = (\n                    \"`score_threshold` is not specified with a float value(0~1) \"\n                    \"in `search_kwargs`.\"\n                )\n                raise ValueError(msg)\n        return values\n\n    def _get_ls_params(self, **kwargs: Any) -> LangSmithRetrieverParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        _kwargs = self.search_kwargs | kwargs\n\n        ls_params = super()._get_ls_params(**_kwargs)\n        ls_params[\"ls_vector_store_provider\"] = self.vectorstore.__class__.__name__\n\n        if self.vectorstore.embeddings:\n            ls_params[\"ls_embedding_provider\"] = (\n                self.vectorstore.embeddings.__class__.__name__\n            )\n        elif hasattr(self.vectorstore, \"embedding\") and isinstance(\n            self.vectorstore.embedding, Embeddings\n        ):\n            ls_params[\"ls_embedding_provider\"] = (\n                self.vectorstore.embedding.__class__.__name__\n            )\n\n        return ls_params\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun, **kwargs: Any\n    ) -> list[Document]:\n        _kwargs = self.search_kwargs | kwargs\n        if self.search_type == \"similarity\":\n            docs = self.vectorstore.similarity_search(query, **_kwargs)\n        elif self.search_type == \"similarity_score_threshold\":\n            docs_and_similarities = (\n                self.vectorstore.similarity_search_with_relevance_scores(\n                    query, **_kwargs\n                )\n            )\n            docs = [doc for doc, _ in docs_and_similarities]\n        elif self.search_type == \"mmr\":\n            docs = self.vectorstore.max_marginal_relevance_search(query, **_kwargs)\n        else:\n            msg = f\"search_type of {self.search_type} not allowed.\"\n            raise ValueError(msg)\n        return docs\n\n    async def _aget_relevant_documents(\n        self,\n        query: str,\n        *,\n        run_manager: AsyncCallbackManagerForRetrieverRun,\n        **kwargs: Any,\n    ) -> list[Document]:\n        _kwargs = self.search_kwargs | kwargs\n        if self.search_type == \"similarity\":\n            docs = await self.vectorstore.asimilarity_search(query, **_kwargs)\n        elif self.search_type == \"similarity_score_threshold\":\n            docs_and_similarities = (\n                await self.vectorstore.asimilarity_search_with_relevance_scores(\n                    query, **_kwargs\n                )\n            )\n            docs = [doc for doc, _ in docs_and_similarities]\n        elif self.search_type == \"mmr\":\n            docs = await self.vectorstore.amax_marginal_relevance_search(\n                query, **_kwargs\n            )\n        else:\n            msg = f\"search_type of {self.search_type} not allowed.\"\n            raise ValueError(msg)\n        return docs\n\n    def add_documents(self, documents: list[Document], **kwargs: Any) -> list[str]:\n        \"\"\"Add documents to the vectorstore.\n\n        Args:\n            documents: Documents to add to the vectorstore.\n            **kwargs: Other keyword arguments that subclasses might use.\n\n        Returns:\n            List of IDs of the added texts.\n        \"\"\"\n        return self.vectorstore.add_documents(documents, **kwargs)\n\n    async def aadd_documents(\n        self, documents: list[Document], **kwargs: Any\n    ) -> list[str]:\n        \"\"\"Async add documents to the vectorstore.\n\n        Args:\n            documents: Documents to add to the vectorstore.\n            **kwargs: Other keyword arguments that subclasses might use.\n\n        Returns:\n            List of IDs of the added texts.\n        \"\"\"\n        return await self.vectorstore.aadd_documents(documents, **kwargs)\n",
        "patch": "@@ -25,7 +25,6 @@\n import math\n import warnings\n from abc import ABC, abstractmethod\n-from collections.abc import Collection, Iterable, Iterator, Sequence\n from itertools import cycle\n from typing import (\n     TYPE_CHECKING,\n@@ -43,6 +42,8 @@\n from langchain_core.runnables.config import run_in_executor\n \n if TYPE_CHECKING:\n+    from collections.abc import Collection, Iterable, Iterator, Sequence\n+\n     from langchain_core.callbacks.manager import (\n         AsyncCallbackManagerForRetrieverRun,\n         CallbackManagerForRetrieverRun,"
      },
      {
        "filename": "libs/core/langchain_core/vectorstores/in_memory.py",
        "content_before": "from __future__ import annotations\n\nimport json\nimport uuid\nfrom collections.abc import Iterator, Sequence\nfrom pathlib import Path\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Optional,\n)\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.load import dumpd, load\nfrom langchain_core.vectorstores import VectorStore\nfrom langchain_core.vectorstores.utils import _cosine_similarity as cosine_similarity\nfrom langchain_core.vectorstores.utils import maximal_marginal_relevance\n\nif TYPE_CHECKING:\n    from langchain_core.indexing import UpsertResponse\n\n\nclass InMemoryVectorStore(VectorStore):\n    \"\"\"In-memory vector store implementation.\n\n    Uses a dictionary, and computes cosine similarity for search using numpy.\n\n    Setup:\n        Install ``langchain-core``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-core\n\n    Key init args \u2014 indexing params:\n        embedding_function: Embeddings\n            Embedding function to use.\n\n    Instantiate:\n        .. code-block:: python\n\n            from langchain_core.vectorstores import InMemoryVectorStore\n            from langchain_openai import OpenAIEmbeddings\n\n            vector_store = InMemoryVectorStore(OpenAIEmbeddings())\n\n    Add Documents:\n        .. code-block:: python\n\n            from langchain_core.documents import Document\n\n            document_1 = Document(id=\"1\", page_content=\"foo\", metadata={\"baz\": \"bar\"})\n            document_2 = Document(id=\"2\", page_content=\"thud\", metadata={\"bar\": \"baz\"})\n            document_3 = Document(id=\"3\", page_content=\"i will be deleted :(\")\n\n            documents = [document_1, document_2, document_3]\n            vector_store.add_documents(documents=documents)\n\n    Inspect documents:\n        .. code-block:: python\n\n            top_n = 10\n            for index, (id, doc) in enumerate(vector_store.store.items()):\n                if index < top_n:\n                    # docs have keys 'id', 'vector', 'text', 'metadata'\n                    print(f\"{id}: {doc['text']}\")\n                else:\n                    break\n\n    Delete Documents:\n        .. code-block:: python\n\n            vector_store.delete(ids=[\"3\"])\n\n    Search:\n        .. code-block:: python\n\n            results = vector_store.similarity_search(query=\"thud\",k=1)\n            for doc in results:\n                print(f\"* {doc.page_content} [{doc.metadata}]\")\n\n        .. code-block:: none\n\n            * thud [{'bar': 'baz'}]\n\n    Search with filter:\n        .. code-block:: python\n\n            def _filter_function(doc: Document) -> bool:\n                return doc.metadata.get(\"bar\") == \"baz\"\n\n            results = vector_store.similarity_search(\n                query=\"thud\", k=1, filter=_filter_function\n            )\n            for doc in results:\n                print(f\"* {doc.page_content} [{doc.metadata}]\")\n\n        .. code-block:: none\n\n            * thud [{'bar': 'baz'}]\n\n\n    Search with score:\n        .. code-block:: python\n\n            results = vector_store.similarity_search_with_score(\n                query=\"qux\", k=1\n            )\n            for doc, score in results:\n                print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")\n\n        .. code-block:: none\n\n            * [SIM=0.832268] foo [{'baz': 'bar'}]\n\n    Async:\n        .. code-block:: python\n\n            # add documents\n            # await vector_store.aadd_documents(documents=documents)\n\n            # delete documents\n            # await vector_store.adelete(ids=[\"3\"])\n\n            # search\n            # results = vector_store.asimilarity_search(query=\"thud\", k=1)\n\n            # search with score\n            results = await vector_store.asimilarity_search_with_score(query=\"qux\", k=1)\n            for doc,score in results:\n                print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")\n\n        .. code-block:: none\n\n            * [SIM=0.832268] foo [{'baz': 'bar'}]\n\n    Use as Retriever:\n        .. code-block:: python\n\n            retriever = vector_store.as_retriever(\n                search_type=\"mmr\",\n                search_kwargs={\"k\": 1, \"fetch_k\": 2, \"lambda_mult\": 0.5},\n            )\n            retriever.invoke(\"thud\")\n\n        .. code-block:: none\n\n            [Document(id='2', metadata={'bar': 'baz'}, page_content='thud')]\n\n    \"\"\"  # noqa: E501\n\n    def __init__(self, embedding: Embeddings) -> None:\n        \"\"\"Initialize with the given embedding function.\n\n        Args:\n            embedding: embedding function to use.\n        \"\"\"\n        # TODO: would be nice to change to\n        # Dict[str, Document] at some point (will be a breaking change)\n        self.store: dict[str, dict[str, Any]] = {}\n        self.embedding = embedding\n\n    @property\n    def embeddings(self) -> Embeddings:\n        return self.embedding\n\n    def delete(self, ids: Optional[Sequence[str]] = None, **kwargs: Any) -> None:\n        if ids:\n            for _id in ids:\n                self.store.pop(_id, None)\n\n    async def adelete(self, ids: Optional[Sequence[str]] = None, **kwargs: Any) -> None:\n        self.delete(ids)\n\n    def add_documents(\n        self,\n        documents: list[Document],\n        ids: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> list[str]:\n        \"\"\"Add documents to the store.\"\"\"\n        texts = [doc.page_content for doc in documents]\n        vectors = self.embedding.embed_documents(texts)\n\n        if ids and len(ids) != len(texts):\n            msg = (\n                f\"ids must be the same length as texts. \"\n                f\"Got {len(ids)} ids and {len(texts)} texts.\"\n            )\n            raise ValueError(msg)\n\n        id_iterator: Iterator[Optional[str]] = (\n            iter(ids) if ids else iter(doc.id for doc in documents)\n        )\n\n        ids_ = []\n\n        for doc, vector in zip(documents, vectors):\n            doc_id = next(id_iterator)\n            doc_id_ = doc_id or str(uuid.uuid4())\n            ids_.append(doc_id_)\n            self.store[doc_id_] = {\n                \"id\": doc_id_,\n                \"vector\": vector,\n                \"text\": doc.page_content,\n                \"metadata\": doc.metadata,\n            }\n\n        return ids_\n\n    async def aadd_documents(\n        self, documents: list[Document], ids: Optional[list[str]] = None, **kwargs: Any\n    ) -> list[str]:\n        \"\"\"Add documents to the store.\"\"\"\n        texts = [doc.page_content for doc in documents]\n        vectors = await self.embedding.aembed_documents(texts)\n\n        if ids and len(ids) != len(texts):\n            msg = (\n                f\"ids must be the same length as texts. \"\n                f\"Got {len(ids)} ids and {len(texts)} texts.\"\n            )\n            raise ValueError(msg)\n\n        id_iterator: Iterator[Optional[str]] = (\n            iter(ids) if ids else iter(doc.id for doc in documents)\n        )\n        ids_: list[str] = []\n\n        for doc, vector in zip(documents, vectors):\n            doc_id = next(id_iterator)\n            doc_id_ = doc_id or str(uuid.uuid4())\n            ids_.append(doc_id_)\n            self.store[doc_id_] = {\n                \"id\": doc_id_,\n                \"vector\": vector,\n                \"text\": doc.page_content,\n                \"metadata\": doc.metadata,\n            }\n\n        return ids_\n\n    def get_by_ids(self, ids: Sequence[str], /) -> list[Document]:\n        \"\"\"Get documents by their ids.\n\n        Args:\n            ids: The ids of the documents to get.\n\n        Returns:\n            A list of Document objects.\n        \"\"\"\n        documents = []\n\n        for doc_id in ids:\n            doc = self.store.get(doc_id)\n            if doc:\n                documents.append(\n                    Document(\n                        id=doc[\"id\"],\n                        page_content=doc[\"text\"],\n                        metadata=doc[\"metadata\"],\n                    )\n                )\n        return documents\n\n    @deprecated(\n        alternative=\"VectorStore.add_documents\",\n        message=(\n            \"This was a beta API that was added in 0.2.11. It'll be removed in 0.3.0.\"\n        ),\n        since=\"0.2.29\",\n        removal=\"1.0\",\n    )\n    def upsert(self, items: Sequence[Document], /, **kwargs: Any) -> UpsertResponse:\n        vectors = self.embedding.embed_documents([item.page_content for item in items])\n        ids = []\n        for item, vector in zip(items, vectors):\n            doc_id = item.id or str(uuid.uuid4())\n            ids.append(doc_id)\n            self.store[doc_id] = {\n                \"id\": doc_id,\n                \"vector\": vector,\n                \"text\": item.page_content,\n                \"metadata\": item.metadata,\n            }\n        return {\n            \"succeeded\": ids,\n            \"failed\": [],\n        }\n\n    @deprecated(\n        alternative=\"VectorStore.aadd_documents\",\n        message=(\n            \"This was a beta API that was added in 0.2.11. It'll be removed in 0.3.0.\"\n        ),\n        since=\"0.2.29\",\n        removal=\"1.0\",\n    )\n    async def aupsert(\n        self, items: Sequence[Document], /, **kwargs: Any\n    ) -> UpsertResponse:\n        vectors = await self.embedding.aembed_documents(\n            [item.page_content for item in items]\n        )\n        ids = []\n        for item, vector in zip(items, vectors):\n            doc_id = item.id or str(uuid.uuid4())\n            ids.append(doc_id)\n            self.store[doc_id] = {\n                \"id\": doc_id,\n                \"vector\": vector,\n                \"text\": item.page_content,\n                \"metadata\": item.metadata,\n            }\n        return {\n            \"succeeded\": ids,\n            \"failed\": [],\n        }\n\n    async def aget_by_ids(self, ids: Sequence[str], /) -> list[Document]:\n        \"\"\"Async get documents by their ids.\n\n        Args:\n            ids: The ids of the documents to get.\n\n        Returns:\n            A list of Document objects.\n        \"\"\"\n        return self.get_by_ids(ids)\n\n    def _similarity_search_with_score_by_vector(\n        self,\n        embedding: list[float],\n        k: int = 4,\n        filter: Optional[Callable[[Document], bool]] = None,\n        **kwargs: Any,\n    ) -> list[tuple[Document, float, list[float]]]:\n        # get all docs with fixed order in list\n        docs = list(self.store.values())\n\n        if filter is not None:\n            docs = [\n                doc\n                for doc in docs\n                if filter(Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"]))\n            ]\n\n        if not docs:\n            return []\n\n        similarity = cosine_similarity([embedding], [doc[\"vector\"] for doc in docs])[0]\n\n        # get the indices ordered by similarity score\n        top_k_idx = similarity.argsort()[::-1][:k]\n\n        return [\n            (\n                Document(\n                    id=doc_dict[\"id\"],\n                    page_content=doc_dict[\"text\"],\n                    metadata=doc_dict[\"metadata\"],\n                ),\n                float(similarity[idx].item()),\n                doc_dict[\"vector\"],\n            )\n            for idx in top_k_idx\n            # Assign using walrus operator to avoid multiple lookups\n            if (doc_dict := docs[idx])\n        ]\n\n    def similarity_search_with_score_by_vector(\n        self,\n        embedding: list[float],\n        k: int = 4,\n        filter: Optional[Callable[[Document], bool]] = None,\n        **kwargs: Any,\n    ) -> list[tuple[Document, float]]:\n        return [\n            (doc, similarity)\n            for doc, similarity, _ in self._similarity_search_with_score_by_vector(\n                embedding=embedding, k=k, filter=filter, **kwargs\n            )\n        ]\n\n    def similarity_search_with_score(\n        self,\n        query: str,\n        k: int = 4,\n        **kwargs: Any,\n    ) -> list[tuple[Document, float]]:\n        embedding = self.embedding.embed_query(query)\n        docs = self.similarity_search_with_score_by_vector(\n            embedding,\n            k,\n            **kwargs,\n        )\n        return docs\n\n    async def asimilarity_search_with_score(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> list[tuple[Document, float]]:\n        embedding = await self.embedding.aembed_query(query)\n        docs = self.similarity_search_with_score_by_vector(\n            embedding,\n            k,\n            **kwargs,\n        )\n        return docs\n\n    def similarity_search_by_vector(\n        self,\n        embedding: list[float],\n        k: int = 4,\n        **kwargs: Any,\n    ) -> list[Document]:\n        docs_and_scores = self.similarity_search_with_score_by_vector(\n            embedding,\n            k,\n            **kwargs,\n        )\n        return [doc for doc, _ in docs_and_scores]\n\n    async def asimilarity_search_by_vector(\n        self, embedding: list[float], k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        return self.similarity_search_by_vector(embedding, k, **kwargs)\n\n    def similarity_search(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        return [doc for doc, _ in self.similarity_search_with_score(query, k, **kwargs)]\n\n    async def asimilarity_search(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        return [\n            doc\n            for doc, _ in await self.asimilarity_search_with_score(query, k, **kwargs)\n        ]\n\n    def max_marginal_relevance_search_by_vector(\n        self,\n        embedding: list[float],\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -> list[Document]:\n        prefetch_hits = self._similarity_search_with_score_by_vector(\n            embedding=embedding,\n            k=fetch_k,\n            **kwargs,\n        )\n\n        try:\n            import numpy as np\n        except ImportError as e:\n            msg = (\n                \"numpy must be installed to use max_marginal_relevance_search \"\n                \"pip install numpy\"\n            )\n            raise ImportError(msg) from e\n\n        mmr_chosen_indices = maximal_marginal_relevance(\n            np.array(embedding, dtype=np.float32),\n            [vector for _, _, vector in prefetch_hits],\n            k=k,\n            lambda_mult=lambda_mult,\n        )\n        return [prefetch_hits[idx][0] for idx in mmr_chosen_indices]\n\n    def max_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -> list[Document]:\n        embedding_vector = self.embedding.embed_query(query)\n        return self.max_marginal_relevance_search_by_vector(\n            embedding_vector,\n            k,\n            fetch_k,\n            lambda_mult=lambda_mult,\n            **kwargs,\n        )\n\n    async def amax_marginal_relevance_search(\n        self,\n        query: str,\n        k: int = 4,\n        fetch_k: int = 20,\n        lambda_mult: float = 0.5,\n        **kwargs: Any,\n    ) -> list[Document]:\n        embedding_vector = await self.embedding.aembed_query(query)\n        return self.max_marginal_relevance_search_by_vector(\n            embedding_vector,\n            k,\n            fetch_k,\n            lambda_mult=lambda_mult,\n            **kwargs,\n        )\n\n    @classmethod\n    def from_texts(\n        cls,\n        texts: list[str],\n        embedding: Embeddings,\n        metadatas: Optional[list[dict]] = None,\n        **kwargs: Any,\n    ) -> InMemoryVectorStore:\n        store = cls(\n            embedding=embedding,\n        )\n        store.add_texts(texts=texts, metadatas=metadatas, **kwargs)\n        return store\n\n    @classmethod\n    async def afrom_texts(\n        cls,\n        texts: list[str],\n        embedding: Embeddings,\n        metadatas: Optional[list[dict]] = None,\n        **kwargs: Any,\n    ) -> InMemoryVectorStore:\n        store = cls(\n            embedding=embedding,\n        )\n        await store.aadd_texts(texts=texts, metadatas=metadatas, **kwargs)\n        return store\n\n    @classmethod\n    def load(\n        cls, path: str, embedding: Embeddings, **kwargs: Any\n    ) -> InMemoryVectorStore:\n        \"\"\"Load a vector store from a file.\n\n        Args:\n            path: The path to load the vector store from.\n            embedding: The embedding to use.\n            kwargs: Additional arguments to pass to the constructor.\n\n        Returns:\n            A VectorStore object.\n        \"\"\"\n        _path: Path = Path(path)\n        with _path.open(\"r\") as f:\n            store = load(json.load(f))\n        vectorstore = cls(embedding=embedding, **kwargs)\n        vectorstore.store = store\n        return vectorstore\n\n    def dump(self, path: str) -> None:\n        \"\"\"Dump the vector store to a file.\n\n        Args:\n            path: The path to dump the vector store to.\n        \"\"\"\n        _path: Path = Path(path)\n        _path.parent.mkdir(exist_ok=True, parents=True)\n        with _path.open(\"w\") as f:\n            json.dump(dumpd(self.store), f, indent=2)\n",
        "patch": "@@ -2,7 +2,6 @@\n \n import json\n import uuid\n-from collections.abc import Iterator, Sequence\n from pathlib import Path\n from typing import (\n     TYPE_CHECKING,\n@@ -13,13 +12,15 @@\n \n from langchain_core._api import deprecated\n from langchain_core.documents import Document\n-from langchain_core.embeddings import Embeddings\n from langchain_core.load import dumpd, load\n from langchain_core.vectorstores import VectorStore\n from langchain_core.vectorstores.utils import _cosine_similarity as cosine_similarity\n from langchain_core.vectorstores.utils import maximal_marginal_relevance\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterator, Sequence\n+\n+    from langchain_core.embeddings import Embeddings\n     from langchain_core.indexing import UpsertResponse\n \n "
      },
      {
        "filename": "libs/core/tests/unit_tests/language_models/chat_models/test_base.py",
        "content_before": "\"\"\"Test base chat model.\"\"\"\n\nimport uuid\nfrom collections.abc import AsyncIterator, Iterator\nfrom typing import Any, Literal, Optional, Union\n\nimport pytest\n\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.language_models import BaseChatModel, FakeListChatModel\nfrom langchain_core.language_models.fake_chat_models import FakeListChatModelError\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n)\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.outputs.llm_result import LLMResult\nfrom langchain_core.tracers import LogStreamCallbackHandler\nfrom langchain_core.tracers.base import BaseTracer\nfrom langchain_core.tracers.context import collect_runs\nfrom langchain_core.tracers.event_stream import _AstreamEventsCallbackHandler\nfrom langchain_core.tracers.schemas import Run\nfrom tests.unit_tests.fake.callbacks import (\n    BaseFakeCallbackHandler,\n    FakeAsyncCallbackHandler,\n    FakeCallbackHandler,\n)\nfrom tests.unit_tests.stubs import _any_id_ai_message, _any_id_ai_message_chunk\n\n\n@pytest.fixture\ndef messages() -> list:\n    return [\n        SystemMessage(content=\"You are a test user.\"),\n        HumanMessage(content=\"Hello, I am a test user.\"),\n    ]\n\n\n@pytest.fixture\ndef messages_2() -> list:\n    return [\n        SystemMessage(content=\"You are a test user.\"),\n        HumanMessage(content=\"Hello, I not a test user.\"),\n    ]\n\n\ndef test_batch_size(messages: list, messages_2: list) -> None:\n    # The base endpoint doesn't support native batching,\n    # so we expect batch_size to always be 1\n    llm = FakeListChatModel(responses=[str(i) for i in range(100)])\n    with collect_runs() as cb:\n        llm.batch([messages, messages_2], {\"callbacks\": [cb]})\n        assert len(cb.traced_runs) == 2\n        assert all((r.extra or {}).get(\"batch_size\") == 1 for r in cb.traced_runs)\n    with collect_runs() as cb:\n        llm.batch([messages], {\"callbacks\": [cb]})\n        assert all((r.extra or {}).get(\"batch_size\") == 1 for r in cb.traced_runs)\n        assert len(cb.traced_runs) == 1\n\n    with collect_runs() as cb:\n        llm.invoke(messages)\n        assert len(cb.traced_runs) == 1\n        assert (cb.traced_runs[0].extra or {}).get(\"batch_size\") == 1\n\n    with collect_runs() as cb:\n        list(llm.stream(messages))\n        assert len(cb.traced_runs) == 1\n        assert (cb.traced_runs[0].extra or {}).get(\"batch_size\") == 1\n\n\nasync def test_async_batch_size(messages: list, messages_2: list) -> None:\n    llm = FakeListChatModel(responses=[str(i) for i in range(100)])\n    # The base endpoint doesn't support native batching,\n    # so we expect batch_size to always be 1\n    with collect_runs() as cb:\n        await llm.abatch([messages, messages_2], {\"callbacks\": [cb]})\n        assert all((r.extra or {}).get(\"batch_size\") == 1 for r in cb.traced_runs)\n        assert len(cb.traced_runs) == 2\n    with collect_runs() as cb:\n        await llm.abatch([messages], {\"callbacks\": [cb]})\n        assert all((r.extra or {}).get(\"batch_size\") == 1 for r in cb.traced_runs)\n        assert len(cb.traced_runs) == 1\n\n    with collect_runs() as cb:\n        await llm.ainvoke(messages)\n        assert len(cb.traced_runs) == 1\n        assert (cb.traced_runs[0].extra or {}).get(\"batch_size\") == 1\n\n    with collect_runs() as cb:\n        async for _ in llm.astream(messages):\n            pass\n        assert len(cb.traced_runs) == 1\n        assert (cb.traced_runs[0].extra or {}).get(\"batch_size\") == 1\n\n\nasync def test_stream_error_callback() -> None:\n    message = \"test\"\n\n    def eval_response(callback: BaseFakeCallbackHandler, i: int) -> None:\n        assert callback.errors == 1\n        assert len(callback.errors_args) == 1\n        llm_result: LLMResult = callback.errors_args[0][\"kwargs\"][\"response\"]\n        if i == 0:\n            assert llm_result.generations == []\n        else:\n            assert llm_result.generations[0][0].text == message[:i]\n\n    for i in range(2):\n        llm = FakeListChatModel(\n            responses=[message],\n            error_on_chunk_number=i,\n        )\n        with pytest.raises(FakeListChatModelError):\n            cb_async = FakeAsyncCallbackHandler()\n            async for _ in llm.astream(\"Dummy message\", callbacks=[cb_async]):\n                pass\n            eval_response(cb_async, i)\n\n            cb_sync = FakeCallbackHandler()\n            for _ in llm.stream(\"Dumy message\", callbacks=[cb_sync]):\n                pass\n\n            eval_response(cb_sync, i)\n\n\nasync def test_astream_fallback_to_ainvoke() -> None:\n    \"\"\"Test astream uses appropriate implementation.\"\"\"\n\n    class ModelWithGenerate(BaseChatModel):\n        def _generate(\n            self,\n            messages: list[BaseMessage],\n            stop: Optional[list[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            **kwargs: Any,\n        ) -> ChatResult:\n            \"\"\"Top Level call.\"\"\"\n            message = AIMessage(content=\"hello\")\n            generation = ChatGeneration(message=message)\n            return ChatResult(generations=[generation])\n\n        @property\n        def _llm_type(self) -> str:\n            return \"fake-chat-model\"\n\n    model = ModelWithGenerate()\n    chunks = list(model.stream(\"anything\"))\n    assert chunks == [_any_id_ai_message(content=\"hello\")]\n\n    chunks = [chunk async for chunk in model.astream(\"anything\")]\n    assert chunks == [_any_id_ai_message(content=\"hello\")]\n\n\nasync def test_astream_implementation_fallback_to_stream() -> None:\n    \"\"\"Test astream uses appropriate implementation.\"\"\"\n\n    class ModelWithSyncStream(BaseChatModel):\n        def _generate(\n            self,\n            messages: list[BaseMessage],\n            stop: Optional[list[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            **kwargs: Any,\n        ) -> ChatResult:\n            \"\"\"Top Level call.\"\"\"\n            raise NotImplementedError\n\n        def _stream(\n            self,\n            messages: list[BaseMessage],\n            stop: Optional[list[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            **kwargs: Any,\n        ) -> Iterator[ChatGenerationChunk]:\n            \"\"\"Stream the output of the model.\"\"\"\n            yield ChatGenerationChunk(message=AIMessageChunk(content=\"a\"))\n            yield ChatGenerationChunk(message=AIMessageChunk(content=\"b\"))\n\n        @property\n        def _llm_type(self) -> str:\n            return \"fake-chat-model\"\n\n    model = ModelWithSyncStream()\n    chunks = list(model.stream(\"anything\"))\n    assert chunks == [\n        _any_id_ai_message_chunk(content=\"a\"),\n        _any_id_ai_message_chunk(content=\"b\"),\n    ]\n    assert len({chunk.id for chunk in chunks}) == 1\n    assert type(model)._astream == BaseChatModel._astream\n    astream_chunks = [chunk async for chunk in model.astream(\"anything\")]\n    assert astream_chunks == [\n        _any_id_ai_message_chunk(content=\"a\"),\n        _any_id_ai_message_chunk(content=\"b\"),\n    ]\n    assert len({chunk.id for chunk in astream_chunks}) == 1\n\n\nasync def test_astream_implementation_uses_astream() -> None:\n    \"\"\"Test astream uses appropriate implementation.\"\"\"\n\n    class ModelWithAsyncStream(BaseChatModel):\n        def _generate(\n            self,\n            messages: list[BaseMessage],\n            stop: Optional[list[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            **kwargs: Any,\n        ) -> ChatResult:\n            \"\"\"Top Level call.\"\"\"\n            raise NotImplementedError\n\n        async def _astream(  # type: ignore\n            self,\n            messages: list[BaseMessage],\n            stop: Optional[list[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            **kwargs: Any,\n        ) -> AsyncIterator[ChatGenerationChunk]:\n            \"\"\"Stream the output of the model.\"\"\"\n            yield ChatGenerationChunk(message=AIMessageChunk(content=\"a\"))\n            yield ChatGenerationChunk(message=AIMessageChunk(content=\"b\"))\n\n        @property\n        def _llm_type(self) -> str:\n            return \"fake-chat-model\"\n\n    model = ModelWithAsyncStream()\n    chunks = [chunk async for chunk in model.astream(\"anything\")]\n    assert chunks == [\n        _any_id_ai_message_chunk(content=\"a\"),\n        _any_id_ai_message_chunk(content=\"b\"),\n    ]\n    assert len({chunk.id for chunk in chunks}) == 1\n\n\nclass FakeTracer(BaseTracer):\n    def __init__(self) -> None:\n        super().__init__()\n        self.traced_run_ids: list = []\n\n    def _persist_run(self, run: Run) -> None:\n        \"\"\"Persist a run.\"\"\"\n        self.traced_run_ids.append(run.id)\n\n\ndef test_pass_run_id() -> None:\n    llm = FakeListChatModel(responses=[\"a\", \"b\", \"c\"])\n    cb = FakeTracer()\n    uid1 = uuid.uuid4()\n    llm.invoke(\"Dummy message\", {\"callbacks\": [cb], \"run_id\": uid1})\n    assert cb.traced_run_ids == [uid1]\n    uid2 = uuid.uuid4()\n    list(llm.stream(\"Dummy message\", {\"callbacks\": [cb], \"run_id\": uid2}))\n    assert cb.traced_run_ids == [uid1, uid2]\n    uid3 = uuid.uuid4()\n    llm.batch([[\"Dummy message\"]], {\"callbacks\": [cb], \"run_id\": uid3})\n    assert cb.traced_run_ids == [uid1, uid2, uid3]\n\n\nasync def test_async_pass_run_id() -> None:\n    llm = FakeListChatModel(responses=[\"a\", \"b\", \"c\"])\n    cb = FakeTracer()\n    uid1 = uuid.uuid4()\n    await llm.ainvoke(\"Dummy message\", {\"callbacks\": [cb], \"run_id\": uid1})\n    assert cb.traced_run_ids == [uid1]\n    uid2 = uuid.uuid4()\n    async for _ in llm.astream(\"Dummy message\", {\"callbacks\": [cb], \"run_id\": uid2}):\n        pass\n    assert cb.traced_run_ids == [uid1, uid2]\n\n    uid3 = uuid.uuid4()\n    await llm.abatch([[\"Dummy message\"]], {\"callbacks\": [cb], \"run_id\": uid3})\n    assert cb.traced_run_ids == [uid1, uid2, uid3]\n\n\nclass NoStreamingModel(BaseChatModel):\n    def _generate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        return ChatResult(generations=[ChatGeneration(message=AIMessage(\"invoke\"))])\n\n    @property\n    def _llm_type(self) -> str:\n        return \"model1\"\n\n\nclass StreamingModel(NoStreamingModel):\n    def _stream(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        yield ChatGenerationChunk(message=AIMessageChunk(content=\"stream\"))\n\n\n@pytest.mark.parametrize(\"disable_streaming\", [True, False, \"tool_calling\"])\ndef test_disable_streaming(\n    disable_streaming: Union[bool, Literal[\"tool_calling\"]],\n) -> None:\n    model = StreamingModel(disable_streaming=disable_streaming)\n    assert model.invoke([]).content == \"invoke\"\n\n    expected = \"invoke\" if disable_streaming is True else \"stream\"\n    assert next(model.stream([])).content == expected\n    assert (\n        model.invoke([], config={\"callbacks\": [LogStreamCallbackHandler()]}).content\n        == expected\n    )\n\n    expected = \"invoke\" if disable_streaming in (\"tool_calling\", True) else \"stream\"\n    assert next(model.stream([], tools=[{\"type\": \"function\"}])).content == expected\n    assert (\n        model.invoke(\n            [], config={\"callbacks\": [LogStreamCallbackHandler()]}, tools=[{}]\n        ).content\n        == expected\n    )\n\n\n@pytest.mark.parametrize(\"disable_streaming\", [True, False, \"tool_calling\"])\nasync def test_disable_streaming_async(\n    disable_streaming: Union[bool, Literal[\"tool_calling\"]],\n) -> None:\n    model = StreamingModel(disable_streaming=disable_streaming)\n    assert (await model.ainvoke([])).content == \"invoke\"\n\n    expected = \"invoke\" if disable_streaming is True else \"stream\"\n    async for c in model.astream([]):\n        assert c.content == expected\n        break\n    assert (\n        await model.ainvoke([], config={\"callbacks\": [_AstreamEventsCallbackHandler()]})\n    ).content == expected\n\n    expected = \"invoke\" if disable_streaming in (\"tool_calling\", True) else \"stream\"\n    async for c in model.astream([], tools=[{}]):\n        assert c.content == expected\n        break\n    assert (\n        await model.ainvoke(\n            [], config={\"callbacks\": [_AstreamEventsCallbackHandler()]}, tools=[{}]\n        )\n    ).content == expected\n\n\n@pytest.mark.parametrize(\"disable_streaming\", [True, False, \"tool_calling\"])\ndef test_disable_streaming_no_streaming_model(\n    disable_streaming: Union[bool, Literal[\"tool_calling\"]],\n) -> None:\n    model = NoStreamingModel(disable_streaming=disable_streaming)\n    assert model.invoke([]).content == \"invoke\"\n    assert next(model.stream([])).content == \"invoke\"\n    assert (\n        model.invoke([], config={\"callbacks\": [LogStreamCallbackHandler()]}).content\n        == \"invoke\"\n    )\n    assert next(model.stream([], tools=[{}])).content == \"invoke\"\n\n\n@pytest.mark.parametrize(\"disable_streaming\", [True, False, \"tool_calling\"])\nasync def test_disable_streaming_no_streaming_model_async(\n    disable_streaming: Union[bool, Literal[\"tool_calling\"]],\n) -> None:\n    model = NoStreamingModel(disable_streaming=disable_streaming)\n    assert (await model.ainvoke([])).content == \"invoke\"\n    async for c in model.astream([]):\n        assert c.content == \"invoke\"\n        break\n    assert (\n        await model.ainvoke([], config={\"callbacks\": [_AstreamEventsCallbackHandler()]})\n    ).content == \"invoke\"\n    async for c in model.astream([], tools=[{}]):\n        assert c.content == \"invoke\"\n        break\n",
        "patch": "@@ -2,7 +2,7 @@\n \n import uuid\n from collections.abc import AsyncIterator, Iterator\n-from typing import Any, Literal, Optional, Union\n+from typing import TYPE_CHECKING, Any, Literal, Optional, Union\n \n import pytest\n \n@@ -30,6 +30,9 @@\n )\n from tests.unit_tests.stubs import _any_id_ai_message, _any_id_ai_message_chunk\n \n+if TYPE_CHECKING:\n+    from langchain_core.outputs.llm_result import LLMResult\n+\n \n @pytest.fixture\n def messages() -> list:"
      },
      {
        "filename": "libs/core/tests/unit_tests/vectorstores/test_vectorstore.py",
        "content_before": "\"\"\"Set of tests that complement the standard tests for vectorstore.\n\nThese tests verify that the base abstraction does appropriate delegation to\nthe relevant methods.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport uuid\nfrom collections.abc import Iterable, Sequence\nfrom typing import Any, Optional\n\nimport pytest\n\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings, FakeEmbeddings\nfrom langchain_core.vectorstores import VectorStore\n\n\nclass CustomAddTextsVectorstore(VectorStore):\n    \"\"\"A vectorstore that only implements add texts.\"\"\"\n\n    def __init__(self) -> None:\n        self.store: dict[str, Document] = {}\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[list[dict]] = None,\n        ids: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> list[str]:\n        if not isinstance(texts, list):\n            texts = list(texts)\n        ids_iter = iter(ids or [])\n\n        ids_ = []\n\n        metadatas_ = metadatas or [{} for _ in texts]\n\n        for text, metadata in zip(texts, metadatas_ or []):\n            next_id = next(ids_iter, None)\n            id_ = next_id or str(uuid.uuid4())\n            self.store[id_] = Document(page_content=text, metadata=metadata, id=id_)\n            ids_.append(id_)\n        return ids_\n\n    def get_by_ids(self, ids: Sequence[str], /) -> list[Document]:\n        return [self.store[id] for id in ids if id in self.store]\n\n    @classmethod\n    def from_texts(  # type: ignore\n        cls,\n        texts: list[str],\n        embedding: Embeddings,\n        metadatas: Optional[list[dict]] = None,\n        **kwargs: Any,\n    ) -> CustomAddTextsVectorstore:\n        vectorstore = CustomAddTextsVectorstore()\n        vectorstore.add_texts(texts, metadatas=metadatas, **kwargs)\n        return vectorstore\n\n    def similarity_search(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        raise NotImplementedError\n\n\nclass CustomAddDocumentsVectorstore(VectorStore):\n    \"\"\"A vectorstore that only implements add documents.\"\"\"\n\n    def __init__(self) -> None:\n        self.store: dict[str, Document] = {}\n\n    def add_documents(\n        self,\n        documents: list[Document],\n        *,\n        ids: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> list[str]:\n        ids_ = []\n        ids_iter = iter(ids or [])\n        for document in documents:\n            id_ = next(ids_iter) if ids else document.id or str(uuid.uuid4())\n            self.store[id_] = Document(\n                id=id_, page_content=document.page_content, metadata=document.metadata\n            )\n            ids_.append(id_)\n        return ids_\n\n    def get_by_ids(self, ids: Sequence[str], /) -> list[Document]:\n        return [self.store[id] for id in ids if id in self.store]\n\n    @classmethod\n    def from_texts(  # type: ignore\n        cls,\n        texts: list[str],\n        embedding: Embeddings,\n        metadatas: Optional[list[dict]] = None,\n        **kwargs: Any,\n    ) -> CustomAddDocumentsVectorstore:\n        vectorstore = CustomAddDocumentsVectorstore()\n        vectorstore.add_texts(texts, metadatas=metadatas, **kwargs)\n        return vectorstore\n\n    def similarity_search(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> list[Document]:\n        raise NotImplementedError\n\n\n@pytest.mark.parametrize(\n    \"vs_class\", [CustomAddTextsVectorstore, CustomAddDocumentsVectorstore]\n)\ndef test_default_add_documents(vs_class: type[VectorStore]) -> None:\n    \"\"\"Test that we can implement the upsert method of the CustomVectorStore\n    class without violating the Liskov Substitution Principle.\n    \"\"\"\n    store = vs_class()\n\n    # Check upsert with id\n    assert store.add_documents([Document(id=\"1\", page_content=\"hello\")]) == [\"1\"]\n\n    assert store.get_by_ids([\"1\"]) == [Document(id=\"1\", page_content=\"hello\")]\n\n    # Check upsert without id\n    ids = store.add_documents([Document(page_content=\"world\")])\n    assert len(ids) == 1\n    assert store.get_by_ids(ids) == [Document(id=ids[0], page_content=\"world\")]\n\n    # Check that add_documents works\n    assert store.add_documents([Document(id=\"5\", page_content=\"baz\")]) == [\"5\"]\n\n    # Test add documents with id specified in both document and ids\n    original_document = Document(id=\"7\", page_content=\"baz\")\n    assert store.add_documents([original_document], ids=[\"6\"]) == [\"6\"]\n    assert original_document.id == \"7\"  # original document should not be modified\n    assert store.get_by_ids([\"6\"]) == [Document(id=\"6\", page_content=\"baz\")]\n\n\n@pytest.mark.parametrize(\n    \"vs_class\", [CustomAddTextsVectorstore, CustomAddDocumentsVectorstore]\n)\ndef test_default_add_texts(vs_class: type[VectorStore]) -> None:\n    store = vs_class()\n    # Check that default implementation of add_texts works\n    assert store.add_texts([\"hello\", \"world\"], ids=[\"3\", \"4\"]) == [\"3\", \"4\"]\n\n    assert store.get_by_ids([\"3\", \"4\"]) == [\n        Document(id=\"3\", page_content=\"hello\"),\n        Document(id=\"4\", page_content=\"world\"),\n    ]\n\n    # Add texts without ids\n    ids_ = store.add_texts([\"foo\", \"bar\"])\n    assert len(ids_) == 2\n    assert store.get_by_ids(ids_) == [\n        Document(id=ids_[0], page_content=\"foo\"),\n        Document(id=ids_[1], page_content=\"bar\"),\n    ]\n\n    # Add texts with metadatas\n    ids_2 = store.add_texts([\"foo\", \"bar\"], metadatas=[{\"foo\": \"bar\"}] * 2)\n    assert len(ids_2) == 2\n    assert store.get_by_ids(ids_2) == [\n        Document(id=ids_2[0], page_content=\"foo\", metadata={\"foo\": \"bar\"}),\n        Document(id=ids_2[1], page_content=\"bar\", metadata={\"foo\": \"bar\"}),\n    ]\n\n\n@pytest.mark.parametrize(\n    \"vs_class\", [CustomAddTextsVectorstore, CustomAddDocumentsVectorstore]\n)\nasync def test_default_aadd_documents(vs_class: type[VectorStore]) -> None:\n    \"\"\"Test delegation to the synchronous method.\"\"\"\n    store = vs_class()\n\n    # Check upsert with id\n    assert await store.aadd_documents([Document(id=\"1\", page_content=\"hello\")]) == [\"1\"]\n\n    assert await store.aget_by_ids([\"1\"]) == [Document(id=\"1\", page_content=\"hello\")]\n\n    # Check upsert without id\n    ids = await store.aadd_documents([Document(page_content=\"world\")])\n    assert len(ids) == 1\n    assert await store.aget_by_ids(ids) == [Document(id=ids[0], page_content=\"world\")]\n\n    # Check that add_documents works\n    assert await store.aadd_documents([Document(id=\"5\", page_content=\"baz\")]) == [\"5\"]\n\n    # Test add documents with id specified in both document and ids\n    original_document = Document(id=\"7\", page_content=\"baz\")\n    assert await store.aadd_documents([original_document], ids=[\"6\"]) == [\"6\"]\n    assert original_document.id == \"7\"  # original document should not be modified\n    assert await store.aget_by_ids([\"6\"]) == [Document(id=\"6\", page_content=\"baz\")]\n\n\n@pytest.mark.parametrize(\n    \"vs_class\", [CustomAddTextsVectorstore, CustomAddDocumentsVectorstore]\n)\nasync def test_default_aadd_texts(vs_class: type[VectorStore]) -> None:\n    \"\"\"Test delegation to the synchronous method.\"\"\"\n    store = vs_class()\n    # Check that default implementation of aadd_texts works\n    assert await store.aadd_texts([\"hello\", \"world\"], ids=[\"3\", \"4\"]) == [\"3\", \"4\"]\n\n    assert await store.aget_by_ids([\"3\", \"4\"]) == [\n        Document(id=\"3\", page_content=\"hello\"),\n        Document(id=\"4\", page_content=\"world\"),\n    ]\n\n    # Add texts without ids\n    ids_ = await store.aadd_texts([\"foo\", \"bar\"])\n    assert len(ids_) == 2\n    assert await store.aget_by_ids(ids_) == [\n        Document(id=ids_[0], page_content=\"foo\"),\n        Document(id=ids_[1], page_content=\"bar\"),\n    ]\n\n    # Add texts with metadatas\n    ids_2 = await store.aadd_texts([\"foo\", \"bar\"], metadatas=[{\"foo\": \"bar\"}] * 2)\n    assert len(ids_2) == 2\n    assert await store.aget_by_ids(ids_2) == [\n        Document(id=ids_2[0], page_content=\"foo\", metadata={\"foo\": \"bar\"}),\n        Document(id=ids_2[1], page_content=\"bar\", metadata={\"foo\": \"bar\"}),\n    ]\n\n\n@pytest.mark.parametrize(\n    \"vs_class\", [CustomAddTextsVectorstore, CustomAddDocumentsVectorstore]\n)\ndef test_default_from_documents(vs_class: type[VectorStore]) -> None:\n    embeddings = FakeEmbeddings(size=1)\n    store = vs_class.from_documents(\n        [Document(id=\"1\", page_content=\"hello\", metadata={\"foo\": \"bar\"})], embeddings\n    )\n\n    assert store.get_by_ids([\"1\"]) == [\n        Document(id=\"1\", page_content=\"hello\", metadata={\"foo\": \"bar\"})\n    ]\n\n    # from_documents with ids in args\n    store = vs_class.from_documents(\n        [Document(page_content=\"hello\", metadata={\"foo\": \"bar\"})], embeddings, ids=[\"1\"]\n    )\n\n    assert store.get_by_ids([\"1\"]) == [\n        Document(id=\"1\", page_content=\"hello\", metadata={\"foo\": \"bar\"})\n    ]\n\n    # Test from_documents with id specified in both document and ids\n    original_document = Document(id=\"7\", page_content=\"baz\")\n    store = vs_class.from_documents([original_document], embeddings, ids=[\"6\"])\n    assert original_document.id == \"7\"  # original document should not be modified\n    assert store.get_by_ids([\"6\"]) == [Document(id=\"6\", page_content=\"baz\")]\n\n\n@pytest.mark.parametrize(\n    \"vs_class\", [CustomAddTextsVectorstore, CustomAddDocumentsVectorstore]\n)\nasync def test_default_afrom_documents(vs_class: type[VectorStore]) -> None:\n    embeddings = FakeEmbeddings(size=1)\n    store = await vs_class.afrom_documents(\n        [Document(id=\"1\", page_content=\"hello\", metadata={\"foo\": \"bar\"})], embeddings\n    )\n\n    assert await store.aget_by_ids([\"1\"]) == [\n        Document(id=\"1\", page_content=\"hello\", metadata={\"foo\": \"bar\"})\n    ]\n\n    # from_documents with ids in args\n    store = await vs_class.afrom_documents(\n        [Document(page_content=\"hello\", metadata={\"foo\": \"bar\"})], embeddings, ids=[\"1\"]\n    )\n\n    assert await store.aget_by_ids([\"1\"]) == [\n        Document(id=\"1\", page_content=\"hello\", metadata={\"foo\": \"bar\"})\n    ]\n\n    # Test afrom_documents with id specified in both document and ids\n    original_document = Document(id=\"7\", page_content=\"baz\")\n    store = await vs_class.afrom_documents([original_document], embeddings, ids=[\"6\"])\n    assert original_document.id == \"7\"  # original document should not be modified\n    assert await store.aget_by_ids([\"6\"]) == [Document(id=\"6\", page_content=\"baz\")]\n",
        "patch": "@@ -7,15 +7,17 @@\n from __future__ import annotations\n \n import uuid\n-from collections.abc import Iterable, Sequence\n-from typing import Any, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n \n import pytest\n \n from langchain_core.documents import Document\n from langchain_core.embeddings import Embeddings, FakeEmbeddings\n from langchain_core.vectorstores import VectorStore\n \n+if TYPE_CHECKING:\n+    from collections.abc import Iterable, Sequence\n+\n \n class CustomAddTextsVectorstore(VectorStore):\n     \"\"\"A vectorstore that only implements add texts.\"\"\""
      }
    ]
  },
  {
    "number": 29942,
    "title": "core[patch]: Fix FileCallbackHandler name resolution, Fixes #29941",
    "body": "- **Description:** Same changes as #26593 but for FileCallbackHandler\r\n- **Issue:**  Fixes #29941\r\n- **Dependencies:** None\r\n- **Twitter handle:** None\r\n\r\n- [x] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n",
    "issue_title": "core[patch]: Fix FileCallbackHandler name resolution, Fixes #29941",
    "issue_body": "- **Description:** Same changes as #26593 but for FileCallbackHandler\r\n- **Issue:**  Fixes #29941\r\n- **Dependencies:** None\r\n- **Twitter handle:** None\r\n\r\n- [x] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n",
    "files": [
      {
        "filename": "libs/core/langchain_core/callbacks/file.py",
        "content_before": "\"\"\"Callback Handler that writes to a file.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Optional, TextIO, cast\n\nfrom langchain_core.agents import AgentAction, AgentFinish\nfrom langchain_core.callbacks import BaseCallbackHandler\nfrom langchain_core.utils.input import print_text\n\n\nclass FileCallbackHandler(BaseCallbackHandler):\n    \"\"\"Callback Handler that writes to a file.\n\n    Parameters:\n        filename: The file to write to.\n        mode: The mode to open the file in. Defaults to \"a\".\n        color: The color to use for the text.\n    \"\"\"\n\n    def __init__(\n        self, filename: str, mode: str = \"a\", color: Optional[str] = None\n    ) -> None:\n        \"\"\"Initialize callback handler.\n\n        Args:\n            filename: The filename to write to.\n            mode: The mode to open the file in. Defaults to \"a\".\n            color: The color to use for the text. Defaults to None.\n        \"\"\"\n        self.file = cast(TextIO, open(filename, mode, encoding=\"utf-8\"))  # noqa: SIM115\n        self.color = color\n\n    def __del__(self) -> None:\n        \"\"\"Destructor to cleanup when done.\"\"\"\n        self.file.close()\n\n    def on_chain_start(\n        self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out that we are entering a chain.\n\n        Args:\n            serialized (Dict[str, Any]): The serialized chain.\n            inputs (Dict[str, Any]): The inputs to the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n        print_text(\n            f\"\\n\\n\\033[1m> Entering new {class_name} chain...\\033[0m\",\n            end=\"\\n\",\n            file=self.file,\n        )\n\n    def on_chain_end(self, outputs: dict[str, Any], **kwargs: Any) -> None:\n        \"\"\"Print out that we finished a chain.\n\n        Args:\n            outputs (Dict[str, Any]): The outputs of the chain.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(\"\\n\\033[1m> Finished chain.\\033[0m\", end=\"\\n\", file=self.file)\n\n    def on_agent_action(\n        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Run on agent action.\n\n        Args:\n            action (AgentAction): The agent action.\n            color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(action.log, color=color or self.color, file=self.file)\n\n    def on_tool_end(\n        self,\n        output: str,\n        color: Optional[str] = None,\n        observation_prefix: Optional[str] = None,\n        llm_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"If not the final action, print out observation.\n\n        Args:\n           output (str): The output to print.\n           color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n           observation_prefix (Optional[str], optional): The observation prefix.\n            Defaults to None.\n           llm_prefix (Optional[str], optional): The LLM prefix.\n                Defaults to None.\n           **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        if observation_prefix is not None:\n            print_text(f\"\\n{observation_prefix}\", file=self.file)\n        print_text(output, color=color or self.color, file=self.file)\n        if llm_prefix is not None:\n            print_text(f\"\\n{llm_prefix}\", file=self.file)\n\n    def on_text(\n        self, text: str, color: Optional[str] = None, end: str = \"\", **kwargs: Any\n    ) -> None:\n        \"\"\"Run when the agent ends.\n\n        Args:\n           text (str): The text to print.\n           color (Optional[str], optional): The color to use for the text.\n            Defaults to None.\n           end (str, optional): The end character. Defaults to \"\".\n           **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(text, color=color or self.color, end=end, file=self.file)\n\n    def on_agent_finish(\n        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n    ) -> None:\n        \"\"\"Run on the agent end.\n\n        Args:\n            finish (AgentFinish): The agent finish.\n            color (Optional[str], optional): The color to use for the text.\n                Defaults to None.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        print_text(finish.log, color=color or self.color, end=\"\\n\", file=self.file)\n",
        "patch": "@@ -45,9 +45,15 @@ def on_chain_start(\n             inputs (Dict[str, Any]): The inputs to the chain.\n             **kwargs (Any): Additional keyword arguments.\n         \"\"\"\n-        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n+        if \"name\" in kwargs:\n+            name = kwargs[\"name\"]\n+        else:\n+            if serialized:\n+                name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n+            else:\n+                name = \"<unknown>\"\n         print_text(\n-            f\"\\n\\n\\033[1m> Entering new {class_name} chain...\\033[0m\",\n+            f\"\\n\\n\\033[1m> Entering new {name} chain...\\033[0m\",\n             end=\"\\n\",\n             file=self.file,\n         )"
      }
    ]
  },
  {
    "number": 29914,
    "title": "community: Update SQLiteVec table trigger",
    "body": "**Issue**:  This trigger can only be used by the first table created. Cannot create additional triggers for other tables.\r\n\r\n**fixed**:  Update the trigger name so that it can be used for new tables.\r\n\r\n\r\n\r\n",
    "issue_title": "community: Update SQLiteVec table trigger",
    "issue_body": "**Issue**:  This trigger can only be used by the first table created. Cannot create additional triggers for other tables.\r\n\r\n**fixed**:  Update the trigger name so that it can be used for new tables.\r\n\r\n\r\n\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/vectorstores/sqlitevec.py",
        "content_before": "from __future__ import annotations\n\nimport json\nimport logging\nimport struct\nimport warnings\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Iterable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n)\n\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.vectorstores import VectorStore\n\nif TYPE_CHECKING:\n    import sqlite3\n\nlogger = logging.getLogger(__name__)\n\n\ndef serialize_f32(vector: List[float]) -> bytes:\n    \"\"\"Serializes a list of floats into a compact \"raw bytes\" format\n\n    Source: https://github.com/asg017/sqlite-vec/blob/21c5a14fc71c83f135f5b00c84115139fd12c492/examples/simple-python/demo.py#L8-L10\n    \"\"\"\n    return struct.pack(\"%sf\" % len(vector), *vector)\n\n\nclass SQLiteVec(VectorStore):\n    \"\"\"SQLite with Vec extension as a vector database.\n\n    To use, you should have the ``sqlite-vec`` python package installed.\n    Example:\n        .. code-block:: python\n            from langchain_community.vectorstores import SQLiteVec\n            from langchain_community.embeddings.openai import OpenAIEmbeddings\n            ...\n    \"\"\"\n\n    def __init__(\n        self,\n        table: str,\n        connection: Optional[sqlite3.Connection],\n        embedding: Embeddings,\n        db_file: str = \"vec.db\",\n    ):\n        \"\"\"Initialize with sqlite client with vss extension.\"\"\"\n        try:\n            import sqlite_vec  # noqa  # pylint: disable=unused-import\n        except ImportError:\n            raise ImportError(\n                \"Could not import sqlite-vec python package. \"\n                \"Please install it with `pip install sqlite-vec`.\"\n            )\n\n        if not connection:\n            connection = self.create_connection(db_file)\n\n        if not isinstance(embedding, Embeddings):\n            warnings.warn(\"embeddings input must be Embeddings object.\")\n\n        self._connection = connection\n        self._table = table\n        self._embedding = embedding\n\n        self.create_table_if_not_exists()\n\n    def create_table_if_not_exists(self) -> None:\n        self._connection.execute(\n            f\"\"\"\n            CREATE TABLE IF NOT EXISTS {self._table}\n            (\n                rowid INTEGER PRIMARY KEY AUTOINCREMENT,\n                text TEXT,\n                metadata BLOB,\n                text_embedding BLOB\n            )\n            ;\n            \"\"\"\n        )\n        self._connection.execute(\n            f\"\"\"\n            CREATE VIRTUAL TABLE IF NOT EXISTS {self._table}_vec USING vec0(\n                rowid INTEGER PRIMARY KEY,\n                text_embedding float[{self.get_dimensionality()}]\n            )\n            ;\n            \"\"\"\n        )\n        self._connection.execute(\n            f\"\"\"\n                CREATE TRIGGER IF NOT EXISTS embed_text \n                AFTER INSERT ON {self._table}\n                BEGIN\n                    INSERT INTO {self._table}_vec(rowid, text_embedding)\n                    VALUES (new.rowid, new.text_embedding) \n                    ;\n                END;\n            \"\"\"\n        )\n        self._connection.commit()\n\n    def add_texts(\n        self,\n        texts: Iterable[str],\n        metadatas: Optional[List[dict]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Add more texts to the vectorstore index.\n        Args:\n            texts: Iterable of strings to add to the vectorstore.\n            metadatas: Optional list of metadatas associated with the texts.\n            kwargs: vectorstore specific parameters\n        \"\"\"\n        max_id = self._connection.execute(\n            f\"SELECT max(rowid) as rowid FROM {self._table}\"\n        ).fetchone()[\"rowid\"]\n        if max_id is None:  # no text added yet\n            max_id = 0\n\n        embeds = self._embedding.embed_documents(list(texts))\n        if not metadatas:\n            metadatas = [{} for _ in texts]\n        data_input = [\n            (text, json.dumps(metadata), serialize_f32(embed))\n            for text, metadata, embed in zip(texts, metadatas, embeds)\n        ]\n        self._connection.executemany(\n            f\"INSERT INTO {self._table}(text, metadata, text_embedding) VALUES (?,?,?)\",\n            data_input,\n        )\n        self._connection.commit()\n        # pulling every ids we just inserted\n        results = self._connection.execute(\n            f\"SELECT rowid FROM {self._table} WHERE rowid > {max_id}\"\n        )\n        return [row[\"rowid\"] for row in results]\n\n    def similarity_search_with_score_by_vector(\n        self, embedding: List[float], k: int = 4, **kwargs: Any\n    ) -> List[Tuple[Document, float]]:\n        sql_query = f\"\"\"\n            SELECT \n                text,\n                metadata,\n                distance\n            FROM {self._table} AS e\n            INNER JOIN {self._table}_vec AS v on v.rowid = e.rowid  \n            WHERE\n                v.text_embedding MATCH ?\n                AND k = ?\n            ORDER BY distance\n        \"\"\"\n        cursor = self._connection.cursor()\n        cursor.execute(\n            sql_query,\n            [serialize_f32(embedding), k],\n        )\n        results = cursor.fetchall()\n\n        documents = []\n        for row in results:\n            metadata = json.loads(row[\"metadata\"]) or {}\n            doc = Document(page_content=row[\"text\"], metadata=metadata)\n            documents.append((doc, row[\"distance\"]))\n\n        return documents\n\n    def similarity_search(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> List[Document]:\n        \"\"\"Return docs most similar to query.\"\"\"\n        embedding = self._embedding.embed_query(query)\n        documents = self.similarity_search_with_score_by_vector(\n            embedding=embedding, k=k\n        )\n        return [doc for doc, _ in documents]\n\n    def similarity_search_with_score(\n        self, query: str, k: int = 4, **kwargs: Any\n    ) -> List[Tuple[Document, float]]:\n        \"\"\"Return docs most similar to query.\"\"\"\n        embedding = self._embedding.embed_query(query)\n        documents = self.similarity_search_with_score_by_vector(\n            embedding=embedding, k=k\n        )\n        return documents\n\n    def similarity_search_by_vector(\n        self, embedding: List[float], k: int = 4, **kwargs: Any\n    ) -> List[Document]:\n        documents = self.similarity_search_with_score_by_vector(\n            embedding=embedding, k=k\n        )\n        return [doc for doc, _ in documents]\n\n    @classmethod\n    def from_texts(\n        cls: Type[SQLiteVec],\n        texts: List[str],\n        embedding: Embeddings,\n        metadatas: Optional[List[dict]] = None,\n        table: str = \"langchain\",\n        db_file: str = \"vec.db\",\n        **kwargs: Any,\n    ) -> SQLiteVec:\n        \"\"\"Return VectorStore initialized from texts and embeddings.\"\"\"\n        connection = cls.create_connection(db_file)\n        vec = cls(\n            table=table, connection=connection, db_file=db_file, embedding=embedding\n        )\n        vec.add_texts(texts=texts, metadatas=metadatas)\n        return vec\n\n    @staticmethod\n    def create_connection(db_file: str) -> sqlite3.Connection:\n        import sqlite3\n\n        import sqlite_vec\n\n        connection = sqlite3.connect(db_file)\n        connection.row_factory = sqlite3.Row\n        connection.enable_load_extension(True)\n        sqlite_vec.load(connection)\n        connection.enable_load_extension(False)\n        return connection\n\n    def get_dimensionality(self) -> int:\n        \"\"\"\n        Function that does a dummy embedding to figure out how many dimensions\n        this embedding function returns. Needed for the virtual table DDL.\n        \"\"\"\n        dummy_text = \"This is a dummy text\"\n        dummy_embedding = self._embedding.embed_query(dummy_text)\n        return len(dummy_embedding)\n",
        "patch": "@@ -95,7 +95,7 @@ def create_table_if_not_exists(self) -> None:\n         )\n         self._connection.execute(\n             f\"\"\"\n-                CREATE TRIGGER IF NOT EXISTS embed_text \n+                CREATE TRIGGER IF NOT EXISTS {self._table}_embed_text \n                 AFTER INSERT ON {self._table}\n                 BEGIN\n                     INSERT INTO {self._table}_vec(rowid, text_embedding)"
      },
      {
        "filename": "libs/community/tests/integration_tests/vectorstores/test_sqlitevec.py",
        "content_before": "from typing import List, Optional\n\nimport pytest\nfrom langchain_core.documents import Document\n\nfrom langchain_community.vectorstores import SQLiteVec\nfrom tests.integration_tests.vectorstores.fake_embeddings import (\n    FakeEmbeddings,\n    fake_texts,\n)\n\n\ndef _sqlite_vec_from_texts(\n    metadatas: Optional[List[dict]] = None, drop: bool = True\n) -> SQLiteVec:\n    return SQLiteVec.from_texts(\n        fake_texts,\n        FakeEmbeddings(),\n        metadatas=metadatas,\n        table=\"test\",\n        db_file=\":memory:\",\n    )\n\n\n@pytest.mark.requires(\"sqlite-vec\")\ndef test_sqlitevec() -> None:\n    \"\"\"Test end to end construction and search.\"\"\"\n    docsearch = _sqlite_vec_from_texts()\n    output = docsearch.similarity_search(\"foo\", k=1)\n    assert output == [Document(page_content=\"foo\", metadata={})]\n\n\n@pytest.mark.requires(\"sqlite-vec\")\ndef test_sqlitevec_with_score() -> None:\n    \"\"\"Test end to end construction and search with scores and IDs.\"\"\"\n    texts = [\"foo\", \"bar\", \"baz\"]\n    metadatas = [{\"page\": i} for i in range(len(texts))]\n    docsearch = _sqlite_vec_from_texts(metadatas=metadatas)\n    output = docsearch.similarity_search_with_score(\"foo\", k=3)\n    docs = [o[0] for o in output]\n    distances = [o[1] for o in output]\n    assert docs == [\n        Document(page_content=\"foo\", metadata={\"page\": 0}),\n        Document(page_content=\"bar\", metadata={\"page\": 1}),\n        Document(page_content=\"baz\", metadata={\"page\": 2}),\n    ]\n    assert distances[0] < distances[1] < distances[2]\n\n\n@pytest.mark.requires(\"sqlite-vec\")\ndef test_sqlitevec_add_extra() -> None:\n    \"\"\"Test end to end construction and MRR search.\"\"\"\n    texts = [\"foo\", \"bar\", \"baz\"]\n    metadatas = [{\"page\": i} for i in range(len(texts))]\n    docsearch = _sqlite_vec_from_texts(metadatas=metadatas)\n    docsearch.add_texts(texts, metadatas)\n    output = docsearch.similarity_search(\"foo\", k=10)\n    assert len(output) == 6\n",
        "patch": "@@ -56,3 +56,27 @@ def test_sqlitevec_add_extra() -> None:\n     docsearch.add_texts(texts, metadatas)\n     output = docsearch.similarity_search(\"foo\", k=10)\n     assert len(output) == 6\n+\n+\n+@pytest.mark.requires(\"sqlite-vec\")\n+def test_sqlitevec_search_multiple_tables() -> None:\n+    \"\"\"Test end to end construction and search with multiple tables.\"\"\"\n+    docsearch_1 = SQLiteVec.from_texts(\n+        fake_texts,\n+        FakeEmbeddings(),\n+        table=\"table_1\",\n+        db_file=\":memory:\",  ## change to local storage for testing\n+    )\n+\n+    docsearch_2 = SQLiteVec.from_texts(\n+        fake_texts,\n+        FakeEmbeddings(),\n+        table=\"table_2\",\n+        db_file=\":memory:\",\n+    )\n+\n+    output_1 = docsearch_1.similarity_search(\"foo\", k=1)\n+    output_2 = docsearch_2.similarity_search(\"foo\", k=1)\n+\n+    assert output_1 == [Document(page_content=\"foo\", metadata={})]\n+    assert output_2 == [Document(page_content=\"foo\", metadata={})]"
      }
    ]
  },
  {
    "number": 29182,
    "title": "langchain: make numpy optional",
    "body": null,
    "issue_title": "langchain: make numpy optional",
    "issue_body": null,
    "files": [
      {
        "filename": "libs/langchain/langchain/chains/flare/base.py",
        "content_before": "from __future__ import annotations\n\nimport re\nfrom typing import Any, Dict, List, Optional, Sequence, Tuple\n\nimport numpy as np\nfrom langchain_core.callbacks import (\n    CallbackManagerForChainRun,\n)\nfrom langchain_core.language_models import BaseLanguageModel\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import BasePromptTemplate\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.runnables import Runnable\nfrom pydantic import Field\n\nfrom langchain.chains.base import Chain\nfrom langchain.chains.flare.prompts import (\n    PROMPT,\n    QUESTION_GENERATOR_PROMPT,\n    FinishedOutputParser,\n)\nfrom langchain.chains.llm import LLMChain\n\n\ndef _extract_tokens_and_log_probs(response: AIMessage) -> Tuple[List[str], List[float]]:\n    \"\"\"Extract tokens and log probabilities from chat model response.\"\"\"\n    tokens = []\n    log_probs = []\n    for token in response.response_metadata[\"logprobs\"][\"content\"]:\n        tokens.append(token[\"token\"])\n        log_probs.append(token[\"logprob\"])\n    return tokens, log_probs\n\n\nclass QuestionGeneratorChain(LLMChain):\n    \"\"\"Chain that generates questions from uncertain spans.\"\"\"\n\n    prompt: BasePromptTemplate = QUESTION_GENERATOR_PROMPT\n    \"\"\"Prompt template for the chain.\"\"\"\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return False\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Input keys for the chain.\"\"\"\n        return [\"user_input\", \"context\", \"response\"]\n\n\ndef _low_confidence_spans(\n    tokens: Sequence[str],\n    log_probs: Sequence[float],\n    min_prob: float,\n    min_token_gap: int,\n    num_pad_tokens: int,\n) -> List[str]:\n    _low_idx = np.where(np.exp(log_probs) < min_prob)[0]\n    low_idx = [i for i in _low_idx if re.search(r\"\\w\", tokens[i])]\n    if len(low_idx) == 0:\n        return []\n    spans = [[low_idx[0], low_idx[0] + num_pad_tokens + 1]]\n    for i, idx in enumerate(low_idx[1:]):\n        end = idx + num_pad_tokens + 1\n        if idx - low_idx[i] < min_token_gap:\n            spans[-1][1] = end\n        else:\n            spans.append([idx, end])\n    return [\"\".join(tokens[start:end]) for start, end in spans]\n\n\nclass FlareChain(Chain):\n    \"\"\"Chain that combines a retriever, a question generator,\n    and a response generator.\n\n    See [Active Retrieval Augmented Generation](https://arxiv.org/abs/2305.06983) paper.\n    \"\"\"\n\n    question_generator_chain: Runnable\n    \"\"\"Chain that generates questions from uncertain spans.\"\"\"\n    response_chain: Runnable\n    \"\"\"Chain that generates responses from user input and context.\"\"\"\n    output_parser: FinishedOutputParser = Field(default_factory=FinishedOutputParser)\n    \"\"\"Parser that determines whether the chain is finished.\"\"\"\n    retriever: BaseRetriever\n    \"\"\"Retriever that retrieves relevant documents from a user input.\"\"\"\n    min_prob: float = 0.2\n    \"\"\"Minimum probability for a token to be considered low confidence.\"\"\"\n    min_token_gap: int = 5\n    \"\"\"Minimum number of tokens between two low confidence spans.\"\"\"\n    num_pad_tokens: int = 2\n    \"\"\"Number of tokens to pad around a low confidence span.\"\"\"\n    max_iter: int = 10\n    \"\"\"Maximum number of iterations.\"\"\"\n    start_with_retrieval: bool = True\n    \"\"\"Whether to start with retrieval.\"\"\"\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Input keys for the chain.\"\"\"\n        return [\"user_input\"]\n\n    @property\n    def output_keys(self) -> List[str]:\n        \"\"\"Output keys for the chain.\"\"\"\n        return [\"response\"]\n\n    def _do_generation(\n        self,\n        questions: List[str],\n        user_input: str,\n        response: str,\n        _run_manager: CallbackManagerForChainRun,\n    ) -> Tuple[str, bool]:\n        callbacks = _run_manager.get_child()\n        docs = []\n        for question in questions:\n            docs.extend(self.retriever.invoke(question))\n        context = \"\\n\\n\".join(d.page_content for d in docs)\n        result = self.response_chain.invoke(\n            {\n                \"user_input\": user_input,\n                \"context\": context,\n                \"response\": response,\n            },\n            {\"callbacks\": callbacks},\n        )\n        if isinstance(result, AIMessage):\n            result = result.content\n        marginal, finished = self.output_parser.parse(result)\n        return marginal, finished\n\n    def _do_retrieval(\n        self,\n        low_confidence_spans: List[str],\n        _run_manager: CallbackManagerForChainRun,\n        user_input: str,\n        response: str,\n        initial_response: str,\n    ) -> Tuple[str, bool]:\n        question_gen_inputs = [\n            {\n                \"user_input\": user_input,\n                \"current_response\": initial_response,\n                \"uncertain_span\": span,\n            }\n            for span in low_confidence_spans\n        ]\n        callbacks = _run_manager.get_child()\n        if isinstance(self.question_generator_chain, LLMChain):\n            question_gen_outputs = self.question_generator_chain.apply(\n                question_gen_inputs, callbacks=callbacks\n            )\n            questions = [\n                output[self.question_generator_chain.output_keys[0]]\n                for output in question_gen_outputs\n            ]\n        else:\n            questions = self.question_generator_chain.batch(\n                question_gen_inputs, config={\"callbacks\": callbacks}\n            )\n        _run_manager.on_text(\n            f\"Generated Questions: {questions}\", color=\"yellow\", end=\"\\n\"\n        )\n        return self._do_generation(questions, user_input, response, _run_manager)\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, Any]:\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n\n        user_input = inputs[self.input_keys[0]]\n\n        response = \"\"\n\n        for i in range(self.max_iter):\n            _run_manager.on_text(\n                f\"Current Response: {response}\", color=\"blue\", end=\"\\n\"\n            )\n            _input = {\"user_input\": user_input, \"context\": \"\", \"response\": response}\n            tokens, log_probs = _extract_tokens_and_log_probs(\n                self.response_chain.invoke(\n                    _input, {\"callbacks\": _run_manager.get_child()}\n                )\n            )\n            low_confidence_spans = _low_confidence_spans(\n                tokens,\n                log_probs,\n                self.min_prob,\n                self.min_token_gap,\n                self.num_pad_tokens,\n            )\n            initial_response = response.strip() + \" \" + \"\".join(tokens)\n            if not low_confidence_spans:\n                response = initial_response\n                final_response, finished = self.output_parser.parse(response)\n                if finished:\n                    return {self.output_keys[0]: final_response}\n                continue\n\n            marginal, finished = self._do_retrieval(\n                low_confidence_spans,\n                _run_manager,\n                user_input,\n                response,\n                initial_response,\n            )\n            response = response.strip() + \" \" + marginal\n            if finished:\n                break\n        return {self.output_keys[0]: response}\n\n    @classmethod\n    def from_llm(\n        cls, llm: BaseLanguageModel, max_generation_len: int = 32, **kwargs: Any\n    ) -> FlareChain:\n        \"\"\"Creates a FlareChain from a language model.\n\n        Args:\n            llm: Language model to use.\n            max_generation_len: Maximum length of the generated response.\n            kwargs: Additional arguments to pass to the constructor.\n\n        Returns:\n            FlareChain class with the given language model.\n        \"\"\"\n        try:\n            from langchain_openai import ChatOpenAI\n        except ImportError:\n            raise ImportError(\n                \"OpenAI is required for FlareChain. \"\n                \"Please install langchain-openai.\"\n                \"pip install langchain-openai\"\n            )\n        llm = ChatOpenAI(\n            max_completion_tokens=max_generation_len, logprobs=True, temperature=0\n        )\n        response_chain = PROMPT | llm\n        question_gen_chain = QUESTION_GENERATOR_PROMPT | llm | StrOutputParser()\n        return cls(\n            question_generator_chain=question_gen_chain,\n            response_chain=response_chain,\n            **kwargs,\n        )\n",
        "patch": "@@ -1,9 +1,9 @@\n from __future__ import annotations\n \n+import logging\n import re\n from typing import Any, Dict, List, Optional, Sequence, Tuple\n \n-import numpy as np\n from langchain_core.callbacks import (\n     CallbackManagerForChainRun,\n )\n@@ -23,6 +23,8 @@\n )\n from langchain.chains.llm import LLMChain\n \n+logger = logging.getLogger(__name__)\n+\n \n def _extract_tokens_and_log_probs(response: AIMessage) -> Tuple[List[str], List[float]]:\n     \"\"\"Extract tokens and log probabilities from chat model response.\"\"\"\n@@ -57,7 +59,24 @@ def _low_confidence_spans(\n     min_token_gap: int,\n     num_pad_tokens: int,\n ) -> List[str]:\n-    _low_idx = np.where(np.exp(log_probs) < min_prob)[0]\n+    try:\n+        import numpy as np\n+\n+        _low_idx = np.where(np.exp(log_probs) < min_prob)[0]\n+    except ImportError:\n+        logger.warning(\n+            \"NumPy not found in the current Python environment. FlareChain will use a \"\n+            \"pure Python implementation for internal calculations, which may \"\n+            \"significantly impact performance, especially for large datasets. For \"\n+            \"optimal speed and efficiency, consider installing NumPy: pip install numpy\"\n+        )\n+        import math\n+\n+        _low_idx = [  # type: ignore[assignment]\n+            idx\n+            for idx, log_prob in enumerate(log_probs)\n+            if math.exp(log_prob) < min_prob\n+        ]\n     low_idx = [i for i in _low_idx if re.search(r\"\\w\", tokens[i])]\n     if len(low_idx) == 0:\n         return []"
      },
      {
        "filename": "libs/langchain/langchain/chains/hyde/base.py",
        "content_before": "\"\"\"Hypothetical Document Embeddings.\n\nhttps://arxiv.org/abs/2212.10496\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nfrom langchain_core.callbacks import CallbackManagerForChainRun\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.language_models import BaseLanguageModel\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import BasePromptTemplate\nfrom langchain_core.runnables import Runnable\nfrom pydantic import ConfigDict\n\nfrom langchain.chains.base import Chain\nfrom langchain.chains.hyde.prompts import PROMPT_MAP\nfrom langchain.chains.llm import LLMChain\n\n\nclass HypotheticalDocumentEmbedder(Chain, Embeddings):\n    \"\"\"Generate hypothetical document for query, and then embed that.\n\n    Based on https://arxiv.org/abs/2212.10496\n    \"\"\"\n\n    base_embeddings: Embeddings\n    llm_chain: Runnable\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n        extra=\"forbid\",\n    )\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Input keys for Hyde's LLM chain.\"\"\"\n        return self.llm_chain.input_schema.model_json_schema()[\"required\"]\n\n    @property\n    def output_keys(self) -> List[str]:\n        \"\"\"Output keys for Hyde's LLM chain.\"\"\"\n        if isinstance(self.llm_chain, LLMChain):\n            return self.llm_chain.output_keys\n        else:\n            return [\"text\"]\n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Call the base embeddings.\"\"\"\n        return self.base_embeddings.embed_documents(texts)\n\n    def combine_embeddings(self, embeddings: List[List[float]]) -> List[float]:\n        \"\"\"Combine embeddings into final embeddings.\"\"\"\n        return list(np.array(embeddings).mean(axis=0))\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Generate a hypothetical document and embedded it.\"\"\"\n        var_name = self.input_keys[0]\n        result = self.llm_chain.invoke({var_name: text})\n        if isinstance(self.llm_chain, LLMChain):\n            documents = [result[self.output_keys[0]]]\n        else:\n            documents = [result]\n        embeddings = self.embed_documents(documents)\n        return self.combine_embeddings(embeddings)\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, str]:\n        \"\"\"Call the internal llm chain.\"\"\"\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        return self.llm_chain.invoke(\n            inputs, config={\"callbacks\": _run_manager.get_child()}\n        )\n\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        base_embeddings: Embeddings,\n        prompt_key: Optional[str] = None,\n        custom_prompt: Optional[BasePromptTemplate] = None,\n        **kwargs: Any,\n    ) -> HypotheticalDocumentEmbedder:\n        \"\"\"Load and use LLMChain with either a specific prompt key or custom prompt.\"\"\"\n        if custom_prompt is not None:\n            prompt = custom_prompt\n        elif prompt_key is not None and prompt_key in PROMPT_MAP:\n            prompt = PROMPT_MAP[prompt_key]\n        else:\n            raise ValueError(\n                f\"Must specify prompt_key if custom_prompt not provided. Should be one \"\n                f\"of {list(PROMPT_MAP.keys())}.\"\n            )\n\n        llm_chain = prompt | llm | StrOutputParser()\n        return cls(base_embeddings=base_embeddings, llm_chain=llm_chain, **kwargs)\n\n    @property\n    def _chain_type(self) -> str:\n        return \"hyde_chain\"\n",
        "patch": "@@ -5,9 +5,9 @@\n \n from __future__ import annotations\n \n+import logging\n from typing import Any, Dict, List, Optional\n \n-import numpy as np\n from langchain_core.callbacks import CallbackManagerForChainRun\n from langchain_core.embeddings import Embeddings\n from langchain_core.language_models import BaseLanguageModel\n@@ -20,6 +20,8 @@\n from langchain.chains.hyde.prompts import PROMPT_MAP\n from langchain.chains.llm import LLMChain\n \n+logger = logging.getLogger(__name__)\n+\n \n class HypotheticalDocumentEmbedder(Chain, Embeddings):\n     \"\"\"Generate hypothetical document for query, and then embed that.\n@@ -54,7 +56,22 @@ def embed_documents(self, texts: List[str]) -> List[List[float]]:\n \n     def combine_embeddings(self, embeddings: List[List[float]]) -> List[float]:\n         \"\"\"Combine embeddings into final embeddings.\"\"\"\n-        return list(np.array(embeddings).mean(axis=0))\n+        try:\n+            import numpy as np\n+\n+            return list(np.array(embeddings).mean(axis=0))\n+        except ImportError:\n+            logger.warning(\n+                \"NumPy not found in the current Python environment. \"\n+                \"HypotheticalDocumentEmbedder will use a pure Python implementation \"\n+                \"for internal calculations, which may significantly impact \"\n+                \"performance, especially for large datasets. For optimal speed and \"\n+                \"efficiency, consider installing NumPy: pip install numpy\"\n+            )\n+            if not embeddings:\n+                return []\n+            num_vectors = len(embeddings)\n+            return [sum(dim_values) / num_vectors for dim_values in zip(*embeddings)]\n \n     def embed_query(self, text: str) -> List[float]:\n         \"\"\"Generate a hypothetical document and embedded it.\"\"\""
      },
      {
        "filename": "libs/langchain/langchain/evaluation/embedding_distance/base.py",
        "content_before": "\"\"\"A chain for comparing the output of two models using embeddings.\"\"\"\n\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nfrom langchain_core.callbacks.manager import (\n    AsyncCallbackManagerForChainRun,\n    CallbackManagerForChainRun,\n    Callbacks,\n)\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.utils import pre_init\nfrom pydantic import ConfigDict, Field\n\nfrom langchain.chains.base import Chain\nfrom langchain.evaluation.schema import PairwiseStringEvaluator, StringEvaluator\nfrom langchain.schema import RUN_KEY\n\n\ndef _embedding_factory() -> Embeddings:\n    \"\"\"Create an Embeddings object.\n    Returns:\n        Embeddings: The created Embeddings object.\n    \"\"\"\n    # Here for backwards compatibility.\n    # Generally, we do not want to be seeing imports from langchain community\n    # or partner packages in langchain.\n    try:\n        from langchain_openai import OpenAIEmbeddings\n    except ImportError:\n        try:\n            from langchain_community.embeddings.openai import (  # type: ignore[no-redef]\n                OpenAIEmbeddings,\n            )\n        except ImportError:\n            raise ImportError(\n                \"Could not import OpenAIEmbeddings. Please install the \"\n                \"OpenAIEmbeddings package using `pip install langchain-openai`.\"\n            )\n    return OpenAIEmbeddings()\n\n\nclass EmbeddingDistance(str, Enum):\n    \"\"\"Embedding Distance Metric.\n\n    Attributes:\n        COSINE: Cosine distance metric.\n        EUCLIDEAN: Euclidean distance metric.\n        MANHATTAN: Manhattan distance metric.\n        CHEBYSHEV: Chebyshev distance metric.\n        HAMMING: Hamming distance metric.\n    \"\"\"\n\n    COSINE = \"cosine\"\n    EUCLIDEAN = \"euclidean\"\n    MANHATTAN = \"manhattan\"\n    CHEBYSHEV = \"chebyshev\"\n    HAMMING = \"hamming\"\n\n\nclass _EmbeddingDistanceChainMixin(Chain):\n    \"\"\"Shared functionality for embedding distance evaluators.\n\n    Attributes:\n        embeddings (Embeddings): The embedding objects to vectorize the outputs.\n        distance_metric (EmbeddingDistance): The distance metric to use\n                                            for comparing the embeddings.\n    \"\"\"\n\n    embeddings: Embeddings = Field(default_factory=_embedding_factory)\n    distance_metric: EmbeddingDistance = Field(default=EmbeddingDistance.COSINE)\n\n    @pre_init\n    def _validate_tiktoken_installed(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Validate that the TikTok library is installed.\n\n        Args:\n            values (Dict[str, Any]): The values to validate.\n\n        Returns:\n            Dict[str, Any]: The validated values.\n        \"\"\"\n        embeddings = values.get(\"embeddings\")\n        types_ = []\n        try:\n            from langchain_openai import OpenAIEmbeddings\n\n            types_.append(OpenAIEmbeddings)\n        except ImportError:\n            pass\n\n        try:\n            from langchain_community.embeddings.openai import (  # type: ignore[no-redef]\n                OpenAIEmbeddings,\n            )\n\n            types_.append(OpenAIEmbeddings)\n        except ImportError:\n            pass\n\n        if not types_:\n            raise ImportError(\n                \"Could not import OpenAIEmbeddings. Please install the \"\n                \"OpenAIEmbeddings package using `pip install langchain-openai`.\"\n            )\n\n        if isinstance(embeddings, tuple(types_)):\n            try:\n                import tiktoken  # noqa: F401\n            except ImportError:\n                raise ImportError(\n                    \"The tiktoken library is required to use the default \"\n                    \"OpenAI embeddings with embedding distance evaluators.\"\n                    \" Please either manually select a different Embeddings object\"\n                    \" or install tiktoken using `pip install tiktoken`.\"\n                )\n        return values\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    def output_keys(self) -> List[str]:\n        \"\"\"Return the output keys of the chain.\n\n        Returns:\n            List[str]: The output keys.\n        \"\"\"\n        return [\"score\"]\n\n    def _prepare_output(self, result: dict) -> dict:\n        parsed = {\"score\": result[\"score\"]}\n        if RUN_KEY in result:\n            parsed[RUN_KEY] = result[RUN_KEY]\n        return parsed\n\n    def _get_metric(self, metric: EmbeddingDistance) -> Any:\n        \"\"\"Get the metric function for the given metric name.\n\n        Args:\n            metric (EmbeddingDistance): The metric name.\n\n        Returns:\n            Any: The metric function.\n        \"\"\"\n        metrics = {\n            EmbeddingDistance.COSINE: self._cosine_distance,\n            EmbeddingDistance.EUCLIDEAN: self._euclidean_distance,\n            EmbeddingDistance.MANHATTAN: self._manhattan_distance,\n            EmbeddingDistance.CHEBYSHEV: self._chebyshev_distance,\n            EmbeddingDistance.HAMMING: self._hamming_distance,\n        }\n        if metric in metrics:\n            return metrics[metric]\n        else:\n            raise ValueError(f\"Invalid metric: {metric}\")\n\n    @staticmethod\n    def _cosine_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n        \"\"\"Compute the cosine distance between two vectors.\n\n        Args:\n            a (np.ndarray): The first vector.\n            b (np.ndarray): The second vector.\n\n        Returns:\n            np.ndarray: The cosine distance.\n        \"\"\"\n        try:\n            from langchain_community.utils.math import cosine_similarity\n        except ImportError:\n            raise ImportError(\n                \"The cosine_similarity function is required to compute cosine distance.\"\n                \" Please install the langchain-community package using\"\n                \" `pip install langchain-community`.\"\n            )\n        return 1.0 - cosine_similarity(a, b)\n\n    @staticmethod\n    def _euclidean_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n        \"\"\"Compute the Euclidean distance between two vectors.\n\n        Args:\n            a (np.ndarray): The first vector.\n            b (np.ndarray): The second vector.\n\n        Returns:\n            np.floating: The Euclidean distance.\n        \"\"\"\n        return np.linalg.norm(a - b)\n\n    @staticmethod\n    def _manhattan_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n        \"\"\"Compute the Manhattan distance between two vectors.\n\n        Args:\n            a (np.ndarray): The first vector.\n            b (np.ndarray): The second vector.\n\n        Returns:\n            np.floating: The Manhattan distance.\n        \"\"\"\n        return np.sum(np.abs(a - b))\n\n    @staticmethod\n    def _chebyshev_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n        \"\"\"Compute the Chebyshev distance between two vectors.\n\n        Args:\n            a (np.ndarray): The first vector.\n            b (np.ndarray): The second vector.\n\n        Returns:\n            np.floating: The Chebyshev distance.\n        \"\"\"\n        return np.max(np.abs(a - b))\n\n    @staticmethod\n    def _hamming_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n        \"\"\"Compute the Hamming distance between two vectors.\n\n        Args:\n            a (np.ndarray): The first vector.\n            b (np.ndarray): The second vector.\n\n        Returns:\n            np.floating: The Hamming distance.\n        \"\"\"\n        return np.mean(a != b)\n\n    def _compute_score(self, vectors: np.ndarray) -> float:\n        \"\"\"Compute the score based on the distance metric.\n\n        Args:\n            vectors (np.ndarray): The input vectors.\n\n        Returns:\n            float: The computed score.\n        \"\"\"\n        metric = self._get_metric(self.distance_metric)\n        score = metric(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1)).item()\n        return score\n\n\nclass EmbeddingDistanceEvalChain(_EmbeddingDistanceChainMixin, StringEvaluator):\n    \"\"\"Use embedding distances to score semantic difference between\n    a prediction and reference.\n\n    Examples:\n        >>> chain = EmbeddingDistanceEvalChain()\n        >>> result = chain.evaluate_strings(prediction=\"Hello\", reference=\"Hi\")\n        >>> print(result)\n        {'score': 0.5}\n    \"\"\"\n\n    @property\n    def requires_reference(self) -> bool:\n        \"\"\"Return whether the chain requires a reference.\n\n        Returns:\n            bool: True if a reference is required, False otherwise.\n        \"\"\"\n        return True\n\n    @property\n    def evaluation_name(self) -> str:\n        return f\"embedding_{self.distance_metric.value}_distance\"\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Return the input keys of the chain.\n\n        Returns:\n            List[str]: The input keys.\n        \"\"\"\n        return [\"prediction\", \"reference\"]\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Compute the score for a prediction and reference.\n\n        Args:\n            inputs (Dict[str, Any]): The input data.\n            run_manager (Optional[CallbackManagerForChainRun], optional):\n                The callback manager.\n\n        Returns:\n            Dict[str, Any]: The computed score.\n        \"\"\"\n        vectors = np.array(\n            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\n        )\n        score = self._compute_score(vectors)\n        return {\"score\": score}\n\n    async def _acall(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Asynchronously compute the score for a prediction and reference.\n\n        Args:\n            inputs (Dict[str, Any]): The input data.\n            run_manager (AsyncCallbackManagerForChainRun, optional):\n                The callback manager.\n\n        Returns:\n            Dict[str, Any]: The computed score.\n        \"\"\"\n        embedded = await self.embeddings.aembed_documents(\n            [\n                inputs[\"prediction\"],\n                inputs[\"reference\"],\n            ]\n        )\n        vectors = np.array(embedded)\n        score = self._compute_score(vectors)\n        return {\"score\": score}\n\n    def _evaluate_strings(\n        self,\n        *,\n        prediction: str,\n        reference: Optional[str] = None,\n        callbacks: Callbacks = None,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        include_run_info: bool = False,\n        **kwargs: Any,\n    ) -> dict:\n        \"\"\"Evaluate the embedding distance between a prediction and\n        reference.\n\n        Args:\n            prediction (str): The output string from the first model.\n            reference (str): The reference string (required)\n            callbacks (Callbacks, optional): The callbacks to use.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing:\n                - score: The embedding distance between the two\n                    predictions.\n        \"\"\"\n        result = self(\n            inputs={\"prediction\": prediction, \"reference\": reference},\n            callbacks=callbacks,\n            tags=tags,\n            metadata=metadata,\n            include_run_info=include_run_info,\n        )\n        return self._prepare_output(result)\n\n    async def _aevaluate_strings(\n        self,\n        *,\n        prediction: str,\n        reference: Optional[str] = None,\n        callbacks: Callbacks = None,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        include_run_info: bool = False,\n        **kwargs: Any,\n    ) -> dict:\n        \"\"\"Asynchronously evaluate the embedding distance between\n        a prediction and reference.\n\n        Args:\n            prediction (str): The output string from the first model.\n            reference (str): The output string from the second model.\n            callbacks (Callbacks, optional): The callbacks to use.\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing:\n                - score: The embedding distance between the two\n                    predictions.\n        \"\"\"\n        result = await self.acall(\n            inputs={\"prediction\": prediction, \"reference\": reference},\n            callbacks=callbacks,\n            tags=tags,\n            metadata=metadata,\n            include_run_info=include_run_info,\n        )\n        return self._prepare_output(result)\n\n\nclass PairwiseEmbeddingDistanceEvalChain(\n    _EmbeddingDistanceChainMixin, PairwiseStringEvaluator\n):\n    \"\"\"Use embedding distances to score semantic difference between two predictions.\n\n    Examples:\n    >>> chain = PairwiseEmbeddingDistanceEvalChain()\n    >>> result = chain.evaluate_string_pairs(prediction=\"Hello\", prediction_b=\"Hi\")\n    >>> print(result)\n    {'score': 0.5}\n    \"\"\"\n\n    @property\n    def input_keys(self) -> List[str]:\n        \"\"\"Return the input keys of the chain.\n\n        Returns:\n            List[str]: The input keys.\n        \"\"\"\n        return [\"prediction\", \"prediction_b\"]\n\n    @property\n    def evaluation_name(self) -> str:\n        return f\"pairwise_embedding_{self.distance_metric.value}_distance\"\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Compute the score for two predictions.\n\n        Args:\n            inputs (Dict[str, Any]): The input data.\n            run_manager (CallbackManagerForChainRun, optional):\n                The callback manager.\n\n        Returns:\n            Dict[str, Any]: The computed score.\n        \"\"\"\n        vectors = np.array(\n            self.embeddings.embed_documents(\n                [\n                    inputs[\"prediction\"],\n                    inputs[\"prediction_b\"],\n                ]\n            )\n        )\n        score = self._compute_score(vectors)\n        return {\"score\": score}\n\n    async def _acall(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Asynchronously compute the score for two predictions.\n\n        Args:\n            inputs (Dict[str, Any]): The input data.\n            run_manager (AsyncCallbackManagerForChainRun, optional):\n                The callback manager.\n\n        Returns:\n            Dict[str, Any]: The computed score.\n        \"\"\"\n        embedded = await self.embeddings.aembed_documents(\n            [\n                inputs[\"prediction\"],\n                inputs[\"prediction_b\"],\n            ]\n        )\n        vectors = np.array(embedded)\n        score = self._compute_score(vectors)\n        return {\"score\": score}\n\n    def _evaluate_string_pairs(\n        self,\n        *,\n        prediction: str,\n        prediction_b: str,\n        callbacks: Callbacks = None,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        include_run_info: bool = False,\n        **kwargs: Any,\n    ) -> dict:\n        \"\"\"Evaluate the embedding distance between two predictions.\n\n        Args:\n            prediction (str): The output string from the first model.\n            prediction_b (str): The output string from the second model.\n            callbacks (Callbacks, optional): The callbacks to use.\n            tags (List[str], optional): Tags to apply to traces\n            metadata (Dict[str, Any], optional): metadata to apply to\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing:\n                - score: The embedding distance between the two\n                    predictions.\n        \"\"\"\n        result = self(\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\n            callbacks=callbacks,\n            tags=tags,\n            metadata=metadata,\n            include_run_info=include_run_info,\n        )\n        return self._prepare_output(result)\n\n    async def _aevaluate_string_pairs(\n        self,\n        *,\n        prediction: str,\n        prediction_b: str,\n        callbacks: Callbacks = None,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        include_run_info: bool = False,\n        **kwargs: Any,\n    ) -> dict:\n        \"\"\"Asynchronously evaluate the embedding distance\n\n        between two predictions.\n\n        Args:\n            prediction (str): The output string from the first model.\n            prediction_b (str): The output string from the second model.\n            callbacks (Callbacks, optional): The callbacks to use.\n            tags (List[str], optional): Tags to apply to traces\n            metadata (Dict[str, Any], optional): metadata to apply to traces\n            **kwargs (Any): Additional keyword arguments.\n\n        Returns:\n            dict: A dictionary containing:\n                - score: The embedding distance between the two\n                    predictions.\n        \"\"\"\n        result = await self.acall(\n            inputs={\"prediction\": prediction, \"prediction_b\": prediction_b},\n            callbacks=callbacks,\n            tags=tags,\n            metadata=metadata,\n            include_run_info=include_run_info,\n        )\n        return self._prepare_output(result)\n",
        "patch": "@@ -1,9 +1,11 @@\n \"\"\"A chain for comparing the output of two models using embeddings.\"\"\"\n \n+import functools\n+import logging\n from enum import Enum\n+from importlib import util\n from typing import Any, Dict, List, Optional\n \n-import numpy as np\n from langchain_core.callbacks.manager import (\n     AsyncCallbackManagerForChainRun,\n     CallbackManagerForChainRun,\n@@ -18,6 +20,34 @@\n from langchain.schema import RUN_KEY\n \n \n+def _import_numpy() -> Any:\n+    try:\n+        import numpy as np\n+\n+        return np\n+    except ImportError as e:\n+        raise ImportError(\n+            \"Could not import numpy, please install with `pip install numpy`.\"\n+        ) from e\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@functools.lru_cache(maxsize=1)\n+def _check_numpy() -> bool:\n+    if bool(util.find_spec(\"numpy\")):\n+        return True\n+    logger.warning(\n+        \"NumPy not found in the current Python environment. \"\n+        \"langchain will use a pure Python implementation for embedding distance \"\n+        \"operations, which may significantly impact performance, especially for large \"\n+        \"datasets. For optimal speed and efficiency, consider installing NumPy: \"\n+        \"pip install numpy\"\n+    )\n+    return False\n+\n+\n def _embedding_factory() -> Embeddings:\n     \"\"\"Create an Embeddings object.\n     Returns:\n@@ -158,7 +188,7 @@ def _get_metric(self, metric: EmbeddingDistance) -> Any:\n             raise ValueError(f\"Invalid metric: {metric}\")\n \n     @staticmethod\n-    def _cosine_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n+    def _cosine_distance(a: Any, b: Any) -> Any:\n         \"\"\"Compute the cosine distance between two vectors.\n \n         Args:\n@@ -179,7 +209,7 @@ def _cosine_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n         return 1.0 - cosine_similarity(a, b)\n \n     @staticmethod\n-    def _euclidean_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n+    def _euclidean_distance(a: Any, b: Any) -> Any:\n         \"\"\"Compute the Euclidean distance between two vectors.\n \n         Args:\n@@ -189,10 +219,15 @@ def _euclidean_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n         Returns:\n             np.floating: The Euclidean distance.\n         \"\"\"\n-        return np.linalg.norm(a - b)\n+        if _check_numpy():\n+            import numpy as np\n+\n+            return np.linalg.norm(a - b)\n+\n+        return sum((x - y) * (x - y) for x, y in zip(a, b)) ** 0.5\n \n     @staticmethod\n-    def _manhattan_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n+    def _manhattan_distance(a: Any, b: Any) -> Any:\n         \"\"\"Compute the Manhattan distance between two vectors.\n \n         Args:\n@@ -202,10 +237,14 @@ def _manhattan_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n         Returns:\n             np.floating: The Manhattan distance.\n         \"\"\"\n-        return np.sum(np.abs(a - b))\n+        if _check_numpy():\n+            np = _import_numpy()\n+            return np.sum(np.abs(a - b))\n+\n+        return sum(abs(x - y) for x, y in zip(a, b))\n \n     @staticmethod\n-    def _chebyshev_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n+    def _chebyshev_distance(a: Any, b: Any) -> Any:\n         \"\"\"Compute the Chebyshev distance between two vectors.\n \n         Args:\n@@ -215,10 +254,14 @@ def _chebyshev_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n         Returns:\n             np.floating: The Chebyshev distance.\n         \"\"\"\n-        return np.max(np.abs(a - b))\n+        if _check_numpy():\n+            np = _import_numpy()\n+            return np.max(np.abs(a - b))\n+\n+        return max(abs(x - y) for x, y in zip(a, b))\n \n     @staticmethod\n-    def _hamming_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n+    def _hamming_distance(a: Any, b: Any) -> Any:\n         \"\"\"Compute the Hamming distance between two vectors.\n \n         Args:\n@@ -228,9 +271,13 @@ def _hamming_distance(a: np.ndarray, b: np.ndarray) -> np.floating:\n         Returns:\n             np.floating: The Hamming distance.\n         \"\"\"\n-        return np.mean(a != b)\n+        if _check_numpy():\n+            np = _import_numpy()\n+            return np.mean(a != b)\n \n-    def _compute_score(self, vectors: np.ndarray) -> float:\n+        return sum(1 for x, y in zip(a, b) if x != y) / len(a)\n+\n+    def _compute_score(self, vectors: Any) -> float:\n         \"\"\"Compute the score based on the distance metric.\n \n         Args:\n@@ -240,8 +287,11 @@ def _compute_score(self, vectors: np.ndarray) -> float:\n             float: The computed score.\n         \"\"\"\n         metric = self._get_metric(self.distance_metric)\n-        score = metric(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1)).item()\n-        return score\n+        if _check_numpy() and isinstance(vectors, _import_numpy().ndarray):\n+            score = metric(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1)).item()\n+        else:\n+            score = metric(vectors[0], vectors[1])\n+        return float(score)\n \n \n class EmbeddingDistanceEvalChain(_EmbeddingDistanceChainMixin, StringEvaluator):\n@@ -292,9 +342,12 @@ def _call(\n         Returns:\n             Dict[str, Any]: The computed score.\n         \"\"\"\n-        vectors = np.array(\n-            self.embeddings.embed_documents([inputs[\"prediction\"], inputs[\"reference\"]])\n+        vectors = self.embeddings.embed_documents(\n+            [inputs[\"prediction\"], inputs[\"reference\"]]\n         )\n+        if _check_numpy():\n+            np = _import_numpy()\n+            vectors = np.array(vectors)\n         score = self._compute_score(vectors)\n         return {\"score\": score}\n \n@@ -313,13 +366,15 @@ async def _acall(\n         Returns:\n             Dict[str, Any]: The computed score.\n         \"\"\"\n-        embedded = await self.embeddings.aembed_documents(\n+        vectors = await self.embeddings.aembed_documents(\n             [\n                 inputs[\"prediction\"],\n                 inputs[\"reference\"],\n             ]\n         )\n-        vectors = np.array(embedded)\n+        if _check_numpy():\n+            np = _import_numpy()\n+            vectors = np.array(vectors)\n         score = self._compute_score(vectors)\n         return {\"score\": score}\n \n@@ -432,14 +487,15 @@ def _call(\n         Returns:\n             Dict[str, Any]: The computed score.\n         \"\"\"\n-        vectors = np.array(\n-            self.embeddings.embed_documents(\n-                [\n-                    inputs[\"prediction\"],\n-                    inputs[\"prediction_b\"],\n-                ]\n-            )\n+        vectors = self.embeddings.embed_documents(\n+            [\n+                inputs[\"prediction\"],\n+                inputs[\"prediction_b\"],\n+            ]\n         )\n+        if _check_numpy():\n+            np = _import_numpy()\n+            vectors = np.array(vectors)\n         score = self._compute_score(vectors)\n         return {\"score\": score}\n \n@@ -458,13 +514,15 @@ async def _acall(\n         Returns:\n             Dict[str, Any]: The computed score.\n         \"\"\"\n-        embedded = await self.embeddings.aembed_documents(\n+        vectors = await self.embeddings.aembed_documents(\n             [\n                 inputs[\"prediction\"],\n                 inputs[\"prediction_b\"],\n             ]\n         )\n-        vectors = np.array(embedded)\n+        if _check_numpy():\n+            np = _import_numpy()\n+            vectors = np.array(vectors)\n         score = self._compute_score(vectors)\n         return {\"score\": score}\n "
      },
      {
        "filename": "libs/langchain/langchain/retrievers/document_compressors/embeddings_filter.py",
        "content_before": "from typing import Callable, Dict, Optional, Sequence\n\nimport numpy as np\nfrom langchain_core.callbacks.manager import Callbacks\nfrom langchain_core.documents import Document\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.utils import pre_init\nfrom pydantic import ConfigDict, Field\n\nfrom langchain.retrievers.document_compressors.base import (\n    BaseDocumentCompressor,\n)\n\n\ndef _get_similarity_function() -> Callable:\n    try:\n        from langchain_community.utils.math import cosine_similarity\n    except ImportError:\n        raise ImportError(\n            \"To use please install langchain-community \"\n            \"with `pip install langchain-community`.\"\n        )\n    return cosine_similarity\n\n\nclass EmbeddingsFilter(BaseDocumentCompressor):\n    \"\"\"Document compressor that uses embeddings to drop documents\n    unrelated to the query.\"\"\"\n\n    embeddings: Embeddings\n    \"\"\"Embeddings to use for embedding document contents and queries.\"\"\"\n    similarity_fn: Callable = Field(default_factory=_get_similarity_function)\n    \"\"\"Similarity function for comparing documents. Function expected to take as input\n    two matrices (List[List[float]]) and return a matrix of scores where higher values\n    indicate greater similarity.\"\"\"\n    k: Optional[int] = 20\n    \"\"\"The number of relevant documents to return. Can be set to None, in which case\n    `similarity_threshold` must be specified. Defaults to 20.\"\"\"\n    similarity_threshold: Optional[float] = None\n    \"\"\"Threshold for determining when two documents are similar enough\n    to be considered redundant. Defaults to None, must be specified if `k` is set\n    to None.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @pre_init\n    def validate_params(cls, values: Dict) -> Dict:\n        \"\"\"Validate similarity parameters.\"\"\"\n        if values[\"k\"] is None and values[\"similarity_threshold\"] is None:\n            raise ValueError(\"Must specify one of `k` or `similarity_threshold`.\")\n        return values\n\n    def compress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[Callbacks] = None,\n    ) -> Sequence[Document]:\n        \"\"\"Filter documents based on similarity of their embeddings to the query.\"\"\"\n        try:\n            from langchain_community.document_transformers.embeddings_redundant_filter import (  # noqa: E501\n                _get_embeddings_from_stateful_docs,\n                get_stateful_documents,\n            )\n        except ImportError:\n            raise ImportError(\n                \"To use please install langchain-community \"\n                \"with `pip install langchain-community`.\"\n            )\n        stateful_documents = get_stateful_documents(documents)\n        embedded_documents = _get_embeddings_from_stateful_docs(\n            self.embeddings, stateful_documents\n        )\n        embedded_query = self.embeddings.embed_query(query)\n        similarity = self.similarity_fn([embedded_query], embedded_documents)[0]\n        included_idxs: np.ndarray = np.arange(len(embedded_documents))\n        if self.k is not None:\n            included_idxs = np.argsort(similarity)[::-1][: self.k]\n        if self.similarity_threshold is not None:\n            similar_enough = np.where(\n                similarity[included_idxs] > self.similarity_threshold\n            )\n            included_idxs = included_idxs[similar_enough]\n        for i in included_idxs:\n            stateful_documents[i].state[\"query_similarity_score\"] = similarity[i]\n        return [stateful_documents[i] for i in included_idxs]\n\n    async def acompress_documents(\n        self,\n        documents: Sequence[Document],\n        query: str,\n        callbacks: Optional[Callbacks] = None,\n    ) -> Sequence[Document]:\n        \"\"\"Filter documents based on similarity of their embeddings to the query.\"\"\"\n        try:\n            from langchain_community.document_transformers.embeddings_redundant_filter import (  # noqa: E501\n                _aget_embeddings_from_stateful_docs,\n                get_stateful_documents,\n            )\n        except ImportError:\n            raise ImportError(\n                \"To use please install langchain-community \"\n                \"with `pip install langchain-community`.\"\n            )\n        stateful_documents = get_stateful_documents(documents)\n        embedded_documents = await _aget_embeddings_from_stateful_docs(\n            self.embeddings, stateful_documents\n        )\n        embedded_query = await self.embeddings.aembed_query(query)\n        similarity = self.similarity_fn([embedded_query], embedded_documents)[0]\n        included_idxs: np.ndarray = np.arange(len(embedded_documents))\n        if self.k is not None:\n            included_idxs = np.argsort(similarity)[::-1][: self.k]\n        if self.similarity_threshold is not None:\n            similar_enough = np.where(\n                similarity[included_idxs] > self.similarity_threshold\n            )\n            included_idxs = included_idxs[similar_enough]\n        for i in included_idxs:\n            stateful_documents[i].state[\"query_similarity_score\"] = similarity[i]\n        return [stateful_documents[i] for i in included_idxs]\n",
        "patch": "@@ -1,6 +1,5 @@\n from typing import Callable, Dict, Optional, Sequence\n \n-import numpy as np\n from langchain_core.callbacks.manager import Callbacks\n from langchain_core.documents import Document\n from langchain_core.embeddings import Embeddings\n@@ -69,6 +68,13 @@ def compress_documents(\n                 \"To use please install langchain-community \"\n                 \"with `pip install langchain-community`.\"\n             )\n+\n+        try:\n+            import numpy as np\n+        except ImportError as e:\n+            raise ImportError(\n+                \"Could not import numpy, please install with `pip install numpy`.\"\n+            ) from e\n         stateful_documents = get_stateful_documents(documents)\n         embedded_documents = _get_embeddings_from_stateful_docs(\n             self.embeddings, stateful_documents\n@@ -104,6 +110,13 @@ async def acompress_documents(\n                 \"To use please install langchain-community \"\n                 \"with `pip install langchain-community`.\"\n             )\n+\n+        try:\n+            import numpy as np\n+        except ImportError as e:\n+            raise ImportError(\n+                \"Could not import numpy, please install with `pip install numpy`.\"\n+            ) from e\n         stateful_documents = get_stateful_documents(documents)\n         embedded_documents = await _aget_embeddings_from_stateful_docs(\n             self.embeddings, stateful_documents"
      },
      {
        "filename": "libs/langchain/tests/unit_tests/test_dependencies.py",
        "content_before": "\"\"\"A unit test meant to catch accidental introduction of non-optional dependencies.\"\"\"\n\nfrom pathlib import Path\nfrom typing import Any, Dict, Mapping\n\nimport pytest\nimport toml\nfrom packaging.requirements import Requirement\n\nHERE = Path(__file__).parent\n\nPYPROJECT_TOML = HERE / \"../../pyproject.toml\"\n\n\n@pytest.fixture()\ndef uv_conf() -> Dict[str, Any]:\n    \"\"\"Load the pyproject.toml file.\"\"\"\n    with open(PYPROJECT_TOML) as f:\n        return toml.load(f)\n\n\ndef test_required_dependencies(uv_conf: Mapping[str, Any]) -> None:\n    \"\"\"A test that checks if a new non-optional dependency is being introduced.\n\n    If this test is triggered, it means that a contributor is trying to introduce a new\n    required dependency. This should be avoided in most situations.\n    \"\"\"\n    # Get the dependencies from the [tool.poetry.dependencies] section\n    dependencies = uv_conf[\"project\"][\"dependencies\"]\n    required_dependencies = set(Requirement(dep).name for dep in dependencies)\n\n    assert sorted(required_dependencies) == sorted(\n        [\n            \"PyYAML\",\n            \"SQLAlchemy\",\n            \"async-timeout\",\n            \"langchain-core\",\n            \"langchain-text-splitters\",\n            \"langsmith\",\n            \"numpy\",\n            \"pydantic\",\n            \"requests\",\n        ]\n    )\n\n\ndef test_test_group_dependencies(uv_conf: Mapping[str, Any]) -> None:\n    \"\"\"Check if someone is attempting to add additional test dependencies.\n\n    Only dependencies associated with test running infrastructure should be added\n    to the test group; e.g., pytest, pytest-cov etc.\n\n    Examples of dependencies that should NOT be included: boto3, azure, postgres, etc.\n    \"\"\"\n\n    dependencies = uv_conf[\"dependency-groups\"][\"test\"]\n    test_group_deps = set(Requirement(dep).name for dep in dependencies)\n\n    assert sorted(test_group_deps) == sorted(\n        [\n            \"duckdb-engine\",\n            \"freezegun\",\n            \"langchain-core\",\n            \"langchain-tests\",\n            \"langchain-text-splitters\",\n            \"langchain-openai\",\n            \"lark\",\n            \"packaging\",\n            \"pandas\",\n            \"pytest\",\n            \"pytest-asyncio\",\n            \"pytest-cov\",\n            \"pytest-dotenv\",\n            \"pytest-mock\",\n            \"pytest-socket\",\n            \"pytest-watcher\",\n            \"pytest-xdist\",\n            \"blockbuster\",\n            \"responses\",\n            \"syrupy\",\n            \"toml\",\n            \"requests-mock\",\n            # TODO: temporary hack since cffi 1.17.1 doesn't work with py 3.9.\n            \"cffi\",\n        ]\n    )\n",
        "patch": "@@ -37,7 +37,6 @@ def test_required_dependencies(uv_conf: Mapping[str, Any]) -> None:\n             \"langchain-core\",\n             \"langchain-text-splitters\",\n             \"langsmith\",\n-            \"numpy\",\n             \"pydantic\",\n             \"requests\",\n         ]\n@@ -82,5 +81,6 @@ def test_test_group_dependencies(uv_conf: Mapping[str, Any]) -> None:\n             \"requests-mock\",\n             # TODO: temporary hack since cffi 1.17.1 doesn't work with py 3.9.\n             \"cffi\",\n+            \"numpy\",\n         ]\n     )"
      }
    ]
  },
  {
    "number": 29995,
    "title": "community: add title, score and raw_content to tavily search results",
    "body": "**Description:**\r\n\r\nTavily search results returned from API include useful information like title, score and (optionally) raw_content that is missed in wrapper although it's documented there properly. Add this data  to the result structure.\r\n",
    "issue_title": "community: add title, score and raw_content to tavily search results",
    "issue_body": "**Description:**\r\n\r\nTavily search results returned from API include useful information like title, score and (optionally) raw_content that is missed in wrapper although it's documented there properly. Add this data  to the result structure.\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/utilities/tavily_search.py",
        "content_before": "\"\"\"Util that calls Tavily Search API.\n\nIn order to set this up, follow instructions at:\nhttps://docs.tavily.com/docs/tavily-api/introduction\n\"\"\"\n\nimport json\nfrom typing import Any, Dict, List, Optional\n\nimport aiohttp\nimport requests\nfrom langchain_core.utils import get_from_dict_or_env\nfrom pydantic import BaseModel, ConfigDict, SecretStr, model_validator\n\nTAVILY_API_URL = \"https://api.tavily.com\"\n\n\nclass TavilySearchAPIWrapper(BaseModel):\n    \"\"\"Wrapper for Tavily Search API.\"\"\"\n\n    tavily_api_key: SecretStr\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_environment(cls, values: Dict) -> Any:\n        \"\"\"Validate that api key and endpoint exists in environment.\"\"\"\n        tavily_api_key = get_from_dict_or_env(\n            values, \"tavily_api_key\", \"TAVILY_API_KEY\"\n        )\n        values[\"tavily_api_key\"] = tavily_api_key\n\n        return values\n\n    def raw_results(\n        self,\n        query: str,\n        max_results: Optional[int] = 5,\n        search_depth: Optional[str] = \"advanced\",\n        include_domains: Optional[List[str]] = [],\n        exclude_domains: Optional[List[str]] = [],\n        include_answer: Optional[bool] = False,\n        include_raw_content: Optional[bool] = False,\n        include_images: Optional[bool] = False,\n    ) -> Dict:\n        params = {\n            \"api_key\": self.tavily_api_key.get_secret_value(),\n            \"query\": query,\n            \"max_results\": max_results,\n            \"search_depth\": search_depth,\n            \"include_domains\": include_domains,\n            \"exclude_domains\": exclude_domains,\n            \"include_answer\": include_answer,\n            \"include_raw_content\": include_raw_content,\n            \"include_images\": include_images,\n        }\n        response = requests.post(\n            # type: ignore\n            f\"{TAVILY_API_URL}/search\",\n            json=params,\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def results(\n        self,\n        query: str,\n        max_results: Optional[int] = 5,\n        search_depth: Optional[str] = \"advanced\",\n        include_domains: Optional[List[str]] = [],\n        exclude_domains: Optional[List[str]] = [],\n        include_answer: Optional[bool] = False,\n        include_raw_content: Optional[bool] = False,\n        include_images: Optional[bool] = False,\n    ) -> List[Dict]:\n        \"\"\"Run query through Tavily Search and return metadata.\n\n        Args:\n            query: The query to search for.\n            max_results: The maximum number of results to return.\n            search_depth: The depth of the search. Can be \"basic\" or \"advanced\".\n            include_domains: A list of domains to include in the search.\n            exclude_domains: A list of domains to exclude from the search.\n            include_answer: Whether to include the answer in the results.\n            include_raw_content: Whether to include the raw content in the results.\n            include_images: Whether to include images in the results.\n        Returns:\n            query: The query that was searched for.\n            follow_up_questions: A list of follow up questions.\n            response_time: The response time of the query.\n            answer: The answer to the query.\n            images: A list of images.\n            results: A list of dictionaries containing the results:\n                title: The title of the result.\n                url: The url of the result.\n                content: The content of the result.\n                score: The score of the result.\n                raw_content: The raw content of the result.\n        \"\"\"\n        raw_search_results = self.raw_results(\n            query,\n            max_results=max_results,\n            search_depth=search_depth,\n            include_domains=include_domains,\n            exclude_domains=exclude_domains,\n            include_answer=include_answer,\n            include_raw_content=include_raw_content,\n            include_images=include_images,\n        )\n        return self.clean_results(raw_search_results[\"results\"])\n\n    async def raw_results_async(\n        self,\n        query: str,\n        max_results: Optional[int] = 5,\n        search_depth: Optional[str] = \"advanced\",\n        include_domains: Optional[List[str]] = [],\n        exclude_domains: Optional[List[str]] = [],\n        include_answer: Optional[bool] = False,\n        include_raw_content: Optional[bool] = False,\n        include_images: Optional[bool] = False,\n    ) -> Dict:\n        \"\"\"Get results from the Tavily Search API asynchronously.\"\"\"\n\n        # Function to perform the API call\n        async def fetch() -> str:\n            params = {\n                \"api_key\": self.tavily_api_key.get_secret_value(),\n                \"query\": query,\n                \"max_results\": max_results,\n                \"search_depth\": search_depth,\n                \"include_domains\": include_domains,\n                \"exclude_domains\": exclude_domains,\n                \"include_answer\": include_answer,\n                \"include_raw_content\": include_raw_content,\n                \"include_images\": include_images,\n            }\n            async with aiohttp.ClientSession() as session:\n                async with session.post(f\"{TAVILY_API_URL}/search\", json=params) as res:\n                    if res.status == 200:\n                        data = await res.text()\n                        return data\n                    else:\n                        raise Exception(f\"Error {res.status}: {res.reason}\")\n\n        results_json_str = await fetch()\n        return json.loads(results_json_str)\n\n    async def results_async(\n        self,\n        query: str,\n        max_results: Optional[int] = 5,\n        search_depth: Optional[str] = \"advanced\",\n        include_domains: Optional[List[str]] = [],\n        exclude_domains: Optional[List[str]] = [],\n        include_answer: Optional[bool] = False,\n        include_raw_content: Optional[bool] = False,\n        include_images: Optional[bool] = False,\n    ) -> List[Dict]:\n        results_json = await self.raw_results_async(\n            query=query,\n            max_results=max_results,\n            search_depth=search_depth,\n            include_domains=include_domains,\n            exclude_domains=exclude_domains,\n            include_answer=include_answer,\n            include_raw_content=include_raw_content,\n            include_images=include_images,\n        )\n        return self.clean_results(results_json[\"results\"])\n\n    def clean_results(self, results: List[Dict]) -> List[Dict]:\n        \"\"\"Clean results from Tavily Search API.\"\"\"\n        clean_results = []\n        for result in results:\n            clean_results.append(\n                {\n                    \"url\": result[\"url\"],\n                    \"content\": result[\"content\"],\n                }\n            )\n        return clean_results\n",
        "patch": "@@ -176,10 +176,13 @@ def clean_results(self, results: List[Dict]) -> List[Dict]:\n         \"\"\"Clean results from Tavily Search API.\"\"\"\n         clean_results = []\n         for result in results:\n-            clean_results.append(\n-                {\n-                    \"url\": result[\"url\"],\n-                    \"content\": result[\"content\"],\n-                }\n-            )\n+            clean_result = {\n+                \"title\": result[\"title\"],\n+                \"url\": result[\"url\"],\n+                \"content\": result[\"content\"],\n+                \"score\": result[\"score\"],\n+            }\n+            if raw_content := result.get(\"raw_content\"):\n+                clean_result[\"raw_content\"] = raw_content\n+            clean_results.append(clean_result)\n         return clean_results"
      }
    ]
  },
  {
    "number": 29976,
    "title": "core[patch]: allow passing description to @tool decorator",
    "body": null,
    "issue_title": "core[patch]: allow passing description to @tool decorator",
    "issue_body": null,
    "files": [
      {
        "filename": "libs/core/langchain_core/tools/base.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport uuid\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom contextvars import copy_context\nfrom inspect import signature\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    PydanticDeprecationWarning,\n    SkipValidation,\n    ValidationError,\n    model_validator,\n    validate_arguments,\n)\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom pydantic.v1 import ValidationError as ValidationErrorV1\nfrom pydantic.v1 import validate_arguments as validate_arguments_v1\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManager,\n    BaseCallbackManager,\n    CallbackManager,\n    Callbacks,\n)\nfrom langchain_core.messages.tool import ToolCall, ToolMessage, ToolOutputMixin\nfrom langchain_core.runnables import (\n    RunnableConfig,\n    RunnableSerializable,\n    ensure_config,\n    patch_config,\n    run_in_executor,\n)\nfrom langchain_core.runnables.config import _set_config_context\nfrom langchain_core.runnables.utils import asyncio_accepts_context\nfrom langchain_core.utils.function_calling import (\n    _parse_google_docstring,\n    _py_38_safe_origin,\n)\nfrom langchain_core.utils.pydantic import (\n    TypeBaseModel,\n    _create_subset_model,\n    get_fields,\n    is_basemodel_subclass,\n    is_pydantic_v1_subclass,\n    is_pydantic_v2_subclass,\n)\n\nFILTERED_ARGS = (\"run_manager\", \"callbacks\")\n\n\nclass SchemaAnnotationError(TypeError):\n    \"\"\"Raised when 'args_schema' is missing or has an incorrect type annotation.\"\"\"\n\n\ndef _is_annotated_type(typ: type[Any]) -> bool:\n    return get_origin(typ) is Annotated\n\n\ndef _get_annotation_description(arg_type: type) -> str | None:\n    if _is_annotated_type(arg_type):\n        annotated_args = get_args(arg_type)\n        for annotation in annotated_args[1:]:\n            if isinstance(annotation, str):\n                return annotation\n    return None\n\n\ndef _get_filtered_args(\n    inferred_model: type[BaseModel],\n    func: Callable,\n    *,\n    filter_args: Sequence[str],\n    include_injected: bool = True,\n) -> dict:\n    \"\"\"Get the arguments from a function's signature.\"\"\"\n    schema = inferred_model.model_json_schema()[\"properties\"]\n    valid_keys = signature(func).parameters\n    return {\n        k: schema[k]\n        for i, (k, param) in enumerate(valid_keys.items())\n        if k not in filter_args\n        and (i > 0 or param.name not in (\"self\", \"cls\"))\n        and (include_injected or not _is_injected_arg_type(param.annotation))\n    }\n\n\ndef _parse_python_function_docstring(\n    function: Callable, annotations: dict, error_on_invalid_docstring: bool = False\n) -> tuple[str, dict]:\n    \"\"\"Parse the function and argument descriptions from the docstring of a function.\n\n    Assumes the function docstring follows Google Python style guide.\n    \"\"\"\n    docstring = inspect.getdoc(function)\n    return _parse_google_docstring(\n        docstring,\n        list(annotations),\n        error_on_invalid_docstring=error_on_invalid_docstring,\n    )\n\n\ndef _validate_docstring_args_against_annotations(\n    arg_descriptions: dict, annotations: dict\n) -> None:\n    \"\"\"Raise error if docstring arg is not in type annotations.\"\"\"\n    for docstring_arg in arg_descriptions:\n        if docstring_arg not in annotations:\n            msg = f\"Arg {docstring_arg} in docstring not found in function signature.\"\n            raise ValueError(msg)\n\n\ndef _infer_arg_descriptions(\n    fn: Callable,\n    *,\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = False,\n) -> tuple[str, dict]:\n    \"\"\"Infer argument descriptions from a function's docstring.\"\"\"\n    if hasattr(inspect, \"get_annotations\"):\n        # This is for python < 3.10\n        annotations = inspect.get_annotations(fn)  # type: ignore\n    else:\n        annotations = getattr(fn, \"__annotations__\", {})\n    if parse_docstring:\n        description, arg_descriptions = _parse_python_function_docstring(\n            fn, annotations, error_on_invalid_docstring=error_on_invalid_docstring\n        )\n    else:\n        description = inspect.getdoc(fn) or \"\"\n        arg_descriptions = {}\n    if parse_docstring:\n        _validate_docstring_args_against_annotations(arg_descriptions, annotations)\n    for arg, arg_type in annotations.items():\n        if arg in arg_descriptions:\n            continue\n        if desc := _get_annotation_description(arg_type):\n            arg_descriptions[arg] = desc\n    return description, arg_descriptions\n\n\ndef _is_pydantic_annotation(annotation: Any, pydantic_version: str = \"v2\") -> bool:\n    \"\"\"Determine if a type annotation is a Pydantic model.\"\"\"\n    base_model_class = BaseModelV1 if pydantic_version == \"v1\" else BaseModel\n    try:\n        return issubclass(annotation, base_model_class)\n    except TypeError:\n        return False\n\n\ndef _function_annotations_are_pydantic_v1(\n    signature: inspect.Signature, func: Callable\n) -> bool:\n    \"\"\"Determine if all Pydantic annotations in a function signature are from V1.\"\"\"\n    any_v1_annotations = any(\n        _is_pydantic_annotation(parameter.annotation, pydantic_version=\"v1\")\n        for parameter in signature.parameters.values()\n    )\n    any_v2_annotations = any(\n        _is_pydantic_annotation(parameter.annotation, pydantic_version=\"v2\")\n        for parameter in signature.parameters.values()\n    )\n    if any_v1_annotations and any_v2_annotations:\n        msg = (\n            f\"Function {func} contains a mix of Pydantic v1 and v2 annotations. \"\n            \"Only one version of Pydantic annotations per function is supported.\"\n        )\n        raise NotImplementedError(msg)\n    return any_v1_annotations and not any_v2_annotations\n\n\nclass _SchemaConfig:\n    \"\"\"Configuration for the pydantic model.\n\n    This is used to configure the pydantic model created from\n    a function's signature.\n\n    Parameters:\n        extra: Whether to allow extra fields in the model.\n        arbitrary_types_allowed: Whether to allow arbitrary types in the model.\n            Defaults to True.\n    \"\"\"\n\n    extra: str = \"forbid\"\n    arbitrary_types_allowed: bool = True\n\n\ndef create_schema_from_function(\n    model_name: str,\n    func: Callable,\n    *,\n    filter_args: Optional[Sequence[str]] = None,\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = False,\n    include_injected: bool = True,\n) -> type[BaseModel]:\n    \"\"\"Create a pydantic schema from a function's signature.\n\n    Args:\n        model_name: Name to assign to the generated pydantic schema.\n        func: Function to generate the schema from.\n        filter_args: Optional list of arguments to exclude from the schema.\n            Defaults to FILTERED_ARGS.\n        parse_docstring: Whether to parse the function's docstring for descriptions\n            for each argument. Defaults to False.\n        error_on_invalid_docstring: if ``parse_docstring`` is provided, configure\n            whether to raise ValueError on invalid Google Style docstrings.\n            Defaults to False.\n        include_injected: Whether to include injected arguments in the schema.\n            Defaults to True, since we want to include them in the schema\n            when *validating* tool inputs.\n\n    Returns:\n        A pydantic model with the same arguments as the function.\n    \"\"\"\n    sig = inspect.signature(func)\n\n    if _function_annotations_are_pydantic_v1(sig, func):\n        validated = validate_arguments_v1(func, config=_SchemaConfig)  # type: ignore\n    else:\n        # https://docs.pydantic.dev/latest/usage/validation_decorator/\n        with warnings.catch_warnings():\n            # We are using deprecated functionality here.\n            # This code should be re-written to simply construct a pydantic model\n            # using inspect.signature and create_model.\n            warnings.simplefilter(\"ignore\", category=PydanticDeprecationWarning)\n            validated = validate_arguments(func, config=_SchemaConfig)  # type: ignore\n\n    # Let's ignore `self` and `cls` arguments for class and instance methods\n    # If qualified name has a \".\", then it likely belongs in a class namespace\n    in_class = bool(func.__qualname__ and \".\" in func.__qualname__)\n\n    has_args = False\n    has_kwargs = False\n\n    for param in sig.parameters.values():\n        if param.kind == param.VAR_POSITIONAL:\n            has_args = True\n        elif param.kind == param.VAR_KEYWORD:\n            has_kwargs = True\n\n    inferred_model = validated.model  # type: ignore\n\n    if filter_args:\n        filter_args_ = filter_args\n    else:\n        # Handle classmethods and instance methods\n        existing_params: list[str] = list(sig.parameters.keys())\n        if existing_params and existing_params[0] in (\"self\", \"cls\") and in_class:\n            filter_args_ = [existing_params[0]] + list(FILTERED_ARGS)\n        else:\n            filter_args_ = list(FILTERED_ARGS)\n\n        for existing_param in existing_params:\n            if not include_injected and _is_injected_arg_type(\n                sig.parameters[existing_param].annotation\n            ):\n                filter_args_.append(existing_param)\n\n    description, arg_descriptions = _infer_arg_descriptions(\n        func,\n        parse_docstring=parse_docstring,\n        error_on_invalid_docstring=error_on_invalid_docstring,\n    )\n    # Pydantic adds placeholder virtual fields we need to strip\n    valid_properties = []\n    for field in get_fields(inferred_model):\n        if not has_args and field == \"args\":\n            continue\n        if not has_kwargs and field == \"kwargs\":\n            continue\n\n        if field == \"v__duplicate_kwargs\":  # Internal pydantic field\n            continue\n\n        if field not in filter_args_:\n            valid_properties.append(field)\n\n    return _create_subset_model(\n        model_name,\n        inferred_model,\n        list(valid_properties),\n        descriptions=arg_descriptions,\n        fn_description=description,\n    )\n\n\nclass ToolException(Exception):  # noqa: N818\n    \"\"\"Optional exception that tool throws when execution error occurs.\n\n    When this exception is thrown, the agent will not stop working,\n    but it will handle the exception according to the handle_tool_error\n    variable of the tool, and the processing result will be returned\n    to the agent as observation, and printed in red on the console.\n    \"\"\"\n\n\nArgsSchema = Union[TypeBaseModel, dict[str, Any]]\n\n\nclass BaseTool(RunnableSerializable[Union[str, dict, ToolCall], Any]):\n    \"\"\"Interface LangChain tools must implement.\"\"\"\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        \"\"\"Create the definition of the new tool class.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        args_schema_type = cls.__annotations__.get(\"args_schema\", None)\n\n        if args_schema_type is not None and args_schema_type == BaseModel:\n            # Throw errors for common mis-annotations.\n            # TODO: Use get_args / get_origin and fully\n            # specify valid annotations.\n            typehint_mandate = \"\"\"\nclass ChildTool(BaseTool):\n    ...\n    args_schema: Type[BaseModel] = SchemaClass\n    ...\"\"\"\n            name = cls.__name__\n            msg = (\n                f\"Tool definition for {name} must include valid type annotations\"\n                f\" for argument 'args_schema' to behave as expected.\\n\"\n                f\"Expected annotation of 'Type[BaseModel]'\"\n                f\" but got '{args_schema_type}'.\\n\"\n                f\"Expected class looks like:\\n\"\n                f\"{typehint_mandate}\"\n            )\n            raise SchemaAnnotationError(msg)\n\n    name: str\n    \"\"\"The unique name of the tool that clearly communicates its purpose.\"\"\"\n    description: str\n    \"\"\"Used to tell the model how/when/why to use the tool.\n\n    You can provide few-shot examples as a part of the description.\n    \"\"\"\n\n    args_schema: Annotated[Optional[ArgsSchema], SkipValidation()] = Field(\n        default=None, description=\"The tool schema.\"\n    )\n    \"\"\"Pydantic model class to validate and parse the tool's input arguments.\n\n    Args schema should be either:\n\n    - A subclass of pydantic.BaseModel.\n    or\n    - A subclass of pydantic.v1.BaseModel if accessing v1 namespace in pydantic 2\n    or\n    - a JSON schema dict\n    \"\"\"\n    return_direct: bool = False\n    \"\"\"Whether to return the tool's output directly.\n\n    Setting this to True means\n    that after the tool is called, the AgentExecutor will stop looping.\n    \"\"\"\n    verbose: bool = False\n    \"\"\"Whether to log the tool's progress.\"\"\"\n\n    callbacks: Callbacks = Field(default=None, exclude=True)\n    \"\"\"Callbacks to be called during tool execution.\"\"\"\n\n    callback_manager: Optional[BaseCallbackManager] = deprecated(\n        name=\"callback_manager\", since=\"0.1.7\", removal=\"1.0\", alternative=\"callbacks\"\n    )(\n        Field(\n            default=None,\n            exclude=True,\n            description=\"Callback manager to add to the run trace.\",\n        )\n    )\n    tags: Optional[list[str]] = None\n    \"\"\"Optional list of tags associated with the tool. Defaults to None.\n    These tags will be associated with each call to this tool,\n    and passed as arguments to the handlers defined in `callbacks`.\n    You can use these to eg identify a specific instance of a tool with its use case.\n    \"\"\"\n    metadata: Optional[dict[str, Any]] = None\n    \"\"\"Optional metadata associated with the tool. Defaults to None.\n    This metadata will be associated with each call to this tool,\n    and passed as arguments to the handlers defined in `callbacks`.\n    You can use these to eg identify a specific instance of a tool with its use case.\n    \"\"\"\n\n    handle_tool_error: Optional[Union[bool, str, Callable[[ToolException], str]]] = (\n        False\n    )\n    \"\"\"Handle the content of the ToolException thrown.\"\"\"\n\n    handle_validation_error: Optional[\n        Union[bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]]\n    ] = False\n    \"\"\"Handle the content of the ValidationError thrown.\"\"\"\n\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\"\n    \"\"\"The tool response format. Defaults to 'content'.\n\n    If \"content\" then the output of the tool is interpreted as the contents of a\n    ToolMessage. If \"content_and_artifact\" then the output is expected to be a\n    two-tuple corresponding to the (content, artifact) of a ToolMessage.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Initialize the tool.\"\"\"\n        if (\n            \"args_schema\" in kwargs\n            and kwargs[\"args_schema\"] is not None\n            and not is_basemodel_subclass(kwargs[\"args_schema\"])\n            and not isinstance(kwargs[\"args_schema\"], dict)\n        ):\n            msg = (\n                \"args_schema must be a subclass of pydantic BaseModel or \"\n                f\"a JSON schema dict. Got: {kwargs['args_schema']}.\"\n            )\n            raise TypeError(msg)\n        super().__init__(**kwargs)\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    def is_single_input(self) -> bool:\n        \"\"\"Whether the tool only accepts a single input.\"\"\"\n        keys = {k for k in self.args if k != \"kwargs\"}\n        return len(keys) == 1\n\n    @property\n    def args(self) -> dict:\n        if isinstance(self.args_schema, dict):\n            json_schema = self.args_schema\n        else:\n            input_schema = self.get_input_schema()\n            json_schema = input_schema.model_json_schema()\n        return json_schema[\"properties\"]\n\n    @property\n    def tool_call_schema(self) -> ArgsSchema:\n        if isinstance(self.args_schema, dict):\n            return self.args_schema\n\n        full_schema = self.get_input_schema()\n        fields = []\n        for name, type_ in get_all_basemodel_annotations(full_schema).items():\n            if not _is_injected_arg_type(type_):\n                fields.append(name)\n        return _create_subset_model(\n            self.name, full_schema, fields, fn_description=self.description\n        )\n\n    # --- Runnable ---\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"The tool's input schema.\n\n        Args:\n            config: The configuration for the tool.\n\n        Returns:\n            The input schema for the tool.\n        \"\"\"\n        if self.args_schema is not None:\n            if isinstance(self.args_schema, dict):\n                return super().get_input_schema(config)\n            return self.args_schema\n        else:\n            return create_schema_from_function(self.name, self._run)\n\n    def invoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n        return self.run(tool_input, **kwargs)\n\n    async def ainvoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n        return await self.arun(tool_input, **kwargs)\n\n    # --- Tool ---\n\n    def _parse_input(\n        self, tool_input: Union[str, dict], tool_call_id: Optional[str]\n    ) -> Union[str, dict[str, Any]]:\n        \"\"\"Convert tool input to a pydantic model.\n\n        Args:\n            tool_input: The input to the tool.\n        \"\"\"\n        input_args = self.args_schema\n        if isinstance(tool_input, str):\n            if input_args is not None:\n                if isinstance(input_args, dict):\n                    msg = (\n                        \"String tool inputs are not allowed when \"\n                        \"using tools with JSON schema args_schema.\"\n                    )\n                    raise ValueError(msg)\n                key_ = next(iter(get_fields(input_args).keys()))\n                if hasattr(input_args, \"model_validate\"):\n                    input_args.model_validate({key_: tool_input})\n                else:\n                    input_args.parse_obj({key_: tool_input})\n            return tool_input\n        else:\n            if input_args is not None:\n                if isinstance(input_args, dict):\n                    return tool_input\n                elif issubclass(input_args, BaseModel):\n                    for k, v in get_all_basemodel_annotations(input_args).items():\n                        if (\n                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)\n                            and k not in tool_input\n                        ):\n                            if tool_call_id is None:\n                                msg = (\n                                    \"When tool includes an InjectedToolCallId \"\n                                    \"argument, tool must always be invoked with a full \"\n                                    \"model ToolCall of the form: {'args': {...}, \"\n                                    \"'name': '...', 'type': 'tool_call', \"\n                                    \"'tool_call_id': '...'}\"\n                                )\n                                raise ValueError(msg)\n                            tool_input[k] = tool_call_id\n                    result = input_args.model_validate(tool_input)\n                    result_dict = result.model_dump()\n                elif issubclass(input_args, BaseModelV1):\n                    for k, v in get_all_basemodel_annotations(input_args).items():\n                        if (\n                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)\n                            and k not in tool_input\n                        ):\n                            if tool_call_id is None:\n                                msg = (\n                                    \"When tool includes an InjectedToolCallId \"\n                                    \"argument, tool must always be invoked with a full \"\n                                    \"model ToolCall of the form: {'args': {...}, \"\n                                    \"'name': '...', 'type': 'tool_call', \"\n                                    \"'tool_call_id': '...'}\"\n                                )\n                                raise ValueError(msg)\n                            tool_input[k] = tool_call_id\n                    result = input_args.parse_obj(tool_input)\n                    result_dict = result.dict()\n                else:\n                    msg = (\n                        \"args_schema must be a Pydantic BaseModel, \"\n                        f\"got {self.args_schema}\"\n                    )\n                    raise NotImplementedError(msg)\n                return {\n                    k: getattr(result, k)\n                    for k, v in result_dict.items()\n                    if k in tool_input\n                }\n            return tool_input\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def raise_deprecation(cls, values: dict) -> Any:\n        \"\"\"Raise deprecation warning if callback_manager is used.\n\n        Args:\n            values: The values to validate.\n\n        Returns:\n            The validated values.\n        \"\"\"\n        if values.get(\"callback_manager\") is not None:\n            warnings.warn(\n                \"callback_manager is deprecated. Please use callbacks instead.\",\n                DeprecationWarning,\n                stacklevel=6,\n            )\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\n        return values\n\n    @abstractmethod\n    def _run(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Use the tool.\n\n        Add run_manager: Optional[CallbackManagerForToolRun] = None\n        to child implementations to enable tracing.\n        \"\"\"\n\n    async def _arun(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Use the tool asynchronously.\n\n        Add run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n        to child implementations to enable tracing.\n        \"\"\"\n        if kwargs.get(\"run_manager\") and signature(self._run).parameters.get(\n            \"run_manager\"\n        ):\n            kwargs[\"run_manager\"] = kwargs[\"run_manager\"].get_sync()\n        return await run_in_executor(None, self._run, *args, **kwargs)\n\n    def _to_args_and_kwargs(\n        self, tool_input: Union[str, dict], tool_call_id: Optional[str]\n    ) -> tuple[tuple, dict]:\n        if (\n            self.args_schema is not None\n            and isinstance(self.args_schema, type)\n            and is_basemodel_subclass(self.args_schema)\n            and not get_fields(self.args_schema)\n        ):\n            # StructuredTool with no args\n            return (), {}\n        tool_input = self._parse_input(tool_input, tool_call_id)\n        # For backwards compatibility, if run_input is a string,\n        # pass as a positional argument.\n        if isinstance(tool_input, str):\n            return (tool_input,), {}\n        else:\n            return (), tool_input\n\n    def run(\n        self,\n        tool_input: Union[str, dict[str, Any]],\n        verbose: Optional[bool] = None,\n        start_color: Optional[str] = \"green\",\n        color: Optional[str] = \"green\",\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        config: Optional[RunnableConfig] = None,\n        tool_call_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the tool.\n\n        Args:\n            tool_input: The input to the tool.\n            verbose: Whether to log the tool's progress. Defaults to None.\n            start_color: The color to use when starting the tool. Defaults to 'green'.\n            color: The color to use when ending the tool. Defaults to 'green'.\n            callbacks: Callbacks to be called during tool execution. Defaults to None.\n            tags: Optional list of tags associated with the tool. Defaults to None.\n            metadata: Optional metadata associated with the tool. Defaults to None.\n            run_name: The name of the run. Defaults to None.\n            run_id: The id of the run. Defaults to None.\n            config: The configuration for the tool. Defaults to None.\n            tool_call_id: The id of the tool call. Defaults to None.\n            kwargs: Keyword arguments to be passed to tool callbacks\n\n        Returns:\n            The output of the tool.\n\n        Raises:\n            ToolException: If an error occurs during tool execution.\n        \"\"\"\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose or bool(verbose),\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n\n        run_manager = callback_manager.on_tool_start(\n            {\"name\": self.name, \"description\": self.description},\n            tool_input if isinstance(tool_input, str) else str(tool_input),\n            color=start_color,\n            name=run_name,\n            run_id=run_id,\n            # Inputs by definition should always be dicts.\n            # For now, it's unclear whether this assumption is ever violated,\n            # but if it is we will send a `None` value to the callback instead\n            # TODO: will need to address issue via a patch.\n            inputs=tool_input if isinstance(tool_input, dict) else None,\n            **kwargs,\n        )\n\n        content = None\n        artifact = None\n        status = \"success\"\n        error_to_raise: Union[Exception, KeyboardInterrupt, None] = None\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)\n            if signature(self._run).parameters.get(\"run_manager\"):\n                tool_kwargs = tool_kwargs | {\"run_manager\": run_manager}\n            if config_param := _get_runnable_config_param(self._run):\n                tool_kwargs = tool_kwargs | {config_param: config}\n            response = context.run(self._run, *tool_args, **tool_kwargs)\n            if self.response_format == \"content_and_artifact\":\n                if not isinstance(response, tuple) or len(response) != 2:\n                    msg = (\n                        \"Since response_format='content_and_artifact' \"\n                        \"a two-tuple of the message content and raw tool output is \"\n                        f\"expected. Instead generated response of type: \"\n                        f\"{type(response)}.\"\n                    )\n                    error_to_raise = ValueError(msg)\n                else:\n                    content, artifact = response\n            else:\n                content = response\n        except (ValidationError, ValidationErrorV1) as e:\n            if not self.handle_validation_error:\n                error_to_raise = e\n            else:\n                content = _handle_validation_error(e, flag=self.handle_validation_error)\n                status = \"error\"\n        except ToolException as e:\n            if not self.handle_tool_error:\n                error_to_raise = e\n            else:\n                content = _handle_tool_error(e, flag=self.handle_tool_error)\n                status = \"error\"\n        except (Exception, KeyboardInterrupt) as e:\n            error_to_raise = e\n\n        if error_to_raise:\n            run_manager.on_tool_error(error_to_raise)\n            raise error_to_raise\n        output = _format_output(content, artifact, tool_call_id, self.name, status)\n        run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n        return output\n\n    async def arun(\n        self,\n        tool_input: Union[str, dict],\n        verbose: Optional[bool] = None,\n        start_color: Optional[str] = \"green\",\n        color: Optional[str] = \"green\",\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        config: Optional[RunnableConfig] = None,\n        tool_call_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the tool asynchronously.\n\n        Args:\n            tool_input: The input to the tool.\n            verbose: Whether to log the tool's progress. Defaults to None.\n            start_color: The color to use when starting the tool. Defaults to 'green'.\n            color: The color to use when ending the tool. Defaults to 'green'.\n            callbacks: Callbacks to be called during tool execution. Defaults to None.\n            tags: Optional list of tags associated with the tool. Defaults to None.\n            metadata: Optional metadata associated with the tool. Defaults to None.\n            run_name: The name of the run. Defaults to None.\n            run_id: The id of the run. Defaults to None.\n            config: The configuration for the tool. Defaults to None.\n            tool_call_id: The id of the tool call. Defaults to None.\n            kwargs: Keyword arguments to be passed to tool callbacks\n\n        Returns:\n            The output of the tool.\n\n        Raises:\n            ToolException: If an error occurs during tool execution.\n        \"\"\"\n        callback_manager = AsyncCallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose or bool(verbose),\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n        run_manager = await callback_manager.on_tool_start(\n            {\"name\": self.name, \"description\": self.description},\n            tool_input if isinstance(tool_input, str) else str(tool_input),\n            color=start_color,\n            name=run_name,\n            run_id=run_id,\n            # Inputs by definition should always be dicts.\n            # For now, it's unclear whether this assumption is ever violated,\n            # but if it is we will send a `None` value to the callback instead\n            # TODO: will need to address issue via a patch.\n            inputs=tool_input if isinstance(tool_input, dict) else None,\n            **kwargs,\n        )\n        content = None\n        artifact = None\n        status = \"success\"\n        error_to_raise: Optional[Union[Exception, KeyboardInterrupt]] = None\n        try:\n            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            func_to_check = (\n                self._run if self.__class__._arun is BaseTool._arun else self._arun\n            )\n            if signature(func_to_check).parameters.get(\"run_manager\"):\n                tool_kwargs[\"run_manager\"] = run_manager\n            if config_param := _get_runnable_config_param(func_to_check):\n                tool_kwargs[config_param] = config\n\n            coro = context.run(self._arun, *tool_args, **tool_kwargs)\n            if asyncio_accepts_context():\n                response = await asyncio.create_task(coro, context=context)  # type: ignore\n            else:\n                response = await coro\n            if self.response_format == \"content_and_artifact\":\n                if not isinstance(response, tuple) or len(response) != 2:\n                    msg = (\n                        \"Since response_format='content_and_artifact' \"\n                        \"a two-tuple of the message content and raw tool output is \"\n                        f\"expected. Instead generated response of type: \"\n                        f\"{type(response)}.\"\n                    )\n                    error_to_raise = ValueError(msg)\n                else:\n                    content, artifact = response\n            else:\n                content = response\n        except ValidationError as e:\n            if not self.handle_validation_error:\n                error_to_raise = e\n            else:\n                content = _handle_validation_error(e, flag=self.handle_validation_error)\n                status = \"error\"\n        except ToolException as e:\n            if not self.handle_tool_error:\n                error_to_raise = e\n            else:\n                content = _handle_tool_error(e, flag=self.handle_tool_error)\n                status = \"error\"\n        except (Exception, KeyboardInterrupt) as e:\n            error_to_raise = e\n\n        if error_to_raise:\n            await run_manager.on_tool_error(error_to_raise)\n            raise error_to_raise\n\n        output = _format_output(content, artifact, tool_call_id, self.name, status)\n        await run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n        return output\n\n    @deprecated(\"0.1.47\", alternative=\"invoke\", removal=\"1.0\")\n    def __call__(self, tool_input: str, callbacks: Callbacks = None) -> str:\n        \"\"\"Make tool callable.\"\"\"\n        return self.run(tool_input, callbacks=callbacks)\n\n\ndef _is_tool_call(x: Any) -> bool:\n    return isinstance(x, dict) and x.get(\"type\") == \"tool_call\"\n\n\ndef _handle_validation_error(\n    e: Union[ValidationError, ValidationErrorV1],\n    *,\n    flag: Union[\n        Literal[True], str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> str:\n    if isinstance(flag, bool):\n        content = \"Tool input validation error\"\n    elif isinstance(flag, str):\n        content = flag\n    elif callable(flag):\n        content = flag(e)\n    else:\n        msg = (\n            f\"Got unexpected type of `handle_validation_error`. Expected bool, \"\n            f\"str or callable. Received: {flag}\"\n        )\n        raise ValueError(msg)  # noqa: TRY004\n    return content\n\n\ndef _handle_tool_error(\n    e: ToolException,\n    *,\n    flag: Optional[Union[Literal[True], str, Callable[[ToolException], str]]],\n) -> str:\n    if isinstance(flag, bool):\n        content = e.args[0] if e.args else \"Tool execution error\"\n    elif isinstance(flag, str):\n        content = flag\n    elif callable(flag):\n        content = flag(e)\n    else:\n        msg = (\n            f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\n            f\"or callable. Received: {flag}\"\n        )\n        raise ValueError(msg)  # noqa: TRY004\n    return content\n\n\ndef _prep_run_args(\n    input: Union[str, dict, ToolCall],\n    config: Optional[RunnableConfig],\n    **kwargs: Any,\n) -> tuple[Union[str, dict], dict]:\n    config = ensure_config(config)\n    if _is_tool_call(input):\n        tool_call_id: Optional[str] = cast(ToolCall, input)[\"id\"]\n        tool_input: Union[str, dict] = cast(ToolCall, input)[\"args\"].copy()\n    else:\n        tool_call_id = None\n        tool_input = cast(Union[str, dict], input)\n    return (\n        tool_input,\n        dict(\n            callbacks=config.get(\"callbacks\"),\n            tags=config.get(\"tags\"),\n            metadata=config.get(\"metadata\"),\n            run_name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            config=config,\n            tool_call_id=tool_call_id,\n            **kwargs,\n        ),\n    )\n\n\ndef _format_output(\n    content: Any,\n    artifact: Any,\n    tool_call_id: Optional[str],\n    name: str,\n    status: str,\n) -> Union[ToolOutputMixin, Any]:\n    if isinstance(content, ToolOutputMixin) or tool_call_id is None:\n        return content\n    if not _is_message_content_type(content):\n        content = _stringify(content)\n    return ToolMessage(\n        content,\n        artifact=artifact,\n        tool_call_id=tool_call_id,\n        name=name,\n        status=status,\n    )\n\n\ndef _is_message_content_type(obj: Any) -> bool:\n    \"\"\"Check for OpenAI or Anthropic format tool message content.\"\"\"\n    return (\n        isinstance(obj, str)\n        or isinstance(obj, list)\n        and all(_is_message_content_block(e) for e in obj)\n    )\n\n\ndef _is_message_content_block(obj: Any) -> bool:\n    \"\"\"Check for OpenAI or Anthropic format tool message content blocks.\"\"\"\n    if isinstance(obj, str):\n        return True\n    elif isinstance(obj, dict):\n        return obj.get(\"type\", None) in (\"text\", \"image_url\", \"image\", \"json\")\n    else:\n        return False\n\n\ndef _stringify(content: Any) -> str:\n    try:\n        return json.dumps(content, ensure_ascii=False)\n    except Exception:\n        return str(content)\n\n\ndef _get_type_hints(func: Callable) -> Optional[dict[str, type]]:\n    if isinstance(func, functools.partial):\n        func = func.func\n    try:\n        return get_type_hints(func)\n    except Exception:\n        return None\n\n\ndef _get_runnable_config_param(func: Callable) -> Optional[str]:\n    type_hints = _get_type_hints(func)\n    if not type_hints:\n        return None\n    for name, type_ in type_hints.items():\n        if type_ is RunnableConfig:\n            return name\n    return None\n\n\nclass InjectedToolArg:\n    \"\"\"Annotation for a Tool arg that is **not** meant to be generated by a model.\"\"\"\n\n\nclass InjectedToolCallId(InjectedToolArg):\n    r'''Annotation for injecting the tool_call_id.\n\n    Example:\n        ..code-block:: python\n\n            from typing_extensions import Annotated\n\n            from langchain_core.messages import ToolMessage\n            from langchain_core.tools import tool, InjectedToolCallID\n\n            @tool\n            def foo(x: int, tool_call_id: Annotated[str, InjectedToolCallID]) -> ToolMessage:\n                \"\"\"Return x.\"\"\"\n                return ToolMessage(str(x), artifact=x, name=\"foo\", tool_call_id=tool_call_id)\n    '''  # noqa: E501\n\n\ndef _is_injected_arg_type(\n    type_: type, injected_type: Optional[type[InjectedToolArg]] = None\n) -> bool:\n    injected_type = injected_type or InjectedToolArg\n    return any(\n        isinstance(arg, injected_type)\n        or (isinstance(arg, type) and issubclass(arg, injected_type))\n        for arg in get_args(type_)[1:]\n    )\n\n\ndef get_all_basemodel_annotations(\n    cls: Union[TypeBaseModel, Any], *, default_to_bound: bool = True\n) -> dict[str, type]:\n    # cls has no subscript: cls = FooBar\n    if isinstance(cls, type):\n        annotations: dict[str, type] = {}\n        for name, param in inspect.signature(cls).parameters.items():\n            # Exclude hidden init args added by pydantic Config. For example if\n            # BaseModel(extra=\"allow\") then \"extra_data\" will part of init sig.\n            if (\n                fields := getattr(cls, \"model_fields\", {})  # pydantic v2+\n                or getattr(cls, \"__fields__\", {})  # pydantic v1\n            ) and name not in fields:\n                continue\n            annotations[name] = param.annotation\n        orig_bases: tuple = getattr(cls, \"__orig_bases__\", ())\n    # cls has subscript: cls = FooBar[int]\n    else:\n        annotations = get_all_basemodel_annotations(\n            get_origin(cls), default_to_bound=False\n        )\n        orig_bases = (cls,)\n\n    # Pydantic v2 automatically resolves inherited generics, Pydantic v1 does not.\n    if not (isinstance(cls, type) and is_pydantic_v2_subclass(cls)):\n        # if cls = FooBar inherits from Baz[str], orig_bases will contain Baz[str]\n        # if cls = FooBar inherits from Baz, orig_bases will contain Baz\n        # if cls = FooBar[int], orig_bases will contain FooBar[int]\n        for parent in orig_bases:\n            # if class = FooBar inherits from Baz, parent = Baz\n            if isinstance(parent, type) and is_pydantic_v1_subclass(parent):\n                annotations.update(\n                    get_all_basemodel_annotations(parent, default_to_bound=False)\n                )\n                continue\n\n            parent_origin = get_origin(parent)\n\n            # if class = FooBar inherits from non-pydantic class\n            if not parent_origin:\n                continue\n\n            # if class = FooBar inherits from Baz[str]:\n            # parent = Baz[str],\n            # parent_origin = Baz,\n            # generic_type_vars = (type vars in Baz)\n            # generic_map = {type var in Baz: str}\n            generic_type_vars: tuple = getattr(parent_origin, \"__parameters__\", ())\n            generic_map = dict(zip(generic_type_vars, get_args(parent)))\n            for field in getattr(parent_origin, \"__annotations__\", {}):\n                annotations[field] = _replace_type_vars(\n                    annotations[field], generic_map, default_to_bound\n                )\n\n    return {\n        k: _replace_type_vars(v, default_to_bound=default_to_bound)\n        for k, v in annotations.items()\n    }\n\n\ndef _replace_type_vars(\n    type_: type,\n    generic_map: Optional[dict[TypeVar, type]] = None,\n    default_to_bound: bool = True,\n) -> type:\n    generic_map = generic_map or {}\n    if isinstance(type_, TypeVar):\n        if type_ in generic_map:\n            return generic_map[type_]\n        elif default_to_bound:\n            return type_.__bound__ or Any\n        else:\n            return type_\n    elif (origin := get_origin(type_)) and (args := get_args(type_)):\n        new_args = tuple(\n            _replace_type_vars(arg, generic_map, default_to_bound) for arg in args\n        )\n        return _py_38_safe_origin(origin)[new_args]  # type: ignore[index]\n    else:\n        return type_\n\n\nclass BaseToolkit(BaseModel, ABC):\n    \"\"\"Base Toolkit representing a collection of related tools.\"\"\"\n\n    @abstractmethod\n    def get_tools(self) -> list[BaseTool]:\n        \"\"\"Get the tools in the toolkit.\"\"\"\n",
        "patch": "@@ -459,6 +459,12 @@ def args(self) -> dict:\n     @property\n     def tool_call_schema(self) -> ArgsSchema:\n         if isinstance(self.args_schema, dict):\n+            if self.description:\n+                return {\n+                    **self.args_schema,\n+                    \"description\": self.description,\n+                }\n+\n             return self.args_schema\n \n         full_schema = self.get_input_schema()"
      },
      {
        "filename": "libs/core/langchain_core/tools/convert.py",
        "content_before": "import inspect\nfrom typing import Any, Callable, Literal, Optional, Union, get_type_hints, overload\n\nfrom pydantic import BaseModel, Field, create_model\n\nfrom langchain_core.callbacks import Callbacks\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.tools.base import BaseTool\nfrom langchain_core.tools.simple import Tool\nfrom langchain_core.tools.structured import StructuredTool\n\n\n@overload\ndef tool(\n    *,\n    return_direct: bool = False,\n    args_schema: Optional[type] = None,\n    infer_schema: bool = True,\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = True,\n) -> Callable[[Union[Callable, Runnable]], BaseTool]: ...\n\n\n@overload\ndef tool(\n    name_or_callable: str,\n    runnable: Runnable,\n    *,\n    return_direct: bool = False,\n    args_schema: Optional[type] = None,\n    infer_schema: bool = True,\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = True,\n) -> BaseTool: ...\n\n\n@overload\ndef tool(\n    name_or_callable: Callable,\n    *,\n    return_direct: bool = False,\n    args_schema: Optional[type] = None,\n    infer_schema: bool = True,\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = True,\n) -> BaseTool: ...\n\n\n@overload\ndef tool(\n    name_or_callable: str,\n    *,\n    return_direct: bool = False,\n    args_schema: Optional[type] = None,\n    infer_schema: bool = True,\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = True,\n) -> Callable[[Union[Callable, Runnable]], BaseTool]: ...\n\n\ndef tool(\n    name_or_callable: Optional[Union[str, Callable]] = None,\n    runnable: Optional[Runnable] = None,\n    *args: Any,\n    return_direct: bool = False,\n    args_schema: Optional[type] = None,\n    infer_schema: bool = True,\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = True,\n) -> Union[\n    BaseTool,\n    Callable[[Union[Callable, Runnable]], BaseTool],\n]:\n    \"\"\"Make tools out of functions, can be used with or without arguments.\n\n    Args:\n        name_or_callable: Optional name of the tool or the callable to be\n            converted to a tool. Must be provided as a positional argument.\n        runnable: Optional runnable to convert to a tool. Must be provided as a\n            positional argument.\n        return_direct: Whether to return directly from the tool rather\n            than continuing the agent loop. Defaults to False.\n        args_schema: optional argument schema for user to specify.\n            Defaults to None.\n        infer_schema: Whether to infer the schema of the arguments from\n            the function's signature. This also makes the resultant tool\n            accept a dictionary input to its `run()` function.\n            Defaults to True.\n        response_format: The tool response format. If \"content\" then the output of\n            the tool is interpreted as the contents of a ToolMessage. If\n            \"content_and_artifact\" then the output is expected to be a two-tuple\n            corresponding to the (content, artifact) of a ToolMessage.\n            Defaults to \"content\".\n        parse_docstring: if ``infer_schema`` and ``parse_docstring``, will attempt to\n            parse parameter descriptions from Google Style function docstrings.\n            Defaults to False.\n        error_on_invalid_docstring: if ``parse_docstring`` is provided, configure\n            whether to raise ValueError on invalid Google Style docstrings.\n            Defaults to True.\n\n    Returns:\n        The tool.\n\n    Requires:\n        - Function must be of type (str) -> str\n        - Function must have a docstring\n\n    Examples:\n        .. code-block:: python\n\n            @tool\n            def search_api(query: str) -> str:\n                # Searches the API for the query.\n                return\n\n            @tool(\"search\", return_direct=True)\n            def search_api(query: str) -> str:\n                # Searches the API for the query.\n                return\n\n            @tool(response_format=\"content_and_artifact\")\n            def search_api(query: str) -> Tuple[str, dict]:\n                return \"partial json of results\", {\"full\": \"object of results\"}\n\n    .. versionadded:: 0.2.14\n    Parse Google-style docstrings:\n\n        .. code-block:: python\n\n            @tool(parse_docstring=True)\n            def foo(bar: str, baz: int) -> str:\n                \\\"\\\"\\\"The foo.\n\n                Args:\n                    bar: The bar.\n                    baz: The baz.\n                \\\"\\\"\\\"\n                return bar\n\n            foo.args_schema.model_json_schema()\n\n        .. code-block:: python\n\n            {\n                \"title\": \"foo\",\n                \"description\": \"The foo.\",\n                \"type\": \"object\",\n                \"properties\": {\n                    \"bar\": {\n                        \"title\": \"Bar\",\n                        \"description\": \"The bar.\",\n                        \"type\": \"string\"\n                    },\n                    \"baz\": {\n                        \"title\": \"Baz\",\n                        \"description\": \"The baz.\",\n                        \"type\": \"integer\"\n                    }\n                },\n                \"required\": [\n                    \"bar\",\n                    \"baz\"\n                ]\n            }\n\n        Note that parsing by default will raise ``ValueError`` if the docstring\n        is considered invalid. A docstring is considered invalid if it contains\n        arguments not in the function signature, or is unable to be parsed into\n        a summary and \"Args:\" blocks. Examples below:\n\n        .. code-block:: python\n\n            # No args section\n            def invalid_docstring_1(bar: str, baz: int) -> str:\n                \\\"\\\"\\\"The foo.\\\"\\\"\\\"\n                return bar\n\n            # Improper whitespace between summary and args section\n            def invalid_docstring_2(bar: str, baz: int) -> str:\n                \\\"\\\"\\\"The foo.\n                Args:\n                    bar: The bar.\n                    baz: The baz.\n                \\\"\\\"\\\"\n                return bar\n\n            # Documented args absent from function signature\n            def invalid_docstring_3(bar: str, baz: int) -> str:\n                \\\"\\\"\\\"The foo.\n\n                Args:\n                    banana: The bar.\n                    monkey: The baz.\n                \\\"\\\"\\\"\n                return bar\n    \"\"\"\n\n    def _create_tool_factory(\n        tool_name: str,\n    ) -> Callable[[Union[Callable, Runnable]], BaseTool]:\n        \"\"\"Create a decorator that takes a callable and returns a tool.\n\n        Args:\n            tool_name: The name that will be assigned to the tool.\n\n        Returns:\n            A function that takes a callable or Runnable and returns a tool.\n        \"\"\"\n\n        def _tool_factory(dec_func: Union[Callable, Runnable]) -> BaseTool:\n            if isinstance(dec_func, Runnable):\n                runnable = dec_func\n\n                if runnable.input_schema.model_json_schema().get(\"type\") != \"object\":\n                    msg = \"Runnable must have an object schema.\"\n                    raise ValueError(msg)\n\n                async def ainvoke_wrapper(\n                    callbacks: Optional[Callbacks] = None, **kwargs: Any\n                ) -> Any:\n                    return await runnable.ainvoke(kwargs, {\"callbacks\": callbacks})\n\n                def invoke_wrapper(\n                    callbacks: Optional[Callbacks] = None, **kwargs: Any\n                ) -> Any:\n                    return runnable.invoke(kwargs, {\"callbacks\": callbacks})\n\n                coroutine = ainvoke_wrapper\n                func = invoke_wrapper\n                schema: Optional[type[BaseModel]] = runnable.input_schema\n                description = repr(runnable)\n            elif inspect.iscoroutinefunction(dec_func):\n                coroutine = dec_func\n                func = None\n                schema = args_schema\n                description = None\n            else:\n                coroutine = None\n                func = dec_func\n                schema = args_schema\n                description = None\n\n            if infer_schema or args_schema is not None:\n                return StructuredTool.from_function(\n                    func,\n                    coroutine,\n                    name=tool_name,\n                    description=description,\n                    return_direct=return_direct,\n                    args_schema=schema,\n                    infer_schema=infer_schema,\n                    response_format=response_format,\n                    parse_docstring=parse_docstring,\n                    error_on_invalid_docstring=error_on_invalid_docstring,\n                )\n            # If someone doesn't want a schema applied, we must treat it as\n            # a simple string->string function\n            if dec_func.__doc__ is None:\n                msg = (\n                    \"Function must have a docstring if \"\n                    \"description not provided and infer_schema is False.\"\n                )\n                raise ValueError(msg)\n            return Tool(\n                name=tool_name,\n                func=func,\n                description=f\"{tool_name} tool\",\n                return_direct=return_direct,\n                coroutine=coroutine,\n                response_format=response_format,\n            )\n\n        return _tool_factory\n\n    if len(args) != 0:\n        # Triggered if a user attempts to use positional arguments that\n        # do not exist in the function signature\n        # e.g., @tool(\"name\", runnable, \"extra_arg\")\n        # Here, \"extra_arg\" is not a valid argument\n        msg = \"Too many arguments for tool decorator. A decorator \"\n        raise ValueError(msg)\n\n    if runnable is not None:\n        # tool is used as a function\n        # tool_from_runnable = tool(\"name\", runnable)\n        if not name_or_callable:\n            msg = \"Runnable without name for tool constructor\"\n            raise ValueError(msg)\n        if not isinstance(name_or_callable, str):\n            msg = \"Name must be a string for tool constructor\"\n            raise ValueError(msg)\n        return _create_tool_factory(name_or_callable)(runnable)\n    elif name_or_callable is not None:\n        if callable(name_or_callable) and hasattr(name_or_callable, \"__name__\"):\n            # Used as a decorator without parameters\n            # @tool\n            # def my_tool():\n            #    pass\n            return _create_tool_factory(name_or_callable.__name__)(name_or_callable)\n        elif isinstance(name_or_callable, str):\n            # Used with a new name for the tool\n            # @tool(\"search\")\n            # def my_tool():\n            #    pass\n            #\n            # or\n            #\n            # @tool(\"search\", parse_docstring=True)\n            # def my_tool():\n            #    pass\n            return _create_tool_factory(name_or_callable)\n        else:\n            msg = (\n                f\"The first argument must be a string or a callable with a __name__ \"\n                f\"for tool decorator. Got {type(name_or_callable)}\"\n            )\n            raise ValueError(msg)\n    else:\n        # Tool is used as a decorator with parameters specified\n        # @tool(parse_docstring=True)\n        # def my_tool():\n        #    pass\n        def _partial(func: Union[Callable, Runnable]) -> BaseTool:\n            \"\"\"Partial function that takes a callable and returns a tool.\"\"\"\n            name_ = func.get_name() if isinstance(func, Runnable) else func.__name__\n            tool_factory = _create_tool_factory(name_)\n            return tool_factory(func)\n\n        return _partial\n\n\ndef _get_description_from_runnable(runnable: Runnable) -> str:\n    \"\"\"Generate a placeholder description of a runnable.\"\"\"\n    input_schema = runnable.input_schema.model_json_schema()\n    return f\"Takes {input_schema}.\"\n\n\ndef _get_schema_from_runnable_and_arg_types(\n    runnable: Runnable,\n    name: str,\n    arg_types: Optional[dict[str, type]] = None,\n) -> type[BaseModel]:\n    \"\"\"Infer args_schema for tool.\"\"\"\n    if arg_types is None:\n        try:\n            arg_types = get_type_hints(runnable.InputType)\n        except TypeError as e:\n            msg = (\n                \"Tool input must be str or dict. If dict, dict arguments must be \"\n                \"typed. Either annotate types (e.g., with TypedDict) or pass \"\n                f\"arg_types into `.as_tool` to specify. {str(e)}\"\n            )\n            raise TypeError(msg) from e\n    fields = {key: (key_type, Field(...)) for key, key_type in arg_types.items()}\n    return create_model(name, **fields)  # type: ignore\n\n\ndef convert_runnable_to_tool(\n    runnable: Runnable,\n    args_schema: Optional[type[BaseModel]] = None,\n    *,\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n    arg_types: Optional[dict[str, type]] = None,\n) -> BaseTool:\n    \"\"\"Convert a Runnable into a BaseTool.\n\n    Args:\n        runnable: The runnable to convert.\n        args_schema: The schema for the tool's input arguments. Defaults to None.\n        name: The name of the tool. Defaults to None.\n        description: The description of the tool. Defaults to None.\n        arg_types: The types of the arguments. Defaults to None.\n\n    Returns:\n        The tool.\n    \"\"\"\n    if args_schema:\n        runnable = runnable.with_types(input_type=args_schema)\n    description = description or _get_description_from_runnable(runnable)\n    name = name or runnable.get_name()\n\n    schema = runnable.input_schema.model_json_schema()\n    if schema.get(\"type\") == \"string\":\n        return Tool(\n            name=name,\n            func=runnable.invoke,\n            coroutine=runnable.ainvoke,\n            description=description,\n        )\n    else:\n\n        async def ainvoke_wrapper(\n            callbacks: Optional[Callbacks] = None, **kwargs: Any\n        ) -> Any:\n            return await runnable.ainvoke(kwargs, config={\"callbacks\": callbacks})\n\n        def invoke_wrapper(callbacks: Optional[Callbacks] = None, **kwargs: Any) -> Any:\n            return runnable.invoke(kwargs, config={\"callbacks\": callbacks})\n\n        if (\n            arg_types is None\n            and schema.get(\"type\") == \"object\"\n            and schema.get(\"properties\")\n        ):\n            args_schema = runnable.input_schema\n        else:\n            args_schema = _get_schema_from_runnable_and_arg_types(\n                runnable, name, arg_types=arg_types\n            )\n\n        return StructuredTool.from_function(\n            name=name,\n            func=invoke_wrapper,\n            coroutine=ainvoke_wrapper,\n            description=description,\n            args_schema=args_schema,\n        )\n",
        "patch": "@@ -5,16 +5,17 @@\n \n from langchain_core.callbacks import Callbacks\n from langchain_core.runnables import Runnable\n-from langchain_core.tools.base import BaseTool\n+from langchain_core.tools.base import ArgsSchema, BaseTool\n from langchain_core.tools.simple import Tool\n from langchain_core.tools.structured import StructuredTool\n \n \n @overload\n def tool(\n     *,\n+    description: Optional[str] = None,\n     return_direct: bool = False,\n-    args_schema: Optional[type] = None,\n+    args_schema: Optional[ArgsSchema] = None,\n     infer_schema: bool = True,\n     response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n     parse_docstring: bool = False,\n@@ -27,8 +28,9 @@ def tool(\n     name_or_callable: str,\n     runnable: Runnable,\n     *,\n+    description: Optional[str] = None,\n     return_direct: bool = False,\n-    args_schema: Optional[type] = None,\n+    args_schema: Optional[ArgsSchema] = None,\n     infer_schema: bool = True,\n     response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n     parse_docstring: bool = False,\n@@ -40,8 +42,9 @@ def tool(\n def tool(\n     name_or_callable: Callable,\n     *,\n+    description: Optional[str] = None,\n     return_direct: bool = False,\n-    args_schema: Optional[type] = None,\n+    args_schema: Optional[ArgsSchema] = None,\n     infer_schema: bool = True,\n     response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n     parse_docstring: bool = False,\n@@ -53,8 +56,9 @@ def tool(\n def tool(\n     name_or_callable: str,\n     *,\n+    description: Optional[str] = None,\n     return_direct: bool = False,\n-    args_schema: Optional[type] = None,\n+    args_schema: Optional[ArgsSchema] = None,\n     infer_schema: bool = True,\n     response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n     parse_docstring: bool = False,\n@@ -66,8 +70,9 @@ def tool(\n     name_or_callable: Optional[Union[str, Callable]] = None,\n     runnable: Optional[Runnable] = None,\n     *args: Any,\n+    description: Optional[str] = None,\n     return_direct: bool = False,\n-    args_schema: Optional[type] = None,\n+    args_schema: Optional[ArgsSchema] = None,\n     infer_schema: bool = True,\n     response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n     parse_docstring: bool = False,\n@@ -83,6 +88,14 @@ def tool(\n             converted to a tool. Must be provided as a positional argument.\n         runnable: Optional runnable to convert to a tool. Must be provided as a\n             positional argument.\n+        description: Optional description for the tool.\n+            Precedence for the tool description value is as follows:\n+                - `description` argument\n+                    (used even if docstring and/or `args_schema` are provided)\n+                - tool function docstring\n+                    (used even if `args_schema` is provided)\n+                - `args_schema` description\n+                    (used only if `description` / docstring are not provided)\n         return_direct: Whether to return directly from the tool rather\n             than continuing the agent loop. Defaults to False.\n         args_schema: optional argument schema for user to specify.\n@@ -213,6 +226,7 @@ def _create_tool_factory(\n         \"\"\"\n \n         def _tool_factory(dec_func: Union[Callable, Runnable]) -> BaseTool:\n+            tool_description = description\n             if isinstance(dec_func, Runnable):\n                 runnable = dec_func\n \n@@ -232,25 +246,23 @@ def invoke_wrapper(\n \n                 coroutine = ainvoke_wrapper\n                 func = invoke_wrapper\n-                schema: Optional[type[BaseModel]] = runnable.input_schema\n-                description = repr(runnable)\n+                schema: Optional[ArgsSchema] = runnable.input_schema\n+                tool_description = description or repr(runnable)\n             elif inspect.iscoroutinefunction(dec_func):\n                 coroutine = dec_func\n                 func = None\n                 schema = args_schema\n-                description = None\n             else:\n                 coroutine = None\n                 func = dec_func\n                 schema = args_schema\n-                description = None\n \n             if infer_schema or args_schema is not None:\n                 return StructuredTool.from_function(\n                     func,\n                     coroutine,\n                     name=tool_name,\n-                    description=description,\n+                    description=tool_description,\n                     return_direct=return_direct,\n                     args_schema=schema,\n                     infer_schema=infer_schema,"
      },
      {
        "filename": "libs/core/langchain_core/tools/structured.py",
        "content_before": "from __future__ import annotations\n\nimport textwrap\nfrom collections.abc import Awaitable\nfrom inspect import signature\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    Union,\n)\n\nfrom pydantic import Field, SkipValidation\n\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.messages import ToolCall\nfrom langchain_core.runnables import RunnableConfig, run_in_executor\nfrom langchain_core.tools.base import (\n    FILTERED_ARGS,\n    ArgsSchema,\n    BaseTool,\n    _get_runnable_config_param,\n    create_schema_from_function,\n)\n\n\nclass StructuredTool(BaseTool):\n    \"\"\"Tool that can operate on any number of inputs.\"\"\"\n\n    description: str = \"\"\n    args_schema: Annotated[ArgsSchema, SkipValidation()] = Field(\n        ..., description=\"The tool schema.\"\n    )\n    \"\"\"The input arguments' schema.\"\"\"\n    func: Optional[Callable[..., Any]] = None\n    \"\"\"The function to run when the tool is called.\"\"\"\n    coroutine: Optional[Callable[..., Awaitable[Any]]] = None\n    \"\"\"The asynchronous version of the function.\"\"\"\n\n    # --- Runnable ---\n\n    # TODO: Is this needed?\n    async def ainvoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        if not self.coroutine:\n            # If the tool does not implement async, fall back to default implementation\n            return await run_in_executor(config, self.invoke, input, config, **kwargs)\n\n        return await super().ainvoke(input, config, **kwargs)\n\n    # --- Tool ---\n\n    @property\n    def args(self) -> dict:\n        \"\"\"The tool's input arguments.\"\"\"\n        if isinstance(self.args_schema, dict):\n            json_schema = self.args_schema\n        else:\n            input_schema = self.get_input_schema()\n            json_schema = input_schema.model_json_schema()\n        return json_schema[\"properties\"]\n\n    def _run(\n        self,\n        *args: Any,\n        config: RunnableConfig,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Use the tool.\"\"\"\n        if self.func:\n            if run_manager and signature(self.func).parameters.get(\"callbacks\"):\n                kwargs[\"callbacks\"] = run_manager.get_child()\n            if config_param := _get_runnable_config_param(self.func):\n                kwargs[config_param] = config\n            return self.func(*args, **kwargs)\n        msg = \"StructuredTool does not support sync invocation.\"\n        raise NotImplementedError(msg)\n\n    async def _arun(\n        self,\n        *args: Any,\n        config: RunnableConfig,\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Use the tool asynchronously.\"\"\"\n        if self.coroutine:\n            if run_manager and signature(self.coroutine).parameters.get(\"callbacks\"):\n                kwargs[\"callbacks\"] = run_manager.get_child()\n            if config_param := _get_runnable_config_param(self.coroutine):\n                kwargs[config_param] = config\n            return await self.coroutine(*args, **kwargs)\n\n        # If self.coroutine is None, then this will delegate to the default\n        # implementation which is expected to delegate to _run on a separate thread.\n        return await super()._arun(\n            *args, config=config, run_manager=run_manager, **kwargs\n        )\n\n    @classmethod\n    def from_function(\n        cls,\n        func: Optional[Callable] = None,\n        coroutine: Optional[Callable[..., Awaitable[Any]]] = None,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        return_direct: bool = False,\n        args_schema: Optional[ArgsSchema] = None,\n        infer_schema: bool = True,\n        *,\n        response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\",\n        parse_docstring: bool = False,\n        error_on_invalid_docstring: bool = False,\n        **kwargs: Any,\n    ) -> StructuredTool:\n        \"\"\"Create tool from a given function.\n\n        A classmethod that helps to create a tool from a function.\n\n        Args:\n            func: The function from which to create a tool.\n            coroutine: The async function from which to create a tool.\n            name: The name of the tool. Defaults to the function name.\n            description: The description of the tool.\n                Defaults to the function docstring.\n            return_direct: Whether to return the result directly or as a callback.\n                Defaults to False.\n            args_schema: The schema of the tool's input arguments. Defaults to None.\n            infer_schema: Whether to infer the schema from the function's signature.\n                Defaults to True.\n            response_format: The tool response format. If \"content\" then the output of\n                the tool is interpreted as the contents of a ToolMessage. If\n                \"content_and_artifact\" then the output is expected to be a two-tuple\n                corresponding to the (content, artifact) of a ToolMessage.\n                Defaults to \"content\".\n            parse_docstring: if ``infer_schema`` and ``parse_docstring``, will attempt\n                to parse parameter descriptions from Google Style function docstrings.\n                Defaults to False.\n            error_on_invalid_docstring: if ``parse_docstring`` is provided, configure\n                whether to raise ValueError on invalid Google Style docstrings.\n                Defaults to False.\n            kwargs: Additional arguments to pass to the tool\n\n        Returns:\n            The tool.\n\n        Raises:\n            ValueError: If the function is not provided.\n\n        Examples:\n\n            .. code-block:: python\n\n                def add(a: int, b: int) -> int:\n                    \\\"\\\"\\\"Add two numbers\\\"\\\"\\\"\n                    return a + b\n                tool = StructuredTool.from_function(add)\n                tool.run(1, 2) # 3\n        \"\"\"\n        if func is not None:\n            source_function = func\n        elif coroutine is not None:\n            source_function = coroutine\n        else:\n            msg = \"Function and/or coroutine must be provided\"\n            raise ValueError(msg)\n        name = name or source_function.__name__\n        if args_schema is None and infer_schema:\n            # schema name is appended within function\n            args_schema = create_schema_from_function(\n                name,\n                source_function,\n                parse_docstring=parse_docstring,\n                error_on_invalid_docstring=error_on_invalid_docstring,\n                filter_args=_filter_schema_args(source_function),\n            )\n        description_ = description\n        if description is None and not parse_docstring:\n            description_ = source_function.__doc__ or None\n        if description_ is None and args_schema:\n            description_ = args_schema.__doc__ or None\n        if description_ is None:\n            msg = \"Function must have a docstring if description not provided.\"\n            raise ValueError(msg)\n        if description is None:\n            # Only apply if using the function's docstring\n            description_ = textwrap.dedent(description_).strip()\n\n        # Description example:\n        # search_api(query: str) - Searches the API for the query.\n        description_ = f\"{description_.strip()}\"\n        return cls(\n            name=name,\n            func=func,\n            coroutine=coroutine,\n            args_schema=args_schema,  # type: ignore[arg-type]\n            description=description_,\n            return_direct=return_direct,\n            response_format=response_format,\n            **kwargs,\n        )\n\n\ndef _filter_schema_args(func: Callable) -> list[str]:\n    filter_args = list(FILTERED_ARGS)\n    if config_param := _get_runnable_config_param(func):\n        filter_args.append(config_param)\n    # filter_args.extend(_get_non_model_params(type_hints))\n    return filter_args\n",
        "patch": "@@ -27,6 +27,7 @@\n     _get_runnable_config_param,\n     create_schema_from_function,\n )\n+from langchain_core.utils.pydantic import is_basemodel_subclass\n \n \n class StructuredTool(BaseTool):\n@@ -188,7 +189,16 @@ def add(a: int, b: int) -> int:\n         if description is None and not parse_docstring:\n             description_ = source_function.__doc__ or None\n         if description_ is None and args_schema:\n-            description_ = args_schema.__doc__ or None\n+            if isinstance(args_schema, type) and is_basemodel_subclass(args_schema):\n+                description_ = args_schema.__doc__ or None\n+            elif isinstance(args_schema, dict):\n+                description_ = args_schema.get(\"description\")\n+            else:\n+                msg = (\n+                    \"Invalid args_schema: expected BaseModel or dict, \"\n+                    f\"got {args_schema}\"\n+                )\n+                raise TypeError(msg)\n         if description_ is None:\n             msg = \"Function must have a docstring if description not provided.\"\n             raise ValueError(msg)"
      },
      {
        "filename": "libs/core/tests/unit_tests/test_tools.py",
        "content_before": "\"\"\"Test the base tool implementation.\"\"\"\n\nimport inspect\nimport json\nimport sys\nimport textwrap\nimport threading\nfrom datetime import datetime\nfrom enum import Enum\nfrom functools import partial\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Generic,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport pytest\nfrom pydantic import BaseModel, Field, ValidationError\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom pydantic.v1 import ValidationError as ValidationErrorV1\nfrom typing_extensions import TypedDict\n\nfrom langchain_core import tools\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.callbacks.manager import (\n    CallbackManagerForRetrieverRun,\n)\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import ToolCall, ToolMessage\nfrom langchain_core.messages.tool import ToolOutputMixin\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableConfig,\n    RunnableLambda,\n    ensure_config,\n)\nfrom langchain_core.tools import (\n    BaseTool,\n    StructuredTool,\n    Tool,\n    ToolException,\n    tool,\n)\nfrom langchain_core.tools.base import (\n    InjectedToolArg,\n    InjectedToolCallId,\n    SchemaAnnotationError,\n    _is_message_content_block,\n    _is_message_content_type,\n    get_all_basemodel_annotations,\n)\nfrom langchain_core.utils.function_calling import convert_to_openai_function\nfrom langchain_core.utils.pydantic import (\n    PYDANTIC_MAJOR_VERSION,\n    _create_subset_model,\n    create_model_v2,\n)\nfrom tests.unit_tests.fake.callbacks import FakeCallbackHandler\nfrom tests.unit_tests.pydantic_utils import _schema\n\n\ndef _get_tool_call_json_schema(tool: BaseTool) -> dict:\n    tool_schema = tool.tool_call_schema\n    if isinstance(tool_schema, dict):\n        return tool_schema\n\n    if hasattr(tool_schema, \"model_json_schema\"):\n        return tool_schema.model_json_schema()\n    else:\n        return tool_schema.schema()\n\n\ndef test_unnamed_decorator() -> None:\n    \"\"\"Test functionality with unnamed decorator.\"\"\"\n\n    @tool\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search_api\"\n    assert not search_api.return_direct\n    assert search_api.invoke(\"test\") == \"API result\"\n\n\nclass _MockSchema(BaseModel):\n    \"\"\"Return the arguments directly.\"\"\"\n\n    arg1: int\n    arg2: bool\n    arg3: Optional[dict] = None\n\n\nclass _MockSchemaV1(BaseModelV1):\n    \"\"\"Return the arguments directly.\"\"\"\n\n    arg1: int\n    arg2: bool\n    arg3: Optional[dict] = None\n\n\nclass _MockStructuredTool(BaseTool):\n    name: str = \"structured_api\"\n    args_schema: type[BaseModel] = _MockSchema\n    description: str = \"A Structured Tool\"\n\n    def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    async def _arun(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        raise NotImplementedError\n\n\ndef test_structured_args() -> None:\n    \"\"\"Test functionality with structured arguments.\"\"\"\n    structured_api = _MockStructuredTool()\n    assert isinstance(structured_api, BaseTool)\n    assert structured_api.name == \"structured_api\"\n    expected_result = \"1 True {'foo': 'bar'}\"\n    args = {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"foo\": \"bar\"}}\n    assert structured_api.run(args) == expected_result\n\n\ndef test_misannotated_base_tool_raises_error() -> None:\n    \"\"\"Test that a BaseTool with the incorrect typehint raises an exception.\"\"\" \"\"\n    with pytest.raises(SchemaAnnotationError):\n\n        class _MisAnnotatedTool(BaseTool):\n            name: str = \"structured_api\"\n            # This would silently be ignored without the custom metaclass\n            args_schema: BaseModel = _MockSchema  # type: ignore\n            description: str = \"A Structured Tool\"\n\n            def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n                return f\"{arg1} {arg2} {arg3}\"\n\n            async def _arun(\n                self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n            ) -> str:\n                raise NotImplementedError\n\n\ndef test_forward_ref_annotated_base_tool_accepted() -> None:\n    \"\"\"Test that a using forward ref annotation syntax is accepted.\"\"\" \"\"\n\n    class _ForwardRefAnnotatedTool(BaseTool):\n        name: str = \"structured_api\"\n        args_schema: \"type[BaseModel]\" = _MockSchema\n        description: str = \"A Structured Tool\"\n\n        def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n            return f\"{arg1} {arg2} {arg3}\"\n\n        async def _arun(\n            self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n        ) -> str:\n            raise NotImplementedError\n\n\ndef test_subclass_annotated_base_tool_accepted() -> None:\n    \"\"\"Test BaseTool child w/ custom schema isn't overwritten.\"\"\"\n\n    class _ForwardRefAnnotatedTool(BaseTool):\n        name: str = \"structured_api\"\n        args_schema: type[_MockSchema] = _MockSchema\n        description: str = \"A Structured Tool\"\n\n        def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n            return f\"{arg1} {arg2} {arg3}\"\n\n        async def _arun(\n            self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n        ) -> str:\n            raise NotImplementedError\n\n    assert issubclass(_ForwardRefAnnotatedTool, BaseTool)\n    tool = _ForwardRefAnnotatedTool()\n    assert tool.args_schema == _MockSchema\n\n\ndef test_decorator_with_specified_schema() -> None:\n    \"\"\"Test that manually specified schemata are passed through to the tool.\"\"\"\n\n    @tool(args_schema=_MockSchema)\n    def tool_func(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(tool_func, BaseTool)\n    assert tool_func.args_schema == _MockSchema\n\n    @tool(args_schema=_MockSchemaV1)\n    def tool_func_v1(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(tool_func_v1, BaseTool)\n    assert tool_func_v1.args_schema == _MockSchemaV1\n\n\ndef test_decorated_function_schema_equivalent() -> None:\n    \"\"\"Test that a BaseTool without a schema meets expectations.\"\"\"\n\n    @tool\n    def structured_tool_input(\n        arg1: int, arg2: bool, arg3: Optional[dict] = None\n    ) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(structured_tool_input, BaseTool)\n    assert structured_tool_input.args_schema is not None\n    assert (\n        _schema(structured_tool_input.args_schema)[\"properties\"]\n        == _schema(_MockSchema)[\"properties\"]\n        == structured_tool_input.args\n    )\n\n\ndef test_args_kwargs_filtered() -> None:\n    class _SingleArgToolWithKwargs(BaseTool):\n        name: str = \"single_arg_tool\"\n        description: str = \"A  single arged tool with kwargs\"\n\n        def _run(\n            self,\n            some_arg: str,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            return \"foo\"\n\n        async def _arun(\n            self,\n            some_arg: str,\n            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            raise NotImplementedError\n\n    tool = _SingleArgToolWithKwargs()\n    assert tool.is_single_input\n\n    class _VarArgToolWithKwargs(BaseTool):\n        name: str = \"single_arg_tool\"\n        description: str = \"A single arged tool with kwargs\"\n\n        def _run(\n            self,\n            *args: Any,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            return \"foo\"\n\n        async def _arun(\n            self,\n            *args: Any,\n            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            raise NotImplementedError\n\n    tool2 = _VarArgToolWithKwargs()\n    assert tool2.is_single_input\n\n\ndef test_structured_args_decorator_no_infer_schema() -> None:\n    \"\"\"Test functionality with structured arguments parsed as a decorator.\"\"\"\n\n    @tool(infer_schema=False)\n    def structured_tool_input(\n        arg1: int, arg2: Union[float, datetime], opt_arg: Optional[dict] = None\n    ) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        return f\"{arg1}, {arg2}, {opt_arg}\"\n\n    assert isinstance(structured_tool_input, BaseTool)\n    assert structured_tool_input.name == \"structured_tool_input\"\n    args = {\"arg1\": 1, \"arg2\": 0.001, \"opt_arg\": {\"foo\": \"bar\"}}\n    with pytest.raises(ToolException):\n        assert structured_tool_input.run(args)\n\n\ndef test_structured_single_str_decorator_no_infer_schema() -> None:\n    \"\"\"Test functionality with structured arguments parsed as a decorator.\"\"\"\n\n    @tool(infer_schema=False)\n    def unstructured_tool_input(tool_input: str) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        assert isinstance(tool_input, str)\n        return f\"{tool_input}\"\n\n    assert isinstance(unstructured_tool_input, BaseTool)\n    assert unstructured_tool_input.args_schema is None\n    assert unstructured_tool_input.run(\"foo\") == \"foo\"\n\n\ndef test_structured_tool_types_parsed() -> None:\n    \"\"\"Test the non-primitive types are correctly passed to structured tools.\"\"\"\n\n    class SomeEnum(Enum):\n        A = \"a\"\n        B = \"b\"\n\n    class SomeBaseModel(BaseModel):\n        foo: str\n\n    @tool\n    def structured_tool(\n        some_enum: SomeEnum,\n        some_base_model: SomeBaseModel,\n    ) -> dict:\n        \"\"\"Return the arguments directly.\"\"\"\n        return {\n            \"some_enum\": some_enum,\n            \"some_base_model\": some_base_model,\n        }\n\n    assert isinstance(structured_tool, StructuredTool)\n    args = {\n        \"some_enum\": SomeEnum.A.value,\n        \"some_base_model\": SomeBaseModel(foo=\"bar\").model_dump(),\n    }\n    result = structured_tool.run(json.loads(json.dumps(args)))\n    expected = {\n        \"some_enum\": SomeEnum.A,\n        \"some_base_model\": SomeBaseModel(foo=\"bar\"),\n    }\n    assert result == expected\n\n\ndef test_structured_tool_types_parsed_pydantic_v1() -> None:\n    \"\"\"Test the non-primitive types are correctly passed to structured tools.\"\"\"\n\n    class SomeBaseModel(BaseModelV1):\n        foo: str\n\n    class AnotherBaseModel(BaseModelV1):\n        bar: str\n\n    @tool\n    def structured_tool(some_base_model: SomeBaseModel) -> AnotherBaseModel:\n        \"\"\"Return the arguments directly.\"\"\"\n        return AnotherBaseModel(bar=some_base_model.foo)\n\n    assert isinstance(structured_tool, StructuredTool)\n\n    expected = AnotherBaseModel(bar=\"baz\")\n    for arg in [\n        SomeBaseModel(foo=\"baz\"),\n        SomeBaseModel(foo=\"baz\").dict(),\n    ]:\n        args = {\"some_base_model\": arg}\n        result = structured_tool.run(args)\n        assert result == expected\n\n\ndef test_structured_tool_types_parsed_pydantic_mixed() -> None:\n    \"\"\"Test handling of tool with mixed Pydantic version arguments.\"\"\"\n\n    class SomeBaseModel(BaseModelV1):\n        foo: str\n\n    class AnotherBaseModel(BaseModel):\n        bar: str\n\n    with pytest.raises(NotImplementedError):\n\n        @tool\n        def structured_tool(\n            some_base_model: SomeBaseModel, another_base_model: AnotherBaseModel\n        ) -> None:\n            \"\"\"Return the arguments directly.\"\"\"\n\n\ndef test_base_tool_inheritance_base_schema() -> None:\n    \"\"\"Test schema is correctly inferred when inheriting from BaseTool.\"\"\"\n\n    class _MockSimpleTool(BaseTool):\n        name: str = \"simple_tool\"\n        description: str = \"A Simple Tool\"\n\n        def _run(self, tool_input: str) -> str:\n            return f\"{tool_input}\"\n\n        async def _arun(self, tool_input: str) -> str:\n            raise NotImplementedError\n\n    simple_tool = _MockSimpleTool()\n    assert simple_tool.args_schema is None\n    expected_args = {\"tool_input\": {\"title\": \"Tool Input\", \"type\": \"string\"}}\n    assert simple_tool.args == expected_args\n\n\ndef test_tool_lambda_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a lambda function.\"\"\"\n    tool = Tool(\n        name=\"tool\",\n        description=\"A tool\",\n        func=lambda tool_input: tool_input,\n    )\n    assert tool.args_schema is None\n    expected_args = {\"tool_input\": {\"type\": \"string\"}}\n    assert tool.args == expected_args\n\n\ndef test_structured_tool_from_function_docstring() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: str) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: the bar value\n            baz: the baz value\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__.strip())\n\n\ndef test_structured_tool_from_function_docstring_complex_args() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: list[str]) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: int\n            baz: List[str]\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\n            \"title\": \"Baz\",\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n        },\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\n                \"title\": \"Baz\",\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n            },\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__).strip()\n\n\ndef test_structured_tool_lambda_multi_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a lambda function.\"\"\"\n    tool = StructuredTool.from_function(\n        name=\"tool\",\n        description=\"A tool\",\n        func=lambda tool_input, other_arg: f\"{tool_input}{other_arg}\",  # type: ignore\n    )\n    assert tool.args_schema is not None\n    expected_args = {\n        \"tool_input\": {\"title\": \"Tool Input\"},\n        \"other_arg\": {\"title\": \"Other Arg\"},\n    }\n    assert tool.args == expected_args\n\n\ndef test_tool_partial_function_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a partial function.\"\"\"\n\n    def func(tool_input: str, other_arg: str) -> str:\n        assert isinstance(tool_input, str)\n        assert isinstance(other_arg, str)\n        return tool_input + other_arg\n\n    tool = Tool(\n        name=\"tool\",\n        description=\"A tool\",\n        func=partial(func, other_arg=\"foo\"),\n    )\n    assert tool.run(\"bar\") == \"barfoo\"\n\n\ndef test_empty_args_decorator() -> None:\n    \"\"\"Test inferred schema of decorated fn with no args.\"\"\"\n\n    @tool\n    def empty_tool_input() -> str:\n        \"\"\"Return a constant.\"\"\"\n        return \"the empty result\"\n\n    assert isinstance(empty_tool_input, BaseTool)\n    assert empty_tool_input.name == \"empty_tool_input\"\n    assert empty_tool_input.args == {}\n    assert empty_tool_input.run({}) == \"the empty result\"\n\n\ndef test_tool_from_function_with_run_manager() -> None:\n    \"\"\"Test run of tool when using run_manager.\"\"\"\n\n    def foo(bar: str, callbacks: Optional[CallbackManagerForToolRun] = None) -> str:\n        \"\"\"Docstring\n        Args:\n            bar: str.\n        \"\"\"\n        assert callbacks is not None\n        return \"foo\" + bar\n\n    handler = FakeCallbackHandler()\n    tool = Tool.from_function(foo, name=\"foo\", description=\"Docstring\")\n\n    assert tool.run(tool_input={\"bar\": \"bar\"}, run_manager=[handler]) == \"foobar\"\n    assert tool.run(\"baz\", run_manager=[handler]) == \"foobaz\"\n\n\ndef test_structured_tool_from_function_with_run_manager() -> None:\n    \"\"\"Test args and schema of structured tool when using callbacks.\"\"\"\n\n    def foo(\n        bar: int, baz: str, callbacks: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: int\n            baz: str\n        \"\"\"\n        assert callbacks is not None\n        return str(bar) + baz\n\n    handler = FakeCallbackHandler()\n    structured_tool = StructuredTool.from_function(foo)\n\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert (\n        structured_tool.run(\n            tool_input={\"bar\": \"10\", \"baz\": \"baz\"}, run_manger=[handler]\n        )\n        == \"10baz\"\n    )\n\n\ndef test_structured_tool_from_parameterless_function() -> None:\n    \"\"\"Test parameterless function of structured tool.\"\"\"\n\n    def foo() -> str:\n        \"\"\"Docstring.\"\"\"\n        return \"invoke foo\"\n\n    structured_tool = StructuredTool.from_function(foo)\n\n    assert structured_tool.run({}) == \"invoke foo\"\n    assert structured_tool.run(\"\") == \"invoke foo\"\n\n\ndef test_named_tool_decorator() -> None:\n    \"\"\"Test functionality when arguments are provided as input to decorator.\"\"\"\n\n    @tool(\"search\")\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        assert isinstance(query, str)\n        return f\"API result - {query}\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search\"\n    assert not search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result - foo\"\n\n\ndef test_named_tool_decorator_return_direct() -> None:\n    \"\"\"Test functionality when arguments and return direct are provided as input.\"\"\"\n\n    @tool(\"search\", return_direct=True)\n    def search_api(query: str, *args: Any) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search\"\n    assert search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result\"\n\n\ndef test_unnamed_tool_decorator_return_direct() -> None:\n    \"\"\"Test functionality when only return direct is provided.\"\"\"\n\n    @tool(return_direct=True)\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        assert isinstance(query, str)\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search_api\"\n    assert search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result\"\n\n\ndef test_tool_with_kwargs() -> None:\n    \"\"\"Test functionality when only return direct is provided.\"\"\"\n\n    @tool(return_direct=True)\n    def search_api(\n        arg_0: str,\n        arg_1: float = 4.3,\n        ping: str = \"hi\",\n    ) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return f\"arg_0={arg_0}, arg_1={arg_1}, ping={ping}\"\n\n    assert isinstance(search_api, BaseTool)\n    result = search_api.run(\n        tool_input={\n            \"arg_0\": \"foo\",\n            \"arg_1\": 3.2,\n            \"ping\": \"pong\",\n        }\n    )\n    assert result == \"arg_0=foo, arg_1=3.2, ping=pong\"\n\n    result = search_api.run(\n        tool_input={\n            \"arg_0\": \"foo\",\n        }\n    )\n    assert result == \"arg_0=foo, arg_1=4.3, ping=hi\"\n    # For backwards compatibility, we still accept a single str arg\n    result = search_api.run(\"foobar\")\n    assert result == \"arg_0=foobar, arg_1=4.3, ping=hi\"\n\n\ndef test_missing_docstring() -> None:\n    \"\"\"Test error is raised when docstring is missing.\"\"\"\n    # expect to throw a value error if there's no docstring\n    with pytest.raises(ValueError, match=\"Function must have a docstring\"):\n\n        @tool\n        def search_api(query: str) -> str:\n            return \"API result\"\n\n\ndef test_create_tool_positional_args() -> None:\n    \"\"\"Test that positional arguments are allowed.\"\"\"\n    test_tool = Tool(\"test_name\", lambda x: x, \"test_description\")\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n    assert test_tool.is_single_input\n\n\ndef test_create_tool_keyword_args() -> None:\n    \"\"\"Test that keyword arguments are allowed.\"\"\"\n    test_tool = Tool(name=\"test_name\", func=lambda x: x, description=\"test_description\")\n    assert test_tool.is_single_input\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n\n\nasync def test_create_async_tool() -> None:\n    \"\"\"Test that async tools are allowed.\"\"\"\n\n    async def _test_func(x: str) -> str:\n        return x\n\n    test_tool = Tool(\n        name=\"test_name\",\n        func=lambda x: x,\n        description=\"test_description\",\n        coroutine=_test_func,\n    )\n    assert test_tool.is_single_input\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n    assert test_tool.coroutine is not None\n    assert await test_tool.arun(\"foo\") == \"foo\"\n\n\nclass _FakeExceptionTool(BaseTool):\n    name: str = \"exception\"\n    description: str = \"an exception-throwing tool\"\n    exception: Exception = ToolException()\n\n    def _run(self) -> str:\n        raise self.exception\n\n    async def _arun(self) -> str:\n        raise self.exception\n\n\ndef test_exception_handling_bool() -> None:\n    _tool = _FakeExceptionTool(handle_tool_error=True)\n    expected = \"Tool execution error\"\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_str() -> None:\n    expected = \"foo bar\"\n    _tool = _FakeExceptionTool(handle_tool_error=expected)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_callable() -> None:\n    expected = \"foo bar\"\n\n    def handling(e: ToolException) -> str:\n        return expected\n\n    _tool = _FakeExceptionTool(handle_tool_error=handling)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_non_tool_exception() -> None:\n    _tool = _FakeExceptionTool(exception=ValueError())\n    with pytest.raises(ValueError):\n        _tool.run({})\n\n\nasync def test_async_exception_handling_bool() -> None:\n    _tool = _FakeExceptionTool(handle_tool_error=True)\n    expected = \"Tool execution error\"\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_str() -> None:\n    expected = \"foo bar\"\n    _tool = _FakeExceptionTool(handle_tool_error=expected)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_callable() -> None:\n    expected = \"foo bar\"\n\n    def handling(e: ToolException) -> str:\n        return expected\n\n    _tool = _FakeExceptionTool(handle_tool_error=handling)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_non_tool_exception() -> None:\n    _tool = _FakeExceptionTool(exception=ValueError())\n    with pytest.raises(ValueError):\n        await _tool.arun({})\n\n\ndef test_structured_tool_from_function() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: str) -> str:\n        \"\"\"Docstring thing.\n\n        Args:\n            bar: the bar value\n            baz: the baz value\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__.strip())\n\n\ndef test_validation_error_handling_bool() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"Tool input validation error\"\n    _tool = _MockStructuredTool(handle_validation_error=True)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_validation_error_handling_str() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n    _tool = _MockStructuredTool(handle_validation_error=expected)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_validation_error_handling_callable() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n\n    def handling(e: Union[ValidationError, ValidationErrorV1]) -> str:\n        return expected\n\n    _tool = _MockStructuredTool(handle_validation_error=handling)\n    actual = _tool.run({})\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\n    \"handler\",\n    [\n        True,\n        \"foo bar\",\n        lambda _: \"foo bar\",\n    ],\n)\ndef test_validation_error_handling_non_validation_error(\n    handler: Union[\n        bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n\n    class _RaiseNonValidationErrorTool(BaseTool):\n        name: str = \"raise_non_validation_error_tool\"\n        description: str = \"A tool that raises a non-validation error\"\n\n        def _parse_input(\n            self,\n            tool_input: Union[str, dict],\n            tool_call_id: Optional[str],\n        ) -> Union[str, dict[str, Any]]:\n            raise NotImplementedError\n\n        def _run(self) -> str:\n            return \"dummy\"\n\n        async def _arun(self) -> str:\n            return \"dummy\"\n\n    _tool = _RaiseNonValidationErrorTool(handle_validation_error=handler)  # type: ignore[call-arg]\n    with pytest.raises(NotImplementedError):\n        _tool.run({})\n\n\nasync def test_async_validation_error_handling_bool() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"Tool input validation error\"\n    _tool = _MockStructuredTool(handle_validation_error=True)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_validation_error_handling_str() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n    _tool = _MockStructuredTool(handle_validation_error=expected)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_validation_error_handling_callable() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n\n    def handling(e: Union[ValidationError, ValidationErrorV1]) -> str:\n        return expected\n\n    _tool = _MockStructuredTool(handle_validation_error=handling)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\n    \"handler\",\n    [\n        True,\n        \"foo bar\",\n        lambda _: \"foo bar\",\n    ],\n)\nasync def test_async_validation_error_handling_non_validation_error(\n    handler: Union[\n        bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n\n    class _RaiseNonValidationErrorTool(BaseTool):\n        name: str = \"raise_non_validation_error_tool\"\n        description: str = \"A tool that raises a non-validation error\"\n\n        def _parse_input(\n            self,\n            tool_input: Union[str, dict],\n            tool_call_id: Optional[str],\n        ) -> Union[str, dict[str, Any]]:\n            raise NotImplementedError\n\n        def _run(self) -> str:\n            return \"dummy\"\n\n        async def _arun(self) -> str:\n            return \"dummy\"\n\n    _tool = _RaiseNonValidationErrorTool(handle_validation_error=handler)  # type: ignore[call-arg]\n    with pytest.raises(NotImplementedError):\n        await _tool.arun({})\n\n\ndef test_optional_subset_model_rewrite() -> None:\n    class MyModel(BaseModel):\n        a: Optional[str] = None\n        b: str\n        c: Optional[list[Optional[str]]] = None\n\n    model2 = _create_subset_model(\"model2\", MyModel, [\"a\", \"b\", \"c\"])\n\n    assert set(_schema(model2)[\"required\"]) == {\"b\"}\n\n\n@pytest.mark.parametrize(\n    \"inputs, expected\",\n    [\n        # Check not required\n        ({\"bar\": \"bar\"}, {\"bar\": \"bar\", \"baz\": 3, \"buzz\": \"buzz\"}),\n        # Check overwritten\n        (\n            {\"bar\": \"bar\", \"baz\": 4, \"buzz\": \"not-buzz\"},\n            {\"bar\": \"bar\", \"baz\": 4, \"buzz\": \"not-buzz\"},\n        ),\n        # Check validation error when missing\n        ({}, None),\n        # Check validation error when wrong type\n        ({\"bar\": \"bar\", \"baz\": \"not-an-int\"}, None),\n        # Check OK when None explicitly passed\n        ({\"bar\": \"bar\", \"baz\": None}, {\"bar\": \"bar\", \"baz\": None, \"buzz\": \"buzz\"}),\n    ],\n)\ndef test_tool_invoke_optional_args(inputs: dict, expected: Optional[dict]) -> None:\n    @tool\n    def foo(bar: str, baz: Optional[int] = 3, buzz: Optional[str] = \"buzz\") -> dict:\n        \"\"\"The foo.\"\"\"\n        return {\n            \"bar\": bar,\n            \"baz\": baz,\n            \"buzz\": buzz,\n        }\n\n    if expected is not None:\n        assert foo.invoke(inputs) == expected  # type: ignore\n    else:\n        with pytest.raises(ValidationError):\n            foo.invoke(inputs)  # type: ignore\n\n\ndef test_tool_pass_context() -> None:\n    @tool\n    def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        assert bar == \"baz\"\n        return bar\n\n    assert foo.invoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"  # type: ignore\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=\"requires python3.11 or higher\",\n)\nasync def test_async_tool_pass_context() -> None:\n    @tool\n    async def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        assert bar == \"baz\"\n        return bar\n\n    assert (\n        await foo.ainvoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"  # type: ignore\n    )\n\n\ndef assert_bar(bar: Any, bar_config: RunnableConfig) -> Any:\n    assert bar_config[\"configurable\"][\"foo\"] == \"not-bar\"\n    assert bar == \"baz\"\n    return bar\n\n\n@tool\ndef foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool\nasync def afoo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool(infer_schema=False)\ndef simple_foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool(infer_schema=False)\nasync def asimple_foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\nclass FooBase(BaseTool):\n    name: str = \"Foo\"\n    description: str = \"Foo\"\n\n    def _run(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return assert_bar(bar, bar_config)\n\n\nFooBase.model_rebuild()\n\n\nclass AFooBase(FooBase):\n    async def _arun(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return assert_bar(bar, bar_config)\n\n\n@pytest.mark.parametrize(\"tool\", [foo, simple_foo, FooBase(), AFooBase()])\ndef test_tool_pass_config(tool: BaseTool) -> None:\n    assert tool.invoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"\n\n    # Test we don't mutate tool calls\n    tool_call = {\n        \"name\": tool.name,\n        \"args\": {\"bar\": \"baz\"},\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    _ = tool.invoke(tool_call, {\"configurable\": {\"foo\": \"not-bar\"}})\n    assert tool_call[\"args\"] == {\"bar\": \"baz\"}\n\n\nclass FooBaseNonPickleable(FooBase):\n    def _run(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return True\n\n\ndef test_tool_pass_config_non_pickleable() -> None:\n    tool = FooBaseNonPickleable()\n\n    args = {\"bar\": threading.Lock()}\n    tool_call = {\n        \"name\": tool.name,\n        \"args\": args,\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    _ = tool.invoke(tool_call, {\"configurable\": {\"foo\": \"not-bar\"}})\n    assert tool_call[\"args\"] == args\n\n\n@pytest.mark.parametrize(\n    \"tool\", [foo, afoo, simple_foo, asimple_foo, FooBase(), AFooBase()]\n)\nasync def test_async_tool_pass_config(tool: BaseTool) -> None:\n    assert (\n        await tool.ainvoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}})\n        == \"baz\"\n    )\n\n\ndef test_tool_description() -> None:\n    def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    assert foo1.description == \"The foo.\"  # type: ignore\n\n    foo2 = StructuredTool.from_function(foo)\n    assert foo2.description == \"The foo.\"\n\n\ndef test_tool_arg_descriptions() -> None:\n    def foo(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    args_schema = _schema(foo1.args_schema)  # type: ignore\n    assert args_schema == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    # Test parses docstring\n    foo2 = tool(foo, parse_docstring=True)\n    args_schema = _schema(foo2.args_schema)  # type: ignore\n    expected = {\n        \"title\": \"foo\",\n        \"description\": \"The foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"description\": \"The bar.\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"description\": \"The baz.\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n    assert args_schema == expected\n\n    # Test parsing with run_manager does not raise error\n    def foo3(\n        bar: str, baz: int, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo3, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n    assert args_schema[\"properties\"] == expected[\"properties\"]\n\n    # Test parameterless tool does not raise error for missing Args section\n    # in docstring.\n    def foo4() -> str:\n        \"\"\"The foo.\"\"\"\n        return \"bar\"\n\n    as_tool = tool(foo4, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n\n    def foo5(run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n        \"\"\"The foo.\"\"\"\n        return \"bar\"\n\n    as_tool = tool(foo5, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n\n\ndef test_docstring_parsing() -> None:\n    expected = {\n        \"title\": \"foo\",\n        \"description\": \"The foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"description\": \"The bar.\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"description\": \"The baz.\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    # Simple case\n    def foo(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == \"The foo.\"\n    assert args_schema[\"properties\"] == expected[\"properties\"]\n\n    # Multi-line description\n    def foo2(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Additional description here.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo2, parse_docstring=True)\n    args_schema2 = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema2[\"description\"] == \"The foo. Additional description here.\"\n    assert args_schema2[\"properties\"] == expected[\"properties\"]\n\n    # Multi-line wth Returns block\n    def foo3(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Additional description here.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n\n        Returns:\n            str: description of returned value.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo3, parse_docstring=True)\n    args_schema3 = _schema(as_tool.args_schema)  # type: ignore\n    args_schema3[\"title\"] = \"foo2\"\n    assert args_schema2 == args_schema3\n\n    # Single argument\n    def foo4(bar: str) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo4, parse_docstring=True)\n    args_schema4 = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema4[\"description\"] == \"The foo.\"\n    assert args_schema4[\"properties\"] == {\n        \"bar\": {\"description\": \"The bar.\", \"title\": \"Bar\", \"type\": \"string\"}\n    }\n\n\ndef test_tool_invalid_docstrings() -> None:\n    # Test invalid docstrings\n    def foo3(bar: str, baz: int) -> str:\n        \"\"\"The foo.\"\"\"\n        return bar\n\n    def foo4(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    def foo5(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            banana: The bar.\n            monkey: The baz.\n        \"\"\"\n        return bar\n\n    for func in [foo3, foo4, foo5]:\n        with pytest.raises(ValueError):\n            _ = tool(func, parse_docstring=True)\n\n\ndef test_tool_annotated_descriptions() -> None:\n    def foo(\n        bar: Annotated[str, \"this is the bar\"], baz: Annotated[int, \"this is the baz\"]\n    ) -> str:\n        \"\"\"The foo.\n\n        Returns:\n            The bar only.\n        \"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    args_schema = _schema(foo1.args_schema)  # type: ignore\n    assert args_schema == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\", \"description\": \"this is the bar\"},\n            \"baz\": {\n                \"title\": \"Baz\",\n                \"type\": \"integer\",\n                \"description\": \"this is the baz\",\n            },\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n\ndef test_tool_call_input_tool_message_output() -> None:\n    tool_call = {\n        \"name\": \"structured_api\",\n        \"args\": {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"img\": \"base64string...\"}},\n        \"id\": \"123\",\n        \"type\": \"tool_call\",\n    }\n    tool = _MockStructuredTool()\n    expected = ToolMessage(\n        \"1 True {'img': 'base64string...'}\", tool_call_id=\"123\", name=\"structured_api\"\n    )\n    actual = tool.invoke(tool_call)\n    assert actual == expected\n\n    tool_call.pop(\"type\")\n    with pytest.raises(ValidationError):\n        tool.invoke(tool_call)\n\n\nclass _MockStructuredToolWithRawOutput(BaseTool):\n    name: str = \"structured_api\"\n    args_schema: type[BaseModel] = _MockSchema\n    description: str = \"A Structured Tool\"\n    response_format: Literal[\"content_and_artifact\"] = \"content_and_artifact\"\n\n    def _run(\n        self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n    ) -> tuple[str, dict]:\n        return f\"{arg1} {arg2}\", {\"arg1\": arg1, \"arg2\": arg2, \"arg3\": arg3}\n\n\n@tool(\"structured_api\", response_format=\"content_and_artifact\")\ndef _mock_structured_tool_with_artifact(\n    arg1: int, arg2: bool, arg3: Optional[dict] = None\n) -> tuple[str, dict]:\n    \"\"\"A Structured Tool.\"\"\"\n    return f\"{arg1} {arg2}\", {\"arg1\": arg1, \"arg2\": arg2, \"arg3\": arg3}\n\n\n@pytest.mark.parametrize(\n    \"tool\", [_MockStructuredToolWithRawOutput(), _mock_structured_tool_with_artifact]\n)\ndef test_tool_call_input_tool_message_with_artifact(tool: BaseTool) -> None:\n    tool_call: dict = {\n        \"name\": \"structured_api\",\n        \"args\": {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"img\": \"base64string...\"}},\n        \"id\": \"123\",\n        \"type\": \"tool_call\",\n    }\n    expected = ToolMessage(\n        \"1 True\", artifact=tool_call[\"args\"], tool_call_id=\"123\", name=\"structured_api\"\n    )\n    actual = tool.invoke(tool_call)\n    assert actual == expected\n\n    tool_call.pop(\"type\")\n    with pytest.raises(ValidationError):\n        tool.invoke(tool_call)\n\n    actual_content = tool.invoke(tool_call[\"args\"])\n    assert actual_content == expected.content\n\n\ndef test_convert_from_runnable_dict() -> None:\n    # Test with typed dict input\n    class Args(TypedDict):\n        a: int\n        b: list[int]\n\n    def f(x: Args) -> str:\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    runnable: Runnable = RunnableLambda(f)\n    as_tool = runnable.as_tool()\n    args_schema = as_tool.args_schema\n    assert args_schema is not None\n    assert _schema(args_schema) == {\n        \"title\": \"f\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"array\", \"items\": {\"type\": \"integer\"}},\n        },\n        \"required\": [\"a\", \"b\"],\n    }\n    assert as_tool.description\n    result = as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n    assert result == \"6\"\n\n    as_tool = runnable.as_tool(name=\"my tool\", description=\"test description\")\n    assert as_tool.name == \"my tool\"\n    assert as_tool.description == \"test description\"\n\n    # Dict without typed input-- must supply schema\n    def g(x: dict[str, Any]) -> str:\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    # Specify via args_schema:\n    class GSchema(BaseModel):\n        \"\"\"Apply a function to an integer and list of integers.\"\"\"\n\n        a: int = Field(..., description=\"Integer\")\n        b: list[int] = Field(..., description=\"List of ints\")\n\n    runnable = RunnableLambda(g)\n    as_tool = runnable.as_tool(GSchema)\n    as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n    # Specify via arg_types:\n    runnable = RunnableLambda(g)\n    as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n    result = as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n    assert result == \"6\"\n\n    # Test with config\n    def h(x: dict[str, Any]) -> str:\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    runnable = RunnableLambda(h)\n    as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n    result = as_tool.invoke(\n        {\"a\": 3, \"b\": [1, 2]}, config={\"configurable\": {\"foo\": \"not-bar\"}}\n    )\n    assert result == \"6\"\n\n\ndef test_convert_from_runnable_other() -> None:\n    # String input\n    def f(x: str) -> str:\n        return x + \"a\"\n\n    def g(x: str) -> str:\n        return x + \"z\"\n\n    runnable: Runnable = RunnableLambda(f) | g\n    as_tool = runnable.as_tool()\n    args_schema = as_tool.args_schema\n    assert args_schema is None\n    assert as_tool.description\n\n    result = as_tool.invoke(\"b\")\n    assert result == \"baz\"\n\n    # Test with config\n    def h(x: str) -> str:\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        return x + \"a\"\n\n    runnable = RunnableLambda(h)\n    as_tool = runnable.as_tool()\n    result = as_tool.invoke(\"b\", config={\"configurable\": {\"foo\": \"not-bar\"}})\n    assert result == \"ba\"\n\n\n@tool(\"foo\", parse_docstring=True)\ndef injected_tool(x: int, y: Annotated[str, InjectedToolArg]) -> str:\n    \"\"\"foo.\n\n    Args:\n        x: abc\n        y: 123\n    \"\"\"\n    return y\n\n\nclass InjectedTool(BaseTool):\n    name: str = \"foo\"\n    description: str = \"foo.\"\n\n    def _run(self, x: int, y: Annotated[str, InjectedToolArg]) -> Any:\n        \"\"\"foo.\n\n        Args:\n            x: abc\n            y: 123\n        \"\"\"\n        return y\n\n\nclass fooSchema(BaseModel):  # noqa: N801\n    \"\"\"foo.\"\"\"\n\n    x: int = Field(..., description=\"abc\")\n    y: Annotated[str, \"foobar comment\", InjectedToolArg()] = Field(\n        ..., description=\"123\"\n    )\n\n\nclass InjectedToolWithSchema(BaseTool):\n    name: str = \"foo\"\n    description: str = \"foo.\"\n    args_schema: type[BaseModel] = fooSchema\n\n    def _run(self, x: int, y: str) -> Any:\n        return y\n\n\n@tool(\"foo\", args_schema=fooSchema)\ndef injected_tool_with_schema(x: int, y: str) -> str:\n    return y\n\n\n@pytest.mark.parametrize(\"tool_\", [InjectedTool()])\ndef test_tool_injected_arg_without_schema(tool_: BaseTool) -> None:\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\\n\\nArgs:\\n    x: abc\\n    y: 123\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\n@pytest.mark.parametrize(\n    \"tool_\",\n    [injected_tool_with_schema, InjectedToolWithSchema()],\n)\ndef test_tool_injected_arg_with_schema(tool_: BaseTool) -> None:\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"fooSchema\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef test_tool_injected_arg() -> None:\n    tool_ = injected_tool\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef test_tool_inherited_injected_arg() -> None:\n    class BarSchema(BaseModel):\n        \"\"\"bar.\"\"\"\n\n        y: Annotated[str, \"foobar comment\", InjectedToolArg()] = Field(\n            ..., description=\"123\"\n        )\n\n    class FooSchema(BarSchema):\n        \"\"\"foo.\"\"\"\n\n        x: int = Field(..., description=\"abc\")\n\n    class InheritedInjectedArgTool(BaseTool):\n        name: str = \"foo\"\n        description: str = \"foo.\"\n        args_schema: type[BaseModel] = FooSchema\n\n        def _run(self, x: int, y: str) -> Any:\n            return y\n\n    tool_ = InheritedInjectedArgTool()\n    assert tool_.get_input_schema().model_json_schema() == {\n        \"title\": \"FooSchema\",  # Matches the title from the provided schema\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"y\", \"x\"],\n    }\n    # Should not include `y` since it's annotated as an injected tool arg\n    assert _get_tool_call_json_schema(tool_) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef _get_parametrized_tools() -> list:\n    def my_tool(x: int, y: str, some_tool: Annotated[Any, InjectedToolArg]) -> str:\n        \"\"\"my_tool.\"\"\"\n        return some_tool\n\n    async def my_async_tool(\n        x: int, y: str, *, some_tool: Annotated[Any, InjectedToolArg]\n    ) -> str:\n        \"\"\"my_tool.\"\"\"\n        return some_tool\n\n    return [my_tool, my_async_tool]\n\n\n@pytest.mark.parametrize(\"tool_\", _get_parametrized_tools())\ndef test_fn_injected_arg_with_schema(tool_: Callable) -> None:\n    assert convert_to_openai_function(tool_) == {\n        \"name\": tool_.__name__,\n        \"description\": \"my_tool.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"x\": {\"type\": \"integer\"},\n                \"y\": {\"type\": \"string\"},\n            },\n            \"required\": [\"x\", \"y\"],\n        },\n    }\n\n\ndef generate_models() -> list[Any]:\n    \"\"\"Generate a list of base models depending on the pydantic version.\"\"\"\n\n    class FooProper(BaseModel):\n        a: int\n        b: str\n\n    return [FooProper]\n\n\ndef generate_backwards_compatible_v1() -> list[Any]:\n    \"\"\"Generate a model with pydantic 2 from the v1 namespace.\"\"\"\n    from pydantic.v1 import BaseModel as BaseModelV1\n\n    class FooV1Namespace(BaseModelV1):\n        a: int\n        b: str\n\n    return [FooV1Namespace]\n\n\n# This generates a list of models that can be used for testing that our APIs\n# behave well with either pydantic 1 proper,\n# pydantic v1 from pydantic 2,\n# or pydantic 2 proper.\nTEST_MODELS = generate_models() + generate_backwards_compatible_v1()\n\n\n@pytest.mark.parametrize(\"pydantic_model\", TEST_MODELS)\ndef test_args_schema_as_pydantic(pydantic_model: Any) -> None:\n    class SomeTool(BaseTool):\n        args_schema: type[pydantic_model] = pydantic_model\n\n        def _run(self, *args: Any, **kwargs: Any) -> str:\n            return \"foo\"\n\n    tool = SomeTool(\n        name=\"some_tool\", description=\"some description\", args_schema=pydantic_model\n    )\n\n    input_schema = tool.get_input_schema()\n    input_json_schema = (\n        input_schema.model_json_schema()\n        if hasattr(input_schema, \"model_json_schema\")\n        else input_schema.schema()\n    )\n    assert input_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n    tool_json_schema = _get_tool_call_json_schema(tool)\n    assert tool_json_schema == {\n        \"description\": \"some description\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"some_tool\",\n        \"type\": \"object\",\n    }\n\n\ndef test_args_schema_explicitly_typed() -> None:\n    \"\"\"This should test that one can type the args schema as a pydantic model.\n\n    Please note that this will test using pydantic 2 even though BaseTool\n    is a pydantic 1 model!\n    \"\"\"\n    # Check with whatever pydantic model is passed in and not via v1 namespace\n    from pydantic import BaseModel\n\n    class Foo(BaseModel):\n        a: int\n        b: str\n\n    class SomeTool(BaseTool):\n        # type ignoring here since we're allowing overriding a type\n        # signature of pydantic.v1.BaseModel with pydantic.BaseModel\n        # for pydantic 2!\n        args_schema: type[BaseModel] = Foo  # type: ignore[assignment]\n\n        def _run(self, *args: Any, **kwargs: Any) -> str:\n            return \"foo\"\n\n    tool = SomeTool(name=\"some_tool\", description=\"some description\")\n\n    assert tool.get_input_schema().model_json_schema() == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"Foo\",\n        \"type\": \"object\",\n    }\n\n    assert _get_tool_call_json_schema(tool) == {\n        \"description\": \"some description\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"some_tool\",\n        \"type\": \"object\",\n    }\n\n\n@pytest.mark.parametrize(\"pydantic_model\", TEST_MODELS)\ndef test_structured_tool_with_different_pydantic_versions(pydantic_model: Any) -> None:\n    \"\"\"This should test that one can type the args schema as a pydantic model.\"\"\"\n    from langchain_core.tools import StructuredTool\n\n    def foo(a: int, b: str) -> str:\n        \"\"\"Hahaha.\"\"\"\n        return \"foo\"\n\n    foo_tool = StructuredTool.from_function(\n        func=foo,\n        args_schema=pydantic_model,\n    )\n\n    assert foo_tool.invoke({\"a\": 5, \"b\": \"hello\"}) == \"foo\"\n\n    args_schema = cast(BaseModel, foo_tool.args_schema)\n    args_json_schema = (\n        args_schema.model_json_schema()\n        if hasattr(args_schema, \"model_json_schema\")\n        else args_schema.schema()\n    )\n    assert args_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n    input_schema = foo_tool.get_input_schema()\n    input_json_schema = (\n        input_schema.model_json_schema()\n        if hasattr(input_schema, \"model_json_schema\")\n        else input_schema.schema()\n    )\n    assert input_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n\nvalid_tool_result_blocks = [\n    \"foo\",\n    {\"type\": \"text\", \"text\": \"foo\"},\n    {\"type\": \"text\", \"blah\": \"foo\"},  # note, only 'type' key is currently checked\n    {\"type\": \"image_url\", \"image_url\": {}},  # openai format\n    {\n        \"type\": \"image\",\n        \"source\": {\n            \"type\": \"base64\",\n            \"media_type\": \"image/jpeg\",\n            \"data\": \"123\",\n        },\n    },  # anthropic format\n    {\"type\": \"json\", \"json\": {}},  # bedrock format\n]\ninvalid_tool_result_blocks = [\n    {\"text\": \"foo\"},  # missing type\n    {\"results\": \"foo\"},  # not content blocks\n]\n\n\n@pytest.mark.parametrize(\n    (\"obj\", \"expected\"),\n    [\n        *([[block, True] for block in valid_tool_result_blocks]),\n        *([[block, False] for block in invalid_tool_result_blocks]),\n    ],\n)\ndef test__is_message_content_block(obj: Any, expected: bool) -> None:\n    assert _is_message_content_block(obj) is expected\n\n\n@pytest.mark.parametrize(\n    (\"obj\", \"expected\"),\n    [\n        [\"foo\", True],\n        [valid_tool_result_blocks, True],\n        [invalid_tool_result_blocks, False],\n    ],\n)\ndef test__is_message_content_type(obj: Any, expected: bool) -> None:\n    assert _is_message_content_type(obj) is expected\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 2, reason=\"Testing pydantic v2.\")\n@pytest.mark.parametrize(\"use_v1_namespace\", [True, False])\ndef test__get_all_basemodel_annotations_v2(use_v1_namespace: bool) -> None:\n    A = TypeVar(\"A\")\n\n    if use_v1_namespace:\n        from pydantic.v1 import BaseModel as BaseModel1\n\n        class ModelA(BaseModel1, Generic[A], extra=\"allow\"):\n            a: A\n\n    else:\n        from pydantic import BaseModel as BaseModel2\n        from pydantic import ConfigDict\n\n        class ModelA(BaseModel2, Generic[A]):  # type: ignore[no-redef]\n            a: A\n            model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"allow\")\n\n    class ModelB(ModelA[str]):\n        b: Annotated[ModelA[dict[str, Any]], \"foo\"]\n\n    class Mixin:\n        def foo(self) -> str:\n            return \"foo\"\n\n    class ModelC(Mixin, ModelB):\n        c: dict\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"], \"c\": dict}\n    actual = get_all_basemodel_annotations(ModelC)\n    assert actual == expected\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"]}\n    actual = get_all_basemodel_annotations(ModelB)\n    assert actual == expected\n\n    expected = {\"a\": Any}\n    actual = get_all_basemodel_annotations(ModelA)\n    assert actual == expected\n\n    expected = {\"a\": int}\n    actual = get_all_basemodel_annotations(ModelA[int])\n    assert actual == expected\n\n    D = TypeVar(\"D\", bound=Union[str, int])\n\n    class ModelD(ModelC, Generic[D]):\n        d: Optional[D]\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[str, int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD)\n    assert actual == expected\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD[int])\n    assert actual == expected\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 1, reason=\"Testing pydantic v1.\")\ndef test__get_all_basemodel_annotations_v1() -> None:\n    A = TypeVar(\"A\")\n\n    class ModelA(BaseModel, Generic[A], extra=\"allow\"):\n        a: A\n\n    class ModelB(ModelA[str]):\n        b: Annotated[ModelA[dict[str, Any]], \"foo\"]\n\n    class Mixin:\n        def foo(self) -> str:\n            return \"foo\"\n\n    class ModelC(Mixin, ModelB):\n        c: dict\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"], \"c\": dict}\n    actual = get_all_basemodel_annotations(ModelC)\n    assert actual == expected\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"]}\n    actual = get_all_basemodel_annotations(ModelB)\n    assert actual == expected\n\n    expected = {\"a\": Any}\n    actual = get_all_basemodel_annotations(ModelA)\n    assert actual == expected\n\n    expected = {\"a\": int}\n    actual = get_all_basemodel_annotations(ModelA[int])\n    assert actual == expected\n\n    D = TypeVar(\"D\", bound=Union[str, int])\n\n    class ModelD(ModelC, Generic[D]):\n        d: Optional[D]\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[str, int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD)\n    assert actual == expected\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD[int])\n    assert actual == expected\n\n\ndef test_tool_annotations_preserved() -> None:\n    \"\"\"Test that annotations are preserved when creating a tool.\"\"\"\n\n    @tool\n    def my_tool(val: int, other_val: Annotated[dict, \"my annotation\"]) -> str:\n        \"\"\"Tool docstring.\"\"\"\n        return \"foo\"\n\n    schema = my_tool.get_input_schema()  # type: ignore[attr-defined]\n\n    func = my_tool.func  # type: ignore[attr-defined]\n\n    expected_type_hints = {\n        name: hint\n        for name, hint in func.__annotations__.items()\n        if name in inspect.signature(func).parameters\n    }\n    assert schema.__annotations__ == expected_type_hints\n\n\ndef test_create_retriever_tool() -> None:\n    class MyRetriever(BaseRetriever):\n        def _get_relevant_documents(\n            self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n        ) -> list[Document]:\n            return [Document(page_content=f\"foo {query}\"), Document(page_content=\"bar\")]\n\n    retriever = MyRetriever()\n    retriever_tool = tools.create_retriever_tool(\n        retriever, \"retriever_tool_content\", \"Retriever Tool Content\"\n    )\n    assert isinstance(retriever_tool, BaseTool)\n    assert retriever_tool.name == \"retriever_tool_content\"\n    assert retriever_tool.description == \"Retriever Tool Content\"\n    assert retriever_tool.invoke(\"bar\") == \"foo bar\\n\\nbar\"\n    assert retriever_tool.invoke(\n        ToolCall(\n            name=\"retriever_tool_content\",\n            args={\"query\": \"bar\"},\n            id=\"123\",\n            type=\"tool_call\",\n        )\n    ) == ToolMessage(\n        \"foo bar\\n\\nbar\", tool_call_id=\"123\", name=\"retriever_tool_content\"\n    )\n\n    retriever_tool_artifact = tools.create_retriever_tool(\n        retriever,\n        \"retriever_tool_artifact\",\n        \"Retriever Tool Artifact\",\n        response_format=\"content_and_artifact\",\n    )\n    assert isinstance(retriever_tool_artifact, BaseTool)\n    assert retriever_tool_artifact.name == \"retriever_tool_artifact\"\n    assert retriever_tool_artifact.description == \"Retriever Tool Artifact\"\n    assert retriever_tool_artifact.invoke(\"bar\") == \"foo bar\\n\\nbar\"\n    assert retriever_tool_artifact.invoke(\n        ToolCall(\n            name=\"retriever_tool_artifact\",\n            args={\"query\": \"bar\"},\n            id=\"123\",\n            type=\"tool_call\",\n        )\n    ) == ToolMessage(\n        \"foo bar\\n\\nbar\",\n        artifact=[Document(page_content=\"foo bar\"), Document(page_content=\"bar\")],\n        tool_call_id=\"123\",\n        name=\"retriever_tool_artifact\",\n    )\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 2, reason=\"Testing pydantic v2.\")\ndef test_tool_args_schema_pydantic_v2_with_metadata() -> None:\n    from pydantic import BaseModel as BaseModelV2\n    from pydantic import Field as FieldV2\n    from pydantic import ValidationError as ValidationErrorV2\n\n    class Foo(BaseModelV2):\n        x: list[int] = FieldV2(\n            description=\"List of integers\", min_length=10, max_length=15\n        )\n\n    @tool(args_schema=Foo)\n    def foo(x) -> list[int]:  # type: ignore[no-untyped-def] # noqa: ANN001\n        \"\"\"Foo.\"\"\"\n        return x\n\n    assert _get_tool_call_json_schema(foo) == {\n        \"description\": \"Foo.\",\n        \"properties\": {\n            \"x\": {\n                \"description\": \"List of integers\",\n                \"items\": {\"type\": \"integer\"},\n                \"maxItems\": 15,\n                \"minItems\": 10,\n                \"title\": \"X\",\n                \"type\": \"array\",\n            }\n        },\n        \"required\": [\"x\"],\n        \"title\": \"foo\",\n        \"type\": \"object\",\n    }\n\n    assert foo.invoke({\"x\": [0] * 10})\n    with pytest.raises(ValidationErrorV2):\n        foo.invoke({\"x\": [0] * 9})\n\n\ndef test_imports() -> None:\n    expected_all = [\n        \"FILTERED_ARGS\",\n        \"SchemaAnnotationError\",\n        \"create_schema_from_function\",\n        \"ToolException\",\n        \"BaseTool\",\n        \"Tool\",\n        \"StructuredTool\",\n        \"tool\",\n        \"RetrieverInput\",\n        \"create_retriever_tool\",\n        \"ToolsRenderer\",\n        \"render_text_description\",\n        \"render_text_description_and_args\",\n        \"BaseToolkit\",\n        \"convert_runnable_to_tool\",\n        \"InjectedToolArg\",\n    ]\n    for module_name in expected_all:\n        assert hasattr(tools, module_name) and getattr(tools, module_name) is not None\n\n\ndef test_structured_tool_direct_init() -> None:\n    def foo(bar: str) -> str:\n        return bar\n\n    async def async_foo(bar: str) -> str:\n        return bar\n\n    class FooSchema(BaseModel):\n        bar: str = Field(..., description=\"The bar\")\n\n    tool = StructuredTool(name=\"foo\", args_schema=FooSchema, coroutine=async_foo)\n\n    with pytest.raises(NotImplementedError):\n        assert tool.invoke(\"hello\") == \"hello\"\n\n\ndef test_injected_arg_with_complex_type() -> None:\n    \"\"\"Test that an injected tool arg can be a complex type.\"\"\"\n\n    class Foo:\n        def __init__(self) -> None:\n            self.value = \"bar\"\n\n    @tool\n    def injected_tool(x: int, foo: Annotated[Foo, InjectedToolArg]) -> str:\n        \"\"\"Tool that has an injected tool arg.\"\"\"\n        return foo.value\n\n    assert injected_tool.invoke({\"x\": 5, \"foo\": Foo()}) == \"bar\"  # type: ignore\n\n\ndef test_tool_injected_tool_call_id() -> None:\n    @tool\n    def foo(x: int, tool_call_id: Annotated[str, InjectedToolCallId]) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"bar\")  # type: ignore\n\n    with pytest.raises(ValueError):\n        assert foo.invoke({\"x\": 0})\n\n    @tool\n    def foo2(x: int, tool_call_id: Annotated[str, InjectedToolCallId()]) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    assert foo2.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"bar\")  # type: ignore\n\n\ndef test_tool_uninjected_tool_call_id() -> None:\n    @tool\n    def foo(x: int, tool_call_id: str) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    with pytest.raises(ValueError):\n        foo.invoke({\"type\": \"tool_call\", \"args\": {\"x\": 0}, \"name\": \"foo\", \"id\": \"bar\"})\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0, \"tool_call_id\": \"zap\"},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"zap\")  # type: ignore\n\n\ndef test_tool_return_output_mixin() -> None:\n    class Bar(ToolOutputMixin):\n        def __init__(self, x: int) -> None:\n            self.x = x\n\n        def __eq__(self, other: Any) -> bool:\n            return isinstance(other, self.__class__) and self.x == other.x\n\n    @tool\n    def foo(x: int) -> Bar:\n        \"\"\"Foo.\"\"\"\n        return Bar(x=x)\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == Bar(x=0)\n\n\ndef test_tool_mutate_input() -> None:\n    class MyTool(BaseTool):\n        name: str = \"MyTool\"\n        description: str = \"a tool\"\n\n        def _run(\n            self,\n            x: str,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n        ) -> str:\n            return \"hi\"\n\n    my_input = {\"x\": \"hi\"}\n    MyTool().invoke(my_input)\n    assert my_input == {\"x\": \"hi\"}\n\n\ndef test_structured_tool_args_schema_dict() -> None:\n    args_schema = {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"integer\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"add\",\n        \"type\": \"object\",\n    }\n    tool = StructuredTool(\n        name=\"add\",\n        description=\"add two numbers\",\n        args_schema=args_schema,\n        func=lambda a, b: a + b,\n    )\n    assert tool.invoke({\"a\": 1, \"b\": 2}) == 3\n    assert tool.args_schema == args_schema\n    # test that the tool call schema is the same as the args schema\n    assert _get_tool_call_json_schema(tool) == args_schema\n    # test that the input schema is the same as the parent (Runnable) input schema\n    assert (\n        tool.get_input_schema().model_json_schema()\n        == create_model_v2(\n            tool.get_name(\"Input\"),\n            root=tool.InputType,\n            module_name=tool.__class__.__module__,\n        ).model_json_schema()\n    )\n    # test that args are extracted correctly\n    assert tool.args == {\n        \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n        \"b\": {\"title\": \"B\", \"type\": \"integer\"},\n    }\n\n\ndef test_simple_tool_args_schema_dict() -> None:\n    args_schema = {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n        },\n        \"required\": [\"a\"],\n        \"title\": \"square\",\n        \"type\": \"object\",\n    }\n    tool = Tool(\n        name=\"square\",\n        description=\"square a number\",\n        args_schema=args_schema,\n        func=lambda a: a * a,\n    )\n    assert tool.invoke({\"a\": 2}) == 4\n    assert tool.args_schema == args_schema\n    # test that the tool call schema is the same as the args schema\n    assert _get_tool_call_json_schema(tool) == args_schema\n    # test that the input schema is the same as the parent (Runnable) input schema\n    assert (\n        tool.get_input_schema().model_json_schema()\n        == create_model_v2(\n            tool.get_name(\"Input\"),\n            root=tool.InputType,\n            module_name=tool.__class__.__module__,\n        ).model_json_schema()\n    )\n    # test that args are extracted correctly\n    assert tool.args == {\n        \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n    }\n\n\ndef test_empty_string_tool_call_id() -> None:\n    @tool\n    def foo(x: int) -> str:\n        \"\"\"Foo.\"\"\"\n        return \"hi\"\n\n    assert foo.invoke({\"type\": \"tool_call\", \"args\": {\"x\": 0}, \"id\": \"\"}) == ToolMessage(\n        content=\"hi\", name=\"foo\", tool_call_id=\"\"\n    )\n",
        "patch": "@@ -52,6 +52,7 @@\n     tool,\n )\n from langchain_core.tools.base import (\n+    ArgsSchema,\n     InjectedToolArg,\n     InjectedToolCallId,\n     SchemaAnnotationError,\n@@ -199,7 +200,7 @@ def tool_func(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n     assert isinstance(tool_func, BaseTool)\n     assert tool_func.args_schema == _MockSchema\n \n-    @tool(args_schema=_MockSchemaV1)\n+    @tool(args_schema=cast(ArgsSchema, _MockSchemaV1))\n     def tool_func_v1(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n         return f\"{arg1} {arg2} {arg3}\"\n \n@@ -2398,10 +2399,10 @@ def test_structured_tool_args_schema_dict() -> None:\n         \"required\": [\"a\", \"b\"],\n         \"title\": \"add\",\n         \"type\": \"object\",\n+        \"description\": \"add two numbers\",\n     }\n     tool = StructuredTool(\n         name=\"add\",\n-        description=\"add two numbers\",\n         args_schema=args_schema,\n         func=lambda a, b: a + b,\n     )\n@@ -2433,6 +2434,7 @@ def test_simple_tool_args_schema_dict() -> None:\n         \"required\": [\"a\"],\n         \"title\": \"square\",\n         \"type\": \"object\",\n+        \"description\": \"square a number\",\n     }\n     tool = Tool(\n         name=\"square\",\n@@ -2468,3 +2470,93 @@ def foo(x: int) -> str:\n     assert foo.invoke({\"type\": \"tool_call\", \"args\": {\"x\": 0}, \"id\": \"\"}) == ToolMessage(\n         content=\"hi\", name=\"foo\", tool_call_id=\"\"\n     )\n+\n+\n+def test_tool_decorator_description() -> None:\n+    # test basic tool\n+    @tool\n+    def foo(x: int) -> str:\n+        \"\"\"Foo.\"\"\"\n+        return \"hi\"\n+\n+    assert foo.description == \"Foo.\"\n+    assert (\n+        cast(BaseModel, foo.tool_call_schema).model_json_schema()[\"description\"]\n+        == \"Foo.\"\n+    )\n+\n+    # test basic tool with description\n+    @tool(description=\"description\")\n+    def foo_description(x: int) -> str:\n+        \"\"\"Foo.\"\"\"\n+        return \"hi\"\n+\n+    assert foo_description.description == \"description\"\n+    assert (\n+        cast(BaseModel, foo_description.tool_call_schema).model_json_schema()[\n+            \"description\"\n+        ]\n+        == \"description\"\n+    )\n+\n+    # test tool with args schema\n+    class ArgsSchema(BaseModel):\n+        \"\"\"Bar.\"\"\"\n+\n+        x: int\n+\n+    @tool(args_schema=ArgsSchema)\n+    def foo_args_schema(x: int) -> str:\n+        return \"hi\"\n+\n+    assert foo_args_schema.description == \"Bar.\"\n+    assert (\n+        cast(BaseModel, foo_args_schema.tool_call_schema).model_json_schema()[\n+            \"description\"\n+        ]\n+        == \"Bar.\"\n+    )\n+\n+    @tool(description=\"description\", args_schema=ArgsSchema)\n+    def foo_args_schema_description(x: int) -> str:\n+        return \"hi\"\n+\n+    assert foo_args_schema_description.description == \"description\"\n+    assert (\n+        cast(\n+            BaseModel, foo_args_schema_description.tool_call_schema\n+        ).model_json_schema()[\"description\"]\n+        == \"description\"\n+    )\n+\n+    args_json_schema = {\n+        \"description\": \"JSON Schema.\",\n+        \"properties\": {\n+            \"x\": {\"description\": \"my field\", \"title\": \"X\", \"type\": \"string\"}\n+        },\n+        \"required\": [\"x\"],\n+        \"title\": \"my_tool\",\n+        \"type\": \"object\",\n+    }\n+\n+    @tool(args_schema=args_json_schema)\n+    def foo_args_jsons_schema(x: int) -> str:\n+        return \"hi\"\n+\n+    @tool(description=\"description\", args_schema=args_json_schema)\n+    def foo_args_jsons_schema_with_description(x: int) -> str:\n+        return \"hi\"\n+\n+    assert foo_args_jsons_schema.description == \"JSON Schema.\"\n+    assert (\n+        cast(dict, foo_args_jsons_schema.tool_call_schema)[\"description\"]\n+        == \"JSON Schema.\"\n+    )\n+\n+    assert foo_args_jsons_schema_with_description.description == \"description\"\n+    assert (\n+        cast(dict, foo_args_jsons_schema_with_description.tool_call_schema)[\n+            \"description\"\n+        ]\n+        == \"description\"\n+    )"
      }
    ]
  },
  {
    "number": 29983,
    "title": "docs: Correct grammatical typos in various documentation files",
    "body": "**Description:**\r\nFixed grammatical typos in various documentation files\r\n\r\n**Issue:**\r\nN/A\r\n\r\n**Dependencies:**\r\nN/A\r\n\r\n**Twitter handle:**\r\n@MrNaveenSK\r\n\r\n",
    "issue_title": "docs: Correct grammatical typos in various documentation files",
    "issue_body": "**Description:**\r\nFixed grammatical typos in various documentation files\r\n\r\n**Issue:**\r\nN/A\r\n\r\n**Dependencies:**\r\nN/A\r\n\r\n**Twitter handle:**\r\n@MrNaveenSK\r\n\r\n",
    "files": [
      {
        "filename": "libs/langchain/langchain/chains/elasticsearch_database/prompts.py",
        "content_before": "# flake8: noqa\nfrom langchain_core.prompts.prompt import PromptTemplate\n\nPROMPT_SUFFIX = \"\"\"Only use the following Elasticsearch indices:\n{indices_info}\n\nQuestion: {input}\nESQuery:\"\"\"\n\nDEFAULT_DSL_TEMPLATE = \"\"\"Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n\nUnless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question.\n\nPay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json.\n\nUse the following format:\n\nQuestion: Question here\nESQuery: Elasticsearch Query formatted as json\n\"\"\"\n\nDSL_PROMPT = PromptTemplate.from_template(DEFAULT_DSL_TEMPLATE + PROMPT_SUFFIX)\n\nDEFAULT_ANSWER_TEMPLATE = \"\"\"Given an input question and relevant data from a database, answer the user question.\n\nUse the following format:\n\nQuestion: Question here\nData: Relevant data here\nAnswer: Final answer here\n\nQuestion: {input}\nData: {data}\nAnswer:\"\"\"\n\nANSWER_PROMPT = PromptTemplate.from_template(DEFAULT_ANSWER_TEMPLATE)\n",
        "patch": "@@ -9,7 +9,7 @@\n \n DEFAULT_DSL_TEMPLATE = \"\"\"Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n \n-Unless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question.\n+Unless told to do not query for all the columns from a specific index, only ask for a few relevant columns given the question.\n \n Pay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json.\n "
      },
      {
        "filename": "libs/langchain/langchain/chains/sql_database/prompt.py",
        "content_before": "# flake8: noqa\nfrom langchain_core.output_parsers.list import CommaSeparatedListOutputParser\nfrom langchain_core.prompts.prompt import PromptTemplate\n\n\nPROMPT_SUFFIX = \"\"\"Only use the following tables:\n{table_info}\n\nQuestion: {input}\"\"\"\n\n_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n\nPay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nPROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"dialect\", \"top_k\"],\n    template=_DEFAULT_TEMPLATE + PROMPT_SUFFIX,\n)\n\n\n_DECIDER_TEMPLATE = \"\"\"Given the below input question and list of potential tables, output a comma separated list of the table names that may be necessary to answer this question.\n\nQuestion: {query}\n\nTable Names: {table_names}\n\nRelevant Table Names:\"\"\"\nDECIDER_PROMPT = PromptTemplate(\n    input_variables=[\"query\", \"table_names\"],\n    template=_DECIDER_TEMPLATE,\n    output_parser=CommaSeparatedListOutputParser(),\n)\n\n_cratedb_prompt = \"\"\"You are a CrateDB expert. Given an input question, first create a syntactically correct CrateDB query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per CrateDB. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use CURRENT_DATE function to get the current date, if the question involves \"today\". \n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nCRATEDB_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_cratedb_prompt + PROMPT_SUFFIX,\n)\n\n_duckdb_prompt = \"\"\"You are a DuckDB expert. Given an input question, first create a syntactically correct DuckDB query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per DuckDB. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use today() function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nDUCKDB_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_duckdb_prompt + PROMPT_SUFFIX,\n)\n\n_googlesql_prompt = \"\"\"You are a GoogleSQL expert. Given an input question, first create a syntactically correct GoogleSQL query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per GoogleSQL. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use CURRENT_DATE() function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nGOOGLESQL_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_googlesql_prompt + PROMPT_SUFFIX,\n)\n\n\n_mssql_prompt = \"\"\"You are an MS SQL expert. Given an input question, first create a syntactically correct MS SQL query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the TOP clause as per MS SQL. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in square brackets ([]) to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use CAST(GETDATE() as date) function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nMSSQL_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_mssql_prompt + PROMPT_SUFFIX,\n)\n\n\n_mysql_prompt = \"\"\"You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nMYSQL_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_mysql_prompt + PROMPT_SUFFIX,\n)\n\n\n_mariadb_prompt = \"\"\"You are a MariaDB expert. Given an input question, first create a syntactically correct MariaDB query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per MariaDB. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nMARIADB_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_mariadb_prompt + PROMPT_SUFFIX,\n)\n\n\n_oracle_prompt = \"\"\"You are an Oracle SQL expert. Given an input question, first create a syntactically correct Oracle SQL query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the FETCH FIRST n ROWS ONLY clause as per Oracle SQL. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use TRUNC(SYSDATE) function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nORACLE_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_oracle_prompt + PROMPT_SUFFIX,\n)\n\n\n_postgres_prompt = \"\"\"You are a PostgreSQL expert. Given an input question, first create a syntactically correct PostgreSQL query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per PostgreSQL. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use CURRENT_DATE function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nPOSTGRES_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_postgres_prompt + PROMPT_SUFFIX,\n)\n\n\n_sqlite_prompt = \"\"\"You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use date('now') function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: Question here\nSQLQuery: SQL Query to run\nSQLResult: Result of the SQLQuery\nAnswer: Final answer here\n\n\"\"\"\n\nSQLITE_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_sqlite_prompt + PROMPT_SUFFIX,\n)\n\n_clickhouse_prompt = \"\"\"You are a ClickHouse expert. Given an input question, first create a syntactically correct Clic query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per ClickHouse. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use today() function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\n\"\"\"\n\nCLICKHOUSE_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_clickhouse_prompt + PROMPT_SUFFIX,\n)\n\n_prestodb_prompt = \"\"\"You are a PrestoDB expert. Given an input question, first create a syntactically correct PrestoDB query to run, then look at the results of the query and return the answer to the input question.\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per PrestoDB. You can order the results to return the most informative data in the database.\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\nPay attention to use current_date function to get the current date, if the question involves \"today\".\n\nUse the following format:\n\nQuestion: \"Question here\"\nSQLQuery: \"SQL Query to run\"\nSQLResult: \"Result of the SQLQuery\"\nAnswer: \"Final answer here\"\n\n\"\"\"\n\nPRESTODB_PROMPT = PromptTemplate(\n    input_variables=[\"input\", \"table_info\", \"top_k\"],\n    template=_prestodb_prompt + PROMPT_SUFFIX,\n)\n\n\nSQL_PROMPTS = {\n    \"crate\": CRATEDB_PROMPT,\n    \"duckdb\": DUCKDB_PROMPT,\n    \"googlesql\": GOOGLESQL_PROMPT,\n    \"mssql\": MSSQL_PROMPT,\n    \"mysql\": MYSQL_PROMPT,\n    \"mariadb\": MARIADB_PROMPT,\n    \"oracle\": ORACLE_PROMPT,\n    \"postgresql\": POSTGRES_PROMPT,\n    \"sqlite\": SQLITE_PROMPT,\n    \"clickhouse\": CLICKHOUSE_PROMPT,\n    \"prestodb\": PRESTODB_PROMPT,\n}\n",
        "patch": "@@ -10,7 +10,7 @@\n \n _DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies in his question a specific number of examples he wishes to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.\n \n-Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n+Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n \n Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n "
      }
    ]
  },
  {
    "number": 29991,
    "title": "langchain[patch]: remove aiohttp",
    "body": "My guess is this was left over from when `community` was in langchain.",
    "issue_title": "langchain[patch]: remove aiohttp",
    "issue_body": "My guess is this was left over from when `community` was in langchain.",
    "files": [
      {
        "filename": "libs/langchain/tests/unit_tests/test_dependencies.py",
        "content_before": "\"\"\"A unit test meant to catch accidental introduction of non-optional dependencies.\"\"\"\n\nfrom pathlib import Path\nfrom typing import Any, Dict, Mapping\n\nimport pytest\nimport toml\nfrom packaging.requirements import Requirement\n\nHERE = Path(__file__).parent\n\nPYPROJECT_TOML = HERE / \"../../pyproject.toml\"\n\n\n@pytest.fixture()\ndef uv_conf() -> Dict[str, Any]:\n    \"\"\"Load the pyproject.toml file.\"\"\"\n    with open(PYPROJECT_TOML) as f:\n        return toml.load(f)\n\n\ndef test_required_dependencies(uv_conf: Mapping[str, Any]) -> None:\n    \"\"\"A test that checks if a new non-optional dependency is being introduced.\n\n    If this test is triggered, it means that a contributor is trying to introduce a new\n    required dependency. This should be avoided in most situations.\n    \"\"\"\n    # Get the dependencies from the [tool.poetry.dependencies] section\n    dependencies = uv_conf[\"project\"][\"dependencies\"]\n    required_dependencies = set(Requirement(dep).name for dep in dependencies)\n\n    assert sorted(required_dependencies) == sorted(\n        [\n            \"PyYAML\",\n            \"SQLAlchemy\",\n            \"aiohttp\",\n            \"async-timeout\",\n            \"langchain-core\",\n            \"langchain-text-splitters\",\n            \"langsmith\",\n            \"numpy\",\n            \"pydantic\",\n            \"requests\",\n        ]\n    )\n\n\ndef test_test_group_dependencies(uv_conf: Mapping[str, Any]) -> None:\n    \"\"\"Check if someone is attempting to add additional test dependencies.\n\n    Only dependencies associated with test running infrastructure should be added\n    to the test group; e.g., pytest, pytest-cov etc.\n\n    Examples of dependencies that should NOT be included: boto3, azure, postgres, etc.\n    \"\"\"\n\n    dependencies = uv_conf[\"dependency-groups\"][\"test\"]\n    test_group_deps = set(Requirement(dep).name for dep in dependencies)\n\n    assert sorted(test_group_deps) == sorted(\n        [\n            \"duckdb-engine\",\n            \"freezegun\",\n            \"langchain-core\",\n            \"langchain-tests\",\n            \"langchain-text-splitters\",\n            \"langchain-openai\",\n            \"lark\",\n            \"packaging\",\n            \"pandas\",\n            \"pytest\",\n            \"pytest-asyncio\",\n            \"pytest-cov\",\n            \"pytest-dotenv\",\n            \"pytest-mock\",\n            \"pytest-socket\",\n            \"pytest-watcher\",\n            \"pytest-xdist\",\n            \"blockbuster\",\n            \"responses\",\n            \"syrupy\",\n            \"toml\",\n            \"requests-mock\",\n            # TODO: temporary hack since cffi 1.17.1 doesn't work with py 3.9.\n            \"cffi\",\n        ]\n    )\n",
        "patch": "@@ -33,7 +33,6 @@ def test_required_dependencies(uv_conf: Mapping[str, Any]) -> None:\n         [\n             \"PyYAML\",\n             \"SQLAlchemy\",\n-            \"aiohttp\",\n             \"async-timeout\",\n             \"langchain-core\",\n             \"langchain-text-splitters\","
      }
    ]
  },
  {
    "number": 29990,
    "title": "langchain[patch]: remove explicit dependency on tenacity",
    "body": "Not used anywhere in `langchain`, already a dependency of langchain-core.",
    "issue_title": "langchain[patch]: remove explicit dependency on tenacity",
    "issue_body": "Not used anywhere in `langchain`, already a dependency of langchain-core.",
    "files": [
      {
        "filename": "libs/langchain/tests/unit_tests/test_dependencies.py",
        "content_before": "\"\"\"A unit test meant to catch accidental introduction of non-optional dependencies.\"\"\"\n\nfrom pathlib import Path\nfrom typing import Any, Dict, Mapping\n\nimport pytest\nimport toml\nfrom packaging.requirements import Requirement\n\nHERE = Path(__file__).parent\n\nPYPROJECT_TOML = HERE / \"../../pyproject.toml\"\n\n\n@pytest.fixture()\ndef uv_conf() -> Dict[str, Any]:\n    \"\"\"Load the pyproject.toml file.\"\"\"\n    with open(PYPROJECT_TOML) as f:\n        return toml.load(f)\n\n\ndef test_required_dependencies(uv_conf: Mapping[str, Any]) -> None:\n    \"\"\"A test that checks if a new non-optional dependency is being introduced.\n\n    If this test is triggered, it means that a contributor is trying to introduce a new\n    required dependency. This should be avoided in most situations.\n    \"\"\"\n    # Get the dependencies from the [tool.poetry.dependencies] section\n    dependencies = uv_conf[\"project\"][\"dependencies\"]\n    required_dependencies = set(Requirement(dep).name for dep in dependencies)\n\n    assert sorted(required_dependencies) == sorted(\n        [\n            \"PyYAML\",\n            \"SQLAlchemy\",\n            \"aiohttp\",\n            \"async-timeout\",\n            \"langchain-core\",\n            \"langchain-text-splitters\",\n            \"langsmith\",\n            \"numpy\",\n            \"pydantic\",\n            \"requests\",\n            \"tenacity\",\n        ]\n    )\n\n\ndef test_test_group_dependencies(uv_conf: Mapping[str, Any]) -> None:\n    \"\"\"Check if someone is attempting to add additional test dependencies.\n\n    Only dependencies associated with test running infrastructure should be added\n    to the test group; e.g., pytest, pytest-cov etc.\n\n    Examples of dependencies that should NOT be included: boto3, azure, postgres, etc.\n    \"\"\"\n\n    dependencies = uv_conf[\"dependency-groups\"][\"test\"]\n    test_group_deps = set(Requirement(dep).name for dep in dependencies)\n\n    assert sorted(test_group_deps) == sorted(\n        [\n            \"duckdb-engine\",\n            \"freezegun\",\n            \"langchain-core\",\n            \"langchain-tests\",\n            \"langchain-text-splitters\",\n            \"langchain-openai\",\n            \"lark\",\n            \"packaging\",\n            \"pandas\",\n            \"pytest\",\n            \"pytest-asyncio\",\n            \"pytest-cov\",\n            \"pytest-dotenv\",\n            \"pytest-mock\",\n            \"pytest-socket\",\n            \"pytest-watcher\",\n            \"pytest-xdist\",\n            \"blockbuster\",\n            \"responses\",\n            \"syrupy\",\n            \"toml\",\n            \"requests-mock\",\n            # TODO: temporary hack since cffi 1.17.1 doesn't work with py 3.9.\n            \"cffi\",\n        ]\n    )\n",
        "patch": "@@ -41,7 +41,6 @@ def test_required_dependencies(uv_conf: Mapping[str, Any]) -> None:\n             \"numpy\",\n             \"pydantic\",\n             \"requests\",\n-            \"tenacity\",\n         ]\n     )\n "
      }
    ]
  },
  {
    "number": 29988,
    "title": "langchain: update extended test",
    "body": null,
    "issue_title": "langchain: update extended test",
    "issue_body": null,
    "files": [
      {
        "filename": "libs/langchain/tests/unit_tests/chat_models/test_base.py",
        "content_before": "import os\nfrom typing import Optional\nfrom unittest import mock\n\nimport pytest\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableConfig, RunnableSequence\nfrom pydantic import SecretStr\n\nfrom langchain.chat_models.base import __all__, init_chat_model\n\nEXPECTED_ALL = [\n    \"BaseChatModel\",\n    \"SimpleChatModel\",\n    \"agenerate_from_stream\",\n    \"generate_from_stream\",\n    \"init_chat_model\",\n]\n\n\ndef test_all_imports() -> None:\n    assert set(__all__) == set(EXPECTED_ALL)\n\n\n@pytest.mark.requires(\n    \"langchain_openai\",\n    \"langchain_anthropic\",\n    \"langchain_fireworks\",\n    \"langchain_groq\",\n)\n@pytest.mark.parametrize(\n    [\"model_name\", \"model_provider\"],\n    [\n        (\"gpt-4o\", \"openai\"),\n        (\"claude-3-opus-20240229\", \"anthropic\"),\n        (\"accounts/fireworks/models/mixtral-8x7b-instruct\", \"fireworks\"),\n        (\"mixtral-8x7b-32768\", \"groq\"),\n    ],\n)\ndef test_init_chat_model(model_name: str, model_provider: Optional[str]) -> None:\n    llm1: BaseChatModel = init_chat_model(\n        model_name, model_provider=model_provider, api_key=\"foo\"\n    )\n    llm2: BaseChatModel = init_chat_model(\n        f\"{model_provider}:{model_name}\", api_key=\"foo\"\n    )\n    assert llm1.dict() == llm2.dict()\n\n\ndef test_init_missing_dep() -> None:\n    with pytest.raises(ImportError):\n        init_chat_model(\"mixtral-8x7b-32768\", model_provider=\"groq\")\n\n\ndef test_init_unknown_provider() -> None:\n    with pytest.raises(ValueError):\n        init_chat_model(\"foo\", model_provider=\"bar\")\n\n\n@pytest.mark.requires(\"langchain_openai\")\n@mock.patch.dict(\n    os.environ, {\"OPENAI_API_KEY\": \"foo\", \"ANTHROPIC_API_KEY\": \"bar\"}, clear=True\n)\ndef test_configurable() -> None:\n    model = init_chat_model()\n\n    for method in (\n        \"invoke\",\n        \"ainvoke\",\n        \"batch\",\n        \"abatch\",\n        \"stream\",\n        \"astream\",\n        \"batch_as_completed\",\n        \"abatch_as_completed\",\n    ):\n        assert hasattr(model, method)\n\n    # Doesn't have access non-configurable, non-declarative methods until a config is\n    # provided.\n    for method in (\"get_num_tokens\", \"get_num_tokens_from_messages\"):\n        with pytest.raises(AttributeError):\n            getattr(model, method)\n\n    # Can call declarative methods even without a default model.\n    model_with_tools = model.bind_tools(\n        [{\"name\": \"foo\", \"description\": \"foo\", \"parameters\": {}}]\n    )\n\n    # Check that original model wasn't mutated by declarative operation.\n    assert model._queued_declarative_operations == []\n\n    # Can iteratively call declarative methods.\n    model_with_config = model_with_tools.with_config(\n        RunnableConfig(tags=[\"foo\"]), configurable={\"model\": \"gpt-4o\"}\n    )\n    assert model_with_config.model_name == \"gpt-4o\"  # type: ignore[attr-defined]\n\n    for method in (\"get_num_tokens\", \"get_num_tokens_from_messages\"):\n        assert hasattr(model_with_config, method)\n\n    assert model_with_config.model_dump() == {  # type: ignore[attr-defined]\n        \"name\": None,\n        \"bound\": {\n            \"name\": None,\n            \"disable_streaming\": False,\n            \"disabled_params\": None,\n            \"model_name\": \"gpt-4o\",\n            \"temperature\": None,\n            \"model_kwargs\": {},\n            \"openai_api_key\": SecretStr(\"foo\"),\n            \"openai_api_base\": None,\n            \"openai_organization\": None,\n            \"openai_proxy\": None,\n            \"request_timeout\": None,\n            \"max_retries\": None,\n            \"presence_penalty\": None,\n            \"reasoning_effort\": None,\n            \"frequency_penalty\": None,\n            \"seed\": None,\n            \"logprobs\": None,\n            \"top_logprobs\": None,\n            \"logit_bias\": None,\n            \"streaming\": False,\n            \"n\": None,\n            \"top_p\": None,\n            \"max_tokens\": None,\n            \"tiktoken_model_name\": None,\n            \"default_headers\": None,\n            \"default_query\": None,\n            \"stop\": None,\n            \"extra_body\": None,\n            \"include_response_headers\": False,\n            \"stream_usage\": False,\n        },\n        \"kwargs\": {\n            \"tools\": [\n                {\n                    \"type\": \"function\",\n                    \"function\": {\"name\": \"foo\", \"description\": \"foo\", \"parameters\": {}},\n                }\n            ]\n        },\n        \"config\": {\"tags\": [\"foo\"], \"configurable\": {}},\n        \"config_factories\": [],\n        \"custom_input_type\": None,\n        \"custom_output_type\": None,\n    }\n\n\n@pytest.mark.requires(\"langchain_openai\", \"langchain_anthropic\")\n@mock.patch.dict(\n    os.environ, {\"OPENAI_API_KEY\": \"foo\", \"ANTHROPIC_API_KEY\": \"bar\"}, clear=True\n)\ndef test_configurable_with_default() -> None:\n    model = init_chat_model(\"gpt-4o\", configurable_fields=\"any\", config_prefix=\"bar\")\n    for method in (\n        \"invoke\",\n        \"ainvoke\",\n        \"batch\",\n        \"abatch\",\n        \"stream\",\n        \"astream\",\n        \"batch_as_completed\",\n        \"abatch_as_completed\",\n    ):\n        assert hasattr(model, method)\n\n    # Does have access non-configurable, non-declarative methods since default params\n    # are provided.\n    for method in (\"get_num_tokens\", \"get_num_tokens_from_messages\", \"dict\"):\n        assert hasattr(model, method)\n\n    assert model.model_name == \"gpt-4o\"  # type: ignore[attr-defined]\n\n    model_with_tools = model.bind_tools(\n        [{\"name\": \"foo\", \"description\": \"foo\", \"parameters\": {}}]\n    )\n\n    model_with_config = model_with_tools.with_config(\n        RunnableConfig(tags=[\"foo\"]),\n        configurable={\"bar_model\": \"claude-3-sonnet-20240229\"},\n    )\n\n    assert model_with_config.model == \"claude-3-sonnet-20240229\"  # type: ignore[attr-defined]\n\n    assert model_with_config.model_dump() == {  # type: ignore[attr-defined]\n        \"name\": None,\n        \"bound\": {\n            \"name\": None,\n            \"disable_streaming\": False,\n            \"model\": \"claude-3-sonnet-20240229\",\n            \"max_tokens\": 1024,\n            \"temperature\": None,\n            \"top_k\": None,\n            \"top_p\": None,\n            \"default_request_timeout\": None,\n            \"max_retries\": 2,\n            \"stop_sequences\": None,\n            \"anthropic_api_url\": \"https://api.anthropic.com\",\n            \"anthropic_api_key\": SecretStr(\"bar\"),\n            \"default_headers\": None,\n            \"model_kwargs\": {},\n            \"streaming\": False,\n            \"stream_usage\": True,\n        },\n        \"kwargs\": {\n            \"tools\": [{\"name\": \"foo\", \"description\": \"foo\", \"input_schema\": {}}]\n        },\n        \"config\": {\"tags\": [\"foo\"], \"configurable\": {}},\n        \"config_factories\": [],\n        \"custom_input_type\": None,\n        \"custom_output_type\": None,\n    }\n    prompt = ChatPromptTemplate.from_messages([(\"system\", \"foo\")])\n    chain = prompt | model_with_config\n    assert isinstance(chain, RunnableSequence)\n",
        "patch": "@@ -193,6 +193,7 @@ def test_configurable_with_default() -> None:\n             \"model\": \"claude-3-sonnet-20240229\",\n             \"max_tokens\": 1024,\n             \"temperature\": None,\n+            \"thinking\": None,\n             \"top_k\": None,\n             \"top_p\": None,\n             \"default_request_timeout\": None,"
      }
    ]
  },
  {
    "number": 29971,
    "title": "anthropic[patch]: support claude 3.7 sonnet",
    "body": null,
    "issue_title": "anthropic[patch]: support claude 3.7 sonnet",
    "issue_body": null,
    "files": [
      {
        "filename": "libs/partners/anthropic/langchain_anthropic/chat_models.py",
        "content_before": "import copy\nimport re\nimport warnings\nfrom functools import cached_property\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport anthropic\nfrom langchain_core._api import beta, deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.ai import InputTokenDetails, UsageMetadata\nfrom langchain_core.messages.tool import tool_call_chunk as create_tool_call_chunk\nfrom langchain_core.output_parsers import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n)\nfrom langchain_core.output_parsers.base import OutputParserLike\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableMap,\n    RunnablePassthrough,\n)\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import (\n    from_env,\n    get_pydantic_field_names,\n    secret_from_env,\n)\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import is_basemodel_subclass\nfrom langchain_core.utils.utils import _build_model_kwargs\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SecretStr,\n    model_validator,\n)\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom langchain_anthropic.output_parsers import extract_tool_calls\n\n_message_type_lookups = {\n    \"human\": \"user\",\n    \"ai\": \"assistant\",\n    \"AIMessageChunk\": \"assistant\",\n    \"HumanMessageChunk\": \"user\",\n}\n\n\ndef _format_image(image_url: str) -> Dict:\n    \"\"\"\n    Formats an image of format data:image/jpeg;base64,{b64_string}\n    to a dict for anthropic api\n\n    {\n      \"type\": \"base64\",\n      \"media_type\": \"image/jpeg\",\n      \"data\": \"/9j/4AAQSkZJRg...\",\n    }\n\n    And throws an error if it's not a b64 image\n    \"\"\"\n    regex = r\"^data:(?P<media_type>image/.+);base64,(?P<data>.+)$\"\n    match = re.match(regex, image_url)\n    if match is None:\n        raise ValueError(\n            \"Anthropic only supports base64-encoded images currently.\"\n            \" Example: data:image/png;base64,'/9j/4AAQSk'...\"\n        )\n    return {\n        \"type\": \"base64\",\n        \"media_type\": match.group(\"media_type\"),\n        \"data\": match.group(\"data\"),\n    }\n\n\ndef _merge_messages(\n    messages: Sequence[BaseMessage],\n) -> List[Union[SystemMessage, AIMessage, HumanMessage]]:\n    \"\"\"Merge runs of human/tool messages into single human messages with content blocks.\"\"\"  # noqa: E501\n    merged: list = []\n    for curr in messages:\n        if isinstance(curr, ToolMessage):\n            if (\n                isinstance(curr.content, list)\n                and curr.content\n                and all(\n                    isinstance(block, dict) and block.get(\"type\") == \"tool_result\"\n                    for block in curr.content\n                )\n            ):\n                curr = HumanMessage(curr.content)  # type: ignore[misc]\n            else:\n                curr = HumanMessage(  # type: ignore[misc]\n                    [\n                        {\n                            \"type\": \"tool_result\",\n                            \"content\": curr.content,\n                            \"tool_use_id\": curr.tool_call_id,\n                            \"is_error\": curr.status == \"error\",\n                        }\n                    ]\n                )\n        last = merged[-1] if merged else None\n        if any(\n            all(isinstance(m, c) for m in (curr, last))\n            for c in (SystemMessage, HumanMessage)\n        ):\n            if isinstance(cast(BaseMessage, last).content, str):\n                new_content: List = [\n                    {\"type\": \"text\", \"text\": cast(BaseMessage, last).content}\n                ]\n            else:\n                new_content = copy.copy(cast(list, cast(BaseMessage, last).content))\n            if isinstance(curr.content, str):\n                new_content.append({\"type\": \"text\", \"text\": curr.content})\n            else:\n                new_content.extend(curr.content)\n            merged[-1] = curr.model_copy(update={\"content\": new_content})\n        else:\n            merged.append(curr)\n    return merged\n\n\ndef _format_messages(\n    messages: List[BaseMessage],\n) -> Tuple[Union[str, List[Dict], None], List[Dict]]:\n    \"\"\"Format messages for anthropic.\"\"\"\n\n    \"\"\"\n    [\n                {\n                    \"role\": _message_type_lookups[m.type],\n                    \"content\": [_AnthropicMessageContent(text=m.content).model_dump()],\n                }\n                for m in messages\n            ]\n    \"\"\"\n    system: Union[str, List[Dict], None] = None\n    formatted_messages: List[Dict] = []\n\n    merged_messages = _merge_messages(messages)\n    for i, message in enumerate(merged_messages):\n        if message.type == \"system\":\n            if system is not None:\n                raise ValueError(\"Received multiple non-consecutive system messages.\")\n            elif isinstance(message.content, list):\n                system = [\n                    (\n                        block\n                        if isinstance(block, dict)\n                        else {\"type\": \"text\", \"text\": block}\n                    )\n                    for block in message.content\n                ]\n            else:\n                system = message.content\n            continue\n\n        role = _message_type_lookups[message.type]\n        content: Union[str, List]\n\n        if not isinstance(message.content, str):\n            # parse as dict\n            assert isinstance(\n                message.content, list\n            ), \"Anthropic message content must be str or list of dicts\"\n\n            # populate content\n            content = []\n            for block in message.content:\n                if isinstance(block, str):\n                    content.append({\"type\": \"text\", \"text\": block})\n                elif isinstance(block, dict):\n                    if \"type\" not in block:\n                        raise ValueError(\"Dict content block must have a type key\")\n                    elif block[\"type\"] == \"image_url\":\n                        # convert format\n                        source = _format_image(block[\"image_url\"][\"url\"])\n                        content.append({\"type\": \"image\", \"source\": source})\n                    elif block[\"type\"] == \"tool_use\":\n                        # If a tool_call with the same id as a tool_use content block\n                        # exists, the tool_call is preferred.\n                        if isinstance(message, AIMessage) and block[\"id\"] in [\n                            tc[\"id\"] for tc in message.tool_calls\n                        ]:\n                            overlapping = [\n                                tc\n                                for tc in message.tool_calls\n                                if tc[\"id\"] == block[\"id\"]\n                            ]\n                            content.extend(\n                                _lc_tool_calls_to_anthropic_tool_use_blocks(overlapping)\n                            )\n                        else:\n                            block.pop(\"text\", None)\n                            content.append(block)\n                    elif block[\"type\"] == \"text\":\n                        text = block.get(\"text\", \"\")\n                        # Only add non-empty strings for now as empty ones are not\n                        # accepted.\n                        # https://github.com/anthropics/anthropic-sdk-python/issues/461\n                        if text.strip():\n                            content.append(\n                                {\n                                    k: v\n                                    for k, v in block.items()\n                                    if k in (\"type\", \"text\", \"cache_control\")\n                                }\n                            )\n                    elif block[\"type\"] == \"tool_result\":\n                        tool_content = _format_messages(\n                            [HumanMessage(block[\"content\"])]\n                        )[1][0][\"content\"]\n                        content.append({**block, **{\"content\": tool_content}})\n                    else:\n                        content.append(block)\n                else:\n                    raise ValueError(\n                        f\"Content blocks must be str or dict, instead was: \"\n                        f\"{type(block)}\"\n                    )\n        else:\n            content = message.content\n\n        # Ensure all tool_calls have a tool_use content block\n        if isinstance(message, AIMessage) and message.tool_calls:\n            content = content or []\n            content = (\n                [{\"type\": \"text\", \"text\": message.content}]\n                if isinstance(content, str) and content\n                else content\n            )\n            tool_use_ids = [\n                cast(dict, block)[\"id\"]\n                for block in content\n                if cast(dict, block)[\"type\"] == \"tool_use\"\n            ]\n            missing_tool_calls = [\n                tc for tc in message.tool_calls if tc[\"id\"] not in tool_use_ids\n            ]\n            cast(list, content).extend(\n                _lc_tool_calls_to_anthropic_tool_use_blocks(missing_tool_calls)\n            )\n\n        formatted_messages.append({\"role\": role, \"content\": content})\n    return system, formatted_messages\n\n\nclass ChatAnthropic(BaseChatModel):\n    \"\"\"Anthropic chat models.\n\n    See https://docs.anthropic.com/en/docs/models-overview for a list of the latest models.\n\n    Setup:\n        Install ``langchain-anthropic`` and set environment variable ``ANTHROPIC_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-anthropic\n            export ANTHROPIC_API_KEY=\"your-api-key\"\n\n    Key init args \u2014 completion params:\n        model: str\n            Name of Anthropic model to use. E.g. \"claude-3-sonnet-20240229\".\n        temperature: float\n            Sampling temperature. Ranges from 0.0 to 1.0.\n        max_tokens: int\n            Max number of tokens to generate.\n\n    Key init args \u2014 client params:\n        timeout: Optional[float]\n            Timeout for requests.\n        max_retries: int\n            Max number of retries if a request fails.\n        api_key: Optional[str]\n            Anthropic API key. If not passed in will be read from env var ANTHROPIC_API_KEY.\n        base_url: Optional[str]\n            Base URL for API requests. Only specify if using a proxy or service\n            emulator.\n\n    See full list of supported init args and their descriptions in the params section.\n\n    Instantiate:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n\n            llm = ChatAnthropic(\n                model=\"claude-3-sonnet-20240229\",\n                temperature=0,\n                max_tokens=1024,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # base_url=\"...\",\n                # other params...\n            )\n\n    **NOTE**: Any param which is not explicitly supported will be passed directly to the\n    ``anthropic.Anthropic.messages.create(...)`` API every time to the model is\n    invoked. For example:\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n            import anthropic\n\n            ChatAnthropic(..., extra_headers={}).invoke(...)\n\n            # results in underlying API call of:\n\n            anthropic.Anthropic(..).messages.create(..., extra_headers={})\n\n            # which is also equivalent to:\n\n            ChatAnthropic(...).invoke(..., extra_headers={})\n\n    Invoke:\n        .. code-block:: python\n\n            messages = [\n                (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Stream:\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            AIMessageChunk(content='J', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=\"'\", id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='a', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ime', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' la', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content=' programm', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='ation', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n            AIMessageChunk(content='.', id='run-272ff5f9-8485-402c-b90d-eac8babc5b25')\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n        .. code-block:: python\n\n            AIMessageChunk(content=\"J'aime la programmation.\", id='run-b34faef0-882f-4869-a19c-ed2b856e6361')\n\n    Async:\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n        .. code-block:: python\n\n            AIMessage(content=\"J'aime la programmation.\", response_metadata={'id': 'msg_01Trik66aiQ9Z1higrD5XFx3', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 25, 'output_tokens': 11}}, id='run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0', usage_metadata={'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36})\n\n    Tool calling:\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n            ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [{'name': 'GetWeather',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01KzpPEAgzura7hpBqwHbWdo'},\n             {'name': 'GetWeather',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JtgbVGVJbiSwtZk3Uycezx'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'Los Angeles, CA'},\n              'id': 'toolu_01429aygngesudV9nTbCKGuw'},\n             {'name': 'GetPopulation',\n              'args': {'location': 'New York, NY'},\n              'id': 'toolu_01JPktyd44tVMeBcPPnFSEJG'}]\n\n        See ``ChatAnthropic.bind_tools()`` method for more.\n\n    Structured output:\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        .. code-block:: python\n\n            Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)\n\n        See ``ChatAnthropic.with_structured_output()`` for more.\n\n    Image input:\n        .. code-block:: python\n\n            import base64\n            import httpx\n            from langchain_core.messages import HumanMessage\n\n            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n            message = HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                    },\n                ],\n            )\n            ai_msg = llm.invoke([message])\n            ai_msg.content\n\n        .. code-block:: python\n\n            \"The image depicts a sunny day with a partly cloudy sky. The sky is a brilliant blue color with scattered white clouds drifting across. The lighting and cloud patterns suggest pleasant, mild weather conditions. The scene shows a grassy field or meadow with a wooden boardwalk trail leading through it, indicating an outdoor setting on a nice day well-suited for enjoying nature.\"\n\n    Token usage:\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        Message chunks containing token usage will be included during streaming by\n        default:\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 25, 'output_tokens': 11, 'total_tokens': 36}\n\n        These can be disabled by setting ``stream_usage=False`` in the stream method,\n        or by setting ``stream_usage=False`` when initializing ChatAnthropic.\n\n    Response metadata\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n        .. code-block:: python\n\n            {'id': 'msg_013xU6FHEGEq76aP4RgFerVT',\n             'model': 'claude-3-sonnet-20240229',\n             'stop_reason': 'end_turn',\n             'stop_sequence': None,\n             'usage': {'input_tokens': 25, 'output_tokens': 11}}\n\n    \"\"\"  # noqa: E501\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n    )\n\n    model: str = Field(alias=\"model_name\")\n    \"\"\"Model name to use.\"\"\"\n\n    max_tokens: int = Field(default=1024, alias=\"max_tokens_to_sample\")\n    \"\"\"Denotes the number of tokens to predict per generation.\"\"\"\n\n    temperature: Optional[float] = None\n    \"\"\"A non-negative float that tunes the degree of randomness in generation.\"\"\"\n\n    top_k: Optional[int] = None\n    \"\"\"Number of most likely tokens to consider at each step.\"\"\"\n\n    top_p: Optional[float] = None\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n\n    default_request_timeout: Optional[float] = Field(None, alias=\"timeout\")\n    \"\"\"Timeout for requests to Anthropic Completion API.\"\"\"\n\n    # sdk default = 2: https://github.com/anthropics/anthropic-sdk-python?tab=readme-ov-file#retries\n    max_retries: int = 2\n    \"\"\"Number of retries allowed for requests sent to the Anthropic Completion API.\"\"\"\n\n    stop_sequences: Optional[List[str]] = Field(None, alias=\"stop\")\n    \"\"\"Default stop sequences.\"\"\"\n\n    anthropic_api_url: Optional[str] = Field(\n        alias=\"base_url\",\n        default_factory=from_env(\n            [\"ANTHROPIC_API_URL\", \"ANTHROPIC_BASE_URL\"],\n            default=\"https://api.anthropic.com\",\n        ),\n    )\n    \"\"\"Base URL for API requests. Only specify if using a proxy or service emulator.\n\n    If a value isn't passed in, will attempt to read the value first from\n    ANTHROPIC_API_URL and if that is not set, ANTHROPIC_BASE_URL.\n    If neither are set, the default value of 'https://api.anthropic.com' will\n    be used.\n    \"\"\"\n\n    anthropic_api_key: SecretStr = Field(\n        alias=\"api_key\",\n        default_factory=secret_from_env(\"ANTHROPIC_API_KEY\", default=\"\"),\n    )\n\n    \"\"\"Automatically read from env var `ANTHROPIC_API_KEY` if not provided.\"\"\"\n\n    default_headers: Optional[Mapping[str, str]] = None\n    \"\"\"Headers to pass to the Anthropic clients, will be used for every API call.\"\"\"\n\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n\n    streaming: bool = False\n    \"\"\"Whether to use streaming or not.\"\"\"\n\n    stream_usage: bool = True\n    \"\"\"Whether to include usage metadata in streaming output. If True, additional\n    message chunks will be generated during the stream including usage metadata.\n    \"\"\"\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"anthropic-chat\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"anthropic_api_key\": \"ANTHROPIC_API_KEY\"}\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"anthropic\"]\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"model_kwargs\": self.model_kwargs,\n            \"streaming\": self.streaming,\n            \"max_retries\": self.max_retries,\n            \"default_request_timeout\": self.default_request_timeout,\n        }\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"anthropic\",\n            ls_model_name=self.model,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict) -> Any:\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @cached_property\n    def _client_params(self) -> Dict[str, Any]:\n        client_params: Dict[str, Any] = {\n            \"api_key\": self.anthropic_api_key.get_secret_value(),\n            \"base_url\": self.anthropic_api_url,\n            \"max_retries\": self.max_retries,\n            \"default_headers\": (self.default_headers or None),\n        }\n        # value <= 0 indicates the param should be ignored. None is a meaningful value\n        # for Anthropic client and treated differently than not specifying the param at\n        # all.\n        if self.default_request_timeout is None or self.default_request_timeout > 0:\n            client_params[\"timeout\"] = self.default_request_timeout\n\n        return client_params\n\n    @cached_property\n    def _client(self) -> anthropic.Client:\n        return anthropic.Client(**self._client_params)\n\n    @cached_property\n    def _async_client(self) -> anthropic.AsyncClient:\n        return anthropic.AsyncClient(**self._client_params)\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Dict,\n    ) -> Dict:\n        messages = self._convert_input(input_).to_messages()\n        system, formatted_messages = _format_messages(messages)\n        payload = {\n            \"model\": self.model,\n            \"max_tokens\": self.max_tokens,\n            \"messages\": formatted_messages,\n            \"temperature\": self.temperature,\n            \"top_k\": self.top_k,\n            \"top_p\": self.top_p,\n            \"stop_sequences\": stop or self.stop_sequences,\n            \"system\": system,\n            **self.model_kwargs,\n            **kwargs,\n        }\n        return {k: v for k, v in payload.items() if v is not None}\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = self._client.messages.create(**payload)\n        coerce_content_to_string = not _tools_in_params(\n            payload\n        ) and not _documents_in_params(payload)\n        for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        *,\n        stream_usage: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        if stream_usage is None:\n            stream_usage = self.stream_usage\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        stream = await self._async_client.messages.create(**payload)\n        coerce_content_to_string = not _tools_in_params(\n            payload\n        ) and not _documents_in_params(payload)\n        async for event in stream:\n            msg = _make_message_chunk_from_anthropic_event(\n                event,\n                stream_usage=stream_usage,\n                coerce_content_to_string=coerce_content_to_string,\n            )\n            if msg is not None:\n                chunk = ChatGenerationChunk(message=msg)\n                if run_manager and isinstance(msg.content, str):\n                    await run_manager.on_llm_new_token(msg.content, chunk=chunk)\n                yield chunk\n\n    def _format_output(self, data: Any, **kwargs: Any) -> ChatResult:\n        data_dict = data.model_dump()\n        content = data_dict[\"content\"]\n\n        # Remove citations if they are None - introduced in anthropic sdk 0.45\n        for block in content:\n            if (\n                isinstance(block, dict)\n                and \"citations\" in block\n                and block[\"citations\"] is None\n            ):\n                block.pop(\"citations\")\n\n        llm_output = {\n            k: v for k, v in data_dict.items() if k not in (\"content\", \"role\", \"type\")\n        }\n        if (\n            len(content) == 1\n            and content[0][\"type\"] == \"text\"\n            and not content[0].get(\"citations\")\n        ):\n            msg = AIMessage(content=content[0][\"text\"])\n        elif any(block[\"type\"] == \"tool_use\" for block in content):\n            tool_calls = extract_tool_calls(content)\n            msg = AIMessage(\n                content=content,\n                tool_calls=tool_calls,\n            )\n        else:\n            msg = AIMessage(content=content)\n        msg.usage_metadata = _create_usage_metadata(data.usage)\n        return ChatResult(\n            generations=[ChatGeneration(message=msg)],\n            llm_output=llm_output,\n        )\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = self._client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        data = await self._async_client.messages.create(**payload)\n        return self._format_output(data, **kwargs)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[\n            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n        ] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        r\"\"\"Bind tool-like objects to this chat model.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports Anthropic format tool schemas and any tool definition handled\n                by :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call. Options are:\n\n                - name of the tool as a string or as dict ``{\"type\": \"tool\", \"name\": \"<<tool_name>>\"}``: calls corresponding tool;\n                - ``\"auto\"``, ``{\"type: \"auto\"}``, or None: automatically selects a tool (including no tool);\n                - ``\"any\"`` or ``{\"type: \"any\"}``: force at least one tool to be called;\n            parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n                Defaults to ``None`` (no specification, which allows parallel tool use).\n\n                .. versionadded:: 0.3.2\n            kwargs: Any additional parameters are passed directly to\n                :meth:`~langchain_anthropic.chat_models.ChatAnthropic.bind`.\n\n        Example:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n                # -> AIMessage(\n                #     content=[\n                #         {'text': '<thinking>\\nBased on the user\\'s question, the relevant function to call is GetWeather, which requires the \"location\" parameter.\\n\\nThe user has directly specified the location as \"San Francisco\". Since San Francisco is a well known city, I can reasonably infer they mean San Francisco, CA without needing the state specified.\\n\\nAll the required parameters are provided, so I can proceed with the API call.\\n</thinking>', 'type': 'text'},\n                #         {'text': None, 'type': 'tool_use', 'id': 'toolu_01SCgExKzQ7eqSkMHfygvYuu', 'name': 'GetWeather', 'input': {'location': 'San Francisco, CA'}}\n                #     ],\n                #     response_metadata={'id': 'msg_01GM3zQtoFv8jGQMW7abLnhi', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 487, 'output_tokens': 145}},\n                #     id='run-87b1331e-9251-4a68-acef-f0a018b639cc-0'\n                # )\n\n        Example \u2014 force tool call with tool_choice 'any':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"any\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n\n        Example \u2014 force specific tool call with tool_choice '<name_of_tool>':\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                llm_with_tools = llm.bind_tools([GetWeather, GetPrice], tool_choice=\"GetWeather\")\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n        Example \u2014 cache specific tools:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic, convert_to_anthropic_tool\n                from pydantic import BaseModel, Field\n\n                class GetWeather(BaseModel):\n                    '''Get the current weather in a given location'''\n\n                    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n                class GetPrice(BaseModel):\n                    '''Get the price of a specific product.'''\n\n                    product: str = Field(..., description=\"The product to look up.\")\n\n                # We'll convert our pydantic class to the anthropic tool format\n                # before passing to bind_tools so that we can set the 'cache_control'\n                # field on our tool.\n                cached_price_tool = convert_to_anthropic_tool(GetPrice)\n                # Currently the only supported \"cache_control\" value is\n                # {\"type\": \"ephemeral\"}.\n                cached_price_tool[\"cache_control\"] = {\"type\": \"ephemeral\"}\n\n                # We need to pass in extra headers to enable use of the beta cache\n                # control API.\n                llm = ChatAnthropic(\n                    model=\"claude-3-5-sonnet-20240620\",\n                    temperature=0,\n                    extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n                )\n                llm_with_tools = llm.bind_tools([GetWeather, cached_price_tool])\n                llm_with_tools.invoke(\"what is the weather like in San Francisco\",)\n\n            This outputs:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': \"Certainly! I can help you find out the current weather in San Francisco. To get this information, I'll use the GetWeather function. Let me fetch that data for you right away.\", 'type': 'text'}, {'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_01Xg7Wr5inFWgBxE5jH9rpRo', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 96, 'cache_creation_input_tokens': 1470, 'cache_read_input_tokens': 0}}, id='run-b36a5b54-5d69-470e-a1b0-b932d00b089e-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01TS5h8LNo7p5imcG7yRiaUM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 96, 'total_tokens': 267})\n\n            If we invoke the tool again, we can see that the \"usage\" information in the AIMessage.response_metadata shows that we had a cache hit:\n\n            .. code-block:: python\n\n                AIMessage(content=[{'text': 'To get the current weather in San Francisco, I can use the GetWeather function. Let me check that for you.', 'type': 'text'}, {'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'input': {'location': 'San Francisco, CA'}, 'name': 'GetWeather', 'type': 'tool_use'}], response_metadata={'id': 'msg_016RfWHrRvW6DAGCdwB6Ac64', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 171, 'output_tokens': 82, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 1470}}, id='run-88b1f825-dcb7-4277-ac27-53df55d22001-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'toolu_01HtVtY1qhMFdPprx42qU2eA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 82, 'total_tokens': 253})\n\n        \"\"\"  # noqa: E501\n        formatted_tools = [convert_to_anthropic_tool(tool) for tool in tools]\n        if not tool_choice:\n            pass\n        elif isinstance(tool_choice, dict):\n            kwargs[\"tool_choice\"] = tool_choice\n        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n        elif isinstance(tool_choice, str):\n            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n        else:\n            raise ValueError(\n                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n                f\"str, or None.\"\n            )\n\n        if parallel_tool_calls is not None:\n            disable_parallel_tool_use = not parallel_tool_calls\n            if \"tool_choice\" in kwargs:\n                kwargs[\"tool_choice\"][\"disable_parallel_tool_use\"] = (\n                    disable_parallel_tool_use\n                )\n            else:\n                kwargs[\"tool_choice\"] = {\n                    \"type\": \"auto\",\n                    \"disable_parallel_tool_use\": disable_parallel_tool_use,\n                }\n\n        return self.bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Union[Dict, type],\n        *,\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema: The output schema. Can be passed in as:\n\n                - an Anthropic tool schema,\n                - an OpenAI function/tool schema,\n                - a JSON Schema,\n                - a TypedDict class,\n                - or a Pydantic class.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`~langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            kwargs: Additional keyword arguments are ignored.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`~langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: Pydantic schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example:  Pydantic schema (include_raw=True):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: Dict schema (include_raw=False):\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n\n                schema = {\n                    \"name\": \"AnswerWithJustification\",\n                    \"description\": \"An answer to the user question along with justification for the answer.\",\n                    \"input_schema\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"answer\": {\"type\": \"string\"},\n                            \"justification\": {\"type\": \"string\"},\n                        },\n                        \"required\": [\"answer\", \"justification\"]\n                    }\n                }\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0)\n                structured_llm = llm.with_structured_output(schema)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. versionchanged:: 0.1.22\n\n                Added support for TypedDict class as `schema`.\n\n        \"\"\"  # noqa: E501\n        formatted_tool = convert_to_anthropic_tool(schema)\n        tool_name = formatted_tool[\"name\"]\n        llm = self.bind_tools(\n            [schema],\n            tool_choice=tool_name,\n            structured_output_format={\"kwargs\": {}, \"schema\": formatted_tool},\n        )\n        if isinstance(schema, type) and is_basemodel_subclass(schema):\n            output_parser: OutputParserLike = PydanticToolsParser(\n                tools=[schema], first_tool_only=True\n            )\n        else:\n            output_parser = JsonOutputKeyToolsParser(\n                key_name=tool_name, first_tool_only=True\n            )\n\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    @beta()\n    def get_num_tokens_from_messages(\n        self,\n        messages: List[BaseMessage],\n        tools: Optional[\n            Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]\n        ] = None,\n    ) -> int:\n        \"\"\"Count tokens in a sequence of input messages.\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n\n        Basic usage:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage, SystemMessage\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                messages = [\n                    SystemMessage(content=\"You are a scientist\"),\n                    HumanMessage(content=\"Hello, Claude\"),\n                ]\n                llm.get_num_tokens_from_messages(messages)\n\n            .. code-block:: none\n\n                14\n\n        Pass tool schemas:\n            .. code-block:: python\n\n                from langchain_anthropic import ChatAnthropic\n                from langchain_core.messages import HumanMessage\n                from langchain_core.tools import tool\n\n                llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n                @tool(parse_docstring=True)\n                def get_weather(location: str) -> str:\n                    \\\"\\\"\\\"Get the current weather in a given location\n\n                    Args:\n                        location: The city and state, e.g. San Francisco, CA\n                    \\\"\\\"\\\"\n                    return \"Sunny\"\n\n                messages = [\n                    HumanMessage(content=\"What's the weather like in San Francisco?\"),\n                ]\n                llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n\n            .. code-block:: none\n\n                403\n\n        .. versionchanged:: 0.3.0\n\n                Uses Anthropic's token counting API to count tokens in messages. See:\n                https://docs.anthropic.com/en/docs/build-with-claude/token-counting\n        \"\"\"\n        formatted_system, formatted_messages = _format_messages(messages)\n        kwargs: Dict[str, Any] = {}\n        if isinstance(formatted_system, str):\n            kwargs[\"system\"] = formatted_system\n        if tools:\n            kwargs[\"tools\"] = [convert_to_anthropic_tool(tool) for tool in tools]\n\n        response = self._client.beta.messages.count_tokens(\n            betas=[\"token-counting-2024-11-01\"],\n            model=self.model,\n            messages=formatted_messages,  # type: ignore[arg-type]\n            **kwargs,\n        )\n        return response.input_tokens\n\n\nclass AnthropicTool(TypedDict):\n    \"\"\"Anthropic tool definition.\"\"\"\n\n    name: str\n    description: str\n    input_schema: Dict[str, Any]\n    cache_control: NotRequired[Dict[str, str]]\n\n\ndef convert_to_anthropic_tool(\n    tool: Union[Dict[str, Any], Type, Callable, BaseTool],\n) -> AnthropicTool:\n    \"\"\"Convert a tool-like object to an Anthropic tool definition.\"\"\"\n    # already in Anthropic tool format\n    if isinstance(tool, dict) and all(\n        k in tool for k in (\"name\", \"description\", \"input_schema\")\n    ):\n        anthropic_formatted = AnthropicTool(tool)  # type: ignore\n    else:\n        oai_formatted = convert_to_openai_tool(tool)[\"function\"]\n        anthropic_formatted = AnthropicTool(\n            name=oai_formatted[\"name\"],\n            description=oai_formatted[\"description\"],\n            input_schema=oai_formatted[\"parameters\"],\n        )\n    return anthropic_formatted\n\n\ndef _tools_in_params(params: dict) -> bool:\n    return \"tools\" in params or (\n        \"extra_body\" in params and params[\"extra_body\"].get(\"tools\")\n    )\n\n\ndef _documents_in_params(params: dict) -> bool:\n    for message in params.get(\"messages\", []):\n        if isinstance(message.get(\"content\"), list):\n            for block in message[\"content\"]:\n                if (\n                    isinstance(block, dict)\n                    and block.get(\"type\") == \"document\"\n                    and block.get(\"citations\", {}).get(\"enabled\")\n                ):\n                    return True\n    return False\n\n\nclass _AnthropicToolUse(TypedDict):\n    type: Literal[\"tool_use\"]\n    name: str\n    input: dict\n    id: str\n\n\ndef _lc_tool_calls_to_anthropic_tool_use_blocks(\n    tool_calls: List[ToolCall],\n) -> List[_AnthropicToolUse]:\n    blocks = []\n    for tool_call in tool_calls:\n        blocks.append(\n            _AnthropicToolUse(\n                type=\"tool_use\",\n                name=tool_call[\"name\"],\n                input=tool_call[\"args\"],\n                id=cast(str, tool_call[\"id\"]),\n            )\n        )\n    return blocks\n\n\ndef _make_message_chunk_from_anthropic_event(\n    event: anthropic.types.RawMessageStreamEvent,\n    *,\n    stream_usage: bool = True,\n    coerce_content_to_string: bool,\n) -> Optional[AIMessageChunk]:\n    \"\"\"Convert Anthropic event to AIMessageChunk.\n\n    Note that not all events will result in a message chunk. In these cases\n    we return None.\n    \"\"\"\n    message_chunk: Optional[AIMessageChunk] = None\n    # See https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/streaming/_messages.py  # noqa: E501\n    if event.type == \"message_start\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.message.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\" if coerce_content_to_string else [],\n            usage_metadata=usage_metadata,\n        )\n    elif (\n        event.type == \"content_block_start\"\n        and event.content_block is not None\n        and event.content_block.type in (\"tool_use\", \"document\")\n    ):\n        if coerce_content_to_string:\n            warnings.warn(\"Received unexpected tool content block.\")\n        content_block = event.content_block.model_dump()\n        content_block[\"index\"] = event.index\n        if event.content_block.type == \"tool_use\":\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=event.content_block.id,\n                name=event.content_block.name,\n                args=\"\",\n            )\n            tool_call_chunks = [tool_call_chunk]\n        else:\n            tool_call_chunks = []\n        message_chunk = AIMessageChunk(\n            content=[content_block],\n            tool_call_chunks=tool_call_chunks,  # type: ignore\n        )\n    elif event.type == \"content_block_delta\":\n        if event.delta.type in (\"text_delta\", \"citations_delta\"):\n            if coerce_content_to_string and hasattr(event.delta, \"text\"):\n                text = event.delta.text\n                message_chunk = AIMessageChunk(content=text)\n            else:\n                content_block = event.delta.model_dump()\n                content_block[\"index\"] = event.index\n                content_block[\"type\"] = \"text\"\n                if \"citation\" in content_block:\n                    content_block[\"citations\"] = [content_block.pop(\"citation\")]\n                message_chunk = AIMessageChunk(content=[content_block])\n        elif event.delta.type == \"input_json_delta\":\n            content_block = event.delta.model_dump()\n            content_block[\"index\"] = event.index\n            content_block[\"type\"] = \"tool_use\"\n            tool_call_chunk = create_tool_call_chunk(\n                index=event.index,\n                id=None,\n                name=None,\n                args=event.delta.partial_json,\n            )\n            message_chunk = AIMessageChunk(\n                content=[content_block],\n                tool_call_chunks=[tool_call_chunk],  # type: ignore\n            )\n    elif event.type == \"message_delta\" and stream_usage:\n        usage_metadata = _create_usage_metadata(event.usage)\n        message_chunk = AIMessageChunk(\n            content=\"\",\n            usage_metadata=usage_metadata,\n            response_metadata={\n                \"stop_reason\": event.delta.stop_reason,\n                \"stop_sequence\": event.delta.stop_sequence,\n            },\n        )\n    else:\n        pass\n\n    return message_chunk\n\n\n@deprecated(since=\"0.1.0\", removal=\"1.0.0\", alternative=\"ChatAnthropic\")\nclass ChatAnthropicMessages(ChatAnthropic):\n    pass\n\n\ndef _create_usage_metadata(anthropic_usage: BaseModel) -> UsageMetadata:\n    input_token_details: Dict = {\n        \"cache_read\": getattr(anthropic_usage, \"cache_read_input_tokens\", None),\n        \"cache_creation\": getattr(anthropic_usage, \"cache_creation_input_tokens\", None),\n    }\n\n    # Anthropic input_tokens exclude cached token counts.\n    input_tokens = (\n        getattr(anthropic_usage, \"input_tokens\", 0)\n        + (input_token_details[\"cache_read\"] or 0)\n        + (input_token_details[\"cache_creation\"] or 0)\n    )\n    output_tokens = getattr(anthropic_usage, \"output_tokens\", 0)\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=input_tokens + output_tokens,\n        input_token_details=InputTokenDetails(\n            **{k: v for k, v in input_token_details.items() if v is not None}\n        ),\n    )\n",
        "patch": "@@ -244,6 +244,23 @@ def _format_messages(\n                                     if k in (\"type\", \"text\", \"cache_control\")\n                                 }\n                             )\n+                    elif block[\"type\"] == \"thinking\":\n+                        content.append(\n+                            {\n+                                k: v\n+                                for k, v in block.items()\n+                                if k\n+                                in (\"type\", \"thinking\", \"cache_control\", \"signature\")\n+                            }\n+                        )\n+                    elif block[\"type\"] == \"redacted_thinking\":\n+                        content.append(\n+                            {\n+                                k: v\n+                                for k, v in block.items()\n+                                if k in (\"type\", \"cache_control\", \"data\")\n+                            }\n+                        )\n                     elif block[\"type\"] == \"tool_result\":\n                         tool_content = _format_messages(\n                             [HumanMessage(block[\"content\"])]\n@@ -600,6 +617,10 @@ class Joke(BaseModel):\n     message chunks will be generated during the stream including usage metadata.\n     \"\"\"\n \n+    thinking: Optional[Dict[str, Any]] = Field(default=None)\n+    \"\"\"Parameters for Claude reasoning,\n+    e.g., ``{\"type\": \"enabled\", \"budget_tokens\": 10_000}``\"\"\"\n+\n     @property\n     def _llm_type(self) -> str:\n         \"\"\"Return type of chat model.\"\"\"\n@@ -631,6 +652,7 @@ def _identifying_params(self) -> Dict[str, Any]:\n             \"streaming\": self.streaming,\n             \"max_retries\": self.max_retries,\n             \"default_request_timeout\": self.default_request_timeout,\n+            \"thinking\": self.thinking,\n         }\n \n     def _get_ls_params(\n@@ -702,6 +724,8 @@ def _get_request_payload(\n             **self.model_kwargs,\n             **kwargs,\n         }\n+        if self.thinking is not None:\n+            payload[\"thinking\"] = self.thinking\n         return {k: v for k, v in payload.items() if v is not None}\n \n     def _stream(\n@@ -718,9 +742,11 @@ def _stream(\n         kwargs[\"stream\"] = True\n         payload = self._get_request_payload(messages, stop=stop, **kwargs)\n         stream = self._client.messages.create(**payload)\n-        coerce_content_to_string = not _tools_in_params(\n-            payload\n-        ) and not _documents_in_params(payload)\n+        coerce_content_to_string = (\n+            not _tools_in_params(payload)\n+            and not _documents_in_params(payload)\n+            and not _thinking_in_params(payload)\n+        )\n         for event in stream:\n             msg = _make_message_chunk_from_anthropic_event(\n                 event,\n@@ -747,9 +773,11 @@ async def _astream(\n         kwargs[\"stream\"] = True\n         payload = self._get_request_payload(messages, stop=stop, **kwargs)\n         stream = await self._async_client.messages.create(**payload)\n-        coerce_content_to_string = not _tools_in_params(\n-            payload\n-        ) and not _documents_in_params(payload)\n+        coerce_content_to_string = (\n+            not _tools_in_params(payload)\n+            and not _documents_in_params(payload)\n+            and not _thinking_in_params(payload)\n+        )\n         async for event in stream:\n             msg = _make_message_chunk_from_anthropic_event(\n                 event,\n@@ -774,6 +802,13 @@ def _format_output(self, data: Any, **kwargs: Any) -> ChatResult:\n                 and block[\"citations\"] is None\n             ):\n                 block.pop(\"citations\")\n+            if (\n+                isinstance(block, dict)\n+                and block.get(\"type\") == \"thinking\"\n+                and \"text\" in block\n+                and block[\"text\"] is None\n+            ):\n+                block.pop(\"text\")\n \n         llm_output = {\n             k: v for k, v in data_dict.items() if k not in (\"content\", \"role\", \"type\")\n@@ -1268,6 +1303,10 @@ def _tools_in_params(params: dict) -> bool:\n     )\n \n \n+def _thinking_in_params(params: dict) -> bool:\n+    return params.get(\"thinking\", {}).get(\"type\") == \"enabled\"\n+\n+\n def _documents_in_params(params: dict) -> bool:\n     for message in params.get(\"messages\", []):\n         if isinstance(message.get(\"content\"), list):\n@@ -1326,7 +1365,7 @@ def _make_message_chunk_from_anthropic_event(\n     elif (\n         event.type == \"content_block_start\"\n         and event.content_block is not None\n-        and event.content_block.type in (\"tool_use\", \"document\")\n+        and event.content_block.type in (\"tool_use\", \"document\", \"redacted_thinking\")\n     ):\n         if coerce_content_to_string:\n             warnings.warn(\"Received unexpected tool content block.\")\n@@ -1358,6 +1397,20 @@ def _make_message_chunk_from_anthropic_event(\n                 if \"citation\" in content_block:\n                     content_block[\"citations\"] = [content_block.pop(\"citation\")]\n                 message_chunk = AIMessageChunk(content=[content_block])\n+        elif event.delta.type == \"thinking_delta\":\n+            content_block = event.delta.model_dump()\n+            if \"text\" in content_block and content_block[\"text\"] is None:\n+                content_block.pop(\"text\")\n+            content_block[\"index\"] = event.index\n+            content_block[\"type\"] = \"thinking\"\n+            message_chunk = AIMessageChunk(content=[content_block])\n+        elif event.delta.type == \"signature_delta\":\n+            content_block = event.delta.model_dump()\n+            if \"text\" in content_block and content_block[\"text\"] is None:\n+                content_block.pop(\"text\")\n+            content_block[\"index\"] = event.index\n+            content_block[\"type\"] = \"thinking\"\n+            message_chunk = AIMessageChunk(content=[content_block])\n         elif event.delta.type == \"input_json_delta\":\n             content_block = event.delta.model_dump()\n             content_block[\"index\"] = event.index"
      },
      {
        "filename": "libs/partners/anthropic/tests/integration_tests/test_chat_models.py",
        "content_before": "\"\"\"Test ChatAnthropic chat model.\"\"\"\n\nimport json\nfrom base64 import b64encode\nfrom typing import List, Optional\n\nimport pytest\nimport requests\nfrom langchain_core.callbacks import CallbackManager\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    HumanMessage,\n    SystemMessage,\n    ToolMessage,\n)\nfrom langchain_core.outputs import ChatGeneration, LLMResult\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\nfrom langchain_anthropic import ChatAnthropic, ChatAnthropicMessages\nfrom tests.unit_tests._utils import FakeCallbackHandler\n\nMODEL_NAME = \"claude-3-5-haiku-latest\"\nIMAGE_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n\n\ndef test_stream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    for token in llm.stream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n\nasync def test_astream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    async for token in llm.astream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n    # Check expected raw API output\n    async_client = llm._async_client\n    params: dict = {\n        \"model\": MODEL_NAME,\n        \"max_tokens\": 1024,\n        \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}],\n        \"temperature\": 0.0,\n    }\n    stream = await async_client.messages.create(**params, stream=True)\n    async for event in stream:\n        if event.type == \"message_start\":\n            assert event.message.usage.input_tokens > 1\n            # Note: this single output token included in message start event\n            # does not appear to contribute to overall output token counts. It\n            # is excluded from the total token count.\n            assert event.message.usage.output_tokens == 1\n        elif event.type == \"message_delta\":\n            assert event.usage.output_tokens > 1\n        else:\n            pass\n\n\nasync def test_stream_usage() -> None:\n    \"\"\"Test usage metadata can be excluded.\"\"\"\n    model = ChatAnthropic(model_name=MODEL_NAME, stream_usage=False)  # type: ignore[call-arg]\n    async for token in model.astream(\"hi\"):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n    # check we override with kwarg\n    model = ChatAnthropic(model_name=MODEL_NAME)  # type: ignore[call-arg]\n    assert model.stream_usage\n    async for token in model.astream(\"hi\", stream_usage=False):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n\n\nasync def test_abatch() -> None:\n    \"\"\"Test streaming tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_abatch_tags() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch(\n        [\"I'm Pickle Rick\", \"I'm not Pickle Rick\"], config={\"tags\": [\"foo\"]}\n    )\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_async_tool_use() -> None:\n    llm = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = await llm_with_tools.ainvoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    first = True\n    chunks = []  # type: ignore\n    async for chunk in llm_with_tools.astream(\n        \"what's the weather in san francisco, ca\"\n    ):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_call_chunks, list)\n    assert len(gathered.tool_call_chunks) == 1\n    tool_call_chunk = gathered.tool_call_chunks[0]\n    assert tool_call_chunk[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call_chunk[\"args\"], str)\n    assert \"location\" in json.loads(tool_call_chunk[\"args\"])\n\n\ndef test_batch() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.batch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_ainvoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.ainvoke(\"I'm Pickle Rick\", config={\"tags\": [\"foo\"]})\n    assert isinstance(result.content, str)\n\n\ndef test_invoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.invoke(\"I'm Pickle Rick\", config=dict(tags=[\"foo\"]))\n    assert isinstance(result.content, str)\n\n\ndef test_system_invoke() -> None:\n    \"\"\"Test invoke tokens with a system message\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are an expert cartographer. If asked, you are a cartographer. \"\n                \"STAY IN CHARACTER\",\n            ),\n            (\"human\", \"Are you a mathematician?\"),\n        ]\n    )\n\n    chain = prompt | llm\n\n    result = chain.invoke({})\n    assert isinstance(result.content, str)\n\n\ndef test_anthropic_call() -> None:\n    \"\"\"Test valid call to anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.invoke([message])\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n\n\ndef test_anthropic_generate() -> None:\n    \"\"\"Test generate method of anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    chat_messages: List[List[BaseMessage]] = [\n        [HumanMessage(content=\"How many toes do dogs have?\")]\n    ]\n    messages_copy = [messages.copy() for messages in chat_messages]\n    result: LLMResult = chat.generate(chat_messages)\n    assert isinstance(result, LLMResult)\n    for response in result.generations[0]:\n        assert isinstance(response, ChatGeneration)\n        assert isinstance(response.text, str)\n        assert response.text == response.message.content\n    assert chat_messages == messages_copy\n\n\ndef test_anthropic_streaming() -> None:\n    \"\"\"Test streaming tokens from anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.stream([message])\n    for token in response:\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n\n\ndef test_anthropic_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    message = HumanMessage(content=\"Write me a sentence with 10 words.\")\n    for token in chat.stream([message]):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\nasync def test_anthropic_async_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    chat_messages: List[BaseMessage] = [\n        HumanMessage(content=\"How many toes do dogs have?\")\n    ]\n    async for token in chat.astream(chat_messages):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\ndef test_anthropic_multimodal() -> None:\n    \"\"\"Test that multimodal inputs are handled correctly.\"\"\"\n    chat = ChatAnthropic(model=IMAGE_MODEL_NAME)\n    messages: list[BaseMessage] = [\n        HumanMessage(\n            content=[\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        # langchain logo\n                        \"url\": \"data:image/jpeg;base64,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\",  # noqa: E501\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is this a logo for?\"},\n            ]\n        )\n    ]\n    response = chat.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n    num_tokens = chat.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n\ndef test_streaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = llm.generate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\nasync def test_astreaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = await llm.agenerate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\ndef test_tool_use() -> None:\n    llm = ChatAnthropic(model=MODEL_NAME)\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = llm_with_tools.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    input = \"how are you? what's the weather in san francisco, ca\"\n    first = True\n    chunks = []  # type: ignore\n    for chunk in llm_with_tools.stream(input):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered.content, list)\n    assert len(gathered.content) == 2\n    tool_use_block = None\n    for content_block in gathered.content:\n        assert isinstance(content_block, dict)\n        if content_block[\"type\"] == \"tool_use\":\n            tool_use_block = content_block\n            break\n    assert tool_use_block is not None\n    assert tool_use_block[\"name\"] == \"get_weather\"\n    assert \"location\" in json.loads(tool_use_block[\"partial_json\"])\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_calls, list)\n    assert len(gathered.tool_calls) == 1\n    tool_call = gathered.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n    assert tool_call[\"id\"] is not None\n\n    # Test passing response back to model\n    stream = llm_with_tools.stream(\n        [\n            input,\n            gathered,\n            ToolMessage(content=\"sunny and warm\", tool_call_id=tool_call[\"id\"]),\n        ]\n    )\n    chunks = []  # type: ignore\n    first = True\n    for chunk in stream:\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n\n\nclass GenerateUsername(BaseModel):\n    \"Get a username based on someone's name and hair color.\"\n\n    name: str\n    hair_color: str\n\n\ndef test_disable_parallel_tool_calling() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n    llm_with_tools = llm.bind_tools([GenerateUsername], parallel_tool_calls=False)\n    result = llm_with_tools.invoke(\n        \"Use the GenerateUsername tool to generate user names for:\\n\\n\"\n        \"Sally with green hair\\n\"\n        \"Bob with blue hair\"\n    )\n    assert isinstance(result, AIMessage)\n    assert len(result.tool_calls) == 1\n\n\ndef test_anthropic_with_empty_text_block() -> None:\n    \"\"\"Anthropic SDK can return an empty text block.\"\"\"\n\n    @tool\n    def type_letter(letter: str) -> str:\n        \"\"\"Type the given letter.\"\"\"\n        return \"OK\"\n\n    model = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0).bind_tools(\n        [type_letter]\n    )\n\n    messages = [\n        SystemMessage(\n            content=\"Repeat the given string using the provided tools. Do not write \"\n            \"anything else or provide any explanations. For example, \"\n            \"if the string is 'abc', you must print the \"\n            \"letters 'a', 'b', and 'c' one at a time and in that order. \"\n        ),\n        HumanMessage(content=\"dog\"),\n        AIMessage(\n            content=[\n                {\"text\": \"\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"letter\": \"d\"},\n                    \"name\": \"type_letter\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"type_letter\",\n                    \"args\": {\"letter\": \"d\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"OK\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n\n    model.invoke(messages)\n\n\ndef test_with_structured_output() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-opus-20240229\",\n    )\n\n    structured_llm = llm.with_structured_output(\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather report for a city\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\"location\": {\"type\": \"string\"}},\n            },\n        }\n    )\n    response = structured_llm.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, dict)\n    assert response[\"location\"]\n\n\ndef test_get_num_tokens_from_messages() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n    # Test simple case\n    messages = [\n        SystemMessage(content=\"You are a scientist\"),\n        HumanMessage(content=\"Hello, Claude\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n    # Test tool use\n    @tool(parse_docstring=True)\n    def get_weather(location: str) -> str:\n        \"\"\"Get the current weather in a given location\n\n        Args:\n            location: The city and state, e.g. San Francisco, CA\n        \"\"\"\n        return \"Sunny\"\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n        AIMessage(\n            content=[\n                {\"text\": \"Let's see.\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"location\": \"SF\"},\n                    \"name\": \"get_weather\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"get_weather\",\n                    \"args\": {\"location\": \"SF\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"Sunny\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n\nclass GetWeather(BaseModel):\n    \"\"\"Get the current weather in a given location\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n@pytest.mark.parametrize(\"tool_choice\", [\"GetWeather\", \"auto\", \"any\"])\ndef test_anthropic_bind_tools_tool_choice(tool_choice: str) -> None:\n    chat_model = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n    chat_model_with_tools = chat_model.bind_tools([GetWeather], tool_choice=tool_choice)\n    response = chat_model_with_tools.invoke(\"what's the weather in ny and la\")\n    assert isinstance(response, AIMessage)\n\n\ndef test_pdf_document_input() -> None:\n    url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n    data = b64encode(requests.get(url).content).decode()\n\n    result = ChatAnthropic(model=IMAGE_MODEL_NAME).invoke(\n        [\n            HumanMessage(\n                [\n                    \"summarize this document\",\n                    {\n                        \"type\": \"document\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"data\": data,\n                            \"media_type\": \"application/pdf\",\n                        },\n                    },\n                ]\n            )\n        ]\n    )\n    assert isinstance(result, AIMessage)\n    assert isinstance(result.content, str)\n    assert len(result.content) > 0\n\n\ndef test_citations() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"content\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": \"The grass is green\"},\n                            {\"type\": \"text\", \"text\": \"The sky is blue\"},\n                        ],\n                    },\n                    \"citations\": {\"enabled\": True},\n                },\n                {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n            ],\n        }\n    ]\n    response = llm.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert any(\"citations\" in block for block in response.content)\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(messages):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    assert any(\"citations\" in block for block in full.content)\n    assert not any(\"citation\" in block for block in full.content)\n",
        "patch": "@@ -661,3 +661,67 @@ def test_citations() -> None:\n     assert isinstance(full.content, list)\n     assert any(\"citations\" in block for block in full.content)\n     assert not any(\"citation\" in block for block in full.content)\n+\n+\n+def test_thinking() -> None:\n+    llm = ChatAnthropic(\n+        model=\"claude-3-7-sonnet-latest\",\n+        max_tokens=5_000,\n+        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n+    )\n+    response = llm.invoke(\"Hello\")\n+    assert any(\"thinking\" in block for block in response.content)\n+    for block in response.content:\n+        assert isinstance(block, dict)\n+        if block[\"type\"] == \"thinking\":\n+            assert set(block.keys()) == {\"type\", \"thinking\", \"signature\"}\n+            assert block[\"thinking\"] and isinstance(block[\"thinking\"], str)\n+            assert block[\"signature\"] and isinstance(block[\"signature\"], str)\n+\n+    # Test streaming\n+    full: Optional[BaseMessageChunk] = None\n+    for chunk in llm.stream(\"Hello\"):\n+        full = chunk if full is None else full + chunk\n+    assert isinstance(full, AIMessageChunk)\n+    assert isinstance(full.content, list)\n+    assert any(\"thinking\" in block for block in full.content)\n+    for block in full.content:\n+        assert isinstance(block, dict)\n+        if block[\"type\"] == \"thinking\":\n+            assert set(block.keys()) == {\"type\", \"thinking\", \"signature\", \"index\"}\n+            assert block[\"thinking\"] and isinstance(block[\"thinking\"], str)\n+            assert block[\"signature\"] and isinstance(block[\"signature\"], str)\n+\n+\n+def test_redacted_thinking() -> None:\n+    llm = ChatAnthropic(\n+        model=\"claude-3-7-sonnet-latest\",\n+        max_tokens=5_000,\n+        thinking={\"type\": \"enabled\", \"budget_tokens\": 2_000},\n+    )\n+    query = \"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"  # noqa: E501\n+\n+    response = llm.invoke(query)\n+    has_reasoning = False\n+    for block in response.content:\n+        assert isinstance(block, dict)\n+        if block[\"type\"] == \"redacted_thinking\":\n+            has_reasoning = True\n+            assert set(block.keys()) == {\"type\", \"data\"}\n+            assert block[\"data\"] and isinstance(block[\"data\"], str)\n+    assert has_reasoning\n+\n+    # Test streaming\n+    full: Optional[BaseMessageChunk] = None\n+    for chunk in llm.stream(query):\n+        full = chunk if full is None else full + chunk\n+    assert isinstance(full, AIMessageChunk)\n+    assert isinstance(full.content, list)\n+    stream_has_reasoning = False\n+    for block in full.content:\n+        assert isinstance(block, dict)\n+        if block[\"type\"] == \"redacted_thinking\":\n+            stream_has_reasoning = True\n+            assert set(block.keys()) == {\"type\", \"data\", \"index\"}\n+            assert block[\"data\"] and isinstance(block[\"data\"], str)\n+    assert stream_has_reasoning"
      }
    ]
  },
  {
    "number": 29968,
    "title": "docs[patch]: update disable_streaming docstring",
    "body": null,
    "issue_title": "docs[patch]: update disable_streaming docstring",
    "issue_body": null,
    "files": [
      {
        "filename": "libs/core/langchain_core/language_models/chat_models.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport inspect\nimport json\nimport typing\nimport uuid\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import AsyncIterator, Iterator, Sequence\nfrom functools import cached_property\nfrom operator import itemgetter\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    Union,\n    cast,\n)\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    model_validator,\n)\nfrom typing_extensions import override\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.caches import BaseCache\nfrom langchain_core.callbacks import (\n    AsyncCallbackManager,\n    AsyncCallbackManagerForLLMRun,\n    BaseCallbackManager,\n    CallbackManager,\n    CallbackManagerForLLMRun,\n    Callbacks,\n)\nfrom langchain_core.globals import get_llm_cache\nfrom langchain_core.language_models.base import (\n    BaseLanguageModel,\n    LangSmithParams,\n    LanguageModelInput,\n)\nfrom langchain_core.load import dumpd, dumps\nfrom langchain_core.messages import (\n    AIMessage,\n    AnyMessage,\n    BaseMessage,\n    BaseMessageChunk,\n    HumanMessage,\n    convert_to_messages,\n    message_chunk_to_message,\n)\nfrom langchain_core.outputs import (\n    ChatGeneration,\n    ChatGenerationChunk,\n    ChatResult,\n    LLMResult,\n    RunInfo,\n)\nfrom langchain_core.prompt_values import ChatPromptValue, PromptValue, StringPromptValue\nfrom langchain_core.rate_limiters import BaseRateLimiter\nfrom langchain_core.runnables import RunnableMap, RunnablePassthrough\nfrom langchain_core.runnables.config import ensure_config, run_in_executor\nfrom langchain_core.tracers._streaming import _StreamingCallbackHandler\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import TypeBaseModel, is_basemodel_subclass\n\nif TYPE_CHECKING:\n    from langchain_core.output_parsers.base import OutputParserLike\n    from langchain_core.runnables import Runnable, RunnableConfig\n    from langchain_core.tools import BaseTool\n\n\ndef generate_from_stream(stream: Iterator[ChatGenerationChunk]) -> ChatResult:\n    \"\"\"Generate from a stream.\n\n    Args:\n        stream: Iterator of ChatGenerationChunk.\n\n    Returns:\n        ChatResult: Chat result.\n    \"\"\"\n    generation = next(stream, None)\n    if generation:\n        generation += list(stream)\n    if generation is None:\n        msg = \"No generations found in stream.\"\n        raise ValueError(msg)\n    return ChatResult(\n        generations=[\n            ChatGeneration(\n                message=message_chunk_to_message(generation.message),\n                generation_info=generation.generation_info,\n            )\n        ]\n    )\n\n\nasync def agenerate_from_stream(\n    stream: AsyncIterator[ChatGenerationChunk],\n) -> ChatResult:\n    \"\"\"Async generate from a stream.\n\n    Args:\n        stream: Iterator of ChatGenerationChunk.\n\n    Returns:\n        ChatResult: Chat result.\n    \"\"\"\n    chunks = [chunk async for chunk in stream]\n    return await run_in_executor(None, generate_from_stream, iter(chunks))\n\n\nclass BaseChatModel(BaseLanguageModel[BaseMessage], ABC):\n    \"\"\"Base class for chat models.\n\n    Key imperative methods:\n        Methods that actually call the underlying model.\n\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | Method                    | Input                                                          | Output                                                              | Description                                                                                      |\n        +===========================+================================================================+=====================================================================+==================================================================================================+\n        | `invoke`                  | str | List[dict | tuple | BaseMessage] | PromptValue           | BaseMessage                                                         | A single chat model call.                                                                        |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `ainvoke`                 | '''                                                            | BaseMessage                                                         | Defaults to running invoke in an async executor.                                                 |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `stream`                  | '''                                                            | Iterator[BaseMessageChunk]                                          | Defaults to yielding output of invoke.                                                           |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `astream`                 | '''                                                            | AsyncIterator[BaseMessageChunk]                                     | Defaults to yielding output of ainvoke.                                                          |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `astream_events`          | '''                                                            | AsyncIterator[StreamEvent]                                          | Event types: 'on_chat_model_start', 'on_chat_model_stream', 'on_chat_model_end'.                 |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `batch`                   | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running invoke in concurrent threads.                                                |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `abatch`                  | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running ainvoke in concurrent threads.                                               |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `batch_as_completed`      | List[''']                                                      | Iterator[Tuple[int, Union[BaseMessage, Exception]]]                 | Defaults to running invoke in concurrent threads.                                                |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n        | `abatch_as_completed`     | List[''']                                                      | AsyncIterator[Tuple[int, Union[BaseMessage, Exception]]]            | Defaults to running ainvoke in concurrent threads.                                               |\n        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+\n\n        This table provides a brief overview of the main imperative methods. Please see the base Runnable reference for full documentation.\n\n    Key declarative methods:\n        Methods for creating another Runnable using the ChatModel.\n\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | Method                           | Description                                                                                               |\n        +==================================+===========================================================================================================+\n        | `bind_tools`                     | Create ChatModel that can call tools.                                                                     |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `with_structured_output`         | Create wrapper that structures model output using schema.                                                 |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `with_retry`                     | Create wrapper that retries model calls on failure.                                                       |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `with_fallbacks`                 | Create wrapper that falls back to other models on failure.                                                |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `configurable_fields`            | Specify init args of the model that can be configured at runtime via the RunnableConfig.                  |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n        | `configurable_alternatives`      | Specify alternative models which can be swapped in at runtime via the RunnableConfig.                     |\n        +----------------------------------+-----------------------------------------------------------------------------------------------------------+\n\n        This table provides a brief overview of the main declarative methods. Please see the reference for each method for full documentation.\n\n    Creating custom chat model:\n        Custom chat model implementations should inherit from this class.\n        Please reference the table below for information about which\n        methods and properties are required or optional for implementations.\n\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | Method/Property                  | Description                                                        | Required/Optional |\n        +==================================+====================================================================+===================+\n        | `_generate`                      | Use to generate a chat result from a prompt                        | Required          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_llm_type` (property)           | Used to uniquely identify the type of the model. Used for logging. | Required          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_identifying_params` (property) | Represent model parameterization for tracing purposes.             | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_stream`                        | Use to implement streaming                                         | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_agenerate`                     | Use to implement a native async method                             | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n        | `_astream`                       | Use to implement async version of `_stream`                        | Optional          |\n        +----------------------------------+--------------------------------------------------------------------+-------------------+\n\n        Follow the guide for more information on how to implement a custom Chat Model:\n        [Guide](https://python.langchain.com/docs/how_to/custom_chat_model/).\n\n    \"\"\"  # noqa: E501\n\n    callback_manager: Optional[BaseCallbackManager] = deprecated(\n        name=\"callback_manager\", since=\"0.1.7\", removal=\"1.0\", alternative=\"callbacks\"\n    )(\n        Field(\n            default=None,\n            exclude=True,\n            description=\"Callback manager to add to the run trace.\",\n        )\n    )\n\n    rate_limiter: Optional[BaseRateLimiter] = Field(default=None, exclude=True)\n    \"An optional rate limiter to use for limiting the number of requests.\"\n\n    disable_streaming: Union[bool, Literal[\"tool_calling\"]] = False\n    \"\"\"Whether to disable streaming for this model.\n\n    If streaming is bypassed, then ``stream()/astream()`` will defer to\n    ``invoke()/ainvoke()``.\n\n    - If True, will always bypass streaming case.\n    - If \"tool_calling\", will bypass streaming case only when the model is called\n      with a ``tools`` keyword argument.\n    - If False (default), will always use streaming case if available.\n    \"\"\"\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def raise_deprecation(cls, values: dict) -> Any:\n        \"\"\"Raise deprecation warning if callback_manager is used.\n\n        Args:\n            values (Dict): Values to validate.\n\n        Returns:\n            Dict: Validated values.\n\n        Raises:\n            DeprecationWarning: If callback_manager is used.\n        \"\"\"\n        if values.get(\"callback_manager\") is not None:\n            warnings.warn(\n                \"callback_manager is deprecated. Please use callbacks instead.\",\n                DeprecationWarning,\n                stacklevel=5,\n            )\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\n        return values\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @cached_property\n    def _serialized(self) -> dict[str, Any]:\n        return dumpd(self)\n\n    # --- Runnable methods ---\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        \"\"\"Get the output type for this runnable.\"\"\"\n        return AnyMessage\n\n    def _convert_input(self, input: LanguageModelInput) -> PromptValue:\n        if isinstance(input, PromptValue):\n            return input\n        elif isinstance(input, str):\n            return StringPromptValue(text=input)\n        elif isinstance(input, Sequence):\n            return ChatPromptValue(messages=convert_to_messages(input))\n        else:\n            msg = (\n                f\"Invalid input type {type(input)}. \"\n                \"Must be a PromptValue, str, or list of BaseMessages.\"\n            )\n            raise ValueError(msg)  # noqa: TRY004\n\n    def invoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        config = ensure_config(config)\n        return cast(\n            ChatGeneration,\n            self.generate_prompt(\n                [self._convert_input(input)],\n                stop=stop,\n                callbacks=config.get(\"callbacks\"),\n                tags=config.get(\"tags\"),\n                metadata=config.get(\"metadata\"),\n                run_name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                **kwargs,\n            ).generations[0][0],\n        ).message\n\n    async def ainvoke(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        config = ensure_config(config)\n        llm_result = await self.agenerate_prompt(\n            [self._convert_input(input)],\n            stop=stop,\n            callbacks=config.get(\"callbacks\"),\n            tags=config.get(\"tags\"),\n            metadata=config.get(\"metadata\"),\n            run_name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            **kwargs,\n        )\n        return cast(ChatGeneration, llm_result.generations[0][0]).message\n\n    def _should_stream(\n        self,\n        *,\n        async_api: bool,\n        run_manager: Optional[\n            Union[CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun]\n        ] = None,\n        **kwargs: Any,\n    ) -> bool:\n        \"\"\"Determine if a given model call should hit the streaming API.\"\"\"\n        sync_not_implemented = type(self)._stream == BaseChatModel._stream\n        async_not_implemented = type(self)._astream == BaseChatModel._astream\n\n        # Check if streaming is implemented.\n        if (not async_api) and sync_not_implemented:\n            return False\n        # Note, since async falls back to sync we check both here.\n        if async_api and async_not_implemented and sync_not_implemented:\n            return False\n\n        # Check if streaming has been disabled on this instance.\n        if self.disable_streaming is True:\n            return False\n        # We assume tools are passed in via \"tools\" kwarg in all models.\n        if self.disable_streaming == \"tool_calling\" and kwargs.get(\"tools\"):\n            return False\n\n        # Check if a runtime streaming flag has been passed in.\n        if \"stream\" in kwargs:\n            return kwargs[\"stream\"]\n\n        # Check if any streaming callback handlers have been passed in.\n        handlers = run_manager.handlers if run_manager else []\n        return any(isinstance(h, _StreamingCallbackHandler) for h in handlers)\n\n    def stream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> Iterator[BaseMessageChunk]:\n        if not self._should_stream(async_api=False, **{**kwargs, \"stream\": True}):\n            # model doesn't implement streaming, so use default implementation\n            yield cast(\n                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)\n            )\n        else:\n            config = ensure_config(config)\n            messages = self._convert_input(input).to_messages()\n            structured_output_format = kwargs.pop(\"structured_output_format\", None)\n            if structured_output_format:\n                try:\n                    structured_output_format_dict = {\n                        \"structured_output_format\": {\n                            \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                            \"schema\": convert_to_openai_tool(\n                                structured_output_format[\"schema\"]\n                            ),\n                        }\n                    }\n                except ValueError:\n                    structured_output_format_dict = {}\n            else:\n                structured_output_format_dict = {}\n\n            params = self._get_invocation_params(stop=stop, **kwargs)\n            options = {\"stop\": stop, **kwargs}\n            inheritable_metadata = {\n                **(config.get(\"metadata\") or {}),\n                **self._get_ls_params(stop=stop, **kwargs),\n                **structured_output_format_dict,\n            }\n            callback_manager = CallbackManager.configure(\n                config.get(\"callbacks\"),\n                self.callbacks,\n                self.verbose,\n                config.get(\"tags\"),\n                self.tags,\n                inheritable_metadata,\n                self.metadata,\n            )\n            (run_manager,) = callback_manager.on_chat_model_start(\n                self._serialized,\n                [messages],\n                invocation_params=params,\n                options=options,\n                name=config.get(\"run_name\"),\n                run_id=config.pop(\"run_id\", None),\n                batch_size=1,\n            )\n            generation: Optional[ChatGenerationChunk] = None\n\n            if self.rate_limiter:\n                self.rate_limiter.acquire(blocking=True)\n\n            try:\n                for chunk in self._stream(messages, stop=stop, **kwargs):\n                    if chunk.message.id is None:\n                        chunk.message.id = f\"run-{run_manager.run_id}\"\n                    chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                    run_manager.on_llm_new_token(\n                        cast(str, chunk.message.content), chunk=chunk\n                    )\n                    yield chunk.message\n                    if generation is None:\n                        generation = chunk\n                    else:\n                        generation += chunk\n            except BaseException as e:\n                run_manager.on_llm_error(\n                    e,\n                    response=LLMResult(\n                        generations=[[generation]] if generation else []\n                    ),\n                )\n                raise\n\n            if generation is None:\n                err = ValueError(\"No generation chunks were returned\")\n                run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n                raise err\n\n            run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n\n    async def astream(\n        self,\n        input: LanguageModelInput,\n        config: Optional[RunnableConfig] = None,\n        *,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[BaseMessageChunk]:\n        if not self._should_stream(async_api=True, **{**kwargs, \"stream\": True}):\n            # No async or sync stream is implemented, so fall back to ainvoke\n            yield cast(\n                BaseMessageChunk,\n                await self.ainvoke(input, config=config, stop=stop, **kwargs),\n            )\n            return\n\n        config = ensure_config(config)\n        messages = self._convert_input(input).to_messages()\n\n        structured_output_format = kwargs.pop(\"structured_output_format\", None)\n        if structured_output_format:\n            try:\n                structured_output_format_dict = {\n                    \"structured_output_format\": {\n                        \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                        \"schema\": convert_to_openai_tool(\n                            structured_output_format[\"schema\"]\n                        ),\n                    }\n                }\n            except ValueError:\n                structured_output_format_dict = {}\n        else:\n            structured_output_format_dict = {}\n\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        options = {\"stop\": stop, **kwargs}\n        inheritable_metadata = {\n            **(config.get(\"metadata\") or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n            **structured_output_format_dict,\n        }\n        callback_manager = AsyncCallbackManager.configure(\n            config.get(\"callbacks\"),\n            self.callbacks,\n            self.verbose,\n            config.get(\"tags\"),\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n        (run_manager,) = await callback_manager.on_chat_model_start(\n            self._serialized,\n            [messages],\n            invocation_params=params,\n            options=options,\n            name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            batch_size=1,\n        )\n\n        if self.rate_limiter:\n            await self.rate_limiter.aacquire(blocking=True)\n\n        generation: Optional[ChatGenerationChunk] = None\n        try:\n            async for chunk in self._astream(\n                messages,\n                stop=stop,\n                **kwargs,\n            ):\n                if chunk.message.id is None:\n                    chunk.message.id = f\"run-{run_manager.run_id}\"\n                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                await run_manager.on_llm_new_token(\n                    cast(str, chunk.message.content), chunk=chunk\n                )\n                yield chunk.message\n                if generation is None:\n                    generation = chunk\n                else:\n                    generation += chunk\n        except BaseException as e:\n            await run_manager.on_llm_error(\n                e,\n                response=LLMResult(generations=[[generation]] if generation else []),\n            )\n            raise\n\n        if generation is None:\n            err = ValueError(\"No generation chunks were returned\")\n            await run_manager.on_llm_error(err, response=LLMResult(generations=[]))\n            raise err\n\n        await run_manager.on_llm_end(\n            LLMResult(generations=[[generation]]),\n        )\n\n    # --- Custom methods ---\n\n    def _combine_llm_outputs(self, llm_outputs: list[Optional[dict]]) -> dict:\n        return {}\n\n    def _get_invocation_params(\n        self,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        params = self.dict()\n        params[\"stop\"] = stop\n        return {**params, **kwargs}\n\n    def _get_ls_params(\n        self,\n        stop: Optional[list[str]] = None,\n        **kwargs: Any,\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        # get default provider from class name\n        default_provider = self.__class__.__name__\n        if default_provider.startswith(\"Chat\"):\n            default_provider = default_provider[4:].lower()\n        elif default_provider.endswith(\"Chat\"):\n            default_provider = default_provider[:-4]\n        default_provider = default_provider.lower()\n\n        ls_params = LangSmithParams(ls_provider=default_provider, ls_model_type=\"chat\")\n        if stop:\n            ls_params[\"ls_stop\"] = stop\n\n        # model\n        if hasattr(self, \"model\") and isinstance(self.model, str):\n            ls_params[\"ls_model_name\"] = self.model\n        elif hasattr(self, \"model_name\") and isinstance(self.model_name, str):\n            ls_params[\"ls_model_name\"] = self.model_name\n\n        # temperature\n        if \"temperature\" in kwargs and isinstance(kwargs[\"temperature\"], float):\n            ls_params[\"ls_temperature\"] = kwargs[\"temperature\"]\n        elif hasattr(self, \"temperature\") and isinstance(self.temperature, float):\n            ls_params[\"ls_temperature\"] = self.temperature\n\n        # max_tokens\n        if \"max_tokens\" in kwargs and isinstance(kwargs[\"max_tokens\"], int):\n            ls_params[\"ls_max_tokens\"] = kwargs[\"max_tokens\"]\n        elif hasattr(self, \"max_tokens\") and isinstance(self.max_tokens, int):\n            ls_params[\"ls_max_tokens\"] = self.max_tokens\n\n        return ls_params\n\n    def _get_llm_string(self, stop: Optional[list[str]] = None, **kwargs: Any) -> str:\n        if self.is_lc_serializable():\n            params = {**kwargs, \"stop\": stop}\n            param_string = str(sorted(params.items()))\n            # This code is not super efficient as it goes back and forth between\n            # json and dict.\n            serialized_repr = self._serialized\n            _cleanup_llm_representation(serialized_repr, 1)\n            llm_string = json.dumps(serialized_repr, sort_keys=True)\n            return llm_string + \"---\" + param_string\n        else:\n            params = self._get_invocation_params(stop=stop, **kwargs)\n            params = {**params, **kwargs}\n            return str(sorted(params.items()))\n\n    def generate(\n        self,\n        messages: list[list[BaseMessage]],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Pass a sequence of prompts to the model and return model generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            messages: List of list of messages.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        structured_output_format = kwargs.pop(\"structured_output_format\", None)\n        if structured_output_format:\n            try:\n                structured_output_format_dict = {\n                    \"structured_output_format\": {\n                        \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                        \"schema\": convert_to_openai_tool(\n                            structured_output_format[\"schema\"]\n                        ),\n                    }\n                }\n            except ValueError:\n                structured_output_format_dict = {}\n        else:\n            structured_output_format_dict = {}\n\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        options = {\"stop\": stop}\n        inheritable_metadata = {\n            **(metadata or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n            **structured_output_format_dict,\n        }\n\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n        run_managers = callback_manager.on_chat_model_start(\n            self._serialized,\n            messages,\n            invocation_params=params,\n            options=options,\n            name=run_name,\n            run_id=run_id,\n            batch_size=len(messages),\n        )\n        results = []\n        for i, m in enumerate(messages):\n            try:\n                results.append(\n                    self._generate_with_cache(\n                        m,\n                        stop=stop,\n                        run_manager=run_managers[i] if run_managers else None,\n                        **kwargs,\n                    )\n                )\n            except BaseException as e:\n                if run_managers:\n                    run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n                raise\n        flattened_outputs = [\n            LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]\n            for res in results\n        ]\n        llm_output = self._combine_llm_outputs([res.llm_output for res in results])\n        generations = [res.generations for res in results]\n        output = LLMResult(generations=generations, llm_output=llm_output)  # type: ignore[arg-type]\n        if run_managers:\n            run_infos = []\n            for manager, flattened_output in zip(run_managers, flattened_outputs):\n                manager.on_llm_end(flattened_output)\n                run_infos.append(RunInfo(run_id=manager.run_id))\n            output.run = run_infos\n        return output\n\n    async def agenerate(\n        self,\n        messages: list[list[BaseMessage]],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Asynchronously pass a sequence of prompts to a model and return generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            messages: List of list of messages.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n        structured_output_format = kwargs.pop(\"structured_output_format\", None)\n        if structured_output_format:\n            try:\n                structured_output_format_dict = {\n                    \"structured_output_format\": {\n                        \"kwargs\": structured_output_format.get(\"kwargs\", {}),\n                        \"schema\": convert_to_openai_tool(\n                            structured_output_format[\"schema\"]\n                        ),\n                    }\n                }\n            except ValueError:\n                structured_output_format_dict = {}\n        else:\n            structured_output_format_dict = {}\n\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        options = {\"stop\": stop}\n        inheritable_metadata = {\n            **(metadata or {}),\n            **self._get_ls_params(stop=stop, **kwargs),\n            **structured_output_format_dict,\n        }\n\n        callback_manager = AsyncCallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose,\n            tags,\n            self.tags,\n            inheritable_metadata,\n            self.metadata,\n        )\n\n        run_managers = await callback_manager.on_chat_model_start(\n            self._serialized,\n            messages,\n            invocation_params=params,\n            options=options,\n            name=run_name,\n            batch_size=len(messages),\n            run_id=run_id,\n        )\n\n        results = await asyncio.gather(\n            *[\n                self._agenerate_with_cache(\n                    m,\n                    stop=stop,\n                    run_manager=run_managers[i] if run_managers else None,\n                    **kwargs,\n                )\n                for i, m in enumerate(messages)\n            ],\n            return_exceptions=True,\n        )\n        exceptions = []\n        for i, res in enumerate(results):\n            if isinstance(res, BaseException):\n                if run_managers:\n                    await run_managers[i].on_llm_error(\n                        res, response=LLMResult(generations=[])\n                    )\n                exceptions.append(res)\n        if exceptions:\n            if run_managers:\n                await asyncio.gather(\n                    *[\n                        run_manager.on_llm_end(\n                            LLMResult(\n                                generations=[res.generations],  # type: ignore[list-item, union-attr]\n                                llm_output=res.llm_output,  # type: ignore[list-item, union-attr]\n                            )\n                        )\n                        for run_manager, res in zip(run_managers, results)\n                        if not isinstance(res, Exception)\n                    ]\n                )\n            raise exceptions[0]\n        flattened_outputs = [\n            LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item, union-attr]\n            for res in results\n        ]\n        llm_output = self._combine_llm_outputs([res.llm_output for res in results])  # type: ignore[union-attr]\n        generations = [res.generations for res in results]  # type: ignore[union-attr]\n        output = LLMResult(generations=generations, llm_output=llm_output)  # type: ignore[arg-type]\n        await asyncio.gather(\n            *[\n                run_manager.on_llm_end(flattened_output)\n                for run_manager, flattened_output in zip(\n                    run_managers, flattened_outputs\n                )\n            ]\n        )\n        if run_managers:\n            output.run = [\n                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers\n            ]\n        return output\n\n    def generate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_messages = [p.to_messages() for p in prompts]\n        return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n\n    async def agenerate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        prompt_messages = [p.to_messages() for p in prompts]\n        return await self.agenerate(\n            prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n        )\n\n    def _generate_with_cache(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        llm_cache = self.cache if isinstance(self.cache, BaseCache) else get_llm_cache()\n        # We should check the cache unless it's explicitly set to False\n        # A None cache means we should use the default global cache\n        # if it's configured.\n        check_cache = self.cache or self.cache is None\n        if check_cache:\n            if llm_cache:\n                llm_string = self._get_llm_string(stop=stop, **kwargs)\n                prompt = dumps(messages)\n                cache_val = llm_cache.lookup(prompt, llm_string)\n                if isinstance(cache_val, list):\n                    return ChatResult(generations=cache_val)\n            elif self.cache is None:\n                pass\n            else:\n                msg = \"Asked to cache, but no cache found at `langchain.cache`.\"\n                raise ValueError(msg)\n\n        # Apply the rate limiter after checking the cache, since\n        # we usually don't want to rate limit cache lookups, but\n        # we do want to rate limit API requests.\n        if self.rate_limiter:\n            self.rate_limiter.acquire(blocking=True)\n\n        # If stream is not explicitly set, check if implicitly requested by\n        # astream_events() or astream_log(). Bail out if _stream not implemented\n        if self._should_stream(\n            async_api=False,\n            run_manager=run_manager,\n            **kwargs,\n        ):\n            chunks: list[ChatGenerationChunk] = []\n            for chunk in self._stream(messages, stop=stop, **kwargs):\n                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                if run_manager:\n                    if chunk.message.id is None:\n                        chunk.message.id = f\"run-{run_manager.run_id}\"\n                    run_manager.on_llm_new_token(\n                        cast(str, chunk.message.content), chunk=chunk\n                    )\n                chunks.append(chunk)\n            result = generate_from_stream(iter(chunks))\n        else:\n            if inspect.signature(self._generate).parameters.get(\"run_manager\"):\n                result = self._generate(\n                    messages, stop=stop, run_manager=run_manager, **kwargs\n                )\n            else:\n                result = self._generate(messages, stop=stop, **kwargs)\n\n        # Add response metadata to each generation\n        for idx, generation in enumerate(result.generations):\n            if run_manager and generation.message.id is None:\n                generation.message.id = f\"run-{run_manager.run_id}-{idx}\"\n            generation.message.response_metadata = _gen_info_and_msg_metadata(\n                generation\n            )\n        if len(result.generations) == 1 and result.llm_output is not None:\n            result.generations[0].message.response_metadata = {\n                **result.llm_output,\n                **result.generations[0].message.response_metadata,\n            }\n        if check_cache and llm_cache:\n            llm_cache.update(prompt, llm_string, result.generations)\n        return result\n\n    async def _agenerate_with_cache(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        llm_cache = self.cache if isinstance(self.cache, BaseCache) else get_llm_cache()\n        # We should check the cache unless it's explicitly set to False\n        # A None cache means we should use the default global cache\n        # if it's configured.\n        check_cache = self.cache or self.cache is None\n        if check_cache:\n            if llm_cache:\n                llm_string = self._get_llm_string(stop=stop, **kwargs)\n                prompt = dumps(messages)\n                cache_val = await llm_cache.alookup(prompt, llm_string)\n                if isinstance(cache_val, list):\n                    return ChatResult(generations=cache_val)\n            elif self.cache is None:\n                pass\n            else:\n                msg = \"Asked to cache, but no cache found at `langchain.cache`.\"\n                raise ValueError(msg)\n\n        # Apply the rate limiter after checking the cache, since\n        # we usually don't want to rate limit cache lookups, but\n        # we do want to rate limit API requests.\n        if self.rate_limiter:\n            await self.rate_limiter.aacquire(blocking=True)\n\n        # If stream is not explicitly set, check if implicitly requested by\n        # astream_events() or astream_log(). Bail out if _astream not implemented\n        if self._should_stream(\n            async_api=True,\n            run_manager=run_manager,\n            **kwargs,\n        ):\n            chunks: list[ChatGenerationChunk] = []\n            async for chunk in self._astream(messages, stop=stop, **kwargs):\n                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)\n                if run_manager:\n                    if chunk.message.id is None:\n                        chunk.message.id = f\"run-{run_manager.run_id}\"\n                    await run_manager.on_llm_new_token(\n                        cast(str, chunk.message.content), chunk=chunk\n                    )\n                chunks.append(chunk)\n            result = generate_from_stream(iter(chunks))\n        else:\n            if inspect.signature(self._agenerate).parameters.get(\"run_manager\"):\n                result = await self._agenerate(\n                    messages, stop=stop, run_manager=run_manager, **kwargs\n                )\n            else:\n                result = await self._agenerate(messages, stop=stop, **kwargs)\n\n        # Add response metadata to each generation\n        for idx, generation in enumerate(result.generations):\n            if run_manager and generation.message.id is None:\n                generation.message.id = f\"run-{run_manager.run_id}-{idx}\"\n            generation.message.response_metadata = _gen_info_and_msg_metadata(\n                generation\n            )\n        if len(result.generations) == 1 and result.llm_output is not None:\n            result.generations[0].message.response_metadata = {\n                **result.llm_output,\n                **result.generations[0].message.response_metadata,\n            }\n        if check_cache and llm_cache:\n            await llm_cache.aupdate(prompt, llm_string, result.generations)\n        return result\n\n    @abstractmethod\n    def _generate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        \"\"\"Top Level call.\"\"\"\n\n    async def _agenerate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        \"\"\"Top Level call.\"\"\"\n        return await run_in_executor(\n            None,\n            self._generate,\n            messages,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n    def _stream(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        raise NotImplementedError\n\n    async def _astream(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        iterator = await run_in_executor(\n            None,\n            self._stream,\n            messages,\n            stop,\n            run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n        done = object()\n        while True:\n            item = await run_in_executor(\n                None,\n                next,\n                iterator,\n                done,  # type: ignore[call-arg, arg-type]\n            )\n            if item is done:\n                break\n            yield item  # type: ignore[misc]\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def __call__(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        generation = self.generate(\n            [messages], stop=stop, callbacks=callbacks, **kwargs\n        ).generations[0][0]\n        if isinstance(generation, ChatGeneration):\n            return generation.message\n        else:\n            msg = \"Unexpected generation type\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    async def _call_async(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        result = await self.agenerate(\n            [messages], stop=stop, callbacks=callbacks, **kwargs\n        )\n        generation = result.generations[0][0]\n        if isinstance(generation, ChatGeneration):\n            return generation.message\n        else:\n            msg = \"Unexpected generation type\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def call_as_llm(\n        self, message: str, stop: Optional[list[str]] = None, **kwargs: Any\n    ) -> str:\n        return self.predict(message, stop=stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        result = self([HumanMessage(content=text)], stop=_stop, **kwargs)\n        if isinstance(result.content, str):\n            return result.content\n        else:\n            msg = \"Cannot use predict when output is not a string.\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    def predict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        _stop = None if stop is None else list(stop)\n        return self(messages, stop=_stop, **kwargs)\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        _stop = None if stop is None else list(stop)\n        result = await self._call_async(\n            [HumanMessage(content=text)], stop=_stop, **kwargs\n        )\n        if isinstance(result.content, str):\n            return result.content\n        else:\n            msg = \"Cannot use predict when output is not a string.\"\n            raise ValueError(msg)  # noqa: TRY004\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    async def apredict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        _stop = None if stop is None else list(stop)\n        return await self._call_async(messages, stop=_stop, **kwargs)\n\n    @property\n    @abstractmethod\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n\n    def dict(self, **kwargs: Any) -> dict:\n        \"\"\"Return a dictionary of the LLM.\"\"\"\n        starter_dict = dict(self._identifying_params)\n        starter_dict[\"_type\"] = self._llm_type\n        return starter_dict\n\n    def bind_tools(\n        self,\n        tools: Sequence[\n            Union[typing.Dict[str, Any], type, Callable, BaseTool]  # noqa: UP006\n        ],\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        raise NotImplementedError\n\n    def with_structured_output(\n        self,\n        schema: Union[typing.Dict, type],  # noqa: UP006\n        *,\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[typing.Dict, BaseModel]]:  # noqa: UP006\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n                    - an OpenAI function/tool schema,\n                    - a JSON Schema,\n                    - a TypedDict class,\n                    - or a Pydantic class.\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: Pydantic schema (include_raw=False):\n            .. code-block:: python\n\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatModel(model=\"model-name\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example: Pydantic schema (include_raw=True):\n            .. code-block:: python\n\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                llm = ChatModel(model=\"model-name\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: Dict schema (include_raw=False):\n            .. code-block:: python\n\n                from pydantic import BaseModel\n                from langchain_core.utils.function_calling import convert_to_openai_tool\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n                    answer: str\n                    justification: str\n\n                dict_schema = convert_to_openai_tool(AnswerWithJustification)\n                llm = ChatModel(model=\"model-name\", temperature=0)\n                structured_llm = llm.with_structured_output(dict_schema)\n\n                structured_llm.invoke(\"What weighs more a pound of bricks or a pound of feathers\")\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. versionchanged:: 0.2.26\n\n                Added support for TypedDict class.\n        \"\"\"  # noqa: E501\n        if kwargs:\n            msg = f\"Received unsupported arguments {kwargs}\"\n            raise ValueError(msg)\n\n        from langchain_core.output_parsers.openai_tools import (\n            JsonOutputKeyToolsParser,\n            PydanticToolsParser,\n        )\n\n        if self.bind_tools is BaseChatModel.bind_tools:\n            msg = \"with_structured_output is not implemented for this model.\"\n            raise NotImplementedError(msg)\n\n        llm = self.bind_tools(\n            [schema],\n            tool_choice=\"any\",\n            structured_output_format={\"kwargs\": {}, \"schema\": schema},\n        )\n        if isinstance(schema, type) and is_basemodel_subclass(schema):\n            output_parser: OutputParserLike = PydanticToolsParser(\n                tools=[cast(TypeBaseModel, schema)], first_tool_only=True\n            )\n        else:\n            key_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n            output_parser = JsonOutputKeyToolsParser(\n                key_name=key_name, first_tool_only=True\n            )\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n\nclass SimpleChatModel(BaseChatModel):\n    \"\"\"Simplified implementation for a chat model to inherit from.\n\n    **Note** This implementation is primarily here for backwards compatibility.\n        For new implementations, please use `BaseChatModel` directly.\n    \"\"\"\n\n    def _generate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        output_str = self._call(messages, stop=stop, run_manager=run_manager, **kwargs)\n        message = AIMessage(content=output_str)\n        generation = ChatGeneration(message=message)\n        return ChatResult(generations=[generation])\n\n    @abstractmethod\n    def _call(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        \"\"\"Simpler interface.\"\"\"\n\n    async def _agenerate(\n        self,\n        messages: list[BaseMessage],\n        stop: Optional[list[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        return await run_in_executor(\n            None,\n            self._generate,\n            messages,\n            stop=stop,\n            run_manager=run_manager.get_sync() if run_manager else None,\n            **kwargs,\n        )\n\n\ndef _gen_info_and_msg_metadata(\n    generation: Union[ChatGeneration, ChatGenerationChunk],\n) -> dict:\n    return {\n        **(generation.generation_info or {}),\n        **generation.message.response_metadata,\n    }\n\n\ndef _cleanup_llm_representation(serialized: Any, depth: int) -> None:\n    \"\"\"Remove non-serializable objects from a serialized object.\"\"\"\n    if depth > 100:  # Don't cooperate for pathological cases\n        return\n\n    if not isinstance(serialized, dict):\n        return\n\n    if (\n        \"type\" in serialized\n        and serialized[\"type\"] == \"not_implemented\"\n        and \"repr\" in serialized\n    ):\n        del serialized[\"repr\"]\n\n    if \"graph\" in serialized:\n        del serialized[\"graph\"]\n\n    if \"kwargs\" in serialized:\n        kwargs = serialized[\"kwargs\"]\n\n        for value in kwargs.values():\n            _cleanup_llm_representation(value, depth + 1)\n",
        "patch": "@@ -208,8 +208,8 @@ class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):\n     disable_streaming: Union[bool, Literal[\"tool_calling\"]] = False\n     \"\"\"Whether to disable streaming for this model.\n \n-    If streaming is bypassed, then ``stream()/astream()`` will defer to\n-    ``invoke()/ainvoke()``.\n+    If streaming is bypassed, then ``stream()``/``astream()``/``astream_events()`` will\n+    defer to ``invoke()``/``ainvoke()``.\n \n     - If True, will always bypass streaming case.\n     - If \"tool_calling\", will bypass streaming case only when the model is called"
      }
    ]
  },
  {
    "number": 29932,
    "title": "openai[patch]: set global ssl context",
    "body": "We set \r\n```python\r\nglobal_ssl_context = ssl.create_default_context(cafile=certifi.where())\r\n```\r\nat the module-level and share it among httpx clients.",
    "issue_title": "openai[patch]: set global ssl context",
    "issue_body": "We set \r\n```python\r\nglobal_ssl_context = ssl.create_default_context(cafile=certifi.where())\r\n```\r\nat the module-level and share it among httpx clients.",
    "files": [
      {
        "filename": "libs/partners/openai/langchain_openai/chat_models/base.py",
        "content_before": "\"\"\"OpenAI chat wrapper.\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport warnings\nfrom io import BytesIO\nfrom math import ceil\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nimport openai\nimport tiktoken\nfrom langchain_core._api.deprecation import deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    ChatMessage,\n    ChatMessageChunk,\n    FunctionMessage,\n    FunctionMessageChunk,\n    HumanMessage,\n    HumanMessageChunk,\n    InvalidToolCall,\n    SystemMessage,\n    SystemMessageChunk,\n    ToolCall,\n    ToolMessage,\n    ToolMessageChunk,\n)\nfrom langchain_core.messages.ai import (\n    InputTokenDetails,\n    OutputTokenDetails,\n    UsageMetadata,\n)\nfrom langchain_core.messages.tool import tool_call_chunk\nfrom langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\nfrom langchain_core.output_parsers.openai_tools import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n    make_invalid_tool_call,\n    parse_tool_call,\n)\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough, chain\nfrom langchain_core.runnables.config import run_in_executor\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import get_pydantic_field_names\nfrom langchain_core.utils.function_calling import (\n    convert_to_openai_function,\n    convert_to_openai_tool,\n)\nfrom langchain_core.utils.pydantic import (\n    PydanticBaseModel,\n    TypeBaseModel,\n    is_basemodel_subclass,\n)\nfrom langchain_core.utils.utils import _build_model_kwargs, from_env, secret_from_env\nfrom pydantic import BaseModel, ConfigDict, Field, SecretStr, model_validator\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\ndef _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n    \"\"\"Convert a dictionary to a LangChain message.\n\n    Args:\n        _dict: The dictionary.\n\n    Returns:\n        The LangChain message.\n    \"\"\"\n    role = _dict.get(\"role\")\n    name = _dict.get(\"name\")\n    id_ = _dict.get(\"id\")\n    if role == \"user\":\n        return HumanMessage(content=_dict.get(\"content\", \"\"), id=id_, name=name)\n    elif role == \"assistant\":\n        # Fix for azure\n        # Also OpenAI returns None for tool invocations\n        content = _dict.get(\"content\", \"\") or \"\"\n        additional_kwargs: Dict = {}\n        if function_call := _dict.get(\"function_call\"):\n            additional_kwargs[\"function_call\"] = dict(function_call)\n        tool_calls = []\n        invalid_tool_calls = []\n        if raw_tool_calls := _dict.get(\"tool_calls\"):\n            additional_kwargs[\"tool_calls\"] = raw_tool_calls\n            for raw_tool_call in raw_tool_calls:\n                try:\n                    tool_calls.append(parse_tool_call(raw_tool_call, return_id=True))\n                except Exception as e:\n                    invalid_tool_calls.append(\n                        make_invalid_tool_call(raw_tool_call, str(e))\n                    )\n        if audio := _dict.get(\"audio\"):\n            additional_kwargs[\"audio\"] = audio\n        return AIMessage(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            name=name,\n            id=id_,\n            tool_calls=tool_calls,\n            invalid_tool_calls=invalid_tool_calls,\n        )\n    elif role in (\"system\", \"developer\"):\n        if role == \"developer\":\n            additional_kwargs = {\"__openai_role__\": role}\n        else:\n            additional_kwargs = {}\n        return SystemMessage(\n            content=_dict.get(\"content\", \"\"),\n            name=name,\n            id=id_,\n            additional_kwargs=additional_kwargs,\n        )\n    elif role == \"function\":\n        return FunctionMessage(\n            content=_dict.get(\"content\", \"\"), name=cast(str, _dict.get(\"name\")), id=id_\n        )\n    elif role == \"tool\":\n        additional_kwargs = {}\n        if \"name\" in _dict:\n            additional_kwargs[\"name\"] = _dict[\"name\"]\n        return ToolMessage(\n            content=_dict.get(\"content\", \"\"),\n            tool_call_id=cast(str, _dict.get(\"tool_call_id\")),\n            additional_kwargs=additional_kwargs,\n            name=name,\n            id=id_,\n        )\n    else:\n        return ChatMessage(content=_dict.get(\"content\", \"\"), role=role, id=id_)  # type: ignore[arg-type]\n\n\ndef _format_message_content(content: Any) -> Any:\n    \"\"\"Format message content.\"\"\"\n    if content and isinstance(content, list):\n        # Remove unexpected block types\n        formatted_content = []\n        for block in content:\n            if (\n                isinstance(block, dict)\n                and \"type\" in block\n                and block[\"type\"] == \"tool_use\"\n            ):\n                continue\n            else:\n                formatted_content.append(block)\n    else:\n        formatted_content = content\n\n    return formatted_content\n\n\ndef _convert_message_to_dict(message: BaseMessage) -> dict:\n    \"\"\"Convert a LangChain message to a dictionary.\n\n    Args:\n        message: The LangChain message.\n\n    Returns:\n        The dictionary.\n    \"\"\"\n    message_dict: Dict[str, Any] = {\"content\": _format_message_content(message.content)}\n    if (name := message.name or message.additional_kwargs.get(\"name\")) is not None:\n        message_dict[\"name\"] = name\n\n    # populate role and additional message data\n    if isinstance(message, ChatMessage):\n        message_dict[\"role\"] = message.role\n    elif isinstance(message, HumanMessage):\n        message_dict[\"role\"] = \"user\"\n    elif isinstance(message, AIMessage):\n        message_dict[\"role\"] = \"assistant\"\n        if \"function_call\" in message.additional_kwargs:\n            message_dict[\"function_call\"] = message.additional_kwargs[\"function_call\"]\n        if message.tool_calls or message.invalid_tool_calls:\n            message_dict[\"tool_calls\"] = [\n                _lc_tool_call_to_openai_tool_call(tc) for tc in message.tool_calls\n            ] + [\n                _lc_invalid_tool_call_to_openai_tool_call(tc)\n                for tc in message.invalid_tool_calls\n            ]\n        elif \"tool_calls\" in message.additional_kwargs:\n            message_dict[\"tool_calls\"] = message.additional_kwargs[\"tool_calls\"]\n            tool_call_supported_props = {\"id\", \"type\", \"function\"}\n            message_dict[\"tool_calls\"] = [\n                {k: v for k, v in tool_call.items() if k in tool_call_supported_props}\n                for tool_call in message_dict[\"tool_calls\"]\n            ]\n        else:\n            pass\n        # If tool calls present, content null value should be None not empty string.\n        if \"function_call\" in message_dict or \"tool_calls\" in message_dict:\n            message_dict[\"content\"] = message_dict[\"content\"] or None\n\n        if \"audio\" in message.additional_kwargs:\n            # openai doesn't support passing the data back - only the id\n            # https://platform.openai.com/docs/guides/audio/multi-turn-conversations\n            raw_audio = message.additional_kwargs[\"audio\"]\n            audio = (\n                {\"id\": message.additional_kwargs[\"audio\"][\"id\"]}\n                if \"id\" in raw_audio\n                else raw_audio\n            )\n            message_dict[\"audio\"] = audio\n    elif isinstance(message, SystemMessage):\n        message_dict[\"role\"] = message.additional_kwargs.get(\n            \"__openai_role__\", \"system\"\n        )\n    elif isinstance(message, FunctionMessage):\n        message_dict[\"role\"] = \"function\"\n    elif isinstance(message, ToolMessage):\n        message_dict[\"role\"] = \"tool\"\n        message_dict[\"tool_call_id\"] = message.tool_call_id\n\n        supported_props = {\"content\", \"role\", \"tool_call_id\"}\n        message_dict = {k: v for k, v in message_dict.items() if k in supported_props}\n    else:\n        raise TypeError(f\"Got unknown type {message}\")\n    return message_dict\n\n\ndef _convert_delta_to_message_chunk(\n    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n) -> BaseMessageChunk:\n    id_ = _dict.get(\"id\")\n    role = cast(str, _dict.get(\"role\"))\n    content = cast(str, _dict.get(\"content\") or \"\")\n    additional_kwargs: Dict = {}\n    if _dict.get(\"function_call\"):\n        function_call = dict(_dict[\"function_call\"])\n        if \"name\" in function_call and function_call[\"name\"] is None:\n            function_call[\"name\"] = \"\"\n        additional_kwargs[\"function_call\"] = function_call\n    tool_call_chunks = []\n    if raw_tool_calls := _dict.get(\"tool_calls\"):\n        additional_kwargs[\"tool_calls\"] = raw_tool_calls\n        try:\n            tool_call_chunks = [\n                tool_call_chunk(\n                    name=rtc[\"function\"].get(\"name\"),\n                    args=rtc[\"function\"].get(\"arguments\"),\n                    id=rtc.get(\"id\"),\n                    index=rtc[\"index\"],\n                )\n                for rtc in raw_tool_calls\n            ]\n        except KeyError:\n            pass\n\n    if role == \"user\" or default_class == HumanMessageChunk:\n        return HumanMessageChunk(content=content, id=id_)\n    elif role == \"assistant\" or default_class == AIMessageChunk:\n        return AIMessageChunk(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            id=id_,\n            tool_call_chunks=tool_call_chunks,  # type: ignore[arg-type]\n        )\n    elif role in (\"system\", \"developer\") or default_class == SystemMessageChunk:\n        if role == \"developer\":\n            additional_kwargs = {\"__openai_role__\": \"developer\"}\n        else:\n            additional_kwargs = {}\n        return SystemMessageChunk(\n            content=content, id=id_, additional_kwargs=additional_kwargs\n        )\n    elif role == \"function\" or default_class == FunctionMessageChunk:\n        return FunctionMessageChunk(content=content, name=_dict[\"name\"], id=id_)\n    elif role == \"tool\" or default_class == ToolMessageChunk:\n        return ToolMessageChunk(\n            content=content, tool_call_id=_dict[\"tool_call_id\"], id=id_\n        )\n    elif role or default_class == ChatMessageChunk:\n        return ChatMessageChunk(content=content, role=role, id=id_)\n    else:\n        return default_class(content=content, id=id_)  # type: ignore\n\n\ndef _update_token_usage(\n    overall_token_usage: Union[int, dict], new_usage: Union[int, dict]\n) -> Union[int, dict]:\n    # Token usage is either ints or dictionaries\n    # `reasoning_tokens` is nested inside `completion_tokens_details`\n    if isinstance(new_usage, int):\n        if not isinstance(overall_token_usage, int):\n            raise ValueError(\n                f\"Got different types for token usage: \"\n                f\"{type(new_usage)} and {type(overall_token_usage)}\"\n            )\n        return new_usage + overall_token_usage\n    elif isinstance(new_usage, dict):\n        if not isinstance(overall_token_usage, dict):\n            raise ValueError(\n                f\"Got different types for token usage: \"\n                f\"{type(new_usage)} and {type(overall_token_usage)}\"\n            )\n        return {\n            k: _update_token_usage(overall_token_usage.get(k, 0), v)\n            for k, v in new_usage.items()\n        }\n    else:\n        warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n        return new_usage\n\n\ndef _handle_openai_bad_request(e: openai.BadRequestError) -> None:\n    if (\n        \"'response_format' of type 'json_schema' is not supported with this model\"\n    ) in e.message:\n        message = (\n            \"This model does not support OpenAI's structured output feature, which \"\n            \"is the default method for `with_structured_output` as of \"\n            \"langchain-openai==0.3. To use `with_structured_output` with this model, \"\n            'specify `method=\"function_calling\"`.'\n        )\n        warnings.warn(message)\n        raise e\n    elif \"Invalid schema for response_format\" in e.message:\n        message = (\n            \"Invalid schema for OpenAI's structured output feature, which is the \"\n            \"default method for `with_structured_output` as of langchain-openai==0.3. \"\n            'Specify `method=\"function_calling\"` instead or update your schema. '\n            \"See supported schemas: \"\n            \"https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\"  # noqa: E501\n        )\n        warnings.warn(message)\n        raise e\n    else:\n        raise\n\n\nclass _FunctionCall(TypedDict):\n    name: str\n\n\n_BM = TypeVar(\"_BM\", bound=BaseModel)\n_DictOrPydanticClass = Union[Dict[str, Any], Type[_BM], Type]\n_DictOrPydantic = Union[Dict, _BM]\n\n\nclass _AllReturnType(TypedDict):\n    raw: BaseMessage\n    parsed: Optional[_DictOrPydantic]\n    parsing_error: Optional[BaseException]\n\n\nclass BaseChatOpenAI(BaseChatModel):\n    client: Any = Field(default=None, exclude=True)  #: :meta private:\n    async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    root_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    root_async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    model_name: str = Field(default=\"gpt-3.5-turbo\", alias=\"model\")\n    \"\"\"Model name to use.\"\"\"\n    temperature: Optional[float] = None\n    \"\"\"What sampling temperature to use.\"\"\"\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n    openai_api_key: Optional[SecretStr] = Field(\n        alias=\"api_key\", default_factory=secret_from_env(\"OPENAI_API_KEY\", default=None)\n    )\n    openai_api_base: Optional[str] = Field(default=None, alias=\"base_url\")\n    \"\"\"Base URL path for API requests, leave blank if not using a proxy or service \n        emulator.\"\"\"\n    openai_organization: Optional[str] = Field(default=None, alias=\"organization\")\n    \"\"\"Automatically inferred from env var `OPENAI_ORG_ID` if not provided.\"\"\"\n    # to support explicit proxy for OpenAI\n    openai_proxy: Optional[str] = Field(\n        default_factory=from_env(\"OPENAI_PROXY\", default=None)\n    )\n    request_timeout: Union[float, Tuple[float, float], Any, None] = Field(\n        default=None, alias=\"timeout\"\n    )\n    \"\"\"Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or \n        None.\"\"\"\n    max_retries: Optional[int] = None\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\n    presence_penalty: Optional[float] = None\n    \"\"\"Penalizes repeated tokens.\"\"\"\n    frequency_penalty: Optional[float] = None\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\n    seed: Optional[int] = None\n    \"\"\"Seed for generation\"\"\"\n    logprobs: Optional[bool] = None\n    \"\"\"Whether to return logprobs.\"\"\"\n    top_logprobs: Optional[int] = None\n    \"\"\"Number of most likely tokens to return at each token position, each with\n     an associated log probability. `logprobs` must be set to true \n     if this parameter is used.\"\"\"\n    logit_bias: Optional[Dict[int, int]] = None\n    \"\"\"Modify the likelihood of specified tokens appearing in the completion.\"\"\"\n    streaming: bool = False\n    \"\"\"Whether to stream the results or not.\"\"\"\n    n: Optional[int] = None\n    \"\"\"Number of chat completions to generate for each prompt.\"\"\"\n    top_p: Optional[float] = None\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n    max_tokens: Optional[int] = Field(default=None)\n    \"\"\"Maximum number of tokens to generate.\"\"\"\n    reasoning_effort: Optional[str] = None\n    \"\"\"Constrains effort on reasoning for reasoning models. \n    \n    Reasoning models only, like OpenAI o1 and o3-mini.\n\n    Currently supported values are low, medium, and high. Reducing reasoning effort \n    can result in faster responses and fewer tokens used on reasoning in a response.\n    \n    .. versionadded:: 0.2.14\n    \"\"\"\n    tiktoken_model_name: Optional[str] = None\n    \"\"\"The model name to pass to tiktoken when using this class. \n    Tiktoken is used to count the number of tokens in documents to constrain \n    them to be under a certain limit. By default, when set to None, this will \n    be the same as the embedding model name. However, there are some cases \n    where you may want to use this Embedding class with a model name not \n    supported by tiktoken. This can include when using Azure embeddings or \n    when using one of the many model providers that expose an OpenAI-like \n    API but with different models. In those cases, in order to avoid erroring \n    when tiktoken is called, you can specify a model name to use here.\"\"\"\n    default_headers: Union[Mapping[str, str], None] = None\n    default_query: Union[Mapping[str, object], None] = None\n    # Configure a custom httpx client. See the\n    # [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n    http_client: Union[Any, None] = Field(default=None, exclude=True)\n    \"\"\"Optional httpx.Client. Only used for sync invocations. Must specify \n        http_async_client as well if you'd like a custom client for async invocations.\n    \"\"\"\n    http_async_client: Union[Any, None] = Field(default=None, exclude=True)\n    \"\"\"Optional httpx.AsyncClient. Only used for async invocations. Must specify \n        http_client as well if you'd like a custom client for sync invocations.\"\"\"\n    stop: Optional[Union[List[str], str]] = Field(default=None, alias=\"stop_sequences\")\n    \"\"\"Default stop sequences.\"\"\"\n    extra_body: Optional[Mapping[str, Any]] = None\n    \"\"\"Optional additional JSON properties to include in the request parameters when\n    making requests to OpenAI compatible APIs, such as vLLM.\"\"\"\n    include_response_headers: bool = False\n    \"\"\"Whether to include response headers in the output message response_metadata.\"\"\"\n    disabled_params: Optional[Dict[str, Any]] = Field(default=None)\n    \"\"\"Parameters of the OpenAI client or chat.completions endpoint that should be \n    disabled for the given model.\n    \n    Should be specified as ``{\"param\": None | ['val1', 'val2']}`` where the key is the \n    parameter and the value is either None, meaning that parameter should never be\n    used, or it's a list of disabled values for the parameter.\n    \n    For example, older models may not support the 'parallel_tool_calls' parameter at \n    all, in which case ``disabled_params={\"parallel_tool_calls\": None}`` can ben passed \n    in.\n    \n    If a parameter is disabled then it will not be used by default in any methods, e.g.\n    in :meth:`~langchain_openai.chat_models.base.ChatOpenAI.with_structured_output`.\n    However this does not prevent a user from directly passed in the parameter during\n    invocation. \n    \"\"\"\n\n    model_config = ConfigDict(populate_by_name=True)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict[str, Any]) -> Any:\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_temperature(cls, values: Dict[str, Any]) -> Any:\n        \"\"\"Currently o1 models only allow temperature=1.\"\"\"\n        model = values.get(\"model_name\") or values.get(\"model\") or \"\"\n        if model.startswith(\"o1\") and \"temperature\" not in values:\n            values[\"temperature\"] = 1\n        return values\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n        if self.n is not None and self.n < 1:\n            raise ValueError(\"n must be at least 1.\")\n        elif self.n is not None and self.n > 1 and self.streaming:\n            raise ValueError(\"n must be 1 when streaming.\")\n\n        # Check OPENAI_ORGANIZATION for backwards compatibility.\n        self.openai_organization = (\n            self.openai_organization\n            or os.getenv(\"OPENAI_ORG_ID\")\n            or os.getenv(\"OPENAI_ORGANIZATION\")\n        )\n        self.openai_api_base = self.openai_api_base or os.getenv(\"OPENAI_API_BASE\")\n        client_params: dict = {\n            \"api_key\": (\n                self.openai_api_key.get_secret_value() if self.openai_api_key else None\n            ),\n            \"organization\": self.openai_organization,\n            \"base_url\": self.openai_api_base,\n            \"timeout\": self.request_timeout,\n            \"default_headers\": self.default_headers,\n            \"default_query\": self.default_query,\n        }\n        if self.max_retries is not None:\n            client_params[\"max_retries\"] = self.max_retries\n\n        if self.openai_proxy and (self.http_client or self.http_async_client):\n            openai_proxy = self.openai_proxy\n            http_client = self.http_client\n            http_async_client = self.http_async_client\n            raise ValueError(\n                \"Cannot specify 'openai_proxy' if one of \"\n                \"'http_client'/'http_async_client' is already specified. Received:\\n\"\n                f\"{openai_proxy=}\\n{http_client=}\\n{http_async_client=}\"\n            )\n        if not self.client:\n            if self.openai_proxy and not self.http_client:\n                try:\n                    import httpx\n                except ImportError as e:\n                    raise ImportError(\n                        \"Could not import httpx python package. \"\n                        \"Please install it with `pip install httpx`.\"\n                    ) from e\n                self.http_client = httpx.Client(proxy=self.openai_proxy)\n            sync_specific = {\"http_client\": self.http_client}\n            self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]\n            self.client = self.root_client.chat.completions\n        if not self.async_client:\n            if self.openai_proxy and not self.http_async_client:\n                try:\n                    import httpx\n                except ImportError as e:\n                    raise ImportError(\n                        \"Could not import httpx python package. \"\n                        \"Please install it with `pip install httpx`.\"\n                    ) from e\n                self.http_async_client = httpx.AsyncClient(proxy=self.openai_proxy)\n            async_specific = {\"http_client\": self.http_async_client}\n            self.root_async_client = openai.AsyncOpenAI(\n                **client_params,\n                **async_specific,  # type: ignore[arg-type]\n            )\n            self.async_client = self.root_async_client.chat.completions\n        return self\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        exclude_if_none = {\n            \"presence_penalty\": self.presence_penalty,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"seed\": self.seed,\n            \"top_p\": self.top_p,\n            \"logprobs\": self.logprobs,\n            \"top_logprobs\": self.top_logprobs,\n            \"logit_bias\": self.logit_bias,\n            \"stop\": self.stop or None,  # also exclude empty list for this\n            \"max_tokens\": self.max_tokens,\n            \"extra_body\": self.extra_body,\n            \"n\": self.n,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n        }\n\n        params = {\n            \"model\": self.model_name,\n            \"stream\": self.streaming,\n            **{k: v for k, v in exclude_if_none.items() if v is not None},\n            **self.model_kwargs,\n        }\n\n        return params\n\n    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:\n        overall_token_usage: dict = {}\n        system_fingerprint = None\n        for output in llm_outputs:\n            if output is None:\n                # Happens in streaming\n                continue\n            token_usage = output[\"token_usage\"]\n            if token_usage is not None:\n                for k, v in token_usage.items():\n                    if v is None:\n                        continue\n                    if k in overall_token_usage:\n                        overall_token_usage[k] = _update_token_usage(\n                            overall_token_usage[k], v\n                        )\n                    else:\n                        overall_token_usage[k] = v\n            if system_fingerprint is None:\n                system_fingerprint = output.get(\"system_fingerprint\")\n        combined = {\"token_usage\": overall_token_usage, \"model_name\": self.model_name}\n        if system_fingerprint:\n            combined[\"system_fingerprint\"] = system_fingerprint\n        return combined\n\n    def _convert_chunk_to_generation_chunk(\n        self,\n        chunk: dict,\n        default_chunk_class: Type,\n        base_generation_info: Optional[Dict],\n    ) -> Optional[ChatGenerationChunk]:\n        if chunk.get(\"type\") == \"content.delta\":  # from beta.chat.completions.stream\n            return None\n        token_usage = chunk.get(\"usage\")\n        choices = (\n            chunk.get(\"choices\", [])\n            # from beta.chat.completions.stream\n            or chunk.get(\"chunk\", {}).get(\"choices\", [])\n        )\n\n        usage_metadata: Optional[UsageMetadata] = (\n            _create_usage_metadata(token_usage) if token_usage else None\n        )\n        if len(choices) == 0:\n            # logprobs is implicitly None\n            generation_chunk = ChatGenerationChunk(\n                message=default_chunk_class(content=\"\", usage_metadata=usage_metadata)\n            )\n            return generation_chunk\n\n        choice = choices[0]\n        if choice[\"delta\"] is None:\n            return None\n\n        message_chunk = _convert_delta_to_message_chunk(\n            choice[\"delta\"], default_chunk_class\n        )\n        generation_info = {**base_generation_info} if base_generation_info else {}\n\n        if finish_reason := choice.get(\"finish_reason\"):\n            generation_info[\"finish_reason\"] = finish_reason\n            if model_name := chunk.get(\"model\"):\n                generation_info[\"model_name\"] = model_name\n            if system_fingerprint := chunk.get(\"system_fingerprint\"):\n                generation_info[\"system_fingerprint\"] = system_fingerprint\n\n        logprobs = choice.get(\"logprobs\")\n        if logprobs:\n            generation_info[\"logprobs\"] = logprobs\n\n        if usage_metadata and isinstance(message_chunk, AIMessageChunk):\n            message_chunk.usage_metadata = usage_metadata\n\n        generation_chunk = ChatGenerationChunk(\n            message=message_chunk, generation_info=generation_info or None\n        )\n        return generation_chunk\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        base_generation_info = {}\n\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            response_stream = self.root_client.beta.chat.completions.stream(**payload)\n            context_manager = response_stream\n        else:\n            if self.include_response_headers:\n                raw_response = self.client.with_raw_response.create(**payload)\n                response = raw_response.parse()\n                base_generation_info = {\"headers\": dict(raw_response.headers)}\n            else:\n                response = self.client.create(**payload)\n            context_manager = response\n        try:\n            with context_manager as response:\n                is_first_chunk = True\n                for chunk in response:\n                    if not isinstance(chunk, dict):\n                        chunk = chunk.model_dump()\n                    generation_chunk = self._convert_chunk_to_generation_chunk(\n                        chunk,\n                        default_chunk_class,\n                        base_generation_info if is_first_chunk else {},\n                    )\n                    if generation_chunk is None:\n                        continue\n                    default_chunk_class = generation_chunk.message.__class__\n                    logprobs = (generation_chunk.generation_info or {}).get(\"logprobs\")\n                    if run_manager:\n                        run_manager.on_llm_new_token(\n                            generation_chunk.text,\n                            chunk=generation_chunk,\n                            logprobs=logprobs,\n                        )\n                    is_first_chunk = False\n                    yield generation_chunk\n        except openai.BadRequestError as e:\n            _handle_openai_bad_request(e)\n        if hasattr(response, \"get_final_completion\") and \"response_format\" in payload:\n            final_completion = response.get_final_completion()\n            generation_chunk = self._get_generation_chunk_from_completion(\n                final_completion\n            )\n            if run_manager:\n                run_manager.on_llm_new_token(\n                    generation_chunk.text, chunk=generation_chunk\n                )\n            yield generation_chunk\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        generation_info = None\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            try:\n                response = self.root_client.beta.chat.completions.parse(**payload)\n            except openai.BadRequestError as e:\n                _handle_openai_bad_request(e)\n        elif self.include_response_headers:\n            raw_response = self.client.with_raw_response.create(**payload)\n            response = raw_response.parse()\n            generation_info = {\"headers\": dict(raw_response.headers)}\n        else:\n            response = self.client.create(**payload)\n        return self._create_chat_result(response, generation_info)\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        messages = self._convert_input(input_).to_messages()\n        if stop is not None:\n            kwargs[\"stop\"] = stop\n\n        return {\n            \"messages\": [_convert_message_to_dict(m) for m in messages],\n            **self._default_params,\n            **kwargs,\n        }\n\n    def _create_chat_result(\n        self,\n        response: Union[dict, openai.BaseModel],\n        generation_info: Optional[Dict] = None,\n    ) -> ChatResult:\n        generations = []\n\n        response_dict = (\n            response if isinstance(response, dict) else response.model_dump()\n        )\n        # Sometimes the AI Model calling will get error, we should raise it.\n        # Otherwise, the next code 'choices.extend(response[\"choices\"])'\n        # will throw a \"TypeError: 'NoneType' object is not iterable\" error\n        # to mask the true error. Because 'response[\"choices\"]' is None.\n        if response_dict.get(\"error\"):\n            raise ValueError(response_dict.get(\"error\"))\n\n        token_usage = response_dict.get(\"usage\")\n        for res in response_dict[\"choices\"]:\n            message = _convert_dict_to_message(res[\"message\"])\n            if token_usage and isinstance(message, AIMessage):\n                message.usage_metadata = _create_usage_metadata(token_usage)\n            generation_info = generation_info or {}\n            generation_info[\"finish_reason\"] = (\n                res.get(\"finish_reason\")\n                if res.get(\"finish_reason\") is not None\n                else generation_info.get(\"finish_reason\")\n            )\n            if \"logprobs\" in res:\n                generation_info[\"logprobs\"] = res[\"logprobs\"]\n            gen = ChatGeneration(message=message, generation_info=generation_info)\n            generations.append(gen)\n        llm_output = {\n            \"token_usage\": token_usage,\n            \"model_name\": response_dict.get(\"model\", self.model_name),\n            \"system_fingerprint\": response_dict.get(\"system_fingerprint\", \"\"),\n        }\n\n        if isinstance(response, openai.BaseModel) and getattr(\n            response, \"choices\", None\n        ):\n            message = response.choices[0].message  # type: ignore[attr-defined]\n            if hasattr(message, \"parsed\"):\n                generations[0].message.additional_kwargs[\"parsed\"] = message.parsed\n            if hasattr(message, \"refusal\"):\n                generations[0].message.additional_kwargs[\"refusal\"] = message.refusal\n\n        return ChatResult(generations=generations, llm_output=llm_output)\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        base_generation_info = {}\n\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            response_stream = self.root_async_client.beta.chat.completions.stream(\n                **payload\n            )\n            context_manager = response_stream\n        else:\n            if self.include_response_headers:\n                raw_response = await self.async_client.with_raw_response.create(\n                    **payload\n                )\n                response = raw_response.parse()\n                base_generation_info = {\"headers\": dict(raw_response.headers)}\n            else:\n                response = await self.async_client.create(**payload)\n            context_manager = response\n        try:\n            async with context_manager as response:\n                is_first_chunk = True\n                async for chunk in response:\n                    if not isinstance(chunk, dict):\n                        chunk = chunk.model_dump()\n                    generation_chunk = self._convert_chunk_to_generation_chunk(\n                        chunk,\n                        default_chunk_class,\n                        base_generation_info if is_first_chunk else {},\n                    )\n                    if generation_chunk is None:\n                        continue\n                    default_chunk_class = generation_chunk.message.__class__\n                    logprobs = (generation_chunk.generation_info or {}).get(\"logprobs\")\n                    if run_manager:\n                        await run_manager.on_llm_new_token(\n                            generation_chunk.text,\n                            chunk=generation_chunk,\n                            logprobs=logprobs,\n                        )\n                    is_first_chunk = False\n                    yield generation_chunk\n        except openai.BadRequestError as e:\n            _handle_openai_bad_request(e)\n        if hasattr(response, \"get_final_completion\") and \"response_format\" in payload:\n            final_completion = await response.get_final_completion()\n            generation_chunk = self._get_generation_chunk_from_completion(\n                final_completion\n            )\n            if run_manager:\n                await run_manager.on_llm_new_token(\n                    generation_chunk.text, chunk=generation_chunk\n                )\n            yield generation_chunk\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        generation_info = None\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            try:\n                response = await self.root_async_client.beta.chat.completions.parse(\n                    **payload\n                )\n            except openai.BadRequestError as e:\n                _handle_openai_bad_request(e)\n        elif self.include_response_headers:\n            raw_response = await self.async_client.with_raw_response.create(**payload)\n            response = raw_response.parse()\n            generation_info = {\"headers\": dict(raw_response.headers)}\n        else:\n            response = await self.async_client.create(**payload)\n        return await run_in_executor(\n            None, self._create_chat_result, response, generation_info\n        )\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model_name\": self.model_name, **self._default_params}\n\n    def _get_invocation_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> Dict[str, Any]:\n        \"\"\"Get the parameters used to invoke the model.\"\"\"\n        return {\n            \"model\": self.model_name,\n            **super()._get_invocation_params(stop=stop),\n            **self._default_params,\n            **kwargs,\n        }\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"openai\",\n            ls_model_name=self.model_name,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens) or params.get(\n            \"max_completion_tokens\", self.max_tokens\n        ):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"openai-chat\"\n\n    def _get_encoding_model(self) -> Tuple[str, tiktoken.Encoding]:\n        if self.tiktoken_model_name is not None:\n            model = self.tiktoken_model_name\n        else:\n            model = self.model_name\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n        except KeyError:\n            model = \"cl100k_base\"\n            encoding = tiktoken.get_encoding(model)\n        return model, encoding\n\n    def get_token_ids(self, text: str) -> List[int]:\n        \"\"\"Get the tokens present in the text with tiktoken package.\"\"\"\n        if self.custom_get_token_ids is not None:\n            return self.custom_get_token_ids(text)\n        # tiktoken NOT supported for Python 3.7 or below\n        if sys.version_info[1] <= 7:\n            return super().get_token_ids(text)\n        _, encoding_model = self._get_encoding_model()\n        return encoding_model.encode(text)\n\n    def get_num_tokens_from_messages(\n        self,\n        messages: List[BaseMessage],\n        tools: Optional[\n            Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]\n        ] = None,\n    ) -> int:\n        \"\"\"Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n\n        **Requirements**: You must have the ``pillow`` installed if you want to count\n        image tokens if you are specifying the image as a base64 string, and you must\n        have both ``pillow`` and ``httpx`` installed if you are specifying the image\n        as a URL. If these aren't installed image inputs will be ignored in token\n        counting.\n\n        OpenAI reference: https://github.com/openai/openai-cookbook/blob/\n        main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n        \"\"\"\n        # TODO: Count bound tools as part of input.\n        if tools is not None:\n            warnings.warn(\n                \"Counting tokens in tool schemas is not yet supported. Ignoring tools.\"\n            )\n        if sys.version_info[1] <= 7:\n            return super().get_num_tokens_from_messages(messages)\n        model, encoding = self._get_encoding_model()\n        if model.startswith(\"gpt-3.5-turbo-0301\"):\n            # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            tokens_per_message = 4\n            # if there's a name, the role is omitted\n            tokens_per_name = -1\n        elif model.startswith(\"gpt-3.5-turbo\") or model.startswith(\"gpt-4\"):\n            tokens_per_message = 3\n            tokens_per_name = 1\n        else:\n            raise NotImplementedError(\n                f\"get_num_tokens_from_messages() is not presently implemented \"\n                f\"for model {model}. See \"\n                \"https://platform.openai.com/docs/guides/text-generation/managing-tokens\"  # noqa: E501\n                \" for information on how messages are converted to tokens.\"\n            )\n        num_tokens = 0\n        messages_dict = [_convert_message_to_dict(m) for m in messages]\n        for message in messages_dict:\n            num_tokens += tokens_per_message\n            for key, value in message.items():\n                # This is an inferred approximation. OpenAI does not document how to\n                # count tool message tokens.\n                if key == \"tool_call_id\":\n                    num_tokens += 3\n                    continue\n                if isinstance(value, list):\n                    # content or tool calls\n                    for val in value:\n                        if isinstance(val, str) or val[\"type\"] == \"text\":\n                            text = val[\"text\"] if isinstance(val, dict) else val\n                            num_tokens += len(encoding.encode(text))\n                        elif val[\"type\"] == \"image_url\":\n                            if val[\"image_url\"].get(\"detail\") == \"low\":\n                                num_tokens += 85\n                            else:\n                                image_size = _url_to_size(val[\"image_url\"][\"url\"])\n                                if not image_size:\n                                    continue\n                                num_tokens += _count_image_tokens(*image_size)\n                        # Tool/function call token counting is not documented by OpenAI.\n                        # This is an approximation.\n                        elif val[\"type\"] == \"function\":\n                            num_tokens += len(\n                                encoding.encode(val[\"function\"][\"arguments\"])\n                            )\n                            num_tokens += len(encoding.encode(val[\"function\"][\"name\"]))\n                        else:\n                            raise ValueError(\n                                f\"Unrecognized content block type\\n\\n{val}\"\n                            )\n                elif not value:\n                    continue\n                else:\n                    # Cast str(value) in case the message value is not a string\n                    # This occurs with function messages\n                    num_tokens += len(encoding.encode(str(value)))\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        # every reply is primed with <im_start>assistant\n        num_tokens += 3\n        return num_tokens\n\n    @deprecated(\n        since=\"0.2.1\",\n        alternative=\"langchain_openai.chat_models.base.ChatOpenAI.bind_tools\",\n        removal=\"1.0.0\",\n    )\n    def bind_functions(\n        self,\n        functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n        function_call: Optional[\n            Union[_FunctionCall, str, Literal[\"auto\", \"none\"]]\n        ] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind functions (and other objects) to this chat model.\n\n        Assumes model is compatible with OpenAI function-calling API.\n\n        NOTE: Using bind_tools is recommended instead, as the `functions` and\n            `function_call` request parameters are officially marked as deprecated by\n            OpenAI.\n\n        Args:\n            functions: A list of function definitions to bind to this chat model.\n                Can be  a dictionary, pydantic model, or callable. Pydantic\n                models and callables will be automatically converted to\n                their schema dictionary representation.\n            function_call: Which function to require the model to call.\n                Must be the name of the single provided function or\n                \"auto\" to automatically determine which function to call\n                (if any).\n            **kwargs: Any additional parameters to pass to the\n                :class:`~langchain.runnable.Runnable` constructor.\n        \"\"\"\n\n        formatted_functions = [convert_to_openai_function(fn) for fn in functions]\n        if function_call is not None:\n            function_call = (\n                {\"name\": function_call}\n                if isinstance(function_call, str)\n                and function_call not in (\"auto\", \"none\")\n                else function_call\n            )\n            if isinstance(function_call, dict) and len(formatted_functions) != 1:\n                raise ValueError(\n                    \"When specifying `function_call`, you must provide exactly one \"\n                    \"function.\"\n                )\n            if (\n                isinstance(function_call, dict)\n                and formatted_functions[0][\"name\"] != function_call[\"name\"]\n            ):\n                raise ValueError(\n                    f\"Function call {function_call} was specified, but the only \"\n                    f\"provided function was {formatted_functions[0]['name']}.\"\n                )\n            kwargs = {**kwargs, \"function_call\": function_call}\n        return super().bind(functions=formatted_functions, **kwargs)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[\n            Union[dict, str, Literal[\"auto\", \"none\", \"required\", \"any\"], bool]\n        ] = None,\n        strict: Optional[bool] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        Assumes model is compatible with OpenAI tool-calling API.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports any tool definition handled by\n                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call. Options are:\n\n                - str of the form ``\"<<tool_name>>\"``: calls <<tool_name>> tool.\n                - ``\"auto\"``: automatically selects a tool (including no tool).\n                - ``\"none\"``: does not call a tool.\n                - ``\"any\"`` or ``\"required\"`` or ``True``: force at least one tool to be called.\n                - dict of the form ``{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}``: calls <<tool_name>> tool.\n                - ``False`` or ``None``: no effect, default OpenAI behavior.\n            strict: If True, model output is guaranteed to exactly match the JSON Schema\n                provided in the tool definition. If True, the input schema will be\n                validated according to\n                https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n                If False, input schema will not be validated and model output will not\n                be validated.\n                If None, ``strict`` argument will not be passed to the model.\n            parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n                Defaults to ``None`` (no specification, which allows parallel tool use).\n            kwargs: Any additional parameters are passed directly to\n                :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind`.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n\n        \"\"\"  # noqa: E501\n\n        if parallel_tool_calls is not None:\n            kwargs[\"parallel_tool_calls\"] = parallel_tool_calls\n        formatted_tools = [\n            convert_to_openai_tool(tool, strict=strict) for tool in tools\n        ]\n        if tool_choice:\n            if isinstance(tool_choice, str):\n                # tool_choice is a tool/function name\n                if tool_choice not in (\"auto\", \"none\", \"any\", \"required\"):\n                    tool_choice = {\n                        \"type\": \"function\",\n                        \"function\": {\"name\": tool_choice},\n                    }\n                # 'any' is not natively supported by OpenAI API.\n                # We support 'any' since other models use this instead of 'required'.\n                if tool_choice == \"any\":\n                    tool_choice = \"required\"\n            elif isinstance(tool_choice, bool):\n                tool_choice = \"required\"\n            elif isinstance(tool_choice, dict):\n                tool_names = [\n                    formatted_tool[\"function\"][\"name\"]\n                    for formatted_tool in formatted_tools\n                ]\n                if not any(\n                    tool_name == tool_choice[\"function\"][\"name\"]\n                    for tool_name in tool_names\n                ):\n                    raise ValueError(\n                        f\"Tool choice {tool_choice} was specified, but the only \"\n                        f\"provided tools were {tool_names}.\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n                    f\"Received: {tool_choice}\"\n                )\n            kwargs[\"tool_choice\"] = tool_choice\n        return super().bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Optional[_DictOrPydanticClass] = None,\n        *,\n        method: Literal[\n            \"function_calling\", \"json_mode\", \"json_schema\"\n        ] = \"function_calling\",\n        include_raw: bool = False,\n        strict: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, _DictOrPydantic]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n\n                - an OpenAI function/tool schema,\n                - a JSON Schema,\n                - a TypedDict class (support added in 0.1.20),\n                - or a Pydantic class.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"function_calling\":\n                    Uses OpenAI's tool-calling (formerly called function calling)\n                    API: https://platform.openai.com/docs/guides/function-calling\n                - \"json_schema\":\n                    Uses OpenAI's Structured Output API: https://platform.openai.com/docs/guides/structured-outputs\n                    Supported for \"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"o1\", and later\n                    models.\n                - \"json_mode\":\n                    Uses OpenAI's JSON mode. Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call:\n                    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\n                Learn more about the differences between the methods and which models\n                support which methods here:\n\n                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode\n                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            strict:\n\n                - True:\n                    Model output is guaranteed to exactly match the schema.\n                    The input schema will also be validated according to\n                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n                - False:\n                    Input schema will not be validated and model output will not be\n                    validated.\n                - None:\n                    ``strict`` argument will not be passed to the model.\n\n            kwargs: Additional keyword args aren't supported.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n\n            - \"raw\": BaseMessage\n            - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n            - \"parsing_error\": Optional[BaseException]\n\n        .. versionchanged:: 0.1.20\n\n            Added support for TypedDict class ``schema``.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n            Support for ``method`` = \"json_schema\" added.\n        \"\"\"  # noqa: E501\n        if kwargs:\n            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n        if strict is not None and method == \"json_mode\":\n            raise ValueError(\n                \"Argument `strict` is not supported with `method`='json_mode'\"\n            )\n        is_pydantic_schema = _is_pydantic_class(schema)\n\n        if method == \"json_schema\":\n            # Check for Pydantic BaseModel V1\n            if (\n                is_pydantic_schema and issubclass(schema, BaseModelV1)  # type: ignore[arg-type]\n            ):\n                warnings.warn(\n                    \"Received a Pydantic BaseModel V1 schema. This is not supported by \"\n                    'method=\"json_schema\". Please use method=\"function_calling\" '\n                    \"or specify schema via JSON Schema or Pydantic V2 BaseModel. \"\n                    'Overriding to method=\"function_calling\".'\n                )\n                method = \"function_calling\"\n            # Check for incompatible model\n            if self.model_name and (\n                self.model_name.startswith(\"gpt-3\")\n                or self.model_name.startswith(\"gpt-4-\")\n                or self.model_name == \"gpt-4\"\n            ):\n                warnings.warn(\n                    f\"Cannot use method='json_schema' with model {self.model_name} \"\n                    f\"since it doesn't support OpenAI's Structured Output API. You can \"\n                    f\"see supported models here: \"\n                    f\"https://platform.openai.com/docs/guides/structured-outputs#supported-models. \"  # noqa: E501\n                    \"To fix this warning, set `method='function_calling'. \"\n                    \"Overriding to method='function_calling'.\"\n                )\n                method = \"function_calling\"\n\n        if method == \"function_calling\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is not 'json_mode'. \"\n                    \"Received None.\"\n                )\n            tool_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n            bind_kwargs = self._filter_disabled_params(\n                tool_choice=tool_name,\n                parallel_tool_calls=False,\n                strict=strict,\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": schema,\n                },\n            )\n\n            llm = self.bind_tools([schema], **bind_kwargs)\n            if is_pydantic_schema:\n                output_parser: Runnable = PydanticToolsParser(\n                    tools=[schema],  # type: ignore[list-item]\n                    first_tool_only=True,  # type: ignore[list-item]\n                )\n            else:\n                output_parser = JsonOutputKeyToolsParser(\n                    key_name=tool_name, first_tool_only=True\n                )\n        elif method == \"json_mode\":\n            llm = self.bind(\n                response_format={\"type\": \"json_object\"},\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": schema,\n                },\n            )\n            output_parser = (\n                PydanticOutputParser(pydantic_object=schema)  # type: ignore[arg-type]\n                if is_pydantic_schema\n                else JsonOutputParser()\n            )\n        elif method == \"json_schema\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is not 'json_mode'. \"\n                    \"Received None.\"\n                )\n            response_format = _convert_to_openai_response_format(schema, strict=strict)\n            llm = self.bind(\n                response_format=response_format,\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": convert_to_openai_tool(schema),\n                },\n            )\n            if is_pydantic_schema:\n                output_parser = _oai_structured_outputs_parser.with_types(\n                    output_type=cast(type, schema)\n                )\n            else:\n                output_parser = JsonOutputParser()\n        else:\n            raise ValueError(\n                f\"Unrecognized method argument. Expected one of 'function_calling' or \"\n                f\"'json_mode'. Received: '{method}'\"\n            )\n\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    def _filter_disabled_params(self, **kwargs: Any) -> Dict[str, Any]:\n        if not self.disabled_params:\n            return kwargs\n        filtered = {}\n        for k, v in kwargs.items():\n            # Skip param\n            if k in self.disabled_params and (\n                self.disabled_params[k] is None or v in self.disabled_params[k]\n            ):\n                continue\n            # Keep param\n            else:\n                filtered[k] = v\n        return filtered\n\n    def _get_generation_chunk_from_completion(\n        self, completion: openai.BaseModel\n    ) -> ChatGenerationChunk:\n        \"\"\"Get chunk from completion (e.g., from final completion of a stream).\"\"\"\n        chat_result = self._create_chat_result(completion)\n        chat_message = chat_result.generations[0].message\n        if isinstance(chat_message, AIMessage):\n            usage_metadata = chat_message.usage_metadata\n            # Skip tool_calls, already sent as chunks\n            if \"tool_calls\" in chat_message.additional_kwargs:\n                chat_message.additional_kwargs.pop(\"tool_calls\")\n        else:\n            usage_metadata = None\n        message = AIMessageChunk(\n            content=\"\",\n            additional_kwargs=chat_message.additional_kwargs,\n            usage_metadata=usage_metadata,\n        )\n        return ChatGenerationChunk(\n            message=message, generation_info=chat_result.llm_output\n        )\n\n\nclass ChatOpenAI(BaseChatOpenAI):  # type: ignore[override]\n    \"\"\"OpenAI chat model integration.\n\n    .. dropdown:: Setup\n        :open:\n\n        Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-openai\n            export OPENAI_API_KEY=\"your-api-key\"\n\n    .. dropdown:: Key init args \u2014 completion params\n\n        model: str\n            Name of OpenAI model to use.\n        temperature: float\n            Sampling temperature.\n        max_tokens: Optional[int]\n            Max number of tokens to generate.\n        logprobs: Optional[bool]\n            Whether to return logprobs.\n        stream_options: Dict\n            Configure streaming outputs, like whether to return token usage when\n            streaming (``{\"include_usage\": True}``).\n\n        See full list of supported init args and their descriptions in the params section.\n\n    .. dropdown:: Key init args \u2014 client params\n\n        timeout: Union[float, Tuple[float, float], Any, None]\n            Timeout for requests.\n        max_retries: Optional[int]\n            Max number of retries.\n        api_key: Optional[str]\n            OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.\n        base_url: Optional[str]\n            Base URL for API requests. Only specify if using a proxy or service\n            emulator.\n        organization: Optional[str]\n            OpenAI organization ID. If not passed in will be read from env\n            var OPENAI_ORG_ID.\n\n        See full list of supported init args and their descriptions in the params section.\n\n    .. dropdown:: Instantiate\n\n        .. code-block:: python\n\n            from langchain_openai import ChatOpenAI\n\n            llm = ChatOpenAI(\n                model=\"gpt-4o\",\n                temperature=0,\n                max_tokens=None,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # base_url=\"...\",\n                # organization=\"...\",\n                # other params...\n            )\n\n        **NOTE**: Any param which is not explicitly supported will be passed directly to the\n        ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is\n        invoked. For example:\n\n        .. code-block:: python\n\n            from langchain_openai import ChatOpenAI\n            import openai\n\n            ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\n\n            # results in underlying API call of:\n\n            openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\n\n            # which is also equivalent to:\n\n            ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\n\n    .. dropdown:: Invoke\n\n        .. code-block:: python\n\n            messages = [\n                (\n                    \"system\",\n                    \"You are a helpful translator. Translate the user sentence to French.\",\n                ),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n        .. code-block:: pycon\n\n            AIMessage(\n                content=\"J'adore la programmation.\",\n                response_metadata={\n                    \"token_usage\": {\n                        \"completion_tokens\": 5,\n                        \"prompt_tokens\": 31,\n                        \"total_tokens\": 36,\n                    },\n                    \"model_name\": \"gpt-4o\",\n                    \"system_fingerprint\": \"fp_43dfabdef1\",\n                    \"finish_reason\": \"stop\",\n                    \"logprobs\": None,\n                },\n                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n            )\n\n    .. dropdown:: Stream\n\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(\n                content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n            )\n            AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(\n                content=\"\",\n                response_metadata={\"finish_reason\": \"stop\"},\n                id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\n            )\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n        .. code-block:: python\n\n            AIMessageChunk(\n                content=\"J'adore la programmation.\",\n                response_metadata={\"finish_reason\": \"stop\"},\n                id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\n            )\n\n    .. dropdown:: Async\n\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n        .. code-block:: python\n\n            AIMessage(\n                content=\"J'adore la programmation.\",\n                response_metadata={\n                    \"token_usage\": {\n                        \"completion_tokens\": 5,\n                        \"prompt_tokens\": 31,\n                        \"total_tokens\": 36,\n                    },\n                    \"model_name\": \"gpt-4o\",\n                    \"system_fingerprint\": \"fp_43dfabdef1\",\n                    \"finish_reason\": \"stop\",\n                    \"logprobs\": None,\n                },\n                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n            )\n\n    .. dropdown:: Tool calling\n\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(\n                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n                )\n\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(\n                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n                )\n\n\n            llm_with_tools = llm.bind_tools(\n                [GetWeather, GetPopulation]\n                # strict = True  # enforce tool args schema is respected\n            )\n            ai_msg = llm_with_tools.invoke(\n                \"Which city is hotter today and which is bigger: LA or NY?\"\n            )\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\n                },\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"New York, NY\"},\n                    \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\n                },\n                {\n                    \"name\": \"GetPopulation\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\n                },\n                {\n                    \"name\": \"GetPopulation\",\n                    \"args\": {\"location\": \"New York, NY\"},\n                    \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\n                },\n            ]\n\n        Note that ``openai >= 1.32`` supports a ``parallel_tool_calls`` parameter\n        that defaults to ``True``. This parameter can be set to ``False`` to\n        disable parallel tool calls:\n\n        .. code-block:: python\n\n            ai_msg = llm_with_tools.invoke(\n                \"What is the weather in LA and NY?\", parallel_tool_calls=False\n            )\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\n                }\n            ]\n\n        Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model\n        using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by\n        setting ``model_kwargs``.\n\n        See ``ChatOpenAI.bind_tools()`` method for more.\n\n    .. dropdown:: Structured output\n\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        .. code-block:: python\n\n            Joke(\n                setup=\"Why was the cat sitting on the computer?\",\n                punchline=\"To keep an eye on the mouse!\",\n                rating=None,\n            )\n\n        See ``ChatOpenAI.with_structured_output()`` for more.\n\n    .. dropdown:: JSON mode\n\n        .. code-block:: python\n\n            json_llm = llm.bind(response_format={\"type\": \"json_object\"})\n            ai_msg = json_llm.invoke(\n                \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n            )\n            ai_msg.content\n\n        .. code-block:: python\n\n            '\\\\n{\\\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\\\n}'\n\n    .. dropdown:: Image input\n\n        .. code-block:: python\n\n            import base64\n            import httpx\n            from langchain_core.messages import HumanMessage\n\n            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n            message = HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                    },\n                ]\n            )\n            ai_msg = llm.invoke([message])\n            ai_msg.content\n\n        .. code-block:: python\n\n            \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\n\n    .. dropdown:: Token usage\n\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n\n        When streaming, set the ``stream_usage`` kwarg:\n\n        .. code-block:: python\n\n            stream = llm.stream(messages, stream_usage=True)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full.usage_metadata\n\n        .. code-block:: python\n\n            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n\n        Alternatively, setting ``stream_usage`` when instantiating the model can be\n        useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using\n        methods like ``.with_structured_output``, which generate chains under the\n        hood.\n\n        .. code-block:: python\n\n            llm = ChatOpenAI(model=\"gpt-4o\", stream_usage=True)\n            structured_llm = llm.with_structured_output(...)\n\n    .. dropdown:: Logprobs\n\n        .. code-block:: python\n\n            logprobs_llm = llm.bind(logprobs=True)\n            ai_msg = logprobs_llm.invoke(messages)\n            ai_msg.response_metadata[\"logprobs\"]\n\n        .. code-block:: python\n\n            {\n                \"content\": [\n                    {\n                        \"token\": \"J\",\n                        \"bytes\": [74],\n                        \"logprob\": -4.9617593e-06,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \"'adore\",\n                        \"bytes\": [39, 97, 100, 111, 114, 101],\n                        \"logprob\": -0.25202933,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \" la\",\n                        \"bytes\": [32, 108, 97],\n                        \"logprob\": -0.20141791,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \" programmation\",\n                        \"bytes\": [\n                            32,\n                            112,\n                            114,\n                            111,\n                            103,\n                            114,\n                            97,\n                            109,\n                            109,\n                            97,\n                            116,\n                            105,\n                            111,\n                            110,\n                        ],\n                        \"logprob\": -1.9361265e-07,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \".\",\n                        \"bytes\": [46],\n                        \"logprob\": -1.2233183e-05,\n                        \"top_logprobs\": [],\n                    },\n                ]\n            }\n\n    .. dropdown:: Response metadata\n\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n        .. code-block:: python\n\n            {\n                \"token_usage\": {\n                    \"completion_tokens\": 5,\n                    \"prompt_tokens\": 28,\n                    \"total_tokens\": 33,\n                },\n                \"model_name\": \"gpt-4o\",\n                \"system_fingerprint\": \"fp_319be4768e\",\n                \"finish_reason\": \"stop\",\n                \"logprobs\": None,\n            }\n\n    \"\"\"  # noqa: E501\n\n    stream_usage: bool = False\n    \"\"\"Whether to include usage metadata in streaming output. If True, additional\n    message chunks will be generated during the stream including usage metadata.\n    \"\"\"\n\n    max_tokens: Optional[int] = Field(default=None, alias=\"max_completion_tokens\")\n    \"\"\"Maximum number of tokens to generate.\"\"\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"openai_api_key\": \"OPENAI_API_KEY\"}\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"openai\"]\n\n    @property\n    def lc_attributes(self) -> Dict[str, Any]:\n        attributes: Dict[str, Any] = {}\n\n        if self.openai_organization:\n            attributes[\"openai_organization\"] = self.openai_organization\n\n        if self.openai_api_base:\n            attributes[\"openai_api_base\"] = self.openai_api_base\n\n        if self.openai_proxy:\n            attributes[\"openai_proxy\"] = self.openai_proxy\n\n        return attributes\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this model can be serialized by Langchain.\"\"\"\n        return True\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        params = super()._default_params\n        if \"max_tokens\" in params:\n            params[\"max_completion_tokens\"] = params.pop(\"max_tokens\")\n\n        return params\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        payload = super()._get_request_payload(input_, stop=stop, **kwargs)\n        # max_tokens was deprecated in favor of max_completion_tokens\n        # in September 2024 release\n        if \"max_tokens\" in payload:\n            payload[\"max_completion_tokens\"] = payload.pop(\"max_tokens\")\n\n        # Mutate system message role to \"developer\" for o-series models\n        if self.model_name and re.match(r\"^o\\d\", self.model_name):\n            for message in payload.get(\"messages\", []):\n                if message[\"role\"] == \"system\":\n                    message[\"role\"] = \"developer\"\n        return payload\n\n    def _should_stream_usage(\n        self, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> bool:\n        \"\"\"Determine whether to include usage metadata in streaming output.\n\n        For backwards compatibility, we check for `stream_options` passed\n        explicitly to kwargs or in the model_kwargs and override self.stream_usage.\n        \"\"\"\n        stream_usage_sources = [  # order of preference\n            stream_usage,\n            kwargs.get(\"stream_options\", {}).get(\"include_usage\"),\n            self.model_kwargs.get(\"stream_options\", {}).get(\"include_usage\"),\n            self.stream_usage,\n        ]\n        for source in stream_usage_sources:\n            if isinstance(source, bool):\n                return source\n        return self.stream_usage\n\n    def _stream(\n        self, *args: Any, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> Iterator[ChatGenerationChunk]:\n        \"\"\"Set default stream_options.\"\"\"\n        stream_usage = self._should_stream_usage(stream_usage, **kwargs)\n        # Note: stream_options is not a valid parameter for Azure OpenAI.\n        # To support users proxying Azure through ChatOpenAI, here we only specify\n        # stream_options if include_usage is set to True.\n        # See https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new\n        # for release notes.\n        if stream_usage:\n            kwargs[\"stream_options\"] = {\"include_usage\": stream_usage}\n\n        return super()._stream(*args, **kwargs)\n\n    async def _astream(\n        self, *args: Any, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        \"\"\"Set default stream_options.\"\"\"\n        stream_usage = self._should_stream_usage(stream_usage, **kwargs)\n        if stream_usage:\n            kwargs[\"stream_options\"] = {\"include_usage\": stream_usage}\n\n        async for chunk in super()._astream(*args, **kwargs):\n            yield chunk\n\n    def with_structured_output(\n        self,\n        schema: Optional[_DictOrPydanticClass] = None,\n        *,\n        method: Literal[\"function_calling\", \"json_mode\", \"json_schema\"] = \"json_schema\",\n        include_raw: bool = False,\n        strict: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, _DictOrPydantic]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n\n                - a JSON Schema,\n                - a TypedDict class,\n                - or a Pydantic class,\n                - an OpenAI function/tool schema.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"json_schema\":\n                    Uses OpenAI's Structured Output API:\n                    https://platform.openai.com/docs/guides/structured-outputs\n                    Supported for \"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"o1\", and later\n                    models.\n                - \"function_calling\":\n                    Uses OpenAI's tool-calling (formerly called function calling)\n                    API: https://platform.openai.com/docs/guides/function-calling\n                - \"json_mode\":\n                    Uses OpenAI's JSON mode. Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call:\n                    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\n                Learn more about the differences between the methods and which models\n                support which methods here:\n\n                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode\n                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            strict:\n\n                - True:\n                    Model output is guaranteed to exactly match the schema.\n                    The input schema will also be validated according to\n                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n                - False:\n                    Input schema will not be validated and model output will not be\n                    validated.\n                - None:\n                    ``strict`` argument will not be passed to the model.\n\n                If schema is specified via TypedDict or JSON schema, ``strict`` is not\n                enabled by default. Pass ``strict=True`` to enable it.\n\n                Note: ``strict`` can only be non-null if ``method`` is\n                ``\"json_schema\"`` or ``\"function_calling\"``.\n\n            kwargs: Additional keyword args aren't supported.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n\n            - \"raw\": BaseMessage\n            - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n            - \"parsing_error\": Optional[BaseException]\n\n        .. versionchanged:: 0.1.20\n\n            Added support for TypedDict class ``schema``.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n            Support for ``method=\"json_schema\"`` added.\n\n        .. versionchanged:: 0.3.0\n\n            ``method`` default changed from \"function_calling\" to \"json_schema\".\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=False, strict=True\n\n            Note, OpenAI has a number of restrictions on what types of schemas can be\n            provided if ``strict`` = True. When using Pydantic, our model cannot\n            specify any Field metadata (like min/max constraints) and fields cannot\n            have default values.\n\n            See all constraints here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Optional[str] = Field(\n                        default=..., description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=False, strict=False\n\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Optional[str] = Field(\n                        default=..., description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, method=\"function_calling\"\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=True\n\n            .. code-block:: python\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: str\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        .. dropdown:: Example: schema=TypedDict class, method=\"json_schema\", include_raw=False, strict=False\n\n            .. code-block:: python\n\n                # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n                # from typing_extensions, not from typing.\n                from typing_extensions import Annotated, TypedDict\n\n                from langchain_openai import ChatOpenAI\n\n\n                class AnswerWithJustification(TypedDict):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Annotated[\n                        Optional[str], None, \"A justification for the answer.\"\n                    ]\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. dropdown:: Example: schema=OpenAI function schema, method=\"json_schema\", include_raw=False\n\n            .. code-block:: python\n\n                from langchain_openai import ChatOpenAI\n\n                oai_schema = {\n                    'name': 'AnswerWithJustification',\n                    'description': 'An answer to the user question along with justification for the answer.',\n                    'parameters': {\n                        'type': 'object',\n                        'properties': {\n                            'answer': {'type': 'string'},\n                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n                        },\n                       'required': ['answer']\n                   }\n               }\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(oai_schema)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_mode\", include_raw=True\n\n            .. code-block::\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    answer: str\n                    justification: str\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification,\n                    method=\"json_mode\",\n                    include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n                #     'parsing_error': None\n                # }\n\n        .. dropdown:: Example: schema=None, method=\"json_mode\", include_raw=True\n\n            .. code-block::\n\n                structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': {\n                #         'answer': 'They are both the same weight.',\n                #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n                #     },\n                #     'parsing_error': None\n                # }\n        \"\"\"  # noqa: E501\n        return super().with_structured_output(\n            schema, method=method, include_raw=include_raw, strict=strict, **kwargs\n        )\n\n\ndef _is_pydantic_class(obj: Any) -> bool:\n    return isinstance(obj, type) and is_basemodel_subclass(obj)\n\n\ndef _lc_tool_call_to_openai_tool_call(tool_call: ToolCall) -> dict:\n    return {\n        \"type\": \"function\",\n        \"id\": tool_call[\"id\"],\n        \"function\": {\n            \"name\": tool_call[\"name\"],\n            \"arguments\": json.dumps(tool_call[\"args\"]),\n        },\n    }\n\n\ndef _lc_invalid_tool_call_to_openai_tool_call(\n    invalid_tool_call: InvalidToolCall,\n) -> dict:\n    return {\n        \"type\": \"function\",\n        \"id\": invalid_tool_call[\"id\"],\n        \"function\": {\n            \"name\": invalid_tool_call[\"name\"],\n            \"arguments\": invalid_tool_call[\"args\"],\n        },\n    }\n\n\ndef _url_to_size(image_source: str) -> Optional[Tuple[int, int]]:\n    try:\n        from PIL import Image  # type: ignore[import]\n    except ImportError:\n        logger.info(\n            \"Unable to count image tokens. To count image tokens please install \"\n            \"`pip install -U pillow httpx`.\"\n        )\n        return None\n    if _is_url(image_source):\n        try:\n            import httpx\n        except ImportError:\n            logger.info(\n                \"Unable to count image tokens. To count image tokens please install \"\n                \"`pip install -U httpx`.\"\n            )\n            return None\n        response = httpx.get(image_source)\n        response.raise_for_status()\n        width, height = Image.open(BytesIO(response.content)).size\n        return width, height\n    elif _is_b64(image_source):\n        _, encoded = image_source.split(\",\", 1)\n        data = base64.b64decode(encoded)\n        width, height = Image.open(BytesIO(data)).size\n        return width, height\n    else:\n        return None\n\n\ndef _count_image_tokens(width: int, height: int) -> int:\n    # Reference: https://platform.openai.com/docs/guides/vision/calculating-costs\n    width, height = _resize(width, height)\n    h = ceil(height / 512)\n    w = ceil(width / 512)\n    return (170 * h * w) + 85\n\n\ndef _is_url(s: str) -> bool:\n    try:\n        result = urlparse(s)\n        return all([result.scheme, result.netloc])\n    except Exception as e:\n        logger.debug(f\"Unable to parse URL: {e}\")\n        return False\n\n\ndef _is_b64(s: str) -> bool:\n    return s.startswith(\"data:image\")\n\n\ndef _resize(width: int, height: int) -> Tuple[int, int]:\n    # larger side must be <= 2048\n    if width > 2048 or height > 2048:\n        if width > height:\n            height = (height * 2048) // width\n            width = 2048\n        else:\n            width = (width * 2048) // height\n            height = 2048\n    # smaller side must be <= 768\n    if width > 768 and height > 768:\n        if width > height:\n            width = (width * 768) // height\n            height = 768\n        else:\n            height = (width * 768) // height\n            width = 768\n    return width, height\n\n\ndef _convert_to_openai_response_format(\n    schema: Union[Dict[str, Any], Type], *, strict: Optional[bool] = None\n) -> Union[Dict, TypeBaseModel]:\n    if isinstance(schema, type) and is_basemodel_subclass(schema):\n        return schema\n\n    if (\n        isinstance(schema, dict)\n        and \"json_schema\" in schema\n        and schema.get(\"type\") == \"json_schema\"\n    ):\n        response_format = schema\n    elif isinstance(schema, dict) and \"name\" in schema and \"schema\" in schema:\n        response_format = {\"type\": \"json_schema\", \"json_schema\": schema}\n    else:\n        if strict is None:\n            if isinstance(schema, dict) and isinstance(schema.get(\"strict\"), bool):\n                strict = schema[\"strict\"]\n            else:\n                strict = False\n        function = convert_to_openai_function(schema, strict=strict)\n        function[\"schema\"] = function.pop(\"parameters\")\n        response_format = {\"type\": \"json_schema\", \"json_schema\": function}\n\n    if strict is not None and strict is not response_format[\"json_schema\"].get(\n        \"strict\"\n    ):\n        msg = (\n            f\"Output schema already has 'strict' value set to \"\n            f\"{schema['json_schema']['strict']} but 'strict' also passed in to \"\n            f\"with_structured_output as {strict}. Please make sure that \"\n            f\"'strict' is only specified in one place.\"\n        )\n        raise ValueError(msg)\n    return response_format\n\n\n@chain\ndef _oai_structured_outputs_parser(ai_msg: AIMessage) -> PydanticBaseModel:\n    if ai_msg.additional_kwargs.get(\"parsed\"):\n        return ai_msg.additional_kwargs[\"parsed\"]\n    elif ai_msg.additional_kwargs.get(\"refusal\"):\n        raise OpenAIRefusalError(ai_msg.additional_kwargs[\"refusal\"])\n    else:\n        raise ValueError(\n            \"Structured Output response does not have a 'parsed' field nor a 'refusal' \"\n            f\"field. Received message:\\n\\n{ai_msg}\"\n        )\n\n\nclass OpenAIRefusalError(Exception):\n    \"\"\"Error raised when OpenAI Structured Outputs API returns a refusal.\n\n    When using OpenAI's Structured Outputs API with user-generated input, the model\n    may occasionally refuse to fulfill the request for safety reasons.\n\n    See here for more on refusals:\n    https://platform.openai.com/docs/guides/structured-outputs/refusals\n\n    .. versionadded:: 0.1.21\n    \"\"\"\n\n\ndef _create_usage_metadata(oai_token_usage: dict) -> UsageMetadata:\n    input_tokens = oai_token_usage.get(\"prompt_tokens\", 0)\n    output_tokens = oai_token_usage.get(\"completion_tokens\", 0)\n    total_tokens = oai_token_usage.get(\"total_tokens\", input_tokens + output_tokens)\n    input_token_details: dict = {\n        \"audio\": (oai_token_usage.get(\"prompt_tokens_details\") or {}).get(\n            \"audio_tokens\"\n        ),\n        \"cache_read\": (oai_token_usage.get(\"prompt_tokens_details\") or {}).get(\n            \"cached_tokens\"\n        ),\n    }\n    output_token_details: dict = {\n        \"audio\": (oai_token_usage.get(\"completion_tokens_details\") or {}).get(\n            \"audio_tokens\"\n        ),\n        \"reasoning\": (oai_token_usage.get(\"completion_tokens_details\") or {}).get(\n            \"reasoning_tokens\"\n        ),\n    }\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=total_tokens,\n        input_token_details=InputTokenDetails(\n            **{k: v for k, v in input_token_details.items() if v is not None}\n        ),\n        output_token_details=OutputTokenDetails(\n            **{k: v for k, v in output_token_details.items() if v is not None}\n        ),\n    )\n",
        "patch": "@@ -7,6 +7,7 @@\n import logging\n import os\n import re\n+import ssl\n import sys\n import warnings\n from io import BytesIO\n@@ -32,6 +33,7 @@\n )\n from urllib.parse import urlparse\n \n+import certifi\n import openai\n import tiktoken\n from langchain_core._api.deprecation import deprecated\n@@ -98,6 +100,10 @@\n \n logger = logging.getLogger(__name__)\n \n+# This SSL context is equivelent to the default `verify=True`.\n+# https://www.python-httpx.org/advanced/ssl/#configuring-client-instances\n+global_ssl_context = ssl.create_default_context(cafile=certifi.where())\n+\n \n def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n     \"\"\"Convert a dictionary to a LangChain message.\n@@ -558,7 +564,9 @@ def validate_environment(self) -> Self:\n                         \"Could not import httpx python package. \"\n                         \"Please install it with `pip install httpx`.\"\n                     ) from e\n-                self.http_client = httpx.Client(proxy=self.openai_proxy)\n+                self.http_client = httpx.Client(\n+                    proxy=self.openai_proxy, verify=global_ssl_context\n+                )\n             sync_specific = {\"http_client\": self.http_client}\n             self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]\n             self.client = self.root_client.chat.completions\n@@ -571,7 +579,9 @@ def validate_environment(self) -> Self:\n                         \"Could not import httpx python package. \"\n                         \"Please install it with `pip install httpx`.\"\n                     ) from e\n-                self.http_async_client = httpx.AsyncClient(proxy=self.openai_proxy)\n+                self.http_async_client = httpx.AsyncClient(\n+                    proxy=self.openai_proxy, verify=global_ssl_context\n+                )\n             async_specific = {\"http_client\": self.http_async_client}\n             self.root_async_client = openai.AsyncOpenAI(\n                 **client_params,"
      }
    ]
  },
  {
    "number": 29963,
    "title": "core[patch]: pydantic 2.11 compat",
    "body": "Resolves https://github.com/langchain-ai/langchain/issues/29951\r\n\r\nWas able to reproduce the issue with Anthropic installing from pydantic `main` and correct it with the fix recommended in the issue.\r\n\r\nThanks very much @Viicos for finding the bug and the detailed writeup!",
    "issue_title": "core[patch]: pydantic 2.11 compat",
    "issue_body": "Resolves https://github.com/langchain-ai/langchain/issues/29951\r\n\r\nWas able to reproduce the issue with Anthropic installing from pydantic `main` and correct it with the fix recommended in the issue.\r\n\r\nThanks very much @Viicos for finding the bug and the detailed writeup!",
    "files": [
      {
        "filename": "libs/core/langchain_core/language_models/base.py",
        "content_before": "from __future__ import annotations\n\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Mapping, Sequence\nfrom functools import cache\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, field_validator\nfrom typing_extensions import TypeAlias, TypedDict, override\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.messages import (\n    AnyMessage,\n    BaseMessage,\n    MessageLikeRepresentation,\n    get_buffer_string,\n)\nfrom langchain_core.prompt_values import PromptValue\nfrom langchain_core.runnables import Runnable, RunnableSerializable\nfrom langchain_core.utils import get_pydantic_field_names\n\nif TYPE_CHECKING:\n    from langchain_core.caches import BaseCache\n    from langchain_core.callbacks import Callbacks\n    from langchain_core.outputs import LLMResult\n\n\nclass LangSmithParams(TypedDict, total=False):\n    \"\"\"LangSmith parameters for tracing.\"\"\"\n\n    ls_provider: str\n    \"\"\"Provider of the model.\"\"\"\n    ls_model_name: str\n    \"\"\"Name of the model.\"\"\"\n    ls_model_type: Literal[\"chat\", \"llm\"]\n    \"\"\"Type of the model. Should be 'chat' or 'llm'.\"\"\"\n    ls_temperature: Optional[float]\n    \"\"\"Temperature for generation.\"\"\"\n    ls_max_tokens: Optional[int]\n    \"\"\"Max tokens for generation.\"\"\"\n    ls_stop: Optional[list[str]]\n    \"\"\"Stop words for generation.\"\"\"\n\n\n@cache  # Cache the tokenizer\ndef get_tokenizer() -> Any:\n    \"\"\"Get a GPT-2 tokenizer instance.\n\n    This function is cached to avoid re-loading the tokenizer\n    every time it is called.\n    \"\"\"\n    try:\n        from transformers import GPT2TokenizerFast  # type: ignore[import]\n    except ImportError as e:\n        msg = (\n            \"Could not import transformers python package. \"\n            \"This is needed in order to calculate get_token_ids. \"\n            \"Please install it with `pip install transformers`.\"\n        )\n        raise ImportError(msg) from e\n    # create a GPT-2 tokenizer instance\n    return GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\n\ndef _get_token_ids_default_method(text: str) -> list[int]:\n    \"\"\"Encode the text into token IDs.\"\"\"\n    # get the cached tokenizer\n    tokenizer = get_tokenizer()\n\n    # tokenize the text using the GPT-2 tokenizer\n    return tokenizer.encode(text)\n\n\nLanguageModelInput = Union[PromptValue, str, Sequence[MessageLikeRepresentation]]\nLanguageModelOutput = Union[BaseMessage, str]\nLanguageModelLike = Runnable[LanguageModelInput, LanguageModelOutput]\nLanguageModelOutputVar = TypeVar(\"LanguageModelOutputVar\", BaseMessage, str)\n\n\ndef _get_verbosity() -> bool:\n    from langchain_core.globals import get_verbose\n\n    return get_verbose()\n\n\nclass BaseLanguageModel(\n    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC\n):\n    \"\"\"Abstract base class for interfacing with language models.\n\n    All language model wrappers inherited from BaseLanguageModel.\n    \"\"\"\n\n    cache: Union[BaseCache, bool, None] = Field(default=None, exclude=True)\n    \"\"\"Whether to cache the response.\n\n    * If true, will use the global cache.\n    * If false, will not use a cache\n    * If None, will use the global cache if it's set, otherwise no cache.\n    * If instance of BaseCache, will use the provided cache.\n\n    Caching is not currently supported for streaming methods of models.\n    \"\"\"\n    verbose: bool = Field(default_factory=_get_verbosity, exclude=True, repr=False)\n    \"\"\"Whether to print out response text.\"\"\"\n    callbacks: Callbacks = Field(default=None, exclude=True)\n    \"\"\"Callbacks to add to the run trace.\"\"\"\n    tags: Optional[list[str]] = Field(default=None, exclude=True)\n    \"\"\"Tags to add to the run trace.\"\"\"\n    metadata: Optional[dict[str, Any]] = Field(default=None, exclude=True)\n    \"\"\"Metadata to add to the run trace.\"\"\"\n    custom_get_token_ids: Optional[Callable[[str], list[int]]] = Field(\n        default=None, exclude=True\n    )\n    \"\"\"Optional encoder to use for counting tokens.\"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @field_validator(\"verbose\", mode=\"before\")\n    def set_verbose(cls, verbose: Optional[bool]) -> bool:\n        \"\"\"If verbose is None, set it.\n\n        This allows users to pass in None as verbose to access the global setting.\n\n        Args:\n            verbose: The verbosity setting to use.\n\n        Returns:\n            The verbosity setting to use.\n        \"\"\"\n        if verbose is None:\n            return _get_verbosity()\n        else:\n            return verbose\n\n    @property\n    @override\n    def InputType(self) -> TypeAlias:\n        \"\"\"Get the input type for this runnable.\"\"\"\n        from langchain_core.prompt_values import (\n            ChatPromptValueConcrete,\n            StringPromptValue,\n        )\n\n        # This is a version of LanguageModelInput which replaces the abstract\n        # base class BaseMessage with a union of its subclasses, which makes\n        # for a much better schema.\n        return Union[\n            str,\n            Union[StringPromptValue, ChatPromptValueConcrete],\n            list[AnyMessage],\n        ]\n\n    @abstractmethod\n    def generate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Pass a sequence of prompts to the model and return model generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            prompts: List of PromptValues. A PromptValue is an object that can be\n                converted to match the format of any language model (string for pure\n                text generation models and BaseMessages for chat models).\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n\n    @abstractmethod\n    async def agenerate_prompt(\n        self,\n        prompts: list[PromptValue],\n        stop: Optional[list[str]] = None,\n        callbacks: Callbacks = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        \"\"\"Asynchronously pass a sequence of prompts and return model generations.\n\n        This method should make use of batched calls for models that expose a batched\n        API.\n\n        Use this method when you want to:\n            1. take advantage of batched calls,\n            2. need more output from the model than just the top generated value,\n            3. are building chains that are agnostic to the underlying language model\n                type (e.g., pure text completion models vs chat models).\n\n        Args:\n            prompts: List of PromptValues. A PromptValue is an object that can be\n                converted to match the format of any language model (string for pure\n                text generation models and BaseMessages for chat models).\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            callbacks: Callbacks to pass through. Used for executing additional\n                functionality, such as logging or streaming, throughout generation.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            An LLMResult, which contains a list of candidate Generations for each input\n                prompt and additional model provider-specific output.\n        \"\"\"\n\n    def with_structured_output(\n        self, schema: Union[dict, type], **kwargs: Any\n    ) -> Runnable[LanguageModelInput, Union[dict, BaseModel]]:\n        \"\"\"Not implemented on this class.\"\"\"\n        # Implement this on child class if there is a way of steering the model to\n        # generate responses that match a given schema.\n        raise NotImplementedError\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    @abstractmethod\n    def predict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        \"\"\"Pass a single string input to the model and return a string.\n\n         Use this method when passing in raw text. If you want to pass in specific\n            types of chat messages, use predict_messages.\n\n        Args:\n            text: String input to pass to the model.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            Top model prediction as a string.\n        \"\"\"\n\n    @deprecated(\"0.1.7\", alternative=\"invoke\", removal=\"1.0\")\n    @abstractmethod\n    def predict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        \"\"\"Pass a message sequence to the model and return a message.\n\n        Use this method when passing in chat messages. If you want to pass in raw text,\n            use predict.\n\n        Args:\n            messages: A sequence of chat messages corresponding to a single model input.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            Top model prediction as a message.\n        \"\"\"\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    @abstractmethod\n    async def apredict(\n        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n    ) -> str:\n        \"\"\"Asynchronously pass a string to the model and return a string.\n\n        Use this method when calling pure text generation models and only the top\n            candidate generation is needed.\n\n        Args:\n            text: String input to pass to the model.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            Top model prediction as a string.\n        \"\"\"\n\n    @deprecated(\"0.1.7\", alternative=\"ainvoke\", removal=\"1.0\")\n    @abstractmethod\n    async def apredict_messages(\n        self,\n        messages: list[BaseMessage],\n        *,\n        stop: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> BaseMessage:\n        \"\"\"Asynchronously pass messages to the model and return a message.\n\n        Use this method when calling chat models and only the top\n            candidate generation is needed.\n\n        Args:\n            messages: A sequence of chat messages corresponding to a single model input.\n            stop: Stop words to use when generating. Model output is cut off at the\n                first occurrence of any of these substrings.\n            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n                to the model provider API call.\n\n        Returns:\n            Top model prediction as a message.\n        \"\"\"\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return self.lc_attributes\n\n    def get_token_ids(self, text: str) -> list[int]:\n        \"\"\"Return the ordered ids of the tokens in a text.\n\n        Args:\n            text: The string input to tokenize.\n\n        Returns:\n            A list of ids corresponding to the tokens in the text, in order they occur\n                in the text.\n        \"\"\"\n        if self.custom_get_token_ids is not None:\n            return self.custom_get_token_ids(text)\n        else:\n            return _get_token_ids_default_method(text)\n\n    def get_num_tokens(self, text: str) -> int:\n        \"\"\"Get the number of tokens present in the text.\n\n        Useful for checking if an input fits in a model's context window.\n\n        Args:\n            text: The string input to tokenize.\n\n        Returns:\n            The integer number of tokens in the text.\n        \"\"\"\n        return len(self.get_token_ids(text))\n\n    def get_num_tokens_from_messages(\n        self,\n        messages: list[BaseMessage],\n        tools: Optional[Sequence] = None,\n    ) -> int:\n        \"\"\"Get the number of tokens in the messages.\n\n        Useful for checking if an input fits in a model's context window.\n\n        **Note**: the base implementation of get_num_tokens_from_messages ignores\n        tool schemas.\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n\n        Returns:\n            The sum of the number of tokens across the messages.\n        \"\"\"\n        if tools is not None:\n            warnings.warn(\n                \"Counting tokens in tool schemas is not yet supported. Ignoring tools.\",\n                stacklevel=2,\n            )\n        return sum(self.get_num_tokens(get_buffer_string([m])) for m in messages)\n\n    @classmethod\n    def _all_required_field_names(cls) -> set:\n        \"\"\"DEPRECATED: Kept for backwards compatibility.\n\n        Use get_pydantic_field_names.\n        \"\"\"\n        return get_pydantic_field_names(cls)\n",
        "patch": "@@ -18,6 +18,8 @@\n from typing_extensions import TypeAlias, TypedDict, override\n \n from langchain_core._api import deprecated\n+from langchain_core.caches import BaseCache\n+from langchain_core.callbacks import Callbacks\n from langchain_core.messages import (\n     AnyMessage,\n     BaseMessage,\n@@ -29,8 +31,6 @@\n from langchain_core.utils import get_pydantic_field_names\n \n if TYPE_CHECKING:\n-    from langchain_core.caches import BaseCache\n-    from langchain_core.callbacks import Callbacks\n     from langchain_core.outputs import LLMResult\n \n "
      }
    ]
  },
  {
    "number": 29940,
    "title": "core, openai[patch]: support serialization of pydantic models in messages",
    "body": "Resolves https://github.com/langchain-ai/langchain/issues/29003, https://github.com/langchain-ai/langchain/issues/27264\r\nRelated: https://github.com/langchain-ai/langchain-redis/issues/52\r\n\r\n```python\r\nfrom langchain.chat_models import init_chat_model\r\nfrom langchain.globals import set_llm_cache\r\nfrom langchain_community.cache import SQLiteCache\r\nfrom pydantic import BaseModel\r\n\r\ncache = SQLiteCache()\r\n\r\nset_llm_cache(cache)\r\n\r\nclass Temperature(BaseModel):\r\n    value: int\r\n    city: str\r\n\r\nllm = init_chat_model(\"openai:gpt-4o-mini\")\r\nstructured_llm = llm.with_structured_output(Temperature)\r\n```\r\n```python\r\n# 681 ms\r\nresponse = structured_llm.invoke(\"What is the average temperature of Rome in May?\")\r\n```\r\n```python\r\n# 6.98 ms\r\nresponse = structured_llm.invoke(\"What is the average temperature of Rome in May?\")\r\n```",
    "issue_title": "core, openai[patch]: support serialization of pydantic models in messages",
    "issue_body": "Resolves https://github.com/langchain-ai/langchain/issues/29003, https://github.com/langchain-ai/langchain/issues/27264\r\nRelated: https://github.com/langchain-ai/langchain-redis/issues/52\r\n\r\n```python\r\nfrom langchain.chat_models import init_chat_model\r\nfrom langchain.globals import set_llm_cache\r\nfrom langchain_community.cache import SQLiteCache\r\nfrom pydantic import BaseModel\r\n\r\ncache = SQLiteCache()\r\n\r\nset_llm_cache(cache)\r\n\r\nclass Temperature(BaseModel):\r\n    value: int\r\n    city: str\r\n\r\nllm = init_chat_model(\"openai:gpt-4o-mini\")\r\nstructured_llm = llm.with_structured_output(Temperature)\r\n```\r\n```python\r\n# 681 ms\r\nresponse = structured_llm.invoke(\"What is the average temperature of Rome in May?\")\r\n```\r\n```python\r\n# 6.98 ms\r\nresponse = structured_llm.invoke(\"What is the average temperature of Rome in May?\")\r\n```",
    "files": [
      {
        "filename": "libs/core/langchain_core/load/dump.py",
        "content_before": "import json\nfrom typing import Any\n\nfrom langchain_core.load.serializable import Serializable, to_json_not_implemented\n\n\ndef default(obj: Any) -> Any:\n    \"\"\"Return a default value for a Serializable object or\n    a SerializedNotImplemented object.\n\n    Args:\n        obj: The object to serialize to json if it is a Serializable object.\n\n    Returns:\n        A json serializable object or a SerializedNotImplemented object.\n    \"\"\"\n    if isinstance(obj, Serializable):\n        return obj.to_json()\n    else:\n        return to_json_not_implemented(obj)\n\n\ndef dumps(obj: Any, *, pretty: bool = False, **kwargs: Any) -> str:\n    \"\"\"Return a json string representation of an object.\n\n    Args:\n        obj: The object to dump.\n        pretty: Whether to pretty print the json. If true, the json will be\n            indented with 2 spaces (if no indent is provided as part of kwargs).\n            Default is False.\n        kwargs: Additional arguments to pass to json.dumps\n\n    Returns:\n        A json string representation of the object.\n\n    Raises:\n        ValueError: If `default` is passed as a kwarg.\n    \"\"\"\n    if \"default\" in kwargs:\n        msg = \"`default` should not be passed to dumps\"\n        raise ValueError(msg)\n    try:\n        if pretty:\n            indent = kwargs.pop(\"indent\", 2)\n            return json.dumps(obj, default=default, indent=indent, **kwargs)\n        else:\n            return json.dumps(obj, default=default, **kwargs)\n    except TypeError:\n        if pretty:\n            indent = kwargs.pop(\"indent\", 2)\n            return json.dumps(to_json_not_implemented(obj), indent=indent, **kwargs)\n        else:\n            return json.dumps(to_json_not_implemented(obj), **kwargs)\n\n\ndef dumpd(obj: Any) -> Any:\n    \"\"\"Return a dict representation of an object.\n\n    Note:\n        Unfortunately this function is not as efficient as it could be\n        because it first dumps the object to a json string and then loads it\n        back into a dictionary.\n\n    Args:\n        obj: The object to dump.\n\n    Returns:\n        dictionary that can be serialized to json using json.dumps\n    \"\"\"\n    return json.loads(dumps(obj))\n",
        "patch": "@@ -1,6 +1,8 @@\n import json\n from typing import Any\n \n+from pydantic import BaseModel\n+\n from langchain_core.load.serializable import Serializable, to_json_not_implemented\n \n \n@@ -20,6 +22,23 @@ def default(obj: Any) -> Any:\n         return to_json_not_implemented(obj)\n \n \n+def _dump_pydantic_models(obj: Any) -> Any:\n+    from langchain_core.messages import AIMessage\n+    from langchain_core.outputs import ChatGeneration\n+\n+    if (\n+        isinstance(obj, ChatGeneration)\n+        and isinstance(obj.message, AIMessage)\n+        and (parsed := obj.message.additional_kwargs.get(\"parsed\"))\n+        and isinstance(parsed, BaseModel)\n+    ):\n+        obj_copy = obj.model_copy(deep=True)\n+        obj_copy.message.additional_kwargs[\"parsed\"] = parsed.model_dump()\n+        return obj_copy\n+    else:\n+        return obj\n+\n+\n def dumps(obj: Any, *, pretty: bool = False, **kwargs: Any) -> str:\n     \"\"\"Return a json string representation of an object.\n \n@@ -40,6 +59,7 @@ def dumps(obj: Any, *, pretty: bool = False, **kwargs: Any) -> str:\n         msg = \"`default` should not be passed to dumps\"\n         raise ValueError(msg)\n     try:\n+        obj = _dump_pydantic_models(obj)\n         if pretty:\n             indent = kwargs.pop(\"indent\", 2)\n             return json.dumps(obj, default=default, indent=indent, **kwargs)"
      },
      {
        "filename": "libs/core/tests/unit_tests/load/test_serializable.py",
        "content_before": "from pydantic import ConfigDict, Field\n\nfrom langchain_core.load import Serializable, dumpd, load\nfrom langchain_core.load.serializable import _is_field_useful\n\n\nclass NonBoolObj:\n    def __bool__(self) -> bool:\n        msg = \"Truthiness can't be determined\"\n        raise ValueError(msg)\n\n    def __eq__(self, other: object) -> bool:\n        msg = \"Equality can't be determined\"\n        raise ValueError(msg)\n\n    def __str__(self) -> str:\n        return self.__class__.__name__\n\n    def __repr__(self) -> str:\n        return self.__class__.__name__\n\n\ndef test_simple_serialization() -> None:\n    class Foo(Serializable):\n        bar: int\n        baz: str\n\n    foo = Foo(bar=1, baz=\"hello\")\n    assert dumpd(foo) == {\n        \"id\": [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"],\n        \"lc\": 1,\n        \"repr\": \"Foo(bar=1, baz='hello')\",\n        \"type\": \"not_implemented\",\n    }\n\n\ndef test_simple_serialization_is_serializable() -> None:\n    class Foo(Serializable):\n        bar: int\n        baz: str\n\n        @classmethod\n        def is_lc_serializable(cls) -> bool:\n            return True\n\n    foo = Foo(bar=1, baz=\"hello\")\n    assert foo.lc_id() == [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"]\n    assert dumpd(foo) == {\n        \"id\": [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"],\n        \"kwargs\": {\"bar\": 1, \"baz\": \"hello\"},\n        \"lc\": 1,\n        \"type\": \"constructor\",\n    }\n\n\ndef test_simple_serialization_secret() -> None:\n    \"\"\"Test handling of secrets.\"\"\"\n    from pydantic import SecretStr\n\n    from langchain_core.load import Serializable\n\n    class Foo(Serializable):\n        bar: int\n        baz: str\n        secret: SecretStr\n        secret_2: str\n\n        @classmethod\n        def is_lc_serializable(cls) -> bool:\n            return True\n\n        @property\n        def lc_secrets(self) -> dict[str, str]:\n            return {\"secret\": \"MASKED_SECRET\", \"secret_2\": \"MASKED_SECRET_2\"}\n\n    foo = Foo(\n        bar=1, baz=\"baz\", secret=SecretStr(\"SUPER_SECRET\"), secret_2=\"SUPER_SECRET\"\n    )\n    assert dumpd(foo) == {\n        \"id\": [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"],\n        \"kwargs\": {\n            \"bar\": 1,\n            \"baz\": \"baz\",\n            \"secret\": {\"id\": [\"MASKED_SECRET\"], \"lc\": 1, \"type\": \"secret\"},\n            \"secret_2\": {\"id\": [\"MASKED_SECRET_2\"], \"lc\": 1, \"type\": \"secret\"},\n        },\n        \"lc\": 1,\n        \"type\": \"constructor\",\n    }\n\n\ndef test__is_field_useful() -> None:\n    class ArrayObj:\n        def __bool__(self) -> bool:\n            msg = \"Truthiness can't be determined\"\n            raise ValueError(msg)\n\n        def __eq__(self, other: object) -> bool:\n            return self  # type: ignore[return-value]\n\n    default_x = ArrayObj()\n    default_y = NonBoolObj()\n\n    class Foo(Serializable):\n        x: ArrayObj = Field(default=default_x)\n        y: NonBoolObj = Field(default=default_y)\n        # Make sure works for fields without default.\n        z: ArrayObj\n\n        model_config = ConfigDict(\n            arbitrary_types_allowed=True,\n        )\n\n    foo = Foo(x=ArrayObj(), y=NonBoolObj(), z=ArrayObj())\n    assert _is_field_useful(foo, \"x\", foo.x)\n    assert _is_field_useful(foo, \"y\", foo.y)\n\n    foo = Foo(x=default_x, y=default_y, z=ArrayObj())\n    assert not _is_field_useful(foo, \"x\", foo.x)\n    assert not _is_field_useful(foo, \"y\", foo.y)\n\n\nclass Foo(Serializable):\n    bar: int\n    baz: str\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n\ndef test_simple_deserialization() -> None:\n    foo = Foo(bar=1, baz=\"hello\")\n    assert foo.lc_id() == [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"]\n    serialized_foo = dumpd(foo)\n    assert serialized_foo == {\n        \"id\": [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"],\n        \"kwargs\": {\"bar\": 1, \"baz\": \"hello\"},\n        \"lc\": 1,\n        \"type\": \"constructor\",\n    }\n    new_foo = load(serialized_foo, valid_namespaces=[\"tests\"])\n    assert new_foo == foo\n\n\nclass Foo2(Serializable):\n    bar: int\n    baz: str\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n\ndef test_simple_deserialization_with_additional_imports() -> None:\n    foo = Foo(bar=1, baz=\"hello\")\n    assert foo.lc_id() == [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"]\n    serialized_foo = dumpd(foo)\n    assert serialized_foo == {\n        \"id\": [\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"],\n        \"kwargs\": {\"bar\": 1, \"baz\": \"hello\"},\n        \"lc\": 1,\n        \"type\": \"constructor\",\n    }\n    new_foo = load(\n        serialized_foo,\n        valid_namespaces=[\"tests\"],\n        additional_import_mappings={\n            (\"tests\", \"unit_tests\", \"load\", \"test_serializable\", \"Foo\"): (\n                \"tests\",\n                \"unit_tests\",\n                \"load\",\n                \"test_serializable\",\n                \"Foo2\",\n            )\n        },\n    )\n    assert isinstance(new_foo, Foo2)\n\n\nclass Foo3(Serializable):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    content: str\n    non_bool: NonBoolObj\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n\ndef test_repr() -> None:\n    foo = Foo3(\n        content=\"repr\",\n        non_bool=NonBoolObj(),\n    )\n    assert repr(foo) == \"Foo3(content='repr', non_bool=NonBoolObj)\"\n\n\ndef test_str() -> None:\n    foo = Foo3(\n        content=\"str\",\n        non_bool=NonBoolObj(),\n    )\n    assert str(foo) == \"content='str' non_bool=NonBoolObj\"\n",
        "patch": "@@ -1,7 +1,9 @@\n-from pydantic import ConfigDict, Field\n+from pydantic import BaseModel, ConfigDict, Field\n \n from langchain_core.load import Serializable, dumpd, load\n from langchain_core.load.serializable import _is_field_useful\n+from langchain_core.messages import AIMessage\n+from langchain_core.outputs import ChatGeneration\n \n \n class NonBoolObj:\n@@ -203,3 +205,21 @@ def test_str() -> None:\n         non_bool=NonBoolObj(),\n     )\n     assert str(foo) == \"content='str' non_bool=NonBoolObj\"\n+\n+\n+def test_serialization_with_pydantic() -> None:\n+    class MyModel(BaseModel):\n+        x: int\n+        y: str\n+\n+    my_model = MyModel(x=1, y=\"hello\")\n+    llm_response = ChatGeneration(\n+        message=AIMessage(\n+            content='{\"x\": 1, \"y\": \"hello\"}', additional_kwargs={\"parsed\": my_model}\n+        )\n+    )\n+    ser = dumpd(llm_response)\n+    deser = load(ser)\n+    assert isinstance(deser, ChatGeneration)\n+    assert deser.message.content\n+    assert deser.message.additional_kwargs[\"parsed\"] == my_model.model_dump()"
      },
      {
        "filename": "libs/partners/openai/langchain_openai/chat_models/base.py",
        "content_before": "\"\"\"OpenAI chat wrapper.\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport json\nimport logging\nimport os\nimport sys\nimport warnings\nfrom io import BytesIO\nfrom math import ceil\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nimport openai\nimport tiktoken\nfrom langchain_core._api.deprecation import deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    ChatMessage,\n    ChatMessageChunk,\n    FunctionMessage,\n    FunctionMessageChunk,\n    HumanMessage,\n    HumanMessageChunk,\n    InvalidToolCall,\n    SystemMessage,\n    SystemMessageChunk,\n    ToolCall,\n    ToolMessage,\n    ToolMessageChunk,\n)\nfrom langchain_core.messages.ai import (\n    InputTokenDetails,\n    OutputTokenDetails,\n    UsageMetadata,\n)\nfrom langchain_core.messages.tool import tool_call_chunk\nfrom langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\nfrom langchain_core.output_parsers.openai_tools import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n    make_invalid_tool_call,\n    parse_tool_call,\n)\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough, chain\nfrom langchain_core.runnables.config import run_in_executor\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import get_pydantic_field_names\nfrom langchain_core.utils.function_calling import (\n    convert_to_openai_function,\n    convert_to_openai_tool,\n)\nfrom langchain_core.utils.pydantic import (\n    PydanticBaseModel,\n    TypeBaseModel,\n    is_basemodel_subclass,\n)\nfrom langchain_core.utils.utils import _build_model_kwargs, from_env, secret_from_env\nfrom pydantic import BaseModel, ConfigDict, Field, SecretStr, model_validator\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\ndef _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n    \"\"\"Convert a dictionary to a LangChain message.\n\n    Args:\n        _dict: The dictionary.\n\n    Returns:\n        The LangChain message.\n    \"\"\"\n    role = _dict.get(\"role\")\n    name = _dict.get(\"name\")\n    id_ = _dict.get(\"id\")\n    if role == \"user\":\n        return HumanMessage(content=_dict.get(\"content\", \"\"), id=id_, name=name)\n    elif role == \"assistant\":\n        # Fix for azure\n        # Also OpenAI returns None for tool invocations\n        content = _dict.get(\"content\", \"\") or \"\"\n        additional_kwargs: Dict = {}\n        if function_call := _dict.get(\"function_call\"):\n            additional_kwargs[\"function_call\"] = dict(function_call)\n        tool_calls = []\n        invalid_tool_calls = []\n        if raw_tool_calls := _dict.get(\"tool_calls\"):\n            additional_kwargs[\"tool_calls\"] = raw_tool_calls\n            for raw_tool_call in raw_tool_calls:\n                try:\n                    tool_calls.append(parse_tool_call(raw_tool_call, return_id=True))\n                except Exception as e:\n                    invalid_tool_calls.append(\n                        make_invalid_tool_call(raw_tool_call, str(e))\n                    )\n        if audio := _dict.get(\"audio\"):\n            additional_kwargs[\"audio\"] = audio\n        return AIMessage(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            name=name,\n            id=id_,\n            tool_calls=tool_calls,\n            invalid_tool_calls=invalid_tool_calls,\n        )\n    elif role in (\"system\", \"developer\"):\n        if role == \"developer\":\n            additional_kwargs = {\"__openai_role__\": role}\n        else:\n            additional_kwargs = {}\n        return SystemMessage(\n            content=_dict.get(\"content\", \"\"),\n            name=name,\n            id=id_,\n            additional_kwargs=additional_kwargs,\n        )\n    elif role == \"function\":\n        return FunctionMessage(\n            content=_dict.get(\"content\", \"\"), name=cast(str, _dict.get(\"name\")), id=id_\n        )\n    elif role == \"tool\":\n        additional_kwargs = {}\n        if \"name\" in _dict:\n            additional_kwargs[\"name\"] = _dict[\"name\"]\n        return ToolMessage(\n            content=_dict.get(\"content\", \"\"),\n            tool_call_id=cast(str, _dict.get(\"tool_call_id\")),\n            additional_kwargs=additional_kwargs,\n            name=name,\n            id=id_,\n        )\n    else:\n        return ChatMessage(content=_dict.get(\"content\", \"\"), role=role, id=id_)  # type: ignore[arg-type]\n\n\ndef _format_message_content(content: Any) -> Any:\n    \"\"\"Format message content.\"\"\"\n    if content and isinstance(content, list):\n        # Remove unexpected block types\n        formatted_content = []\n        for block in content:\n            if (\n                isinstance(block, dict)\n                and \"type\" in block\n                and block[\"type\"] == \"tool_use\"\n            ):\n                continue\n            else:\n                formatted_content.append(block)\n    else:\n        formatted_content = content\n\n    return formatted_content\n\n\ndef _convert_message_to_dict(message: BaseMessage) -> dict:\n    \"\"\"Convert a LangChain message to a dictionary.\n\n    Args:\n        message: The LangChain message.\n\n    Returns:\n        The dictionary.\n    \"\"\"\n    message_dict: Dict[str, Any] = {\"content\": _format_message_content(message.content)}\n    if (name := message.name or message.additional_kwargs.get(\"name\")) is not None:\n        message_dict[\"name\"] = name\n\n    # populate role and additional message data\n    if isinstance(message, ChatMessage):\n        message_dict[\"role\"] = message.role\n    elif isinstance(message, HumanMessage):\n        message_dict[\"role\"] = \"user\"\n    elif isinstance(message, AIMessage):\n        message_dict[\"role\"] = \"assistant\"\n        if \"function_call\" in message.additional_kwargs:\n            message_dict[\"function_call\"] = message.additional_kwargs[\"function_call\"]\n        if message.tool_calls or message.invalid_tool_calls:\n            message_dict[\"tool_calls\"] = [\n                _lc_tool_call_to_openai_tool_call(tc) for tc in message.tool_calls\n            ] + [\n                _lc_invalid_tool_call_to_openai_tool_call(tc)\n                for tc in message.invalid_tool_calls\n            ]\n        elif \"tool_calls\" in message.additional_kwargs:\n            message_dict[\"tool_calls\"] = message.additional_kwargs[\"tool_calls\"]\n            tool_call_supported_props = {\"id\", \"type\", \"function\"}\n            message_dict[\"tool_calls\"] = [\n                {k: v for k, v in tool_call.items() if k in tool_call_supported_props}\n                for tool_call in message_dict[\"tool_calls\"]\n            ]\n        else:\n            pass\n        # If tool calls present, content null value should be None not empty string.\n        if \"function_call\" in message_dict or \"tool_calls\" in message_dict:\n            message_dict[\"content\"] = message_dict[\"content\"] or None\n\n        if \"audio\" in message.additional_kwargs:\n            # openai doesn't support passing the data back - only the id\n            # https://platform.openai.com/docs/guides/audio/multi-turn-conversations\n            raw_audio = message.additional_kwargs[\"audio\"]\n            audio = (\n                {\"id\": message.additional_kwargs[\"audio\"][\"id\"]}\n                if \"id\" in raw_audio\n                else raw_audio\n            )\n            message_dict[\"audio\"] = audio\n    elif isinstance(message, SystemMessage):\n        message_dict[\"role\"] = message.additional_kwargs.get(\n            \"__openai_role__\", \"system\"\n        )\n    elif isinstance(message, FunctionMessage):\n        message_dict[\"role\"] = \"function\"\n    elif isinstance(message, ToolMessage):\n        message_dict[\"role\"] = \"tool\"\n        message_dict[\"tool_call_id\"] = message.tool_call_id\n\n        supported_props = {\"content\", \"role\", \"tool_call_id\"}\n        message_dict = {k: v for k, v in message_dict.items() if k in supported_props}\n    else:\n        raise TypeError(f\"Got unknown type {message}\")\n    return message_dict\n\n\ndef _convert_delta_to_message_chunk(\n    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n) -> BaseMessageChunk:\n    id_ = _dict.get(\"id\")\n    role = cast(str, _dict.get(\"role\"))\n    content = cast(str, _dict.get(\"content\") or \"\")\n    additional_kwargs: Dict = {}\n    if _dict.get(\"function_call\"):\n        function_call = dict(_dict[\"function_call\"])\n        if \"name\" in function_call and function_call[\"name\"] is None:\n            function_call[\"name\"] = \"\"\n        additional_kwargs[\"function_call\"] = function_call\n    tool_call_chunks = []\n    if raw_tool_calls := _dict.get(\"tool_calls\"):\n        additional_kwargs[\"tool_calls\"] = raw_tool_calls\n        try:\n            tool_call_chunks = [\n                tool_call_chunk(\n                    name=rtc[\"function\"].get(\"name\"),\n                    args=rtc[\"function\"].get(\"arguments\"),\n                    id=rtc.get(\"id\"),\n                    index=rtc[\"index\"],\n                )\n                for rtc in raw_tool_calls\n            ]\n        except KeyError:\n            pass\n\n    if role == \"user\" or default_class == HumanMessageChunk:\n        return HumanMessageChunk(content=content, id=id_)\n    elif role == \"assistant\" or default_class == AIMessageChunk:\n        return AIMessageChunk(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            id=id_,\n            tool_call_chunks=tool_call_chunks,  # type: ignore[arg-type]\n        )\n    elif role in (\"system\", \"developer\") or default_class == SystemMessageChunk:\n        if role == \"developer\":\n            additional_kwargs = {\"__openai_role__\": \"developer\"}\n        else:\n            additional_kwargs = {}\n        return SystemMessageChunk(\n            content=content, id=id_, additional_kwargs=additional_kwargs\n        )\n    elif role == \"function\" or default_class == FunctionMessageChunk:\n        return FunctionMessageChunk(content=content, name=_dict[\"name\"], id=id_)\n    elif role == \"tool\" or default_class == ToolMessageChunk:\n        return ToolMessageChunk(\n            content=content, tool_call_id=_dict[\"tool_call_id\"], id=id_\n        )\n    elif role or default_class == ChatMessageChunk:\n        return ChatMessageChunk(content=content, role=role, id=id_)\n    else:\n        return default_class(content=content, id=id_)  # type: ignore\n\n\ndef _update_token_usage(\n    overall_token_usage: Union[int, dict], new_usage: Union[int, dict]\n) -> Union[int, dict]:\n    # Token usage is either ints or dictionaries\n    # `reasoning_tokens` is nested inside `completion_tokens_details`\n    if isinstance(new_usage, int):\n        if not isinstance(overall_token_usage, int):\n            raise ValueError(\n                f\"Got different types for token usage: \"\n                f\"{type(new_usage)} and {type(overall_token_usage)}\"\n            )\n        return new_usage + overall_token_usage\n    elif isinstance(new_usage, dict):\n        if not isinstance(overall_token_usage, dict):\n            raise ValueError(\n                f\"Got different types for token usage: \"\n                f\"{type(new_usage)} and {type(overall_token_usage)}\"\n            )\n        return {\n            k: _update_token_usage(overall_token_usage.get(k, 0), v)\n            for k, v in new_usage.items()\n        }\n    else:\n        warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n        return new_usage\n\n\ndef _handle_openai_bad_request(e: openai.BadRequestError) -> None:\n    if (\n        \"'response_format' of type 'json_schema' is not supported with this model\"\n    ) in e.message:\n        message = (\n            \"This model does not support OpenAI's structured output feature, which \"\n            \"is the default method for `with_structured_output` as of \"\n            \"langchain-openai==0.3. To use `with_structured_output` with this model, \"\n            'specify `method=\"function_calling\"`.'\n        )\n        warnings.warn(message)\n        raise e\n    elif \"Invalid schema for response_format\" in e.message:\n        message = (\n            \"Invalid schema for OpenAI's structured output feature, which is the \"\n            \"default method for `with_structured_output` as of langchain-openai==0.3. \"\n            'Specify `method=\"function_calling\"` instead or update your schema. '\n            \"See supported schemas: \"\n            \"https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\"  # noqa: E501\n        )\n        warnings.warn(message)\n        raise e\n    else:\n        raise\n\n\nclass _FunctionCall(TypedDict):\n    name: str\n\n\n_BM = TypeVar(\"_BM\", bound=BaseModel)\n_DictOrPydanticClass = Union[Dict[str, Any], Type[_BM], Type]\n_DictOrPydantic = Union[Dict, _BM]\n\n\nclass _AllReturnType(TypedDict):\n    raw: BaseMessage\n    parsed: Optional[_DictOrPydantic]\n    parsing_error: Optional[BaseException]\n\n\nclass BaseChatOpenAI(BaseChatModel):\n    client: Any = Field(default=None, exclude=True)  #: :meta private:\n    async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    root_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    root_async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    model_name: str = Field(default=\"gpt-3.5-turbo\", alias=\"model\")\n    \"\"\"Model name to use.\"\"\"\n    temperature: Optional[float] = None\n    \"\"\"What sampling temperature to use.\"\"\"\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n    openai_api_key: Optional[SecretStr] = Field(\n        alias=\"api_key\", default_factory=secret_from_env(\"OPENAI_API_KEY\", default=None)\n    )\n    openai_api_base: Optional[str] = Field(default=None, alias=\"base_url\")\n    \"\"\"Base URL path for API requests, leave blank if not using a proxy or service \n        emulator.\"\"\"\n    openai_organization: Optional[str] = Field(default=None, alias=\"organization\")\n    \"\"\"Automatically inferred from env var `OPENAI_ORG_ID` if not provided.\"\"\"\n    # to support explicit proxy for OpenAI\n    openai_proxy: Optional[str] = Field(\n        default_factory=from_env(\"OPENAI_PROXY\", default=None)\n    )\n    request_timeout: Union[float, Tuple[float, float], Any, None] = Field(\n        default=None, alias=\"timeout\"\n    )\n    \"\"\"Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or \n        None.\"\"\"\n    max_retries: Optional[int] = None\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\n    presence_penalty: Optional[float] = None\n    \"\"\"Penalizes repeated tokens.\"\"\"\n    frequency_penalty: Optional[float] = None\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\n    seed: Optional[int] = None\n    \"\"\"Seed for generation\"\"\"\n    logprobs: Optional[bool] = None\n    \"\"\"Whether to return logprobs.\"\"\"\n    top_logprobs: Optional[int] = None\n    \"\"\"Number of most likely tokens to return at each token position, each with\n     an associated log probability. `logprobs` must be set to true \n     if this parameter is used.\"\"\"\n    logit_bias: Optional[Dict[int, int]] = None\n    \"\"\"Modify the likelihood of specified tokens appearing in the completion.\"\"\"\n    streaming: bool = False\n    \"\"\"Whether to stream the results or not.\"\"\"\n    n: Optional[int] = None\n    \"\"\"Number of chat completions to generate for each prompt.\"\"\"\n    top_p: Optional[float] = None\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n    max_tokens: Optional[int] = Field(default=None)\n    \"\"\"Maximum number of tokens to generate.\"\"\"\n    reasoning_effort: Optional[str] = None\n    \"\"\"Constrains effort on reasoning for reasoning models. \n    \n    Reasoning models only, like OpenAI o1 and o3-mini.\n\n    Currently supported values are low, medium, and high. Reducing reasoning effort \n    can result in faster responses and fewer tokens used on reasoning in a response.\n    \n    .. versionadded:: 0.2.14\n    \"\"\"\n    tiktoken_model_name: Optional[str] = None\n    \"\"\"The model name to pass to tiktoken when using this class. \n    Tiktoken is used to count the number of tokens in documents to constrain \n    them to be under a certain limit. By default, when set to None, this will \n    be the same as the embedding model name. However, there are some cases \n    where you may want to use this Embedding class with a model name not \n    supported by tiktoken. This can include when using Azure embeddings or \n    when using one of the many model providers that expose an OpenAI-like \n    API but with different models. In those cases, in order to avoid erroring \n    when tiktoken is called, you can specify a model name to use here.\"\"\"\n    default_headers: Union[Mapping[str, str], None] = None\n    default_query: Union[Mapping[str, object], None] = None\n    # Configure a custom httpx client. See the\n    # [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n    http_client: Union[Any, None] = Field(default=None, exclude=True)\n    \"\"\"Optional httpx.Client. Only used for sync invocations. Must specify \n        http_async_client as well if you'd like a custom client for async invocations.\n    \"\"\"\n    http_async_client: Union[Any, None] = Field(default=None, exclude=True)\n    \"\"\"Optional httpx.AsyncClient. Only used for async invocations. Must specify \n        http_client as well if you'd like a custom client for sync invocations.\"\"\"\n    stop: Optional[Union[List[str], str]] = Field(default=None, alias=\"stop_sequences\")\n    \"\"\"Default stop sequences.\"\"\"\n    extra_body: Optional[Mapping[str, Any]] = None\n    \"\"\"Optional additional JSON properties to include in the request parameters when\n    making requests to OpenAI compatible APIs, such as vLLM.\"\"\"\n    include_response_headers: bool = False\n    \"\"\"Whether to include response headers in the output message response_metadata.\"\"\"\n    disabled_params: Optional[Dict[str, Any]] = Field(default=None)\n    \"\"\"Parameters of the OpenAI client or chat.completions endpoint that should be \n    disabled for the given model.\n    \n    Should be specified as ``{\"param\": None | ['val1', 'val2']}`` where the key is the \n    parameter and the value is either None, meaning that parameter should never be\n    used, or it's a list of disabled values for the parameter.\n    \n    For example, older models may not support the 'parallel_tool_calls' parameter at \n    all, in which case ``disabled_params={\"parallel_tool_calls\": None}`` can ben passed \n    in.\n    \n    If a parameter is disabled then it will not be used by default in any methods, e.g.\n    in :meth:`~langchain_openai.chat_models.base.ChatOpenAI.with_structured_output`.\n    However this does not prevent a user from directly passed in the parameter during\n    invocation. \n    \"\"\"\n\n    model_config = ConfigDict(populate_by_name=True)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict[str, Any]) -> Any:\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_temperature(cls, values: Dict[str, Any]) -> Any:\n        \"\"\"Currently o1 models only allow temperature=1.\"\"\"\n        model = values.get(\"model_name\") or values.get(\"model\") or \"\"\n        if model.startswith(\"o1\") and \"temperature\" not in values:\n            values[\"temperature\"] = 1\n        return values\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n        if self.n is not None and self.n < 1:\n            raise ValueError(\"n must be at least 1.\")\n        elif self.n is not None and self.n > 1 and self.streaming:\n            raise ValueError(\"n must be 1 when streaming.\")\n\n        # Check OPENAI_ORGANIZATION for backwards compatibility.\n        self.openai_organization = (\n            self.openai_organization\n            or os.getenv(\"OPENAI_ORG_ID\")\n            or os.getenv(\"OPENAI_ORGANIZATION\")\n        )\n        self.openai_api_base = self.openai_api_base or os.getenv(\"OPENAI_API_BASE\")\n        client_params: dict = {\n            \"api_key\": (\n                self.openai_api_key.get_secret_value() if self.openai_api_key else None\n            ),\n            \"organization\": self.openai_organization,\n            \"base_url\": self.openai_api_base,\n            \"timeout\": self.request_timeout,\n            \"default_headers\": self.default_headers,\n            \"default_query\": self.default_query,\n        }\n        if self.max_retries is not None:\n            client_params[\"max_retries\"] = self.max_retries\n\n        if self.openai_proxy and (self.http_client or self.http_async_client):\n            openai_proxy = self.openai_proxy\n            http_client = self.http_client\n            http_async_client = self.http_async_client\n            raise ValueError(\n                \"Cannot specify 'openai_proxy' if one of \"\n                \"'http_client'/'http_async_client' is already specified. Received:\\n\"\n                f\"{openai_proxy=}\\n{http_client=}\\n{http_async_client=}\"\n            )\n        if not self.client:\n            if self.openai_proxy and not self.http_client:\n                try:\n                    import httpx\n                except ImportError as e:\n                    raise ImportError(\n                        \"Could not import httpx python package. \"\n                        \"Please install it with `pip install httpx`.\"\n                    ) from e\n                self.http_client = httpx.Client(proxy=self.openai_proxy)\n            sync_specific = {\"http_client\": self.http_client}\n            self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]\n            self.client = self.root_client.chat.completions\n        if not self.async_client:\n            if self.openai_proxy and not self.http_async_client:\n                try:\n                    import httpx\n                except ImportError as e:\n                    raise ImportError(\n                        \"Could not import httpx python package. \"\n                        \"Please install it with `pip install httpx`.\"\n                    ) from e\n                self.http_async_client = httpx.AsyncClient(proxy=self.openai_proxy)\n            async_specific = {\"http_client\": self.http_async_client}\n            self.root_async_client = openai.AsyncOpenAI(\n                **client_params,\n                **async_specific,  # type: ignore[arg-type]\n            )\n            self.async_client = self.root_async_client.chat.completions\n        return self\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        exclude_if_none = {\n            \"presence_penalty\": self.presence_penalty,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"seed\": self.seed,\n            \"top_p\": self.top_p,\n            \"logprobs\": self.logprobs,\n            \"top_logprobs\": self.top_logprobs,\n            \"logit_bias\": self.logit_bias,\n            \"stop\": self.stop or None,  # also exclude empty list for this\n            \"max_tokens\": self.max_tokens,\n            \"extra_body\": self.extra_body,\n            \"n\": self.n,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n        }\n\n        params = {\n            \"model\": self.model_name,\n            \"stream\": self.streaming,\n            **{k: v for k, v in exclude_if_none.items() if v is not None},\n            **self.model_kwargs,\n        }\n\n        return params\n\n    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:\n        overall_token_usage: dict = {}\n        system_fingerprint = None\n        for output in llm_outputs:\n            if output is None:\n                # Happens in streaming\n                continue\n            token_usage = output[\"token_usage\"]\n            if token_usage is not None:\n                for k, v in token_usage.items():\n                    if v is None:\n                        continue\n                    if k in overall_token_usage:\n                        overall_token_usage[k] = _update_token_usage(\n                            overall_token_usage[k], v\n                        )\n                    else:\n                        overall_token_usage[k] = v\n            if system_fingerprint is None:\n                system_fingerprint = output.get(\"system_fingerprint\")\n        combined = {\"token_usage\": overall_token_usage, \"model_name\": self.model_name}\n        if system_fingerprint:\n            combined[\"system_fingerprint\"] = system_fingerprint\n        return combined\n\n    def _convert_chunk_to_generation_chunk(\n        self,\n        chunk: dict,\n        default_chunk_class: Type,\n        base_generation_info: Optional[Dict],\n    ) -> Optional[ChatGenerationChunk]:\n        if chunk.get(\"type\") == \"content.delta\":  # from beta.chat.completions.stream\n            return None\n        token_usage = chunk.get(\"usage\")\n        choices = (\n            chunk.get(\"choices\", [])\n            # from beta.chat.completions.stream\n            or chunk.get(\"chunk\", {}).get(\"choices\", [])\n        )\n\n        usage_metadata: Optional[UsageMetadata] = (\n            _create_usage_metadata(token_usage) if token_usage else None\n        )\n        if len(choices) == 0:\n            # logprobs is implicitly None\n            generation_chunk = ChatGenerationChunk(\n                message=default_chunk_class(content=\"\", usage_metadata=usage_metadata)\n            )\n            return generation_chunk\n\n        choice = choices[0]\n        if choice[\"delta\"] is None:\n            return None\n\n        message_chunk = _convert_delta_to_message_chunk(\n            choice[\"delta\"], default_chunk_class\n        )\n        generation_info = {**base_generation_info} if base_generation_info else {}\n\n        if finish_reason := choice.get(\"finish_reason\"):\n            generation_info[\"finish_reason\"] = finish_reason\n            if model_name := chunk.get(\"model\"):\n                generation_info[\"model_name\"] = model_name\n            if system_fingerprint := chunk.get(\"system_fingerprint\"):\n                generation_info[\"system_fingerprint\"] = system_fingerprint\n\n        logprobs = choice.get(\"logprobs\")\n        if logprobs:\n            generation_info[\"logprobs\"] = logprobs\n\n        if usage_metadata and isinstance(message_chunk, AIMessageChunk):\n            message_chunk.usage_metadata = usage_metadata\n\n        generation_chunk = ChatGenerationChunk(\n            message=message_chunk, generation_info=generation_info or None\n        )\n        return generation_chunk\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        base_generation_info = {}\n\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            response_stream = self.root_client.beta.chat.completions.stream(**payload)\n            context_manager = response_stream\n        else:\n            if self.include_response_headers:\n                raw_response = self.client.with_raw_response.create(**payload)\n                response = raw_response.parse()\n                base_generation_info = {\"headers\": dict(raw_response.headers)}\n            else:\n                response = self.client.create(**payload)\n            context_manager = response\n        try:\n            with context_manager as response:\n                is_first_chunk = True\n                for chunk in response:\n                    if not isinstance(chunk, dict):\n                        chunk = chunk.model_dump()\n                    generation_chunk = self._convert_chunk_to_generation_chunk(\n                        chunk,\n                        default_chunk_class,\n                        base_generation_info if is_first_chunk else {},\n                    )\n                    if generation_chunk is None:\n                        continue\n                    default_chunk_class = generation_chunk.message.__class__\n                    logprobs = (generation_chunk.generation_info or {}).get(\"logprobs\")\n                    if run_manager:\n                        run_manager.on_llm_new_token(\n                            generation_chunk.text,\n                            chunk=generation_chunk,\n                            logprobs=logprobs,\n                        )\n                    is_first_chunk = False\n                    yield generation_chunk\n        except openai.BadRequestError as e:\n            _handle_openai_bad_request(e)\n        if hasattr(response, \"get_final_completion\") and \"response_format\" in payload:\n            final_completion = response.get_final_completion()\n            generation_chunk = self._get_generation_chunk_from_completion(\n                final_completion\n            )\n            if run_manager:\n                run_manager.on_llm_new_token(\n                    generation_chunk.text, chunk=generation_chunk\n                )\n            yield generation_chunk\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        generation_info = None\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            try:\n                response = self.root_client.beta.chat.completions.parse(**payload)\n            except openai.BadRequestError as e:\n                _handle_openai_bad_request(e)\n        elif self.include_response_headers:\n            raw_response = self.client.with_raw_response.create(**payload)\n            response = raw_response.parse()\n            generation_info = {\"headers\": dict(raw_response.headers)}\n        else:\n            response = self.client.create(**payload)\n        return self._create_chat_result(response, generation_info)\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        messages = self._convert_input(input_).to_messages()\n        if stop is not None:\n            kwargs[\"stop\"] = stop\n\n        return {\n            \"messages\": [_convert_message_to_dict(m) for m in messages],\n            **self._default_params,\n            **kwargs,\n        }\n\n    def _create_chat_result(\n        self,\n        response: Union[dict, openai.BaseModel],\n        generation_info: Optional[Dict] = None,\n    ) -> ChatResult:\n        generations = []\n\n        response_dict = (\n            response if isinstance(response, dict) else response.model_dump()\n        )\n        # Sometimes the AI Model calling will get error, we should raise it.\n        # Otherwise, the next code 'choices.extend(response[\"choices\"])'\n        # will throw a \"TypeError: 'NoneType' object is not iterable\" error\n        # to mask the true error. Because 'response[\"choices\"]' is None.\n        if response_dict.get(\"error\"):\n            raise ValueError(response_dict.get(\"error\"))\n\n        token_usage = response_dict.get(\"usage\")\n        for res in response_dict[\"choices\"]:\n            message = _convert_dict_to_message(res[\"message\"])\n            if token_usage and isinstance(message, AIMessage):\n                message.usage_metadata = _create_usage_metadata(token_usage)\n            generation_info = generation_info or {}\n            generation_info[\"finish_reason\"] = (\n                res.get(\"finish_reason\")\n                if res.get(\"finish_reason\") is not None\n                else generation_info.get(\"finish_reason\")\n            )\n            if \"logprobs\" in res:\n                generation_info[\"logprobs\"] = res[\"logprobs\"]\n            gen = ChatGeneration(message=message, generation_info=generation_info)\n            generations.append(gen)\n        llm_output = {\n            \"token_usage\": token_usage,\n            \"model_name\": response_dict.get(\"model\", self.model_name),\n            \"system_fingerprint\": response_dict.get(\"system_fingerprint\", \"\"),\n        }\n\n        if isinstance(response, openai.BaseModel) and getattr(\n            response, \"choices\", None\n        ):\n            message = response.choices[0].message  # type: ignore[attr-defined]\n            if hasattr(message, \"parsed\"):\n                generations[0].message.additional_kwargs[\"parsed\"] = message.parsed\n            if hasattr(message, \"refusal\"):\n                generations[0].message.additional_kwargs[\"refusal\"] = message.refusal\n\n        return ChatResult(generations=generations, llm_output=llm_output)\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        base_generation_info = {}\n\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            response_stream = self.root_async_client.beta.chat.completions.stream(\n                **payload\n            )\n            context_manager = response_stream\n        else:\n            if self.include_response_headers:\n                raw_response = await self.async_client.with_raw_response.create(\n                    **payload\n                )\n                response = raw_response.parse()\n                base_generation_info = {\"headers\": dict(raw_response.headers)}\n            else:\n                response = await self.async_client.create(**payload)\n            context_manager = response\n        try:\n            async with context_manager as response:\n                is_first_chunk = True\n                async for chunk in response:\n                    if not isinstance(chunk, dict):\n                        chunk = chunk.model_dump()\n                    generation_chunk = self._convert_chunk_to_generation_chunk(\n                        chunk,\n                        default_chunk_class,\n                        base_generation_info if is_first_chunk else {},\n                    )\n                    if generation_chunk is None:\n                        continue\n                    default_chunk_class = generation_chunk.message.__class__\n                    logprobs = (generation_chunk.generation_info or {}).get(\"logprobs\")\n                    if run_manager:\n                        await run_manager.on_llm_new_token(\n                            generation_chunk.text,\n                            chunk=generation_chunk,\n                            logprobs=logprobs,\n                        )\n                    is_first_chunk = False\n                    yield generation_chunk\n        except openai.BadRequestError as e:\n            _handle_openai_bad_request(e)\n        if hasattr(response, \"get_final_completion\") and \"response_format\" in payload:\n            final_completion = await response.get_final_completion()\n            generation_chunk = self._get_generation_chunk_from_completion(\n                final_completion\n            )\n            if run_manager:\n                await run_manager.on_llm_new_token(\n                    generation_chunk.text, chunk=generation_chunk\n                )\n            yield generation_chunk\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        generation_info = None\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            try:\n                response = await self.root_async_client.beta.chat.completions.parse(\n                    **payload\n                )\n            except openai.BadRequestError as e:\n                _handle_openai_bad_request(e)\n        elif self.include_response_headers:\n            raw_response = await self.async_client.with_raw_response.create(**payload)\n            response = raw_response.parse()\n            generation_info = {\"headers\": dict(raw_response.headers)}\n        else:\n            response = await self.async_client.create(**payload)\n        return await run_in_executor(\n            None, self._create_chat_result, response, generation_info\n        )\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model_name\": self.model_name, **self._default_params}\n\n    def _get_invocation_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> Dict[str, Any]:\n        \"\"\"Get the parameters used to invoke the model.\"\"\"\n        return {\n            \"model\": self.model_name,\n            **super()._get_invocation_params(stop=stop),\n            **self._default_params,\n            **kwargs,\n        }\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"openai\",\n            ls_model_name=self.model_name,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens) or params.get(\n            \"max_completion_tokens\", self.max_tokens\n        ):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"openai-chat\"\n\n    def _get_encoding_model(self) -> Tuple[str, tiktoken.Encoding]:\n        if self.tiktoken_model_name is not None:\n            model = self.tiktoken_model_name\n        else:\n            model = self.model_name\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n        except KeyError:\n            model = \"cl100k_base\"\n            encoding = tiktoken.get_encoding(model)\n        return model, encoding\n\n    def get_token_ids(self, text: str) -> List[int]:\n        \"\"\"Get the tokens present in the text with tiktoken package.\"\"\"\n        if self.custom_get_token_ids is not None:\n            return self.custom_get_token_ids(text)\n        # tiktoken NOT supported for Python 3.7 or below\n        if sys.version_info[1] <= 7:\n            return super().get_token_ids(text)\n        _, encoding_model = self._get_encoding_model()\n        return encoding_model.encode(text)\n\n    def get_num_tokens_from_messages(\n        self,\n        messages: List[BaseMessage],\n        tools: Optional[\n            Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]\n        ] = None,\n    ) -> int:\n        \"\"\"Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n\n        **Requirements**: You must have the ``pillow`` installed if you want to count\n        image tokens if you are specifying the image as a base64 string, and you must\n        have both ``pillow`` and ``httpx`` installed if you are specifying the image\n        as a URL. If these aren't installed image inputs will be ignored in token\n        counting.\n\n        OpenAI reference: https://github.com/openai/openai-cookbook/blob/\n        main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n        \"\"\"\n        # TODO: Count bound tools as part of input.\n        if tools is not None:\n            warnings.warn(\n                \"Counting tokens in tool schemas is not yet supported. Ignoring tools.\"\n            )\n        if sys.version_info[1] <= 7:\n            return super().get_num_tokens_from_messages(messages)\n        model, encoding = self._get_encoding_model()\n        if model.startswith(\"gpt-3.5-turbo-0301\"):\n            # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            tokens_per_message = 4\n            # if there's a name, the role is omitted\n            tokens_per_name = -1\n        elif model.startswith(\"gpt-3.5-turbo\") or model.startswith(\"gpt-4\"):\n            tokens_per_message = 3\n            tokens_per_name = 1\n        else:\n            raise NotImplementedError(\n                f\"get_num_tokens_from_messages() is not presently implemented \"\n                f\"for model {model}. See \"\n                \"https://platform.openai.com/docs/guides/text-generation/managing-tokens\"  # noqa: E501\n                \" for information on how messages are converted to tokens.\"\n            )\n        num_tokens = 0\n        messages_dict = [_convert_message_to_dict(m) for m in messages]\n        for message in messages_dict:\n            num_tokens += tokens_per_message\n            for key, value in message.items():\n                # This is an inferred approximation. OpenAI does not document how to\n                # count tool message tokens.\n                if key == \"tool_call_id\":\n                    num_tokens += 3\n                    continue\n                if isinstance(value, list):\n                    # content or tool calls\n                    for val in value:\n                        if isinstance(val, str) or val[\"type\"] == \"text\":\n                            text = val[\"text\"] if isinstance(val, dict) else val\n                            num_tokens += len(encoding.encode(text))\n                        elif val[\"type\"] == \"image_url\":\n                            if val[\"image_url\"].get(\"detail\") == \"low\":\n                                num_tokens += 85\n                            else:\n                                image_size = _url_to_size(val[\"image_url\"][\"url\"])\n                                if not image_size:\n                                    continue\n                                num_tokens += _count_image_tokens(*image_size)\n                        # Tool/function call token counting is not documented by OpenAI.\n                        # This is an approximation.\n                        elif val[\"type\"] == \"function\":\n                            num_tokens += len(\n                                encoding.encode(val[\"function\"][\"arguments\"])\n                            )\n                            num_tokens += len(encoding.encode(val[\"function\"][\"name\"]))\n                        else:\n                            raise ValueError(\n                                f\"Unrecognized content block type\\n\\n{val}\"\n                            )\n                elif not value:\n                    continue\n                else:\n                    # Cast str(value) in case the message value is not a string\n                    # This occurs with function messages\n                    num_tokens += len(encoding.encode(str(value)))\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        # every reply is primed with <im_start>assistant\n        num_tokens += 3\n        return num_tokens\n\n    @deprecated(\n        since=\"0.2.1\",\n        alternative=\"langchain_openai.chat_models.base.ChatOpenAI.bind_tools\",\n        removal=\"1.0.0\",\n    )\n    def bind_functions(\n        self,\n        functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n        function_call: Optional[\n            Union[_FunctionCall, str, Literal[\"auto\", \"none\"]]\n        ] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind functions (and other objects) to this chat model.\n\n        Assumes model is compatible with OpenAI function-calling API.\n\n        NOTE: Using bind_tools is recommended instead, as the `functions` and\n            `function_call` request parameters are officially marked as deprecated by\n            OpenAI.\n\n        Args:\n            functions: A list of function definitions to bind to this chat model.\n                Can be  a dictionary, pydantic model, or callable. Pydantic\n                models and callables will be automatically converted to\n                their schema dictionary representation.\n            function_call: Which function to require the model to call.\n                Must be the name of the single provided function or\n                \"auto\" to automatically determine which function to call\n                (if any).\n            **kwargs: Any additional parameters to pass to the\n                :class:`~langchain.runnable.Runnable` constructor.\n        \"\"\"\n\n        formatted_functions = [convert_to_openai_function(fn) for fn in functions]\n        if function_call is not None:\n            function_call = (\n                {\"name\": function_call}\n                if isinstance(function_call, str)\n                and function_call not in (\"auto\", \"none\")\n                else function_call\n            )\n            if isinstance(function_call, dict) and len(formatted_functions) != 1:\n                raise ValueError(\n                    \"When specifying `function_call`, you must provide exactly one \"\n                    \"function.\"\n                )\n            if (\n                isinstance(function_call, dict)\n                and formatted_functions[0][\"name\"] != function_call[\"name\"]\n            ):\n                raise ValueError(\n                    f\"Function call {function_call} was specified, but the only \"\n                    f\"provided function was {formatted_functions[0]['name']}.\"\n                )\n            kwargs = {**kwargs, \"function_call\": function_call}\n        return super().bind(functions=formatted_functions, **kwargs)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[\n            Union[dict, str, Literal[\"auto\", \"none\", \"required\", \"any\"], bool]\n        ] = None,\n        strict: Optional[bool] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        Assumes model is compatible with OpenAI tool-calling API.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports any tool definition handled by\n                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call. Options are:\n\n                - str of the form ``\"<<tool_name>>\"``: calls <<tool_name>> tool.\n                - ``\"auto\"``: automatically selects a tool (including no tool).\n                - ``\"none\"``: does not call a tool.\n                - ``\"any\"`` or ``\"required\"`` or ``True``: force at least one tool to be called.\n                - dict of the form ``{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}``: calls <<tool_name>> tool.\n                - ``False`` or ``None``: no effect, default OpenAI behavior.\n            strict: If True, model output is guaranteed to exactly match the JSON Schema\n                provided in the tool definition. If True, the input schema will be\n                validated according to\n                https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n                If False, input schema will not be validated and model output will not\n                be validated.\n                If None, ``strict`` argument will not be passed to the model.\n            parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n                Defaults to ``None`` (no specification, which allows parallel tool use).\n            kwargs: Any additional parameters are passed directly to\n                :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind`.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n\n        \"\"\"  # noqa: E501\n\n        if parallel_tool_calls is not None:\n            kwargs[\"parallel_tool_calls\"] = parallel_tool_calls\n        formatted_tools = [\n            convert_to_openai_tool(tool, strict=strict) for tool in tools\n        ]\n        if tool_choice:\n            if isinstance(tool_choice, str):\n                # tool_choice is a tool/function name\n                if tool_choice not in (\"auto\", \"none\", \"any\", \"required\"):\n                    tool_choice = {\n                        \"type\": \"function\",\n                        \"function\": {\"name\": tool_choice},\n                    }\n                # 'any' is not natively supported by OpenAI API.\n                # We support 'any' since other models use this instead of 'required'.\n                if tool_choice == \"any\":\n                    tool_choice = \"required\"\n            elif isinstance(tool_choice, bool):\n                tool_choice = \"required\"\n            elif isinstance(tool_choice, dict):\n                tool_names = [\n                    formatted_tool[\"function\"][\"name\"]\n                    for formatted_tool in formatted_tools\n                ]\n                if not any(\n                    tool_name == tool_choice[\"function\"][\"name\"]\n                    for tool_name in tool_names\n                ):\n                    raise ValueError(\n                        f\"Tool choice {tool_choice} was specified, but the only \"\n                        f\"provided tools were {tool_names}.\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n                    f\"Received: {tool_choice}\"\n                )\n            kwargs[\"tool_choice\"] = tool_choice\n        return super().bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Optional[_DictOrPydanticClass] = None,\n        *,\n        method: Literal[\n            \"function_calling\", \"json_mode\", \"json_schema\"\n        ] = \"function_calling\",\n        include_raw: bool = False,\n        strict: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, _DictOrPydantic]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n\n                - an OpenAI function/tool schema,\n                - a JSON Schema,\n                - a TypedDict class (support added in 0.1.20),\n                - or a Pydantic class.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"function_calling\":\n                    Uses OpenAI's tool-calling (formerly called function calling)\n                    API: https://platform.openai.com/docs/guides/function-calling\n                - \"json_schema\":\n                    Uses OpenAI's Structured Output API: https://platform.openai.com/docs/guides/structured-outputs\n                    Supported for \"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"o1\", and later\n                    models.\n                - \"json_mode\":\n                    Uses OpenAI's JSON mode. Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call:\n                    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\n                Learn more about the differences between the methods and which models\n                support which methods here:\n\n                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode\n                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            strict:\n\n                - True:\n                    Model output is guaranteed to exactly match the schema.\n                    The input schema will also be validated according to\n                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n                - False:\n                    Input schema will not be validated and model output will not be\n                    validated.\n                - None:\n                    ``strict`` argument will not be passed to the model.\n\n            kwargs: Additional keyword args aren't supported.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n\n            - \"raw\": BaseMessage\n            - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n            - \"parsing_error\": Optional[BaseException]\n\n        .. versionchanged:: 0.1.20\n\n            Added support for TypedDict class ``schema``.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n            Support for ``method`` = \"json_schema\" added.\n        \"\"\"  # noqa: E501\n        if kwargs:\n            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n        if strict is not None and method == \"json_mode\":\n            raise ValueError(\n                \"Argument `strict` is not supported with `method`='json_mode'\"\n            )\n        is_pydantic_schema = _is_pydantic_class(schema)\n\n        if method == \"json_schema\":\n            # Check for Pydantic BaseModel V1\n            if (\n                is_pydantic_schema and issubclass(schema, BaseModelV1)  # type: ignore[arg-type]\n            ):\n                warnings.warn(\n                    \"Received a Pydantic BaseModel V1 schema. This is not supported by \"\n                    'method=\"json_schema\". Please use method=\"function_calling\" '\n                    \"or specify schema via JSON Schema or Pydantic V2 BaseModel. \"\n                    'Overriding to method=\"function_calling\".'\n                )\n                method = \"function_calling\"\n            # Check for incompatible model\n            if self.model_name and (\n                self.model_name.startswith(\"gpt-3\")\n                or self.model_name.startswith(\"gpt-4-\")\n                or self.model_name == \"gpt-4\"\n            ):\n                warnings.warn(\n                    f\"Cannot use method='json_schema' with model {self.model_name} \"\n                    f\"since it doesn't support OpenAI's Structured Output API. You can \"\n                    f\"see supported models here: \"\n                    f\"https://platform.openai.com/docs/guides/structured-outputs#supported-models. \"  # noqa: E501\n                    \"To fix this warning, set `method='function_calling'. \"\n                    \"Overriding to method='function_calling'.\"\n                )\n                method = \"function_calling\"\n\n        if method == \"function_calling\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is not 'json_mode'. \"\n                    \"Received None.\"\n                )\n            tool_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n            bind_kwargs = self._filter_disabled_params(\n                tool_choice=tool_name,\n                parallel_tool_calls=False,\n                strict=strict,\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": schema,\n                },\n            )\n\n            llm = self.bind_tools([schema], **bind_kwargs)\n            if is_pydantic_schema:\n                output_parser: Runnable = PydanticToolsParser(\n                    tools=[schema],  # type: ignore[list-item]\n                    first_tool_only=True,  # type: ignore[list-item]\n                )\n            else:\n                output_parser = JsonOutputKeyToolsParser(\n                    key_name=tool_name, first_tool_only=True\n                )\n        elif method == \"json_mode\":\n            llm = self.bind(\n                response_format={\"type\": \"json_object\"},\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": schema,\n                },\n            )\n            output_parser = (\n                PydanticOutputParser(pydantic_object=schema)  # type: ignore[arg-type]\n                if is_pydantic_schema\n                else JsonOutputParser()\n            )\n        elif method == \"json_schema\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is not 'json_mode'. \"\n                    \"Received None.\"\n                )\n            response_format = _convert_to_openai_response_format(schema, strict=strict)\n            llm = self.bind(\n                response_format=response_format,\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": convert_to_openai_tool(schema),\n                },\n            )\n            if is_pydantic_schema:\n                output_parser = _oai_structured_outputs_parser.with_types(\n                    output_type=cast(type, schema)\n                )\n            else:\n                output_parser = JsonOutputParser()\n        else:\n            raise ValueError(\n                f\"Unrecognized method argument. Expected one of 'function_calling' or \"\n                f\"'json_mode'. Received: '{method}'\"\n            )\n\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    def _filter_disabled_params(self, **kwargs: Any) -> Dict[str, Any]:\n        if not self.disabled_params:\n            return kwargs\n        filtered = {}\n        for k, v in kwargs.items():\n            # Skip param\n            if k in self.disabled_params and (\n                self.disabled_params[k] is None or v in self.disabled_params[k]\n            ):\n                continue\n            # Keep param\n            else:\n                filtered[k] = v\n        return filtered\n\n    def _get_generation_chunk_from_completion(\n        self, completion: openai.BaseModel\n    ) -> ChatGenerationChunk:\n        \"\"\"Get chunk from completion (e.g., from final completion of a stream).\"\"\"\n        chat_result = self._create_chat_result(completion)\n        chat_message = chat_result.generations[0].message\n        if isinstance(chat_message, AIMessage):\n            usage_metadata = chat_message.usage_metadata\n            # Skip tool_calls, already sent as chunks\n            if \"tool_calls\" in chat_message.additional_kwargs:\n                chat_message.additional_kwargs.pop(\"tool_calls\")\n        else:\n            usage_metadata = None\n        message = AIMessageChunk(\n            content=\"\",\n            additional_kwargs=chat_message.additional_kwargs,\n            usage_metadata=usage_metadata,\n        )\n        return ChatGenerationChunk(\n            message=message, generation_info=chat_result.llm_output\n        )\n\n\nclass ChatOpenAI(BaseChatOpenAI):  # type: ignore[override]\n    \"\"\"OpenAI chat model integration.\n\n    .. dropdown:: Setup\n        :open:\n\n        Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-openai\n            export OPENAI_API_KEY=\"your-api-key\"\n\n    .. dropdown:: Key init args \u2014 completion params\n\n        model: str\n            Name of OpenAI model to use.\n        temperature: float\n            Sampling temperature.\n        max_tokens: Optional[int]\n            Max number of tokens to generate.\n        logprobs: Optional[bool]\n            Whether to return logprobs.\n        stream_options: Dict\n            Configure streaming outputs, like whether to return token usage when\n            streaming (``{\"include_usage\": True}``).\n\n        See full list of supported init args and their descriptions in the params section.\n\n    .. dropdown:: Key init args \u2014 client params\n\n        timeout: Union[float, Tuple[float, float], Any, None]\n            Timeout for requests.\n        max_retries: Optional[int]\n            Max number of retries.\n        api_key: Optional[str]\n            OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.\n        base_url: Optional[str]\n            Base URL for API requests. Only specify if using a proxy or service\n            emulator.\n        organization: Optional[str]\n            OpenAI organization ID. If not passed in will be read from env\n            var OPENAI_ORG_ID.\n\n        See full list of supported init args and their descriptions in the params section.\n\n    .. dropdown:: Instantiate\n\n        .. code-block:: python\n\n            from langchain_openai import ChatOpenAI\n\n            llm = ChatOpenAI(\n                model=\"gpt-4o\",\n                temperature=0,\n                max_tokens=None,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # base_url=\"...\",\n                # organization=\"...\",\n                # other params...\n            )\n\n        **NOTE**: Any param which is not explicitly supported will be passed directly to the\n        ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is\n        invoked. For example:\n\n        .. code-block:: python\n\n            from langchain_openai import ChatOpenAI\n            import openai\n\n            ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\n\n            # results in underlying API call of:\n\n            openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\n\n            # which is also equivalent to:\n\n            ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\n\n    .. dropdown:: Invoke\n\n        .. code-block:: python\n\n            messages = [\n                (\n                    \"system\",\n                    \"You are a helpful translator. Translate the user sentence to French.\",\n                ),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n        .. code-block:: pycon\n\n            AIMessage(\n                content=\"J'adore la programmation.\",\n                response_metadata={\n                    \"token_usage\": {\n                        \"completion_tokens\": 5,\n                        \"prompt_tokens\": 31,\n                        \"total_tokens\": 36,\n                    },\n                    \"model_name\": \"gpt-4o\",\n                    \"system_fingerprint\": \"fp_43dfabdef1\",\n                    \"finish_reason\": \"stop\",\n                    \"logprobs\": None,\n                },\n                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n            )\n\n    .. dropdown:: Stream\n\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(\n                content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n            )\n            AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(\n                content=\"\",\n                response_metadata={\"finish_reason\": \"stop\"},\n                id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\n            )\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n        .. code-block:: python\n\n            AIMessageChunk(\n                content=\"J'adore la programmation.\",\n                response_metadata={\"finish_reason\": \"stop\"},\n                id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\n            )\n\n    .. dropdown:: Async\n\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n        .. code-block:: python\n\n            AIMessage(\n                content=\"J'adore la programmation.\",\n                response_metadata={\n                    \"token_usage\": {\n                        \"completion_tokens\": 5,\n                        \"prompt_tokens\": 31,\n                        \"total_tokens\": 36,\n                    },\n                    \"model_name\": \"gpt-4o\",\n                    \"system_fingerprint\": \"fp_43dfabdef1\",\n                    \"finish_reason\": \"stop\",\n                    \"logprobs\": None,\n                },\n                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n            )\n\n    .. dropdown:: Tool calling\n\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(\n                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n                )\n\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(\n                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n                )\n\n\n            llm_with_tools = llm.bind_tools(\n                [GetWeather, GetPopulation]\n                # strict = True  # enforce tool args schema is respected\n            )\n            ai_msg = llm_with_tools.invoke(\n                \"Which city is hotter today and which is bigger: LA or NY?\"\n            )\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\n                },\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"New York, NY\"},\n                    \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\n                },\n                {\n                    \"name\": \"GetPopulation\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\n                },\n                {\n                    \"name\": \"GetPopulation\",\n                    \"args\": {\"location\": \"New York, NY\"},\n                    \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\n                },\n            ]\n\n        Note that ``openai >= 1.32`` supports a ``parallel_tool_calls`` parameter\n        that defaults to ``True``. This parameter can be set to ``False`` to\n        disable parallel tool calls:\n\n        .. code-block:: python\n\n            ai_msg = llm_with_tools.invoke(\n                \"What is the weather in LA and NY?\", parallel_tool_calls=False\n            )\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\n                }\n            ]\n\n        Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model\n        using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by\n        setting ``model_kwargs``.\n\n        See ``ChatOpenAI.bind_tools()`` method for more.\n\n    .. dropdown:: Structured output\n\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        .. code-block:: python\n\n            Joke(\n                setup=\"Why was the cat sitting on the computer?\",\n                punchline=\"To keep an eye on the mouse!\",\n                rating=None,\n            )\n\n        See ``ChatOpenAI.with_structured_output()`` for more.\n\n    .. dropdown:: JSON mode\n\n        .. code-block:: python\n\n            json_llm = llm.bind(response_format={\"type\": \"json_object\"})\n            ai_msg = json_llm.invoke(\n                \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n            )\n            ai_msg.content\n\n        .. code-block:: python\n\n            '\\\\n{\\\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\\\n}'\n\n    .. dropdown:: Image input\n\n        .. code-block:: python\n\n            import base64\n            import httpx\n            from langchain_core.messages import HumanMessage\n\n            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n            message = HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                    },\n                ]\n            )\n            ai_msg = llm.invoke([message])\n            ai_msg.content\n\n        .. code-block:: python\n\n            \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\n\n    .. dropdown:: Token usage\n\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n\n        When streaming, set the ``stream_usage`` kwarg:\n\n        .. code-block:: python\n\n            stream = llm.stream(messages, stream_usage=True)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full.usage_metadata\n\n        .. code-block:: python\n\n            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n\n        Alternatively, setting ``stream_usage`` when instantiating the model can be\n        useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using\n        methods like ``.with_structured_output``, which generate chains under the\n        hood.\n\n        .. code-block:: python\n\n            llm = ChatOpenAI(model=\"gpt-4o\", stream_usage=True)\n            structured_llm = llm.with_structured_output(...)\n\n    .. dropdown:: Logprobs\n\n        .. code-block:: python\n\n            logprobs_llm = llm.bind(logprobs=True)\n            ai_msg = logprobs_llm.invoke(messages)\n            ai_msg.response_metadata[\"logprobs\"]\n\n        .. code-block:: python\n\n            {\n                \"content\": [\n                    {\n                        \"token\": \"J\",\n                        \"bytes\": [74],\n                        \"logprob\": -4.9617593e-06,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \"'adore\",\n                        \"bytes\": [39, 97, 100, 111, 114, 101],\n                        \"logprob\": -0.25202933,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \" la\",\n                        \"bytes\": [32, 108, 97],\n                        \"logprob\": -0.20141791,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \" programmation\",\n                        \"bytes\": [\n                            32,\n                            112,\n                            114,\n                            111,\n                            103,\n                            114,\n                            97,\n                            109,\n                            109,\n                            97,\n                            116,\n                            105,\n                            111,\n                            110,\n                        ],\n                        \"logprob\": -1.9361265e-07,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \".\",\n                        \"bytes\": [46],\n                        \"logprob\": -1.2233183e-05,\n                        \"top_logprobs\": [],\n                    },\n                ]\n            }\n\n    .. dropdown:: Response metadata\n\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n        .. code-block:: python\n\n            {\n                \"token_usage\": {\n                    \"completion_tokens\": 5,\n                    \"prompt_tokens\": 28,\n                    \"total_tokens\": 33,\n                },\n                \"model_name\": \"gpt-4o\",\n                \"system_fingerprint\": \"fp_319be4768e\",\n                \"finish_reason\": \"stop\",\n                \"logprobs\": None,\n            }\n\n    \"\"\"  # noqa: E501\n\n    stream_usage: bool = False\n    \"\"\"Whether to include usage metadata in streaming output. If True, additional\n    message chunks will be generated during the stream including usage metadata.\n    \"\"\"\n\n    max_tokens: Optional[int] = Field(default=None, alias=\"max_completion_tokens\")\n    \"\"\"Maximum number of tokens to generate.\"\"\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"openai_api_key\": \"OPENAI_API_KEY\"}\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"openai\"]\n\n    @property\n    def lc_attributes(self) -> Dict[str, Any]:\n        attributes: Dict[str, Any] = {}\n\n        if self.openai_organization:\n            attributes[\"openai_organization\"] = self.openai_organization\n\n        if self.openai_api_base:\n            attributes[\"openai_api_base\"] = self.openai_api_base\n\n        if self.openai_proxy:\n            attributes[\"openai_proxy\"] = self.openai_proxy\n\n        return attributes\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this model can be serialized by Langchain.\"\"\"\n        return True\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        params = super()._default_params\n        if \"max_tokens\" in params:\n            params[\"max_completion_tokens\"] = params.pop(\"max_tokens\")\n\n        return params\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        payload = super()._get_request_payload(input_, stop=stop, **kwargs)\n        # max_tokens was deprecated in favor of max_completion_tokens\n        # in September 2024 release\n        if \"max_tokens\" in payload:\n            payload[\"max_completion_tokens\"] = payload.pop(\"max_tokens\")\n        return payload\n\n    def _should_stream_usage(\n        self, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> bool:\n        \"\"\"Determine whether to include usage metadata in streaming output.\n\n        For backwards compatibility, we check for `stream_options` passed\n        explicitly to kwargs or in the model_kwargs and override self.stream_usage.\n        \"\"\"\n        stream_usage_sources = [  # order of preference\n            stream_usage,\n            kwargs.get(\"stream_options\", {}).get(\"include_usage\"),\n            self.model_kwargs.get(\"stream_options\", {}).get(\"include_usage\"),\n            self.stream_usage,\n        ]\n        for source in stream_usage_sources:\n            if isinstance(source, bool):\n                return source\n        return self.stream_usage\n\n    def _stream(\n        self, *args: Any, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> Iterator[ChatGenerationChunk]:\n        \"\"\"Set default stream_options.\"\"\"\n        stream_usage = self._should_stream_usage(stream_usage, **kwargs)\n        # Note: stream_options is not a valid parameter for Azure OpenAI.\n        # To support users proxying Azure through ChatOpenAI, here we only specify\n        # stream_options if include_usage is set to True.\n        # See https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new\n        # for release notes.\n        if stream_usage:\n            kwargs[\"stream_options\"] = {\"include_usage\": stream_usage}\n\n        return super()._stream(*args, **kwargs)\n\n    async def _astream(\n        self, *args: Any, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        \"\"\"Set default stream_options.\"\"\"\n        stream_usage = self._should_stream_usage(stream_usage, **kwargs)\n        if stream_usage:\n            kwargs[\"stream_options\"] = {\"include_usage\": stream_usage}\n\n        async for chunk in super()._astream(*args, **kwargs):\n            yield chunk\n\n    def with_structured_output(\n        self,\n        schema: Optional[_DictOrPydanticClass] = None,\n        *,\n        method: Literal[\"function_calling\", \"json_mode\", \"json_schema\"] = \"json_schema\",\n        include_raw: bool = False,\n        strict: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, _DictOrPydantic]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n\n                - a JSON Schema,\n                - a TypedDict class,\n                - or a Pydantic class,\n                - an OpenAI function/tool schema.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"json_schema\":\n                    Uses OpenAI's Structured Output API:\n                    https://platform.openai.com/docs/guides/structured-outputs\n                    Supported for \"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"o1\", and later\n                    models.\n                - \"function_calling\":\n                    Uses OpenAI's tool-calling (formerly called function calling)\n                    API: https://platform.openai.com/docs/guides/function-calling\n                - \"json_mode\":\n                    Uses OpenAI's JSON mode. Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call:\n                    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\n                Learn more about the differences between the methods and which models\n                support which methods here:\n\n                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode\n                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            strict:\n\n                - True:\n                    Model output is guaranteed to exactly match the schema.\n                    The input schema will also be validated according to\n                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n                - False:\n                    Input schema will not be validated and model output will not be\n                    validated.\n                - None:\n                    ``strict`` argument will not be passed to the model.\n\n                If schema is specified via TypedDict or JSON schema, ``strict`` is not\n                enabled by default. Pass ``strict=True`` to enable it.\n\n                Note: ``strict`` can only be non-null if ``method`` is\n                ``\"json_schema\"`` or ``\"function_calling\"``.\n\n            kwargs: Additional keyword args aren't supported.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n\n            - \"raw\": BaseMessage\n            - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n            - \"parsing_error\": Optional[BaseException]\n\n        .. versionchanged:: 0.1.20\n\n            Added support for TypedDict class ``schema``.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n            Support for ``method=\"json_schema\"`` added.\n\n        .. versionchanged:: 0.3.0\n\n            ``method`` default changed from \"function_calling\" to \"json_schema\".\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=False, strict=True\n\n            Note, OpenAI has a number of restrictions on what types of schemas can be\n            provided if ``strict`` = True. When using Pydantic, our model cannot\n            specify any Field metadata (like min/max constraints) and fields cannot\n            have default values.\n\n            See all constraints here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Optional[str] = Field(\n                        default=..., description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=False, strict=False\n\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Optional[str] = Field(\n                        default=..., description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, method=\"function_calling\"\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=True\n\n            .. code-block:: python\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: str\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        .. dropdown:: Example: schema=TypedDict class, method=\"json_schema\", include_raw=False, strict=False\n\n            .. code-block:: python\n\n                # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n                # from typing_extensions, not from typing.\n                from typing_extensions import Annotated, TypedDict\n\n                from langchain_openai import ChatOpenAI\n\n\n                class AnswerWithJustification(TypedDict):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Annotated[\n                        Optional[str], None, \"A justification for the answer.\"\n                    ]\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. dropdown:: Example: schema=OpenAI function schema, method=\"json_schema\", include_raw=False\n\n            .. code-block:: python\n\n                from langchain_openai import ChatOpenAI\n\n                oai_schema = {\n                    'name': 'AnswerWithJustification',\n                    'description': 'An answer to the user question along with justification for the answer.',\n                    'parameters': {\n                        'type': 'object',\n                        'properties': {\n                            'answer': {'type': 'string'},\n                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n                        },\n                       'required': ['answer']\n                   }\n               }\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(oai_schema)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_mode\", include_raw=True\n\n            .. code-block::\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    answer: str\n                    justification: str\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification,\n                    method=\"json_mode\",\n                    include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n                #     'parsing_error': None\n                # }\n\n        .. dropdown:: Example: schema=None, method=\"json_mode\", include_raw=True\n\n            .. code-block::\n\n                structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': {\n                #         'answer': 'They are both the same weight.',\n                #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n                #     },\n                #     'parsing_error': None\n                # }\n        \"\"\"  # noqa: E501\n        return super().with_structured_output(\n            schema, method=method, include_raw=include_raw, strict=strict, **kwargs\n        )\n\n\ndef _is_pydantic_class(obj: Any) -> bool:\n    return isinstance(obj, type) and is_basemodel_subclass(obj)\n\n\ndef _lc_tool_call_to_openai_tool_call(tool_call: ToolCall) -> dict:\n    return {\n        \"type\": \"function\",\n        \"id\": tool_call[\"id\"],\n        \"function\": {\n            \"name\": tool_call[\"name\"],\n            \"arguments\": json.dumps(tool_call[\"args\"]),\n        },\n    }\n\n\ndef _lc_invalid_tool_call_to_openai_tool_call(\n    invalid_tool_call: InvalidToolCall,\n) -> dict:\n    return {\n        \"type\": \"function\",\n        \"id\": invalid_tool_call[\"id\"],\n        \"function\": {\n            \"name\": invalid_tool_call[\"name\"],\n            \"arguments\": invalid_tool_call[\"args\"],\n        },\n    }\n\n\ndef _url_to_size(image_source: str) -> Optional[Tuple[int, int]]:\n    try:\n        from PIL import Image  # type: ignore[import]\n    except ImportError:\n        logger.info(\n            \"Unable to count image tokens. To count image tokens please install \"\n            \"`pip install -U pillow httpx`.\"\n        )\n        return None\n    if _is_url(image_source):\n        try:\n            import httpx\n        except ImportError:\n            logger.info(\n                \"Unable to count image tokens. To count image tokens please install \"\n                \"`pip install -U httpx`.\"\n            )\n            return None\n        response = httpx.get(image_source)\n        response.raise_for_status()\n        width, height = Image.open(BytesIO(response.content)).size\n        return width, height\n    elif _is_b64(image_source):\n        _, encoded = image_source.split(\",\", 1)\n        data = base64.b64decode(encoded)\n        width, height = Image.open(BytesIO(data)).size\n        return width, height\n    else:\n        return None\n\n\ndef _count_image_tokens(width: int, height: int) -> int:\n    # Reference: https://platform.openai.com/docs/guides/vision/calculating-costs\n    width, height = _resize(width, height)\n    h = ceil(height / 512)\n    w = ceil(width / 512)\n    return (170 * h * w) + 85\n\n\ndef _is_url(s: str) -> bool:\n    try:\n        result = urlparse(s)\n        return all([result.scheme, result.netloc])\n    except Exception as e:\n        logger.debug(f\"Unable to parse URL: {e}\")\n        return False\n\n\ndef _is_b64(s: str) -> bool:\n    return s.startswith(\"data:image\")\n\n\ndef _resize(width: int, height: int) -> Tuple[int, int]:\n    # larger side must be <= 2048\n    if width > 2048 or height > 2048:\n        if width > height:\n            height = (height * 2048) // width\n            width = 2048\n        else:\n            width = (width * 2048) // height\n            height = 2048\n    # smaller side must be <= 768\n    if width > 768 and height > 768:\n        if width > height:\n            width = (width * 768) // height\n            height = 768\n        else:\n            height = (width * 768) // height\n            width = 768\n    return width, height\n\n\ndef _convert_to_openai_response_format(\n    schema: Union[Dict[str, Any], Type], *, strict: Optional[bool] = None\n) -> Union[Dict, TypeBaseModel]:\n    if isinstance(schema, type) and is_basemodel_subclass(schema):\n        return schema\n\n    if (\n        isinstance(schema, dict)\n        and \"json_schema\" in schema\n        and schema.get(\"type\") == \"json_schema\"\n    ):\n        response_format = schema\n    elif isinstance(schema, dict) and \"name\" in schema and \"schema\" in schema:\n        response_format = {\"type\": \"json_schema\", \"json_schema\": schema}\n    else:\n        if strict is None:\n            if isinstance(schema, dict) and isinstance(schema.get(\"strict\"), bool):\n                strict = schema[\"strict\"]\n            else:\n                strict = False\n        function = convert_to_openai_function(schema, strict=strict)\n        function[\"schema\"] = function.pop(\"parameters\")\n        response_format = {\"type\": \"json_schema\", \"json_schema\": function}\n\n    if strict is not None and strict is not response_format[\"json_schema\"].get(\n        \"strict\"\n    ):\n        msg = (\n            f\"Output schema already has 'strict' value set to \"\n            f\"{schema['json_schema']['strict']} but 'strict' also passed in to \"\n            f\"with_structured_output as {strict}. Please make sure that \"\n            f\"'strict' is only specified in one place.\"\n        )\n        raise ValueError(msg)\n    return response_format\n\n\n@chain\ndef _oai_structured_outputs_parser(ai_msg: AIMessage) -> PydanticBaseModel:\n    if ai_msg.additional_kwargs.get(\"parsed\"):\n        return ai_msg.additional_kwargs[\"parsed\"]\n    elif ai_msg.additional_kwargs.get(\"refusal\"):\n        raise OpenAIRefusalError(ai_msg.additional_kwargs[\"refusal\"])\n    else:\n        raise ValueError(\n            \"Structured Output response does not have a 'parsed' field nor a 'refusal' \"\n            f\"field. Received message:\\n\\n{ai_msg}\"\n        )\n\n\nclass OpenAIRefusalError(Exception):\n    \"\"\"Error raised when OpenAI Structured Outputs API returns a refusal.\n\n    When using OpenAI's Structured Outputs API with user-generated input, the model\n    may occasionally refuse to fulfill the request for safety reasons.\n\n    See here for more on refusals:\n    https://platform.openai.com/docs/guides/structured-outputs/refusals\n\n    .. versionadded:: 0.1.21\n    \"\"\"\n\n\ndef _create_usage_metadata(oai_token_usage: dict) -> UsageMetadata:\n    input_tokens = oai_token_usage.get(\"prompt_tokens\", 0)\n    output_tokens = oai_token_usage.get(\"completion_tokens\", 0)\n    total_tokens = oai_token_usage.get(\"total_tokens\", input_tokens + output_tokens)\n    input_token_details: dict = {\n        \"audio\": (oai_token_usage.get(\"prompt_tokens_details\") or {}).get(\n            \"audio_tokens\"\n        ),\n        \"cache_read\": (oai_token_usage.get(\"prompt_tokens_details\") or {}).get(\n            \"cached_tokens\"\n        ),\n    }\n    output_token_details: dict = {\n        \"audio\": (oai_token_usage.get(\"completion_tokens_details\") or {}).get(\n            \"audio_tokens\"\n        ),\n        \"reasoning\": (oai_token_usage.get(\"completion_tokens_details\") or {}).get(\n            \"reasoning_tokens\"\n        ),\n    }\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=total_tokens,\n        input_token_details=InputTokenDetails(\n            **{k: v for k, v in input_token_details.items() if v is not None}\n        ),\n        output_token_details=OutputTokenDetails(\n            **{k: v for k, v in output_token_details.items() if v is not None}\n        ),\n    )\n",
        "patch": "@@ -8,6 +8,7 @@\n import os\n import sys\n import warnings\n+from functools import partial\n from io import BytesIO\n from math import ceil\n from operator import itemgetter\n@@ -77,7 +78,12 @@\n     parse_tool_call,\n )\n from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n-from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough, chain\n+from langchain_core.runnables import (\n+    Runnable,\n+    RunnableLambda,\n+    RunnableMap,\n+    RunnablePassthrough,\n+)\n from langchain_core.runnables.config import run_in_executor\n from langchain_core.tools import BaseTool\n from langchain_core.utils import get_pydantic_field_names\n@@ -1435,9 +1441,9 @@ def with_structured_output(\n                 },\n             )\n             if is_pydantic_schema:\n-                output_parser = _oai_structured_outputs_parser.with_types(\n-                    output_type=cast(type, schema)\n-                )\n+                output_parser = RunnableLambda(\n+                    partial(_oai_structured_outputs_parser, schema=cast(type, schema))\n+                ).with_types(output_type=cast(type, schema))\n             else:\n                 output_parser = JsonOutputParser()\n         else:\n@@ -2510,10 +2516,14 @@ def _convert_to_openai_response_format(\n     return response_format\n \n \n-@chain\n-def _oai_structured_outputs_parser(ai_msg: AIMessage) -> PydanticBaseModel:\n-    if ai_msg.additional_kwargs.get(\"parsed\"):\n-        return ai_msg.additional_kwargs[\"parsed\"]\n+def _oai_structured_outputs_parser(\n+    ai_msg: AIMessage, schema: Type[_BM]\n+) -> PydanticBaseModel:\n+    if parsed := ai_msg.additional_kwargs.get(\"parsed\"):\n+        if isinstance(parsed, dict):\n+            return schema(**parsed)\n+        else:\n+            return parsed\n     elif ai_msg.additional_kwargs.get(\"refusal\"):\n         raise OpenAIRefusalError(ai_msg.additional_kwargs[\"refusal\"])\n     else:"
      },
      {
        "filename": "libs/partners/openai/tests/unit_tests/chat_models/test_base.py",
        "content_before": "\"\"\"Test OpenAI Chat API wrapper.\"\"\"\n\nimport json\nfrom types import TracebackType\nfrom typing import Any, Dict, List, Literal, Optional, Type, Union\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\nimport pytest\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    FunctionMessage,\n    HumanMessage,\n    InvalidToolCall,\n    SystemMessage,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.ai import UsageMetadata\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai.chat_models.base import (\n    _convert_dict_to_message,\n    _convert_message_to_dict,\n    _convert_to_openai_response_format,\n    _create_usage_metadata,\n    _format_message_content,\n)\n\n\ndef test_openai_model_param() -> None:\n    llm = ChatOpenAI(model=\"foo\")\n    assert llm.model_name == \"foo\"\n    llm = ChatOpenAI(model_name=\"foo\")  # type: ignore[call-arg]\n    assert llm.model_name == \"foo\"\n\n    llm = ChatOpenAI(max_tokens=10)  # type: ignore[call-arg]\n    assert llm.max_tokens == 10\n    llm = ChatOpenAI(max_completion_tokens=10)\n    assert llm.max_tokens == 10\n\n\ndef test_openai_o1_temperature() -> None:\n    llm = ChatOpenAI(model=\"o1-preview\")\n    assert llm.temperature == 1\n    llm = ChatOpenAI(model_name=\"o1-mini\")  # type: ignore[call-arg]\n    assert llm.temperature == 1\n\n\ndef test_function_message_dict_to_function_message() -> None:\n    content = json.dumps({\"result\": \"Example #1\"})\n    name = \"test_function\"\n    result = _convert_dict_to_message(\n        {\"role\": \"function\", \"name\": name, \"content\": content}\n    )\n    assert isinstance(result, FunctionMessage)\n    assert result.name == name\n    assert result.content == content\n\n\ndef test__convert_dict_to_message_human() -> None:\n    message = {\"role\": \"user\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = HumanMessage(content=\"foo\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_human_with_name() -> None:\n    message = {\"role\": \"user\", \"content\": \"foo\", \"name\": \"test\"}\n    result = _convert_dict_to_message(message)\n    expected_output = HumanMessage(content=\"foo\", name=\"test\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_ai() -> None:\n    message = {\"role\": \"assistant\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(content=\"foo\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_ai_with_name() -> None:\n    message = {\"role\": \"assistant\", \"content\": \"foo\", \"name\": \"test\"}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(content=\"foo\", name=\"test\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_system() -> None:\n    message = {\"role\": \"system\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = SystemMessage(content=\"foo\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_developer() -> None:\n    message = {\"role\": \"developer\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = SystemMessage(\n        content=\"foo\", additional_kwargs={\"__openai_role__\": \"developer\"}\n    )\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_system_with_name() -> None:\n    message = {\"role\": \"system\", \"content\": \"foo\", \"name\": \"test\"}\n    result = _convert_dict_to_message(message)\n    expected_output = SystemMessage(content=\"foo\", name=\"test\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_tool() -> None:\n    message = {\"role\": \"tool\", \"content\": \"foo\", \"tool_call_id\": \"bar\"}\n    result = _convert_dict_to_message(message)\n    expected_output = ToolMessage(content=\"foo\", tool_call_id=\"bar\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_tool_call() -> None:\n    raw_tool_call = {\n        \"id\": \"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n        \"function\": {\n            \"arguments\": '{\"name\": \"Sally\", \"hair_color\": \"green\"}',\n            \"name\": \"GenerateUsername\",\n        },\n        \"type\": \"function\",\n    }\n    message = {\"role\": \"assistant\", \"content\": None, \"tool_calls\": [raw_tool_call]}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(\n        content=\"\",\n        additional_kwargs={\"tool_calls\": [raw_tool_call]},\n        tool_calls=[\n            ToolCall(\n                name=\"GenerateUsername\",\n                args={\"name\": \"Sally\", \"hair_color\": \"green\"},\n                id=\"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n                type=\"tool_call\",\n            )\n        ],\n    )\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n    # Test malformed tool call\n    raw_tool_calls: list = [\n        {\n            \"id\": \"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n            \"function\": {\"arguments\": \"oops\", \"name\": \"GenerateUsername\"},\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"call_abc123\",\n            \"function\": {\n                \"arguments\": '{\"name\": \"Sally\", \"hair_color\": \"green\"}',\n                \"name\": \"GenerateUsername\",\n            },\n            \"type\": \"function\",\n        },\n    ]\n    raw_tool_calls = list(sorted(raw_tool_calls, key=lambda x: x[\"id\"]))\n    message = {\"role\": \"assistant\", \"content\": None, \"tool_calls\": raw_tool_calls}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(\n        content=\"\",\n        additional_kwargs={\"tool_calls\": raw_tool_calls},\n        invalid_tool_calls=[\n            InvalidToolCall(\n                name=\"GenerateUsername\",\n                args=\"oops\",\n                id=\"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n                error=(\n                    \"Function GenerateUsername arguments:\\n\\noops\\n\\nare not \"\n                    \"valid JSON. Received JSONDecodeError Expecting value: line 1 \"\n                    \"column 1 (char 0)\\nFor troubleshooting, visit: https://python\"\n                    \".langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \"\n                ),\n                type=\"invalid_tool_call\",\n            )\n        ],\n        tool_calls=[\n            ToolCall(\n                name=\"GenerateUsername\",\n                args={\"name\": \"Sally\", \"hair_color\": \"green\"},\n                id=\"call_abc123\",\n                type=\"tool_call\",\n            )\n        ],\n    )\n    assert result == expected_output\n    reverted_message_dict = _convert_message_to_dict(expected_output)\n    reverted_message_dict[\"tool_calls\"] = list(\n        sorted(reverted_message_dict[\"tool_calls\"], key=lambda x: x[\"id\"])\n    )\n    assert reverted_message_dict == message\n\n\nclass MockAsyncContextManager:\n    def __init__(self, chunk_list: list):\n        self.current_chunk = 0\n        self.chunk_list = chunk_list\n        self.chunk_num = len(chunk_list)\n\n    async def __aenter__(self) -> \"MockAsyncContextManager\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n    def __aiter__(self) -> \"MockAsyncContextManager\":\n        return self\n\n    async def __anext__(self) -> dict:\n        if self.current_chunk < self.chunk_num:\n            chunk = self.chunk_list[self.current_chunk]\n            self.current_chunk += 1\n            return chunk\n        else:\n            raise StopAsyncIteration\n\n\nclass MockSyncContextManager:\n    def __init__(self, chunk_list: list):\n        self.current_chunk = 0\n        self.chunk_list = chunk_list\n        self.chunk_num = len(chunk_list)\n\n    def __enter__(self) -> \"MockSyncContextManager\":\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n    def __iter__(self) -> \"MockSyncContextManager\":\n        return self\n\n    def __next__(self) -> dict:\n        if self.current_chunk < self.chunk_num:\n            chunk = self.chunk_list[self.current_chunk]\n            self.current_chunk += 1\n            return chunk\n        else:\n            raise StopIteration\n\n\nGLM4_STREAM_META = \"\"\"{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u4eba\\u5de5\\u667a\\u80fd\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u52a9\\u624b\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\uff0c\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u4f60\\u53ef\\u4ee5\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u53eb\\u6211\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"AI\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u52a9\\u624b\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\u3002\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{\"role\":\"assistant\",\"content\":\"\"}}],\"usage\":{\"prompt_tokens\":13,\"completion_tokens\":10,\"total_tokens\":23}}\n[DONE]\"\"\"  # noqa: E501\n\n\n@pytest.fixture\ndef mock_glm4_completion() -> list:\n    list_chunk_data = GLM4_STREAM_META.split(\"\\n\")\n    result_list = []\n    for msg in list_chunk_data:\n        if msg != \"[DONE]\":\n            result_list.append(json.loads(msg))\n\n    return result_list\n\n\nasync def test_glm4_astream(mock_glm4_completion: list) -> None:\n    llm_name = \"glm-4\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = AsyncMock()\n\n    async def mock_create(*args: Any, **kwargs: Any) -> MockAsyncContextManager:\n        return MockAsyncContextManager(mock_glm4_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_glm4_completion[-1]\n\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"async_client\", mock_client):\n        async for chunk in llm.astream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\ndef test_glm4_stream(mock_glm4_completion: list) -> None:\n    llm_name = \"glm-4\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = MagicMock()\n\n    def mock_create(*args: Any, **kwargs: Any) -> MockSyncContextManager:\n        return MockSyncContextManager(mock_glm4_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_glm4_completion[-1]\n\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"client\", mock_client):\n        for chunk in llm.stream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\nDEEPSEEK_STREAM_DATA = \"\"\"{\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\",\"role\":\"assistant\"},\"finish_reason\":null,\"logprobs\":null}],\"created\":1721630271,\"model\":\"deepseek-chat\",\"system_fingerprint\":\"fp_7e0991cad4\",\"object\":\"chat.completion.chunk\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u6211\u662f\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"Deep\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"Seek\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\" Chat\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\uff0c\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u4e00\u4e2a\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u7531\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u6df1\u5ea6\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u6c42\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u7d22\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u516c\u53f8\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u5f00\u53d1\u7684\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u667a\u80fd\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u52a9\u624b\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u3002\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\",\"role\":null},\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":{\"completion_tokens\":15,\"prompt_tokens\":11,\"total_tokens\":26}}\n[DONE]\"\"\"  # noqa: E501\n\n\n@pytest.fixture\ndef mock_deepseek_completion() -> List[Dict]:\n    list_chunk_data = DEEPSEEK_STREAM_DATA.split(\"\\n\")\n    result_list = []\n    for msg in list_chunk_data:\n        if msg != \"[DONE]\":\n            result_list.append(json.loads(msg))\n\n    return result_list\n\n\nasync def test_deepseek_astream(mock_deepseek_completion: list) -> None:\n    llm_name = \"deepseek-chat\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = AsyncMock()\n\n    async def mock_create(*args: Any, **kwargs: Any) -> MockAsyncContextManager:\n        return MockAsyncContextManager(mock_deepseek_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_deepseek_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"async_client\", mock_client):\n        async for chunk in llm.astream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\ndef test_deepseek_stream(mock_deepseek_completion: list) -> None:\n    llm_name = \"deepseek-chat\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = MagicMock()\n\n    def mock_create(*args: Any, **kwargs: Any) -> MockSyncContextManager:\n        return MockSyncContextManager(mock_deepseek_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_deepseek_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"client\", mock_client):\n        for chunk in llm.stream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\nOPENAI_STREAM_DATA = \"\"\"{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\u6211\u662f\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\u52a9\u624b\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\u3002\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[],\"usage\":{\"prompt_tokens\":14,\"completion_tokens\":3,\"total_tokens\":17}}\n[DONE]\"\"\"  # noqa: E501\n\n\n@pytest.fixture\ndef mock_openai_completion() -> List[Dict]:\n    list_chunk_data = OPENAI_STREAM_DATA.split(\"\\n\")\n    result_list = []\n    for msg in list_chunk_data:\n        if msg != \"[DONE]\":\n            result_list.append(json.loads(msg))\n\n    return result_list\n\n\nasync def test_openai_astream(mock_openai_completion: list) -> None:\n    llm_name = \"gpt-4o\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = AsyncMock()\n\n    async def mock_create(*args: Any, **kwargs: Any) -> MockAsyncContextManager:\n        return MockAsyncContextManager(mock_openai_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_openai_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"async_client\", mock_client):\n        async for chunk in llm.astream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\ndef test_openai_stream(mock_openai_completion: list) -> None:\n    llm_name = \"gpt-4o\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = MagicMock()\n\n    def mock_create(*args: Any, **kwargs: Any) -> MockSyncContextManager:\n        return MockSyncContextManager(mock_openai_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_openai_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"client\", mock_client):\n        for chunk in llm.stream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\n@pytest.fixture\ndef mock_completion() -> dict:\n    return {\n        \"id\": \"chatcmpl-7fcZavknQda3SQ\",\n        \"object\": \"chat.completion\",\n        \"created\": 1689989000,\n        \"model\": \"gpt-3.5-turbo-0613\",\n        \"choices\": [\n            {\n                \"index\": 0,\n                \"message\": {\"role\": \"assistant\", \"content\": \"Bar Baz\", \"name\": \"Erick\"},\n                \"finish_reason\": \"stop\",\n            }\n        ],\n    }\n\n\n@pytest.fixture\ndef mock_client(mock_completion: dict) -> MagicMock:\n    rtn = MagicMock()\n\n    mock_create = MagicMock()\n\n    mock_resp = MagicMock()\n    mock_resp.headers = {\"content-type\": \"application/json\"}\n    mock_resp.parse.return_value = mock_completion\n    mock_create.return_value = mock_resp\n\n    rtn.with_raw_response.create = mock_create\n    rtn.create.return_value = mock_completion\n    return rtn\n\n\n@pytest.fixture\ndef mock_async_client(mock_completion: dict) -> AsyncMock:\n    rtn = AsyncMock()\n\n    mock_create = AsyncMock()\n    mock_resp = MagicMock()\n    mock_resp.parse.return_value = mock_completion\n    mock_create.return_value = mock_resp\n\n    rtn.with_raw_response.create = mock_create\n    rtn.create.return_value = mock_completion\n    return rtn\n\n\ndef test_openai_invoke(mock_client: MagicMock) -> None:\n    llm = ChatOpenAI()\n\n    with patch.object(llm, \"client\", mock_client):\n        res = llm.invoke(\"bar\")\n        assert res.content == \"Bar Baz\"\n\n        # headers are not in response_metadata if include_response_headers not set\n        assert \"headers\" not in res.response_metadata\n    assert mock_client.create.called\n\n\nasync def test_openai_ainvoke(mock_async_client: AsyncMock) -> None:\n    llm = ChatOpenAI()\n\n    with patch.object(llm, \"async_client\", mock_async_client):\n        res = await llm.ainvoke(\"bar\")\n        assert res.content == \"Bar Baz\"\n\n        # headers are not in response_metadata if include_response_headers not set\n        assert \"headers\" not in res.response_metadata\n    assert mock_async_client.create.called\n\n\n@pytest.mark.parametrize(\n    \"model\",\n    [\n        \"gpt-3.5-turbo\",\n        \"gpt-4\",\n        \"gpt-3.5-0125\",\n        \"gpt-4-0125-preview\",\n        \"gpt-4-turbo-preview\",\n        \"gpt-4-vision-preview\",\n    ],\n)\ndef test__get_encoding_model(model: str) -> None:\n    ChatOpenAI(model=model)._get_encoding_model()\n    return\n\n\ndef test_openai_invoke_name(mock_client: MagicMock) -> None:\n    llm = ChatOpenAI()\n\n    with patch.object(llm, \"client\", mock_client):\n        messages = [HumanMessage(content=\"Foo\", name=\"Katie\")]\n        res = llm.invoke(messages)\n        call_args, call_kwargs = mock_client.create.call_args\n        assert len(call_args) == 0  # no positional args\n        call_messages = call_kwargs[\"messages\"]\n        assert len(call_messages) == 1\n        assert call_messages[0][\"role\"] == \"user\"\n        assert call_messages[0][\"content\"] == \"Foo\"\n        assert call_messages[0][\"name\"] == \"Katie\"\n\n        # check return type has name\n        assert res.content == \"Bar Baz\"\n        assert res.name == \"Erick\"\n\n\ndef test_custom_token_counting() -> None:\n    def token_encoder(text: str) -> List[int]:\n        return [1, 2, 3]\n\n    llm = ChatOpenAI(custom_get_token_ids=token_encoder)\n    assert llm.get_token_ids(\"foo\") == [1, 2, 3]\n\n\ndef test_format_message_content() -> None:\n    content: Any = \"hello\"\n    assert content == _format_message_content(content)\n\n    content = None\n    assert content == _format_message_content(content)\n\n    content = []\n    assert content == _format_message_content(content)\n\n    content = [\n        {\"type\": \"text\", \"text\": \"What is in this image?\"},\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"url.com\"}},\n    ]\n    assert content == _format_message_content(content)\n\n    content = [\n        {\"type\": \"text\", \"text\": \"hello\"},\n        {\n            \"type\": \"tool_use\",\n            \"id\": \"toolu_01A09q90qw90lq917835lq9\",\n            \"name\": \"get_weather\",\n            \"input\": {\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"},\n        },\n    ]\n    assert [{\"type\": \"text\", \"text\": \"hello\"}] == _format_message_content(content)\n\n\nclass GenerateUsername(BaseModel):\n    \"Get a username based on someone's name and hair color.\"\n\n    name: str\n    hair_color: str\n\n\nclass MakeASandwich(BaseModel):\n    \"Make a sandwich given a list of ingredients.\"\n\n    bread_type: str\n    cheese_type: str\n    condiments: List[str]\n    vegetables: List[str]\n\n\n@pytest.mark.parametrize(\n    \"tool_choice\",\n    [\n        \"any\",\n        \"none\",\n        \"auto\",\n        \"required\",\n        \"GenerateUsername\",\n        {\"type\": \"function\", \"function\": {\"name\": \"MakeASandwich\"}},\n        False,\n        None,\n    ],\n)\n@pytest.mark.parametrize(\"strict\", [True, False, None])\ndef test_bind_tools_tool_choice(tool_choice: Any, strict: Optional[bool]) -> None:\n    \"\"\"Test passing in manually construct tool call message.\"\"\"\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n    llm.bind_tools(\n        tools=[GenerateUsername, MakeASandwich], tool_choice=tool_choice, strict=strict\n    )\n\n\n@pytest.mark.parametrize(\n    \"schema\", [GenerateUsername, GenerateUsername.model_json_schema()]\n)\n@pytest.mark.parametrize(\"method\", [\"json_schema\", \"function_calling\", \"json_mode\"])\n@pytest.mark.parametrize(\"include_raw\", [True, False])\n@pytest.mark.parametrize(\"strict\", [True, False, None])\ndef test_with_structured_output(\n    schema: Union[Type, Dict[str, Any], None],\n    method: Literal[\"function_calling\", \"json_mode\", \"json_schema\"],\n    include_raw: bool,\n    strict: Optional[bool],\n) -> None:\n    \"\"\"Test passing in manually construct tool call message.\"\"\"\n    if method == \"json_mode\":\n        strict = None\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n    llm.with_structured_output(\n        schema, method=method, strict=strict, include_raw=include_raw\n    )\n\n\ndef test_get_num_tokens_from_messages() -> None:\n    llm = ChatOpenAI(model=\"gpt-4o\")\n    messages = [\n        SystemMessage(\"you're a good assistant\"),\n        HumanMessage(\"how are you\"),\n        HumanMessage(\n            [\n                {\"type\": \"text\", \"text\": \"what's in this image\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://foobar.com\"}},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": \"https://foobar.com\", \"detail\": \"low\"},\n                },\n            ]\n        ),\n        AIMessage(\"a nice bird\"),\n        AIMessage(\n            \"\",\n            tool_calls=[\n                ToolCall(id=\"foo\", name=\"bar\", args={\"arg1\": \"arg1\"}, type=\"tool_call\")\n            ],\n        ),\n        AIMessage(\n            \"\",\n            additional_kwargs={\n                \"function_call\": {\n                    \"arguments\": json.dumps({\"arg1\": \"arg1\"}),\n                    \"name\": \"fun\",\n                }\n            },\n        ),\n        AIMessage(\n            \"text\",\n            tool_calls=[\n                ToolCall(id=\"foo\", name=\"bar\", args={\"arg1\": \"arg1\"}, type=\"tool_call\")\n            ],\n        ),\n        ToolMessage(\"foobar\", tool_call_id=\"foo\"),\n    ]\n    expected = 176\n    actual = llm.get_num_tokens_from_messages(messages)\n    assert expected == actual\n\n\nclass Foo(BaseModel):\n    bar: int\n\n\n# class FooV1(BaseModelV1):\n#     bar: int\n\n\n@pytest.mark.parametrize(\n    \"schema\",\n    [\n        Foo\n        # FooV1\n    ],\n)\ndef test_schema_from_with_structured_output(schema: Type) -> None:\n    \"\"\"Test schema from with_structured_output.\"\"\"\n\n    llm = ChatOpenAI(model=\"gpt-4o\")\n\n    structured_llm = llm.with_structured_output(\n        schema, method=\"json_schema\", strict=True\n    )\n\n    expected = {\n        \"properties\": {\"bar\": {\"title\": \"Bar\", \"type\": \"integer\"}},\n        \"required\": [\"bar\"],\n        \"title\": schema.__name__,\n        \"type\": \"object\",\n    }\n    actual = structured_llm.get_output_schema().model_json_schema()\n    assert actual == expected\n\n\ndef test__create_usage_metadata() -> None:\n    usage_metadata = {\n        \"completion_tokens\": 15,\n        \"prompt_tokens_details\": None,\n        \"completion_tokens_details\": None,\n        \"prompt_tokens\": 11,\n        \"total_tokens\": 26,\n    }\n    result = _create_usage_metadata(usage_metadata)\n    assert result == UsageMetadata(\n        output_tokens=15,\n        input_tokens=11,\n        total_tokens=26,\n        input_token_details={},\n        output_token_details={},\n    )\n\n\ndef test__convert_to_openai_response_format() -> None:\n    # Test response formats that aren't tool-like.\n    response_format: dict = {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"},\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False,\n                        },\n                    },\n                    \"final_answer\": {\"type\": \"string\"},\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False,\n            },\n            \"strict\": True,\n        },\n    }\n\n    actual = _convert_to_openai_response_format(response_format)\n    assert actual == response_format\n\n    actual = _convert_to_openai_response_format(response_format[\"json_schema\"])\n    assert actual == response_format\n\n    actual = _convert_to_openai_response_format(response_format, strict=True)\n    assert actual == response_format\n\n    with pytest.raises(ValueError):\n        _convert_to_openai_response_format(response_format, strict=False)\n\n\n@pytest.mark.parametrize(\"method\", [\"function_calling\", \"json_schema\"])\n@pytest.mark.parametrize(\"strict\", [True, None])\ndef test_structured_output_strict(\n    method: Literal[\"function_calling\", \"json_schema\"], strict: Optional[bool]\n) -> None:\n    \"\"\"Test to verify structured output with strict=True.\"\"\"\n\n    llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n\n    class Joke(BaseModel):\n        \"\"\"Joke to tell user.\"\"\"\n\n        setup: str = Field(description=\"question to set up a joke\")\n        punchline: str = Field(description=\"answer to resolve the joke\")\n\n    llm.with_structured_output(Joke, method=method, strict=strict)\n    # Schema\n    llm.with_structured_output(Joke.model_json_schema(), method=method, strict=strict)\n\n\ndef test_nested_structured_output_strict() -> None:\n    \"\"\"Test to verify structured output with strict=True for nested object.\"\"\"\n\n    llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n\n    class SelfEvaluation(TypedDict):\n        score: int\n        text: str\n\n    class JokeWithEvaluation(TypedDict):\n        \"\"\"Joke to tell user.\"\"\"\n\n        setup: str\n        punchline: str\n        self_evaluation: SelfEvaluation\n\n    llm.with_structured_output(JokeWithEvaluation, method=\"json_schema\")\n\n\ndef test__get_request_payload() -> None:\n    llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n    messages: list = [\n        SystemMessage(\"hello\"),\n        SystemMessage(\"bye\", additional_kwargs={\"__openai_role__\": \"developer\"}),\n        {\"role\": \"human\", \"content\": \"how are you\"},\n    ]\n    expected = {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"hello\"},\n            {\"role\": \"developer\", \"content\": \"bye\"},\n            {\"role\": \"user\", \"content\": \"how are you\"},\n        ],\n        \"model\": \"gpt-4o-2024-08-06\",\n        \"stream\": False,\n    }\n    payload = llm._get_request_payload(messages)\n    assert payload == expected\n\n\ndef test_init_o1() -> None:\n    with pytest.warns(None) as record:  # type: ignore[call-overload]\n        ChatOpenAI(model=\"o1\", reasoning_effort=\"medium\")\n    assert len(record) == 0\n\n\ndef test_structured_output_old_model() -> None:\n    class Output(TypedDict):\n        \"\"\"output.\"\"\"\n\n        foo: str\n\n    with pytest.warns(match=\"Cannot use method='json_schema'\"):\n        llm = ChatOpenAI(model=\"gpt-4\").with_structured_output(Output)\n    # assert tool calling was used instead of json_schema\n    assert \"tools\" in llm.steps[0].kwargs  # type: ignore\n    assert \"response_format\" not in llm.steps[0].kwargs  # type: ignore\n",
        "patch": "@@ -1,11 +1,13 @@\n \"\"\"Test OpenAI Chat API wrapper.\"\"\"\n \n import json\n+from functools import partial\n from types import TracebackType\n from typing import Any, Dict, List, Literal, Optional, Type, Union\n from unittest.mock import AsyncMock, MagicMock, patch\n \n import pytest\n+from langchain_core.load import dumps, loads\n from langchain_core.messages import (\n     AIMessage,\n     AIMessageChunk,\n@@ -17,6 +19,8 @@\n     ToolMessage,\n )\n from langchain_core.messages.ai import UsageMetadata\n+from langchain_core.outputs import ChatGeneration\n+from langchain_core.runnables import RunnableLambda\n from pydantic import BaseModel, Field\n from typing_extensions import TypedDict\n \n@@ -27,6 +31,7 @@\n     _convert_to_openai_response_format,\n     _create_usage_metadata,\n     _format_message_content,\n+    _oai_structured_outputs_parser,\n )\n \n \n@@ -899,3 +904,21 @@ class Output(TypedDict):\n     # assert tool calling was used instead of json_schema\n     assert \"tools\" in llm.steps[0].kwargs  # type: ignore\n     assert \"response_format\" not in llm.steps[0].kwargs  # type: ignore\n+\n+\n+def test_structured_outputs_parser() -> None:\n+    parsed_response = GenerateUsername(name=\"alice\", hair_color=\"black\")\n+    llm_output = ChatGeneration(\n+        message=AIMessage(\n+            content='{\"name\": \"alice\", \"hair_color\": \"black\"}',\n+            additional_kwargs={\"parsed\": parsed_response},\n+        )\n+    )\n+    output_parser = RunnableLambda(\n+        partial(_oai_structured_outputs_parser, schema=GenerateUsername)\n+    )\n+    serialized = dumps(llm_output)\n+    deserialized = loads(serialized)\n+    assert isinstance(deserialized, ChatGeneration)\n+    result = output_parser.invoke(deserialized.message)\n+    assert result == parsed_response"
      }
    ]
  },
  {
    "number": 29785,
    "title": "openai[patch]: update system role to developer for o-series models",
    "body": "Some o-series models will raise a 400 error for `\"role\": \"system\"` (`o1-mini` and `o1-preview` will raise, `o1` and `o3-mini` will not).\r\n\r\nHere we update `ChatOpenAI` to update the role to `\"developer\"` for all model names matching `^o\\d`.\r\n\r\nWe only make this change on the ChatOpenAI class (not BaseChatOpenAI).",
    "issue_title": "openai[patch]: update system role to developer for o-series models",
    "issue_body": "Some o-series models will raise a 400 error for `\"role\": \"system\"` (`o1-mini` and `o1-preview` will raise, `o1` and `o3-mini` will not).\r\n\r\nHere we update `ChatOpenAI` to update the role to `\"developer\"` for all model names matching `^o\\d`.\r\n\r\nWe only make this change on the ChatOpenAI class (not BaseChatOpenAI).",
    "files": [
      {
        "filename": "libs/partners/openai/langchain_openai/chat_models/base.py",
        "content_before": "\"\"\"OpenAI chat wrapper.\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport json\nimport logging\nimport os\nimport sys\nimport warnings\nfrom io import BytesIO\nfrom math import ceil\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\nfrom urllib.parse import urlparse\n\nimport openai\nimport tiktoken\nfrom langchain_core._api.deprecation import deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    ChatMessage,\n    ChatMessageChunk,\n    FunctionMessage,\n    FunctionMessageChunk,\n    HumanMessage,\n    HumanMessageChunk,\n    InvalidToolCall,\n    SystemMessage,\n    SystemMessageChunk,\n    ToolCall,\n    ToolMessage,\n    ToolMessageChunk,\n)\nfrom langchain_core.messages.ai import (\n    InputTokenDetails,\n    OutputTokenDetails,\n    UsageMetadata,\n)\nfrom langchain_core.messages.tool import tool_call_chunk\nfrom langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\nfrom langchain_core.output_parsers.openai_tools import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n    make_invalid_tool_call,\n    parse_tool_call,\n)\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough, chain\nfrom langchain_core.runnables.config import run_in_executor\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import get_pydantic_field_names\nfrom langchain_core.utils.function_calling import (\n    convert_to_openai_function,\n    convert_to_openai_tool,\n)\nfrom langchain_core.utils.pydantic import (\n    PydanticBaseModel,\n    TypeBaseModel,\n    is_basemodel_subclass,\n)\nfrom langchain_core.utils.utils import _build_model_kwargs, from_env, secret_from_env\nfrom pydantic import BaseModel, ConfigDict, Field, SecretStr, model_validator\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n\ndef _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n    \"\"\"Convert a dictionary to a LangChain message.\n\n    Args:\n        _dict: The dictionary.\n\n    Returns:\n        The LangChain message.\n    \"\"\"\n    role = _dict.get(\"role\")\n    name = _dict.get(\"name\")\n    id_ = _dict.get(\"id\")\n    if role == \"user\":\n        return HumanMessage(content=_dict.get(\"content\", \"\"), id=id_, name=name)\n    elif role == \"assistant\":\n        # Fix for azure\n        # Also OpenAI returns None for tool invocations\n        content = _dict.get(\"content\", \"\") or \"\"\n        additional_kwargs: Dict = {}\n        if function_call := _dict.get(\"function_call\"):\n            additional_kwargs[\"function_call\"] = dict(function_call)\n        tool_calls = []\n        invalid_tool_calls = []\n        if raw_tool_calls := _dict.get(\"tool_calls\"):\n            additional_kwargs[\"tool_calls\"] = raw_tool_calls\n            for raw_tool_call in raw_tool_calls:\n                try:\n                    tool_calls.append(parse_tool_call(raw_tool_call, return_id=True))\n                except Exception as e:\n                    invalid_tool_calls.append(\n                        make_invalid_tool_call(raw_tool_call, str(e))\n                    )\n        if audio := _dict.get(\"audio\"):\n            additional_kwargs[\"audio\"] = audio\n        return AIMessage(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            name=name,\n            id=id_,\n            tool_calls=tool_calls,\n            invalid_tool_calls=invalid_tool_calls,\n        )\n    elif role in (\"system\", \"developer\"):\n        if role == \"developer\":\n            additional_kwargs = {\"__openai_role__\": role}\n        else:\n            additional_kwargs = {}\n        return SystemMessage(\n            content=_dict.get(\"content\", \"\"),\n            name=name,\n            id=id_,\n            additional_kwargs=additional_kwargs,\n        )\n    elif role == \"function\":\n        return FunctionMessage(\n            content=_dict.get(\"content\", \"\"), name=cast(str, _dict.get(\"name\")), id=id_\n        )\n    elif role == \"tool\":\n        additional_kwargs = {}\n        if \"name\" in _dict:\n            additional_kwargs[\"name\"] = _dict[\"name\"]\n        return ToolMessage(\n            content=_dict.get(\"content\", \"\"),\n            tool_call_id=cast(str, _dict.get(\"tool_call_id\")),\n            additional_kwargs=additional_kwargs,\n            name=name,\n            id=id_,\n        )\n    else:\n        return ChatMessage(content=_dict.get(\"content\", \"\"), role=role, id=id_)  # type: ignore[arg-type]\n\n\ndef _format_message_content(content: Any) -> Any:\n    \"\"\"Format message content.\"\"\"\n    if content and isinstance(content, list):\n        # Remove unexpected block types\n        formatted_content = []\n        for block in content:\n            if (\n                isinstance(block, dict)\n                and \"type\" in block\n                and block[\"type\"] == \"tool_use\"\n            ):\n                continue\n            else:\n                formatted_content.append(block)\n    else:\n        formatted_content = content\n\n    return formatted_content\n\n\ndef _convert_message_to_dict(message: BaseMessage) -> dict:\n    \"\"\"Convert a LangChain message to a dictionary.\n\n    Args:\n        message: The LangChain message.\n\n    Returns:\n        The dictionary.\n    \"\"\"\n    message_dict: Dict[str, Any] = {\"content\": _format_message_content(message.content)}\n    if (name := message.name or message.additional_kwargs.get(\"name\")) is not None:\n        message_dict[\"name\"] = name\n\n    # populate role and additional message data\n    if isinstance(message, ChatMessage):\n        message_dict[\"role\"] = message.role\n    elif isinstance(message, HumanMessage):\n        message_dict[\"role\"] = \"user\"\n    elif isinstance(message, AIMessage):\n        message_dict[\"role\"] = \"assistant\"\n        if \"function_call\" in message.additional_kwargs:\n            message_dict[\"function_call\"] = message.additional_kwargs[\"function_call\"]\n        if message.tool_calls or message.invalid_tool_calls:\n            message_dict[\"tool_calls\"] = [\n                _lc_tool_call_to_openai_tool_call(tc) for tc in message.tool_calls\n            ] + [\n                _lc_invalid_tool_call_to_openai_tool_call(tc)\n                for tc in message.invalid_tool_calls\n            ]\n        elif \"tool_calls\" in message.additional_kwargs:\n            message_dict[\"tool_calls\"] = message.additional_kwargs[\"tool_calls\"]\n            tool_call_supported_props = {\"id\", \"type\", \"function\"}\n            message_dict[\"tool_calls\"] = [\n                {k: v for k, v in tool_call.items() if k in tool_call_supported_props}\n                for tool_call in message_dict[\"tool_calls\"]\n            ]\n        else:\n            pass\n        # If tool calls present, content null value should be None not empty string.\n        if \"function_call\" in message_dict or \"tool_calls\" in message_dict:\n            message_dict[\"content\"] = message_dict[\"content\"] or None\n\n        if \"audio\" in message.additional_kwargs:\n            # openai doesn't support passing the data back - only the id\n            # https://platform.openai.com/docs/guides/audio/multi-turn-conversations\n            raw_audio = message.additional_kwargs[\"audio\"]\n            audio = (\n                {\"id\": message.additional_kwargs[\"audio\"][\"id\"]}\n                if \"id\" in raw_audio\n                else raw_audio\n            )\n            message_dict[\"audio\"] = audio\n    elif isinstance(message, SystemMessage):\n        message_dict[\"role\"] = message.additional_kwargs.get(\n            \"__openai_role__\", \"system\"\n        )\n    elif isinstance(message, FunctionMessage):\n        message_dict[\"role\"] = \"function\"\n    elif isinstance(message, ToolMessage):\n        message_dict[\"role\"] = \"tool\"\n        message_dict[\"tool_call_id\"] = message.tool_call_id\n\n        supported_props = {\"content\", \"role\", \"tool_call_id\"}\n        message_dict = {k: v for k, v in message_dict.items() if k in supported_props}\n    else:\n        raise TypeError(f\"Got unknown type {message}\")\n    return message_dict\n\n\ndef _convert_delta_to_message_chunk(\n    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n) -> BaseMessageChunk:\n    id_ = _dict.get(\"id\")\n    role = cast(str, _dict.get(\"role\"))\n    content = cast(str, _dict.get(\"content\") or \"\")\n    additional_kwargs: Dict = {}\n    if _dict.get(\"function_call\"):\n        function_call = dict(_dict[\"function_call\"])\n        if \"name\" in function_call and function_call[\"name\"] is None:\n            function_call[\"name\"] = \"\"\n        additional_kwargs[\"function_call\"] = function_call\n    tool_call_chunks = []\n    if raw_tool_calls := _dict.get(\"tool_calls\"):\n        additional_kwargs[\"tool_calls\"] = raw_tool_calls\n        try:\n            tool_call_chunks = [\n                tool_call_chunk(\n                    name=rtc[\"function\"].get(\"name\"),\n                    args=rtc[\"function\"].get(\"arguments\"),\n                    id=rtc.get(\"id\"),\n                    index=rtc[\"index\"],\n                )\n                for rtc in raw_tool_calls\n            ]\n        except KeyError:\n            pass\n\n    if role == \"user\" or default_class == HumanMessageChunk:\n        return HumanMessageChunk(content=content, id=id_)\n    elif role == \"assistant\" or default_class == AIMessageChunk:\n        return AIMessageChunk(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            id=id_,\n            tool_call_chunks=tool_call_chunks,  # type: ignore[arg-type]\n        )\n    elif role in (\"system\", \"developer\") or default_class == SystemMessageChunk:\n        if role == \"developer\":\n            additional_kwargs = {\"__openai_role__\": \"developer\"}\n        else:\n            additional_kwargs = {}\n        return SystemMessageChunk(\n            content=content, id=id_, additional_kwargs=additional_kwargs\n        )\n    elif role == \"function\" or default_class == FunctionMessageChunk:\n        return FunctionMessageChunk(content=content, name=_dict[\"name\"], id=id_)\n    elif role == \"tool\" or default_class == ToolMessageChunk:\n        return ToolMessageChunk(\n            content=content, tool_call_id=_dict[\"tool_call_id\"], id=id_\n        )\n    elif role or default_class == ChatMessageChunk:\n        return ChatMessageChunk(content=content, role=role, id=id_)\n    else:\n        return default_class(content=content, id=id_)  # type: ignore\n\n\ndef _update_token_usage(\n    overall_token_usage: Union[int, dict], new_usage: Union[int, dict]\n) -> Union[int, dict]:\n    # Token usage is either ints or dictionaries\n    # `reasoning_tokens` is nested inside `completion_tokens_details`\n    if isinstance(new_usage, int):\n        if not isinstance(overall_token_usage, int):\n            raise ValueError(\n                f\"Got different types for token usage: \"\n                f\"{type(new_usage)} and {type(overall_token_usage)}\"\n            )\n        return new_usage + overall_token_usage\n    elif isinstance(new_usage, dict):\n        if not isinstance(overall_token_usage, dict):\n            raise ValueError(\n                f\"Got different types for token usage: \"\n                f\"{type(new_usage)} and {type(overall_token_usage)}\"\n            )\n        return {\n            k: _update_token_usage(overall_token_usage.get(k, 0), v)\n            for k, v in new_usage.items()\n        }\n    else:\n        warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n        return new_usage\n\n\ndef _handle_openai_bad_request(e: openai.BadRequestError) -> None:\n    if (\n        \"'response_format' of type 'json_schema' is not supported with this model\"\n    ) in e.message:\n        message = (\n            \"This model does not support OpenAI's structured output feature, which \"\n            \"is the default method for `with_structured_output` as of \"\n            \"langchain-openai==0.3. To use `with_structured_output` with this model, \"\n            'specify `method=\"function_calling\"`.'\n        )\n        warnings.warn(message)\n        raise e\n    elif \"Invalid schema for response_format\" in e.message:\n        message = (\n            \"Invalid schema for OpenAI's structured output feature, which is the \"\n            \"default method for `with_structured_output` as of langchain-openai==0.3. \"\n            'Specify `method=\"function_calling\"` instead or update your schema. '\n            \"See supported schemas: \"\n            \"https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\"  # noqa: E501\n        )\n        warnings.warn(message)\n        raise e\n    else:\n        raise\n\n\nclass _FunctionCall(TypedDict):\n    name: str\n\n\n_BM = TypeVar(\"_BM\", bound=BaseModel)\n_DictOrPydanticClass = Union[Dict[str, Any], Type[_BM], Type]\n_DictOrPydantic = Union[Dict, _BM]\n\n\nclass _AllReturnType(TypedDict):\n    raw: BaseMessage\n    parsed: Optional[_DictOrPydantic]\n    parsing_error: Optional[BaseException]\n\n\nclass BaseChatOpenAI(BaseChatModel):\n    client: Any = Field(default=None, exclude=True)  #: :meta private:\n    async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    root_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    root_async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    model_name: str = Field(default=\"gpt-3.5-turbo\", alias=\"model\")\n    \"\"\"Model name to use.\"\"\"\n    temperature: Optional[float] = None\n    \"\"\"What sampling temperature to use.\"\"\"\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n    openai_api_key: Optional[SecretStr] = Field(\n        alias=\"api_key\", default_factory=secret_from_env(\"OPENAI_API_KEY\", default=None)\n    )\n    openai_api_base: Optional[str] = Field(default=None, alias=\"base_url\")\n    \"\"\"Base URL path for API requests, leave blank if not using a proxy or service \n        emulator.\"\"\"\n    openai_organization: Optional[str] = Field(default=None, alias=\"organization\")\n    \"\"\"Automatically inferred from env var `OPENAI_ORG_ID` if not provided.\"\"\"\n    # to support explicit proxy for OpenAI\n    openai_proxy: Optional[str] = Field(\n        default_factory=from_env(\"OPENAI_PROXY\", default=None)\n    )\n    request_timeout: Union[float, Tuple[float, float], Any, None] = Field(\n        default=None, alias=\"timeout\"\n    )\n    \"\"\"Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or \n        None.\"\"\"\n    max_retries: Optional[int] = None\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\n    presence_penalty: Optional[float] = None\n    \"\"\"Penalizes repeated tokens.\"\"\"\n    frequency_penalty: Optional[float] = None\n    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\n    seed: Optional[int] = None\n    \"\"\"Seed for generation\"\"\"\n    logprobs: Optional[bool] = None\n    \"\"\"Whether to return logprobs.\"\"\"\n    top_logprobs: Optional[int] = None\n    \"\"\"Number of most likely tokens to return at each token position, each with\n     an associated log probability. `logprobs` must be set to true \n     if this parameter is used.\"\"\"\n    logit_bias: Optional[Dict[int, int]] = None\n    \"\"\"Modify the likelihood of specified tokens appearing in the completion.\"\"\"\n    streaming: bool = False\n    \"\"\"Whether to stream the results or not.\"\"\"\n    n: Optional[int] = None\n    \"\"\"Number of chat completions to generate for each prompt.\"\"\"\n    top_p: Optional[float] = None\n    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n    max_tokens: Optional[int] = Field(default=None)\n    \"\"\"Maximum number of tokens to generate.\"\"\"\n    reasoning_effort: Optional[str] = None\n    \"\"\"Constrains effort on reasoning for reasoning models. \n    \n    Reasoning models only, like OpenAI o1 and o3-mini.\n\n    Currently supported values are low, medium, and high. Reducing reasoning effort \n    can result in faster responses and fewer tokens used on reasoning in a response.\n    \n    .. versionadded:: 0.2.14\n    \"\"\"\n    tiktoken_model_name: Optional[str] = None\n    \"\"\"The model name to pass to tiktoken when using this class. \n    Tiktoken is used to count the number of tokens in documents to constrain \n    them to be under a certain limit. By default, when set to None, this will \n    be the same as the embedding model name. However, there are some cases \n    where you may want to use this Embedding class with a model name not \n    supported by tiktoken. This can include when using Azure embeddings or \n    when using one of the many model providers that expose an OpenAI-like \n    API but with different models. In those cases, in order to avoid erroring \n    when tiktoken is called, you can specify a model name to use here.\"\"\"\n    default_headers: Union[Mapping[str, str], None] = None\n    default_query: Union[Mapping[str, object], None] = None\n    # Configure a custom httpx client. See the\n    # [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n    http_client: Union[Any, None] = Field(default=None, exclude=True)\n    \"\"\"Optional httpx.Client. Only used for sync invocations. Must specify \n        http_async_client as well if you'd like a custom client for async invocations.\n    \"\"\"\n    http_async_client: Union[Any, None] = Field(default=None, exclude=True)\n    \"\"\"Optional httpx.AsyncClient. Only used for async invocations. Must specify \n        http_client as well if you'd like a custom client for sync invocations.\"\"\"\n    stop: Optional[Union[List[str], str]] = Field(default=None, alias=\"stop_sequences\")\n    \"\"\"Default stop sequences.\"\"\"\n    extra_body: Optional[Mapping[str, Any]] = None\n    \"\"\"Optional additional JSON properties to include in the request parameters when\n    making requests to OpenAI compatible APIs, such as vLLM.\"\"\"\n    include_response_headers: bool = False\n    \"\"\"Whether to include response headers in the output message response_metadata.\"\"\"\n    disabled_params: Optional[Dict[str, Any]] = Field(default=None)\n    \"\"\"Parameters of the OpenAI client or chat.completions endpoint that should be \n    disabled for the given model.\n    \n    Should be specified as ``{\"param\": None | ['val1', 'val2']}`` where the key is the \n    parameter and the value is either None, meaning that parameter should never be\n    used, or it's a list of disabled values for the parameter.\n    \n    For example, older models may not support the 'parallel_tool_calls' parameter at \n    all, in which case ``disabled_params={\"parallel_tool_calls\": None}`` can ben passed \n    in.\n    \n    If a parameter is disabled then it will not be used by default in any methods, e.g.\n    in :meth:`~langchain_openai.chat_models.base.ChatOpenAI.with_structured_output`.\n    However this does not prevent a user from directly passed in the parameter during\n    invocation. \n    \"\"\"\n\n    model_config = ConfigDict(populate_by_name=True)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def build_extra(cls, values: Dict[str, Any]) -> Any:\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\n        all_required_field_names = get_pydantic_field_names(cls)\n        values = _build_model_kwargs(values, all_required_field_names)\n        return values\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_temperature(cls, values: Dict[str, Any]) -> Any:\n        \"\"\"Currently o1 models only allow temperature=1.\"\"\"\n        model = values.get(\"model_name\") or values.get(\"model\") or \"\"\n        if model.startswith(\"o1\") and \"temperature\" not in values:\n            values[\"temperature\"] = 1\n        return values\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n        if self.n is not None and self.n < 1:\n            raise ValueError(\"n must be at least 1.\")\n        elif self.n is not None and self.n > 1 and self.streaming:\n            raise ValueError(\"n must be 1 when streaming.\")\n\n        # Check OPENAI_ORGANIZATION for backwards compatibility.\n        self.openai_organization = (\n            self.openai_organization\n            or os.getenv(\"OPENAI_ORG_ID\")\n            or os.getenv(\"OPENAI_ORGANIZATION\")\n        )\n        self.openai_api_base = self.openai_api_base or os.getenv(\"OPENAI_API_BASE\")\n        client_params: dict = {\n            \"api_key\": (\n                self.openai_api_key.get_secret_value() if self.openai_api_key else None\n            ),\n            \"organization\": self.openai_organization,\n            \"base_url\": self.openai_api_base,\n            \"timeout\": self.request_timeout,\n            \"default_headers\": self.default_headers,\n            \"default_query\": self.default_query,\n        }\n        if self.max_retries is not None:\n            client_params[\"max_retries\"] = self.max_retries\n\n        if self.openai_proxy and (self.http_client or self.http_async_client):\n            openai_proxy = self.openai_proxy\n            http_client = self.http_client\n            http_async_client = self.http_async_client\n            raise ValueError(\n                \"Cannot specify 'openai_proxy' if one of \"\n                \"'http_client'/'http_async_client' is already specified. Received:\\n\"\n                f\"{openai_proxy=}\\n{http_client=}\\n{http_async_client=}\"\n            )\n        if not self.client:\n            if self.openai_proxy and not self.http_client:\n                try:\n                    import httpx\n                except ImportError as e:\n                    raise ImportError(\n                        \"Could not import httpx python package. \"\n                        \"Please install it with `pip install httpx`.\"\n                    ) from e\n                self.http_client = httpx.Client(proxy=self.openai_proxy)\n            sync_specific = {\"http_client\": self.http_client}\n            self.root_client = openai.OpenAI(**client_params, **sync_specific)  # type: ignore[arg-type]\n            self.client = self.root_client.chat.completions\n        if not self.async_client:\n            if self.openai_proxy and not self.http_async_client:\n                try:\n                    import httpx\n                except ImportError as e:\n                    raise ImportError(\n                        \"Could not import httpx python package. \"\n                        \"Please install it with `pip install httpx`.\"\n                    ) from e\n                self.http_async_client = httpx.AsyncClient(proxy=self.openai_proxy)\n            async_specific = {\"http_client\": self.http_async_client}\n            self.root_async_client = openai.AsyncOpenAI(\n                **client_params,\n                **async_specific,  # type: ignore[arg-type]\n            )\n            self.async_client = self.root_async_client.chat.completions\n        return self\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        exclude_if_none = {\n            \"presence_penalty\": self.presence_penalty,\n            \"frequency_penalty\": self.frequency_penalty,\n            \"seed\": self.seed,\n            \"top_p\": self.top_p,\n            \"logprobs\": self.logprobs,\n            \"top_logprobs\": self.top_logprobs,\n            \"logit_bias\": self.logit_bias,\n            \"stop\": self.stop or None,  # also exclude empty list for this\n            \"max_tokens\": self.max_tokens,\n            \"extra_body\": self.extra_body,\n            \"n\": self.n,\n            \"temperature\": self.temperature,\n            \"reasoning_effort\": self.reasoning_effort,\n        }\n\n        params = {\n            \"model\": self.model_name,\n            \"stream\": self.streaming,\n            **{k: v for k, v in exclude_if_none.items() if v is not None},\n            **self.model_kwargs,\n        }\n\n        return params\n\n    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:\n        overall_token_usage: dict = {}\n        system_fingerprint = None\n        for output in llm_outputs:\n            if output is None:\n                # Happens in streaming\n                continue\n            token_usage = output[\"token_usage\"]\n            if token_usage is not None:\n                for k, v in token_usage.items():\n                    if v is None:\n                        continue\n                    if k in overall_token_usage:\n                        overall_token_usage[k] = _update_token_usage(\n                            overall_token_usage[k], v\n                        )\n                    else:\n                        overall_token_usage[k] = v\n            if system_fingerprint is None:\n                system_fingerprint = output.get(\"system_fingerprint\")\n        combined = {\"token_usage\": overall_token_usage, \"model_name\": self.model_name}\n        if system_fingerprint:\n            combined[\"system_fingerprint\"] = system_fingerprint\n        return combined\n\n    def _convert_chunk_to_generation_chunk(\n        self,\n        chunk: dict,\n        default_chunk_class: Type,\n        base_generation_info: Optional[Dict],\n    ) -> Optional[ChatGenerationChunk]:\n        if chunk.get(\"type\") == \"content.delta\":  # from beta.chat.completions.stream\n            return None\n        token_usage = chunk.get(\"usage\")\n        choices = (\n            chunk.get(\"choices\", [])\n            # from beta.chat.completions.stream\n            or chunk.get(\"chunk\", {}).get(\"choices\", [])\n        )\n\n        usage_metadata: Optional[UsageMetadata] = (\n            _create_usage_metadata(token_usage) if token_usage else None\n        )\n        if len(choices) == 0:\n            # logprobs is implicitly None\n            generation_chunk = ChatGenerationChunk(\n                message=default_chunk_class(content=\"\", usage_metadata=usage_metadata)\n            )\n            return generation_chunk\n\n        choice = choices[0]\n        if choice[\"delta\"] is None:\n            return None\n\n        message_chunk = _convert_delta_to_message_chunk(\n            choice[\"delta\"], default_chunk_class\n        )\n        generation_info = {**base_generation_info} if base_generation_info else {}\n\n        if finish_reason := choice.get(\"finish_reason\"):\n            generation_info[\"finish_reason\"] = finish_reason\n            if model_name := chunk.get(\"model\"):\n                generation_info[\"model_name\"] = model_name\n            if system_fingerprint := chunk.get(\"system_fingerprint\"):\n                generation_info[\"system_fingerprint\"] = system_fingerprint\n\n        logprobs = choice.get(\"logprobs\")\n        if logprobs:\n            generation_info[\"logprobs\"] = logprobs\n\n        if usage_metadata and isinstance(message_chunk, AIMessageChunk):\n            message_chunk.usage_metadata = usage_metadata\n\n        generation_chunk = ChatGenerationChunk(\n            message=message_chunk, generation_info=generation_info or None\n        )\n        return generation_chunk\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        base_generation_info = {}\n\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            response_stream = self.root_client.beta.chat.completions.stream(**payload)\n            context_manager = response_stream\n        else:\n            if self.include_response_headers:\n                raw_response = self.client.with_raw_response.create(**payload)\n                response = raw_response.parse()\n                base_generation_info = {\"headers\": dict(raw_response.headers)}\n            else:\n                response = self.client.create(**payload)\n            context_manager = response\n        try:\n            with context_manager as response:\n                is_first_chunk = True\n                for chunk in response:\n                    if not isinstance(chunk, dict):\n                        chunk = chunk.model_dump()\n                    generation_chunk = self._convert_chunk_to_generation_chunk(\n                        chunk,\n                        default_chunk_class,\n                        base_generation_info if is_first_chunk else {},\n                    )\n                    if generation_chunk is None:\n                        continue\n                    default_chunk_class = generation_chunk.message.__class__\n                    logprobs = (generation_chunk.generation_info or {}).get(\"logprobs\")\n                    if run_manager:\n                        run_manager.on_llm_new_token(\n                            generation_chunk.text,\n                            chunk=generation_chunk,\n                            logprobs=logprobs,\n                        )\n                    is_first_chunk = False\n                    yield generation_chunk\n        except openai.BadRequestError as e:\n            _handle_openai_bad_request(e)\n        if hasattr(response, \"get_final_completion\") and \"response_format\" in payload:\n            final_completion = response.get_final_completion()\n            generation_chunk = self._get_generation_chunk_from_completion(\n                final_completion\n            )\n            if run_manager:\n                run_manager.on_llm_new_token(\n                    generation_chunk.text, chunk=generation_chunk\n                )\n            yield generation_chunk\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        generation_info = None\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            try:\n                response = self.root_client.beta.chat.completions.parse(**payload)\n            except openai.BadRequestError as e:\n                _handle_openai_bad_request(e)\n        elif self.include_response_headers:\n            raw_response = self.client.with_raw_response.create(**payload)\n            response = raw_response.parse()\n            generation_info = {\"headers\": dict(raw_response.headers)}\n        else:\n            response = self.client.create(**payload)\n        return self._create_chat_result(response, generation_info)\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        messages = self._convert_input(input_).to_messages()\n        if stop is not None:\n            kwargs[\"stop\"] = stop\n\n        return {\n            \"messages\": [_convert_message_to_dict(m) for m in messages],\n            **self._default_params,\n            **kwargs,\n        }\n\n    def _create_chat_result(\n        self,\n        response: Union[dict, openai.BaseModel],\n        generation_info: Optional[Dict] = None,\n    ) -> ChatResult:\n        generations = []\n\n        response_dict = (\n            response if isinstance(response, dict) else response.model_dump()\n        )\n        # Sometimes the AI Model calling will get error, we should raise it.\n        # Otherwise, the next code 'choices.extend(response[\"choices\"])'\n        # will throw a \"TypeError: 'NoneType' object is not iterable\" error\n        # to mask the true error. Because 'response[\"choices\"]' is None.\n        if response_dict.get(\"error\"):\n            raise ValueError(response_dict.get(\"error\"))\n\n        token_usage = response_dict.get(\"usage\")\n        for res in response_dict[\"choices\"]:\n            message = _convert_dict_to_message(res[\"message\"])\n            if token_usage and isinstance(message, AIMessage):\n                message.usage_metadata = _create_usage_metadata(token_usage)\n            generation_info = generation_info or {}\n            generation_info[\"finish_reason\"] = (\n                res.get(\"finish_reason\")\n                if res.get(\"finish_reason\") is not None\n                else generation_info.get(\"finish_reason\")\n            )\n            if \"logprobs\" in res:\n                generation_info[\"logprobs\"] = res[\"logprobs\"]\n            gen = ChatGeneration(message=message, generation_info=generation_info)\n            generations.append(gen)\n        llm_output = {\n            \"token_usage\": token_usage,\n            \"model_name\": response_dict.get(\"model\", self.model_name),\n            \"system_fingerprint\": response_dict.get(\"system_fingerprint\", \"\"),\n        }\n\n        if isinstance(response, openai.BaseModel) and getattr(\n            response, \"choices\", None\n        ):\n            message = response.choices[0].message  # type: ignore[attr-defined]\n            if hasattr(message, \"parsed\"):\n                generations[0].message.additional_kwargs[\"parsed\"] = message.parsed\n            if hasattr(message, \"refusal\"):\n                generations[0].message.additional_kwargs[\"refusal\"] = message.refusal\n\n        return ChatResult(generations=generations, llm_output=llm_output)\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        kwargs[\"stream\"] = True\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        base_generation_info = {}\n\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            response_stream = self.root_async_client.beta.chat.completions.stream(\n                **payload\n            )\n            context_manager = response_stream\n        else:\n            if self.include_response_headers:\n                raw_response = await self.async_client.with_raw_response.create(\n                    **payload\n                )\n                response = raw_response.parse()\n                base_generation_info = {\"headers\": dict(raw_response.headers)}\n            else:\n                response = await self.async_client.create(**payload)\n            context_manager = response\n        try:\n            async with context_manager as response:\n                is_first_chunk = True\n                async for chunk in response:\n                    if not isinstance(chunk, dict):\n                        chunk = chunk.model_dump()\n                    generation_chunk = self._convert_chunk_to_generation_chunk(\n                        chunk,\n                        default_chunk_class,\n                        base_generation_info if is_first_chunk else {},\n                    )\n                    if generation_chunk is None:\n                        continue\n                    default_chunk_class = generation_chunk.message.__class__\n                    logprobs = (generation_chunk.generation_info or {}).get(\"logprobs\")\n                    if run_manager:\n                        await run_manager.on_llm_new_token(\n                            generation_chunk.text,\n                            chunk=generation_chunk,\n                            logprobs=logprobs,\n                        )\n                    is_first_chunk = False\n                    yield generation_chunk\n        except openai.BadRequestError as e:\n            _handle_openai_bad_request(e)\n        if hasattr(response, \"get_final_completion\") and \"response_format\" in payload:\n            final_completion = await response.get_final_completion()\n            generation_chunk = self._get_generation_chunk_from_completion(\n                final_completion\n            )\n            if run_manager:\n                await run_manager.on_llm_new_token(\n                    generation_chunk.text, chunk=generation_chunk\n                )\n            yield generation_chunk\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        if self.streaming:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n        payload = self._get_request_payload(messages, stop=stop, **kwargs)\n        generation_info = None\n        if \"response_format\" in payload:\n            if self.include_response_headers:\n                warnings.warn(\n                    \"Cannot currently include response headers when response_format is \"\n                    \"specified.\"\n                )\n            payload.pop(\"stream\")\n            try:\n                response = await self.root_async_client.beta.chat.completions.parse(\n                    **payload\n                )\n            except openai.BadRequestError as e:\n                _handle_openai_bad_request(e)\n        elif self.include_response_headers:\n            raw_response = await self.async_client.with_raw_response.create(**payload)\n            response = raw_response.parse()\n            generation_info = {\"headers\": dict(raw_response.headers)}\n        else:\n            response = await self.async_client.create(**payload)\n        return await run_in_executor(\n            None, self._create_chat_result, response, generation_info\n        )\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model_name\": self.model_name, **self._default_params}\n\n    def _get_invocation_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> Dict[str, Any]:\n        \"\"\"Get the parameters used to invoke the model.\"\"\"\n        return {\n            \"model\": self.model_name,\n            **super()._get_invocation_params(stop=stop),\n            **self._default_params,\n            **kwargs,\n        }\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"openai\",\n            ls_model_name=self.model_name,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens) or params.get(\n            \"max_completion_tokens\", self.max_tokens\n        ):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"openai-chat\"\n\n    def _get_encoding_model(self) -> Tuple[str, tiktoken.Encoding]:\n        if self.tiktoken_model_name is not None:\n            model = self.tiktoken_model_name\n        else:\n            model = self.model_name\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n        except KeyError:\n            model = \"cl100k_base\"\n            encoding = tiktoken.get_encoding(model)\n        return model, encoding\n\n    def get_token_ids(self, text: str) -> List[int]:\n        \"\"\"Get the tokens present in the text with tiktoken package.\"\"\"\n        if self.custom_get_token_ids is not None:\n            return self.custom_get_token_ids(text)\n        # tiktoken NOT supported for Python 3.7 or below\n        if sys.version_info[1] <= 7:\n            return super().get_token_ids(text)\n        _, encoding_model = self._get_encoding_model()\n        return encoding_model.encode(text)\n\n    def get_num_tokens_from_messages(\n        self,\n        messages: List[BaseMessage],\n        tools: Optional[\n            Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]]\n        ] = None,\n    ) -> int:\n        \"\"\"Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n\n        **Requirements**: You must have the ``pillow`` installed if you want to count\n        image tokens if you are specifying the image as a base64 string, and you must\n        have both ``pillow`` and ``httpx`` installed if you are specifying the image\n        as a URL. If these aren't installed image inputs will be ignored in token\n        counting.\n\n        OpenAI reference: https://github.com/openai/openai-cookbook/blob/\n        main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n\n        Args:\n            messages: The message inputs to tokenize.\n            tools: If provided, sequence of dict, BaseModel, function, or BaseTools\n                to be converted to tool schemas.\n        \"\"\"\n        # TODO: Count bound tools as part of input.\n        if tools is not None:\n            warnings.warn(\n                \"Counting tokens in tool schemas is not yet supported. Ignoring tools.\"\n            )\n        if sys.version_info[1] <= 7:\n            return super().get_num_tokens_from_messages(messages)\n        model, encoding = self._get_encoding_model()\n        if model.startswith(\"gpt-3.5-turbo-0301\"):\n            # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            tokens_per_message = 4\n            # if there's a name, the role is omitted\n            tokens_per_name = -1\n        elif model.startswith(\"gpt-3.5-turbo\") or model.startswith(\"gpt-4\"):\n            tokens_per_message = 3\n            tokens_per_name = 1\n        else:\n            raise NotImplementedError(\n                f\"get_num_tokens_from_messages() is not presently implemented \"\n                f\"for model {model}. See \"\n                \"https://platform.openai.com/docs/guides/text-generation/managing-tokens\"  # noqa: E501\n                \" for information on how messages are converted to tokens.\"\n            )\n        num_tokens = 0\n        messages_dict = [_convert_message_to_dict(m) for m in messages]\n        for message in messages_dict:\n            num_tokens += tokens_per_message\n            for key, value in message.items():\n                # This is an inferred approximation. OpenAI does not document how to\n                # count tool message tokens.\n                if key == \"tool_call_id\":\n                    num_tokens += 3\n                    continue\n                if isinstance(value, list):\n                    # content or tool calls\n                    for val in value:\n                        if isinstance(val, str) or val[\"type\"] == \"text\":\n                            text = val[\"text\"] if isinstance(val, dict) else val\n                            num_tokens += len(encoding.encode(text))\n                        elif val[\"type\"] == \"image_url\":\n                            if val[\"image_url\"].get(\"detail\") == \"low\":\n                                num_tokens += 85\n                            else:\n                                image_size = _url_to_size(val[\"image_url\"][\"url\"])\n                                if not image_size:\n                                    continue\n                                num_tokens += _count_image_tokens(*image_size)\n                        # Tool/function call token counting is not documented by OpenAI.\n                        # This is an approximation.\n                        elif val[\"type\"] == \"function\":\n                            num_tokens += len(\n                                encoding.encode(val[\"function\"][\"arguments\"])\n                            )\n                            num_tokens += len(encoding.encode(val[\"function\"][\"name\"]))\n                        else:\n                            raise ValueError(\n                                f\"Unrecognized content block type\\n\\n{val}\"\n                            )\n                elif not value:\n                    continue\n                else:\n                    # Cast str(value) in case the message value is not a string\n                    # This occurs with function messages\n                    num_tokens += len(encoding.encode(str(value)))\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        # every reply is primed with <im_start>assistant\n        num_tokens += 3\n        return num_tokens\n\n    @deprecated(\n        since=\"0.2.1\",\n        alternative=\"langchain_openai.chat_models.base.ChatOpenAI.bind_tools\",\n        removal=\"1.0.0\",\n    )\n    def bind_functions(\n        self,\n        functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n        function_call: Optional[\n            Union[_FunctionCall, str, Literal[\"auto\", \"none\"]]\n        ] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind functions (and other objects) to this chat model.\n\n        Assumes model is compatible with OpenAI function-calling API.\n\n        NOTE: Using bind_tools is recommended instead, as the `functions` and\n            `function_call` request parameters are officially marked as deprecated by\n            OpenAI.\n\n        Args:\n            functions: A list of function definitions to bind to this chat model.\n                Can be  a dictionary, pydantic model, or callable. Pydantic\n                models and callables will be automatically converted to\n                their schema dictionary representation.\n            function_call: Which function to require the model to call.\n                Must be the name of the single provided function or\n                \"auto\" to automatically determine which function to call\n                (if any).\n            **kwargs: Any additional parameters to pass to the\n                :class:`~langchain.runnable.Runnable` constructor.\n        \"\"\"\n\n        formatted_functions = [convert_to_openai_function(fn) for fn in functions]\n        if function_call is not None:\n            function_call = (\n                {\"name\": function_call}\n                if isinstance(function_call, str)\n                and function_call not in (\"auto\", \"none\")\n                else function_call\n            )\n            if isinstance(function_call, dict) and len(formatted_functions) != 1:\n                raise ValueError(\n                    \"When specifying `function_call`, you must provide exactly one \"\n                    \"function.\"\n                )\n            if (\n                isinstance(function_call, dict)\n                and formatted_functions[0][\"name\"] != function_call[\"name\"]\n            ):\n                raise ValueError(\n                    f\"Function call {function_call} was specified, but the only \"\n                    f\"provided function was {formatted_functions[0]['name']}.\"\n                )\n            kwargs = {**kwargs, \"function_call\": function_call}\n        return super().bind(functions=formatted_functions, **kwargs)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[\n            Union[dict, str, Literal[\"auto\", \"none\", \"required\", \"any\"], bool]\n        ] = None,\n        strict: Optional[bool] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        Assumes model is compatible with OpenAI tool-calling API.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports any tool definition handled by\n                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call. Options are:\n\n                - str of the form ``\"<<tool_name>>\"``: calls <<tool_name>> tool.\n                - ``\"auto\"``: automatically selects a tool (including no tool).\n                - ``\"none\"``: does not call a tool.\n                - ``\"any\"`` or ``\"required\"`` or ``True``: force at least one tool to be called.\n                - dict of the form ``{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}``: calls <<tool_name>> tool.\n                - ``False`` or ``None``: no effect, default OpenAI behavior.\n            strict: If True, model output is guaranteed to exactly match the JSON Schema\n                provided in the tool definition. If True, the input schema will be\n                validated according to\n                https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n                If False, input schema will not be validated and model output will not\n                be validated.\n                If None, ``strict`` argument will not be passed to the model.\n            parallel_tool_calls: Set to ``False`` to disable parallel tool use.\n                Defaults to ``None`` (no specification, which allows parallel tool use).\n            kwargs: Any additional parameters are passed directly to\n                :meth:`~langchain_openai.chat_models.base.ChatOpenAI.bind`.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n\n        \"\"\"  # noqa: E501\n\n        if parallel_tool_calls is not None:\n            kwargs[\"parallel_tool_calls\"] = parallel_tool_calls\n        formatted_tools = [\n            convert_to_openai_tool(tool, strict=strict) for tool in tools\n        ]\n        if tool_choice:\n            if isinstance(tool_choice, str):\n                # tool_choice is a tool/function name\n                if tool_choice not in (\"auto\", \"none\", \"any\", \"required\"):\n                    tool_choice = {\n                        \"type\": \"function\",\n                        \"function\": {\"name\": tool_choice},\n                    }\n                # 'any' is not natively supported by OpenAI API.\n                # We support 'any' since other models use this instead of 'required'.\n                if tool_choice == \"any\":\n                    tool_choice = \"required\"\n            elif isinstance(tool_choice, bool):\n                tool_choice = \"required\"\n            elif isinstance(tool_choice, dict):\n                tool_names = [\n                    formatted_tool[\"function\"][\"name\"]\n                    for formatted_tool in formatted_tools\n                ]\n                if not any(\n                    tool_name == tool_choice[\"function\"][\"name\"]\n                    for tool_name in tool_names\n                ):\n                    raise ValueError(\n                        f\"Tool choice {tool_choice} was specified, but the only \"\n                        f\"provided tools were {tool_names}.\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n                    f\"Received: {tool_choice}\"\n                )\n            kwargs[\"tool_choice\"] = tool_choice\n        return super().bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Optional[_DictOrPydanticClass] = None,\n        *,\n        method: Literal[\n            \"function_calling\", \"json_mode\", \"json_schema\"\n        ] = \"function_calling\",\n        include_raw: bool = False,\n        strict: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, _DictOrPydantic]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n\n                - an OpenAI function/tool schema,\n                - a JSON Schema,\n                - a TypedDict class (support added in 0.1.20),\n                - or a Pydantic class.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"function_calling\":\n                    Uses OpenAI's tool-calling (formerly called function calling)\n                    API: https://platform.openai.com/docs/guides/function-calling\n                - \"json_schema\":\n                    Uses OpenAI's Structured Output API: https://platform.openai.com/docs/guides/structured-outputs\n                    Supported for \"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"o1\", and later\n                    models.\n                - \"json_mode\":\n                    Uses OpenAI's JSON mode. Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call:\n                    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\n                Learn more about the differences between the methods and which models\n                support which methods here:\n\n                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode\n                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            strict:\n\n                - True:\n                    Model output is guaranteed to exactly match the schema.\n                    The input schema will also be validated according to\n                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n                - False:\n                    Input schema will not be validated and model output will not be\n                    validated.\n                - None:\n                    ``strict`` argument will not be passed to the model.\n\n            kwargs: Additional keyword args aren't supported.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n\n            - \"raw\": BaseMessage\n            - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n            - \"parsing_error\": Optional[BaseException]\n\n        .. versionchanged:: 0.1.20\n\n            Added support for TypedDict class ``schema``.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n            Support for ``method`` = \"json_schema\" added.\n        \"\"\"  # noqa: E501\n        if kwargs:\n            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n        if strict is not None and method == \"json_mode\":\n            raise ValueError(\n                \"Argument `strict` is not supported with `method`='json_mode'\"\n            )\n        is_pydantic_schema = _is_pydantic_class(schema)\n\n        if method == \"json_schema\":\n            # Check for Pydantic BaseModel V1\n            if (\n                is_pydantic_schema and issubclass(schema, BaseModelV1)  # type: ignore[arg-type]\n            ):\n                warnings.warn(\n                    \"Received a Pydantic BaseModel V1 schema. This is not supported by \"\n                    'method=\"json_schema\". Please use method=\"function_calling\" '\n                    \"or specify schema via JSON Schema or Pydantic V2 BaseModel. \"\n                    'Overriding to method=\"function_calling\".'\n                )\n                method = \"function_calling\"\n            # Check for incompatible model\n            if self.model_name and (\n                self.model_name.startswith(\"gpt-3\")\n                or self.model_name.startswith(\"gpt-4-\")\n                or self.model_name == \"gpt-4\"\n            ):\n                warnings.warn(\n                    f\"Cannot use method='json_schema' with model {self.model_name} \"\n                    f\"since it doesn't support OpenAI's Structured Output API. You can \"\n                    f\"see supported models here: \"\n                    f\"https://platform.openai.com/docs/guides/structured-outputs#supported-models. \"  # noqa: E501\n                    \"To fix this warning, set `method='function_calling'. \"\n                    \"Overriding to method='function_calling'.\"\n                )\n                method = \"function_calling\"\n\n        if method == \"function_calling\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is not 'json_mode'. \"\n                    \"Received None.\"\n                )\n            tool_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n            bind_kwargs = self._filter_disabled_params(\n                tool_choice=tool_name,\n                parallel_tool_calls=False,\n                strict=strict,\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": schema,\n                },\n            )\n\n            llm = self.bind_tools([schema], **bind_kwargs)\n            if is_pydantic_schema:\n                output_parser: Runnable = PydanticToolsParser(\n                    tools=[schema],  # type: ignore[list-item]\n                    first_tool_only=True,  # type: ignore[list-item]\n                )\n            else:\n                output_parser = JsonOutputKeyToolsParser(\n                    key_name=tool_name, first_tool_only=True\n                )\n        elif method == \"json_mode\":\n            llm = self.bind(\n                response_format={\"type\": \"json_object\"},\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": schema,\n                },\n            )\n            output_parser = (\n                PydanticOutputParser(pydantic_object=schema)  # type: ignore[arg-type]\n                if is_pydantic_schema\n                else JsonOutputParser()\n            )\n        elif method == \"json_schema\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is not 'json_mode'. \"\n                    \"Received None.\"\n                )\n            response_format = _convert_to_openai_response_format(schema, strict=strict)\n            llm = self.bind(\n                response_format=response_format,\n                structured_output_format={\n                    \"kwargs\": {\"method\": method},\n                    \"schema\": convert_to_openai_tool(schema),\n                },\n            )\n            if is_pydantic_schema:\n                output_parser = _oai_structured_outputs_parser.with_types(\n                    output_type=cast(type, schema)\n                )\n            else:\n                output_parser = JsonOutputParser()\n        else:\n            raise ValueError(\n                f\"Unrecognized method argument. Expected one of 'function_calling' or \"\n                f\"'json_mode'. Received: '{method}'\"\n            )\n\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    def _filter_disabled_params(self, **kwargs: Any) -> Dict[str, Any]:\n        if not self.disabled_params:\n            return kwargs\n        filtered = {}\n        for k, v in kwargs.items():\n            # Skip param\n            if k in self.disabled_params and (\n                self.disabled_params[k] is None or v in self.disabled_params[k]\n            ):\n                continue\n            # Keep param\n            else:\n                filtered[k] = v\n        return filtered\n\n    def _get_generation_chunk_from_completion(\n        self, completion: openai.BaseModel\n    ) -> ChatGenerationChunk:\n        \"\"\"Get chunk from completion (e.g., from final completion of a stream).\"\"\"\n        chat_result = self._create_chat_result(completion)\n        chat_message = chat_result.generations[0].message\n        if isinstance(chat_message, AIMessage):\n            usage_metadata = chat_message.usage_metadata\n            # Skip tool_calls, already sent as chunks\n            if \"tool_calls\" in chat_message.additional_kwargs:\n                chat_message.additional_kwargs.pop(\"tool_calls\")\n        else:\n            usage_metadata = None\n        message = AIMessageChunk(\n            content=\"\",\n            additional_kwargs=chat_message.additional_kwargs,\n            usage_metadata=usage_metadata,\n        )\n        return ChatGenerationChunk(\n            message=message, generation_info=chat_result.llm_output\n        )\n\n\nclass ChatOpenAI(BaseChatOpenAI):  # type: ignore[override]\n    \"\"\"OpenAI chat model integration.\n\n    .. dropdown:: Setup\n        :open:\n\n        Install ``langchain-openai`` and set environment variable ``OPENAI_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-openai\n            export OPENAI_API_KEY=\"your-api-key\"\n\n    .. dropdown:: Key init args \u2014 completion params\n\n        model: str\n            Name of OpenAI model to use.\n        temperature: float\n            Sampling temperature.\n        max_tokens: Optional[int]\n            Max number of tokens to generate.\n        logprobs: Optional[bool]\n            Whether to return logprobs.\n        stream_options: Dict\n            Configure streaming outputs, like whether to return token usage when\n            streaming (``{\"include_usage\": True}``).\n\n        See full list of supported init args and their descriptions in the params section.\n\n    .. dropdown:: Key init args \u2014 client params\n\n        timeout: Union[float, Tuple[float, float], Any, None]\n            Timeout for requests.\n        max_retries: Optional[int]\n            Max number of retries.\n        api_key: Optional[str]\n            OpenAI API key. If not passed in will be read from env var OPENAI_API_KEY.\n        base_url: Optional[str]\n            Base URL for API requests. Only specify if using a proxy or service\n            emulator.\n        organization: Optional[str]\n            OpenAI organization ID. If not passed in will be read from env\n            var OPENAI_ORG_ID.\n\n        See full list of supported init args and their descriptions in the params section.\n\n    .. dropdown:: Instantiate\n\n        .. code-block:: python\n\n            from langchain_openai import ChatOpenAI\n\n            llm = ChatOpenAI(\n                model=\"gpt-4o\",\n                temperature=0,\n                max_tokens=None,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # base_url=\"...\",\n                # organization=\"...\",\n                # other params...\n            )\n\n        **NOTE**: Any param which is not explicitly supported will be passed directly to the\n        ``openai.OpenAI.chat.completions.create(...)`` API every time to the model is\n        invoked. For example:\n\n        .. code-block:: python\n\n            from langchain_openai import ChatOpenAI\n            import openai\n\n            ChatOpenAI(..., frequency_penalty=0.2).invoke(...)\n\n            # results in underlying API call of:\n\n            openai.OpenAI(..).chat.completions.create(..., frequency_penalty=0.2)\n\n            # which is also equivalent to:\n\n            ChatOpenAI(...).invoke(..., frequency_penalty=0.2)\n\n    .. dropdown:: Invoke\n\n        .. code-block:: python\n\n            messages = [\n                (\n                    \"system\",\n                    \"You are a helpful translator. Translate the user sentence to French.\",\n                ),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n        .. code-block:: pycon\n\n            AIMessage(\n                content=\"J'adore la programmation.\",\n                response_metadata={\n                    \"token_usage\": {\n                        \"completion_tokens\": 5,\n                        \"prompt_tokens\": 31,\n                        \"total_tokens\": 36,\n                    },\n                    \"model_name\": \"gpt-4o\",\n                    \"system_fingerprint\": \"fp_43dfabdef1\",\n                    \"finish_reason\": \"stop\",\n                    \"logprobs\": None,\n                },\n                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n            )\n\n    .. dropdown:: Stream\n\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            AIMessageChunk(content=\"\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\"J\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\"'adore\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(content=\" la\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(\n                content=\" programmation\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\"\n            )\n            AIMessageChunk(content=\".\", id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\")\n            AIMessageChunk(\n                content=\"\",\n                response_metadata={\"finish_reason\": \"stop\"},\n                id=\"run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0\",\n            )\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n        .. code-block:: python\n\n            AIMessageChunk(\n                content=\"J'adore la programmation.\",\n                response_metadata={\"finish_reason\": \"stop\"},\n                id=\"run-bf917526-7f58-4683-84f7-36a6b671d140\",\n            )\n\n    .. dropdown:: Async\n\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n        .. code-block:: python\n\n            AIMessage(\n                content=\"J'adore la programmation.\",\n                response_metadata={\n                    \"token_usage\": {\n                        \"completion_tokens\": 5,\n                        \"prompt_tokens\": 31,\n                        \"total_tokens\": 36,\n                    },\n                    \"model_name\": \"gpt-4o\",\n                    \"system_fingerprint\": \"fp_43dfabdef1\",\n                    \"finish_reason\": \"stop\",\n                    \"logprobs\": None,\n                },\n                id=\"run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0\",\n                usage_metadata={\"input_tokens\": 31, \"output_tokens\": 5, \"total_tokens\": 36},\n            )\n\n    .. dropdown:: Tool calling\n\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(\n                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n                )\n\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(\n                    ..., description=\"The city and state, e.g. San Francisco, CA\"\n                )\n\n\n            llm_with_tools = llm.bind_tools(\n                [GetWeather, GetPopulation]\n                # strict = True  # enforce tool args schema is respected\n            )\n            ai_msg = llm_with_tools.invoke(\n                \"Which city is hotter today and which is bigger: LA or NY?\"\n            )\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_6XswGD5Pqk8Tt5atYr7tfenU\",\n                },\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"New York, NY\"},\n                    \"id\": \"call_ZVL15vA8Y7kXqOy3dtmQgeCi\",\n                },\n                {\n                    \"name\": \"GetPopulation\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_49CFW8zqC9W7mh7hbMLSIrXw\",\n                },\n                {\n                    \"name\": \"GetPopulation\",\n                    \"args\": {\"location\": \"New York, NY\"},\n                    \"id\": \"call_6ghfKxV264jEfe1mRIkS3PE7\",\n                },\n            ]\n\n        Note that ``openai >= 1.32`` supports a ``parallel_tool_calls`` parameter\n        that defaults to ``True``. This parameter can be set to ``False`` to\n        disable parallel tool calls:\n\n        .. code-block:: python\n\n            ai_msg = llm_with_tools.invoke(\n                \"What is the weather in LA and NY?\", parallel_tool_calls=False\n            )\n            ai_msg.tool_calls\n\n        .. code-block:: python\n\n            [\n                {\n                    \"name\": \"GetWeather\",\n                    \"args\": {\"location\": \"Los Angeles, CA\"},\n                    \"id\": \"call_4OoY0ZR99iEvC7fevsH8Uhtz\",\n                }\n            ]\n\n        Like other runtime parameters, ``parallel_tool_calls`` can be bound to a model\n        using ``llm.bind(parallel_tool_calls=False)`` or during instantiation by\n        setting ``model_kwargs``.\n\n        See ``ChatOpenAI.bind_tools()`` method for more.\n\n    .. dropdown:: Structured output\n\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        .. code-block:: python\n\n            Joke(\n                setup=\"Why was the cat sitting on the computer?\",\n                punchline=\"To keep an eye on the mouse!\",\n                rating=None,\n            )\n\n        See ``ChatOpenAI.with_structured_output()`` for more.\n\n    .. dropdown:: JSON mode\n\n        .. code-block:: python\n\n            json_llm = llm.bind(response_format={\"type\": \"json_object\"})\n            ai_msg = json_llm.invoke(\n                \"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\"\n            )\n            ai_msg.content\n\n        .. code-block:: python\n\n            '\\\\n{\\\\n  \"random_ints\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\\\\n}'\n\n    .. dropdown:: Image input\n\n        .. code-block:: python\n\n            import base64\n            import httpx\n            from langchain_core.messages import HumanMessage\n\n            image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n            message = HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                    },\n                ]\n            )\n            ai_msg = llm.invoke([message])\n            ai_msg.content\n\n        .. code-block:: python\n\n            \"The weather in the image appears to be clear and pleasant. The sky is mostly blue with scattered, light clouds, suggesting a sunny day with minimal cloud cover. There is no indication of rain or strong winds, and the overall scene looks bright and calm. The lush green grass and clear visibility further indicate good weather conditions.\"\n\n    .. dropdown:: Token usage\n\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n\n        When streaming, set the ``stream_usage`` kwarg:\n\n        .. code-block:: python\n\n            stream = llm.stream(messages, stream_usage=True)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full.usage_metadata\n\n        .. code-block:: python\n\n            {\"input_tokens\": 28, \"output_tokens\": 5, \"total_tokens\": 33}\n\n        Alternatively, setting ``stream_usage`` when instantiating the model can be\n        useful when incorporating ``ChatOpenAI`` into LCEL chains-- or when using\n        methods like ``.with_structured_output``, which generate chains under the\n        hood.\n\n        .. code-block:: python\n\n            llm = ChatOpenAI(model=\"gpt-4o\", stream_usage=True)\n            structured_llm = llm.with_structured_output(...)\n\n    .. dropdown:: Logprobs\n\n        .. code-block:: python\n\n            logprobs_llm = llm.bind(logprobs=True)\n            ai_msg = logprobs_llm.invoke(messages)\n            ai_msg.response_metadata[\"logprobs\"]\n\n        .. code-block:: python\n\n            {\n                \"content\": [\n                    {\n                        \"token\": \"J\",\n                        \"bytes\": [74],\n                        \"logprob\": -4.9617593e-06,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \"'adore\",\n                        \"bytes\": [39, 97, 100, 111, 114, 101],\n                        \"logprob\": -0.25202933,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \" la\",\n                        \"bytes\": [32, 108, 97],\n                        \"logprob\": -0.20141791,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \" programmation\",\n                        \"bytes\": [\n                            32,\n                            112,\n                            114,\n                            111,\n                            103,\n                            114,\n                            97,\n                            109,\n                            109,\n                            97,\n                            116,\n                            105,\n                            111,\n                            110,\n                        ],\n                        \"logprob\": -1.9361265e-07,\n                        \"top_logprobs\": [],\n                    },\n                    {\n                        \"token\": \".\",\n                        \"bytes\": [46],\n                        \"logprob\": -1.2233183e-05,\n                        \"top_logprobs\": [],\n                    },\n                ]\n            }\n\n    .. dropdown:: Response metadata\n\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n        .. code-block:: python\n\n            {\n                \"token_usage\": {\n                    \"completion_tokens\": 5,\n                    \"prompt_tokens\": 28,\n                    \"total_tokens\": 33,\n                },\n                \"model_name\": \"gpt-4o\",\n                \"system_fingerprint\": \"fp_319be4768e\",\n                \"finish_reason\": \"stop\",\n                \"logprobs\": None,\n            }\n\n    \"\"\"  # noqa: E501\n\n    stream_usage: bool = False\n    \"\"\"Whether to include usage metadata in streaming output. If True, additional\n    message chunks will be generated during the stream including usage metadata.\n    \"\"\"\n\n    max_tokens: Optional[int] = Field(default=None, alias=\"max_completion_tokens\")\n    \"\"\"Maximum number of tokens to generate.\"\"\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"openai_api_key\": \"OPENAI_API_KEY\"}\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"openai\"]\n\n    @property\n    def lc_attributes(self) -> Dict[str, Any]:\n        attributes: Dict[str, Any] = {}\n\n        if self.openai_organization:\n            attributes[\"openai_organization\"] = self.openai_organization\n\n        if self.openai_api_base:\n            attributes[\"openai_api_base\"] = self.openai_api_base\n\n        if self.openai_proxy:\n            attributes[\"openai_proxy\"] = self.openai_proxy\n\n        return attributes\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this model can be serialized by Langchain.\"\"\"\n        return True\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        params = super()._default_params\n        if \"max_tokens\" in params:\n            params[\"max_completion_tokens\"] = params.pop(\"max_tokens\")\n\n        return params\n\n    def _get_request_payload(\n        self,\n        input_: LanguageModelInput,\n        *,\n        stop: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> dict:\n        payload = super()._get_request_payload(input_, stop=stop, **kwargs)\n        # max_tokens was deprecated in favor of max_completion_tokens\n        # in September 2024 release\n        if \"max_tokens\" in payload:\n            payload[\"max_completion_tokens\"] = payload.pop(\"max_tokens\")\n        return payload\n\n    def _should_stream_usage(\n        self, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> bool:\n        \"\"\"Determine whether to include usage metadata in streaming output.\n\n        For backwards compatibility, we check for `stream_options` passed\n        explicitly to kwargs or in the model_kwargs and override self.stream_usage.\n        \"\"\"\n        stream_usage_sources = [  # order of preference\n            stream_usage,\n            kwargs.get(\"stream_options\", {}).get(\"include_usage\"),\n            self.model_kwargs.get(\"stream_options\", {}).get(\"include_usage\"),\n            self.stream_usage,\n        ]\n        for source in stream_usage_sources:\n            if isinstance(source, bool):\n                return source\n        return self.stream_usage\n\n    def _stream(\n        self, *args: Any, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> Iterator[ChatGenerationChunk]:\n        \"\"\"Set default stream_options.\"\"\"\n        stream_usage = self._should_stream_usage(stream_usage, **kwargs)\n        # Note: stream_options is not a valid parameter for Azure OpenAI.\n        # To support users proxying Azure through ChatOpenAI, here we only specify\n        # stream_options if include_usage is set to True.\n        # See https://learn.microsoft.com/en-us/azure/ai-services/openai/whats-new\n        # for release notes.\n        if stream_usage:\n            kwargs[\"stream_options\"] = {\"include_usage\": stream_usage}\n\n        return super()._stream(*args, **kwargs)\n\n    async def _astream(\n        self, *args: Any, stream_usage: Optional[bool] = None, **kwargs: Any\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        \"\"\"Set default stream_options.\"\"\"\n        stream_usage = self._should_stream_usage(stream_usage, **kwargs)\n        if stream_usage:\n            kwargs[\"stream_options\"] = {\"include_usage\": stream_usage}\n\n        async for chunk in super()._astream(*args, **kwargs):\n            yield chunk\n\n    def with_structured_output(\n        self,\n        schema: Optional[_DictOrPydanticClass] = None,\n        *,\n        method: Literal[\"function_calling\", \"json_mode\", \"json_schema\"] = \"json_schema\",\n        include_raw: bool = False,\n        strict: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, _DictOrPydantic]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n\n                - a JSON Schema,\n                - a TypedDict class,\n                - or a Pydantic class,\n                - an OpenAI function/tool schema.\n\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"json_schema\":\n                    Uses OpenAI's Structured Output API:\n                    https://platform.openai.com/docs/guides/structured-outputs\n                    Supported for \"gpt-4o-mini\", \"gpt-4o-2024-08-06\", \"o1\", and later\n                    models.\n                - \"function_calling\":\n                    Uses OpenAI's tool-calling (formerly called function calling)\n                    API: https://platform.openai.com/docs/guides/function-calling\n                - \"json_mode\":\n                    Uses OpenAI's JSON mode. Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call:\n                    https://platform.openai.com/docs/guides/structured-outputs/json-mode\n\n                Learn more about the differences between the methods and which models\n                support which methods here:\n\n                - https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode\n                - https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n            strict:\n\n                - True:\n                    Model output is guaranteed to exactly match the schema.\n                    The input schema will also be validated according to\n                    https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n                - False:\n                    Input schema will not be validated and model output will not be\n                    validated.\n                - None:\n                    ``strict`` argument will not be passed to the model.\n\n                If schema is specified via TypedDict or JSON schema, ``strict`` is not\n                enabled by default. Pass ``strict=True`` to enable it.\n\n                Note: ``strict`` can only be non-null if ``method`` is\n                ``\"json_schema\"`` or ``\"function_calling\"``.\n\n            kwargs: Additional keyword args aren't supported.\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            | If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs an instance of ``schema`` (i.e., a Pydantic object). Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            | If ``include_raw`` is True, then Runnable outputs a dict with keys:\n\n            - \"raw\": BaseMessage\n            - \"parsed\": None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n            - \"parsing_error\": Optional[BaseException]\n\n        .. versionchanged:: 0.1.20\n\n            Added support for TypedDict class ``schema``.\n\n        .. versionchanged:: 0.1.21\n\n            Support for ``strict`` argument added.\n            Support for ``method=\"json_schema\"`` added.\n\n        .. versionchanged:: 0.3.0\n\n            ``method`` default changed from \"function_calling\" to \"json_schema\".\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=False, strict=True\n\n            Note, OpenAI has a number of restrictions on what types of schemas can be\n            provided if ``strict`` = True. When using Pydantic, our model cannot\n            specify any Field metadata (like min/max constraints) and fields cannot\n            have default values.\n\n            See all constraints here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas\n\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Optional[str] = Field(\n                        default=..., description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"function_calling\", include_raw=False, strict=False\n\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Optional[str] = Field(\n                        default=..., description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, method=\"function_calling\"\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_schema\", include_raw=True\n\n            .. code-block:: python\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: str\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        .. dropdown:: Example: schema=TypedDict class, method=\"json_schema\", include_raw=False, strict=False\n\n            .. code-block:: python\n\n                # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n                # from typing_extensions, not from typing.\n                from typing_extensions import Annotated, TypedDict\n\n                from langchain_openai import ChatOpenAI\n\n\n                class AnswerWithJustification(TypedDict):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Annotated[\n                        Optional[str], None, \"A justification for the answer.\"\n                    ]\n\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. dropdown:: Example: schema=OpenAI function schema, method=\"json_schema\", include_raw=False\n\n            .. code-block:: python\n\n                from langchain_openai import ChatOpenAI\n\n                oai_schema = {\n                    'name': 'AnswerWithJustification',\n                    'description': 'An answer to the user question along with justification for the answer.',\n                    'parameters': {\n                        'type': 'object',\n                        'properties': {\n                            'answer': {'type': 'string'},\n                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n                        },\n                       'required': ['answer']\n                   }\n               }\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(oai_schema)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        .. dropdown:: Example: schema=Pydantic class, method=\"json_mode\", include_raw=True\n\n            .. code-block::\n\n                from langchain_openai import ChatOpenAI\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    answer: str\n                    justification: str\n\n                llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification,\n                    method=\"json_mode\",\n                    include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n                #     'parsing_error': None\n                # }\n\n        .. dropdown:: Example: schema=None, method=\"json_mode\", include_raw=True\n\n            .. code-block::\n\n                structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': {\n                #         'answer': 'They are both the same weight.',\n                #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n                #     },\n                #     'parsing_error': None\n                # }\n        \"\"\"  # noqa: E501\n        return super().with_structured_output(\n            schema, method=method, include_raw=include_raw, strict=strict, **kwargs\n        )\n\n\ndef _is_pydantic_class(obj: Any) -> bool:\n    return isinstance(obj, type) and is_basemodel_subclass(obj)\n\n\ndef _lc_tool_call_to_openai_tool_call(tool_call: ToolCall) -> dict:\n    return {\n        \"type\": \"function\",\n        \"id\": tool_call[\"id\"],\n        \"function\": {\n            \"name\": tool_call[\"name\"],\n            \"arguments\": json.dumps(tool_call[\"args\"]),\n        },\n    }\n\n\ndef _lc_invalid_tool_call_to_openai_tool_call(\n    invalid_tool_call: InvalidToolCall,\n) -> dict:\n    return {\n        \"type\": \"function\",\n        \"id\": invalid_tool_call[\"id\"],\n        \"function\": {\n            \"name\": invalid_tool_call[\"name\"],\n            \"arguments\": invalid_tool_call[\"args\"],\n        },\n    }\n\n\ndef _url_to_size(image_source: str) -> Optional[Tuple[int, int]]:\n    try:\n        from PIL import Image  # type: ignore[import]\n    except ImportError:\n        logger.info(\n            \"Unable to count image tokens. To count image tokens please install \"\n            \"`pip install -U pillow httpx`.\"\n        )\n        return None\n    if _is_url(image_source):\n        try:\n            import httpx\n        except ImportError:\n            logger.info(\n                \"Unable to count image tokens. To count image tokens please install \"\n                \"`pip install -U httpx`.\"\n            )\n            return None\n        response = httpx.get(image_source)\n        response.raise_for_status()\n        width, height = Image.open(BytesIO(response.content)).size\n        return width, height\n    elif _is_b64(image_source):\n        _, encoded = image_source.split(\",\", 1)\n        data = base64.b64decode(encoded)\n        width, height = Image.open(BytesIO(data)).size\n        return width, height\n    else:\n        return None\n\n\ndef _count_image_tokens(width: int, height: int) -> int:\n    # Reference: https://platform.openai.com/docs/guides/vision/calculating-costs\n    width, height = _resize(width, height)\n    h = ceil(height / 512)\n    w = ceil(width / 512)\n    return (170 * h * w) + 85\n\n\ndef _is_url(s: str) -> bool:\n    try:\n        result = urlparse(s)\n        return all([result.scheme, result.netloc])\n    except Exception as e:\n        logger.debug(f\"Unable to parse URL: {e}\")\n        return False\n\n\ndef _is_b64(s: str) -> bool:\n    return s.startswith(\"data:image\")\n\n\ndef _resize(width: int, height: int) -> Tuple[int, int]:\n    # larger side must be <= 2048\n    if width > 2048 or height > 2048:\n        if width > height:\n            height = (height * 2048) // width\n            width = 2048\n        else:\n            width = (width * 2048) // height\n            height = 2048\n    # smaller side must be <= 768\n    if width > 768 and height > 768:\n        if width > height:\n            width = (width * 768) // height\n            height = 768\n        else:\n            height = (width * 768) // height\n            width = 768\n    return width, height\n\n\ndef _convert_to_openai_response_format(\n    schema: Union[Dict[str, Any], Type], *, strict: Optional[bool] = None\n) -> Union[Dict, TypeBaseModel]:\n    if isinstance(schema, type) and is_basemodel_subclass(schema):\n        return schema\n\n    if (\n        isinstance(schema, dict)\n        and \"json_schema\" in schema\n        and schema.get(\"type\") == \"json_schema\"\n    ):\n        response_format = schema\n    elif isinstance(schema, dict) and \"name\" in schema and \"schema\" in schema:\n        response_format = {\"type\": \"json_schema\", \"json_schema\": schema}\n    else:\n        if strict is None:\n            if isinstance(schema, dict) and isinstance(schema.get(\"strict\"), bool):\n                strict = schema[\"strict\"]\n            else:\n                strict = False\n        function = convert_to_openai_function(schema, strict=strict)\n        function[\"schema\"] = function.pop(\"parameters\")\n        response_format = {\"type\": \"json_schema\", \"json_schema\": function}\n\n    if strict is not None and strict is not response_format[\"json_schema\"].get(\n        \"strict\"\n    ):\n        msg = (\n            f\"Output schema already has 'strict' value set to \"\n            f\"{schema['json_schema']['strict']} but 'strict' also passed in to \"\n            f\"with_structured_output as {strict}. Please make sure that \"\n            f\"'strict' is only specified in one place.\"\n        )\n        raise ValueError(msg)\n    return response_format\n\n\n@chain\ndef _oai_structured_outputs_parser(ai_msg: AIMessage) -> PydanticBaseModel:\n    if ai_msg.additional_kwargs.get(\"parsed\"):\n        return ai_msg.additional_kwargs[\"parsed\"]\n    elif ai_msg.additional_kwargs.get(\"refusal\"):\n        raise OpenAIRefusalError(ai_msg.additional_kwargs[\"refusal\"])\n    else:\n        raise ValueError(\n            \"Structured Output response does not have a 'parsed' field nor a 'refusal' \"\n            f\"field. Received message:\\n\\n{ai_msg}\"\n        )\n\n\nclass OpenAIRefusalError(Exception):\n    \"\"\"Error raised when OpenAI Structured Outputs API returns a refusal.\n\n    When using OpenAI's Structured Outputs API with user-generated input, the model\n    may occasionally refuse to fulfill the request for safety reasons.\n\n    See here for more on refusals:\n    https://platform.openai.com/docs/guides/structured-outputs/refusals\n\n    .. versionadded:: 0.1.21\n    \"\"\"\n\n\ndef _create_usage_metadata(oai_token_usage: dict) -> UsageMetadata:\n    input_tokens = oai_token_usage.get(\"prompt_tokens\", 0)\n    output_tokens = oai_token_usage.get(\"completion_tokens\", 0)\n    total_tokens = oai_token_usage.get(\"total_tokens\", input_tokens + output_tokens)\n    input_token_details: dict = {\n        \"audio\": (oai_token_usage.get(\"prompt_tokens_details\") or {}).get(\n            \"audio_tokens\"\n        ),\n        \"cache_read\": (oai_token_usage.get(\"prompt_tokens_details\") or {}).get(\n            \"cached_tokens\"\n        ),\n    }\n    output_token_details: dict = {\n        \"audio\": (oai_token_usage.get(\"completion_tokens_details\") or {}).get(\n            \"audio_tokens\"\n        ),\n        \"reasoning\": (oai_token_usage.get(\"completion_tokens_details\") or {}).get(\n            \"reasoning_tokens\"\n        ),\n    }\n    return UsageMetadata(\n        input_tokens=input_tokens,\n        output_tokens=output_tokens,\n        total_tokens=total_tokens,\n        input_token_details=InputTokenDetails(\n            **{k: v for k, v in input_token_details.items() if v is not None}\n        ),\n        output_token_details=OutputTokenDetails(\n            **{k: v for k, v in output_token_details.items() if v is not None}\n        ),\n    )\n",
        "patch": "@@ -6,6 +6,7 @@\n import json\n import logging\n import os\n+import re\n import sys\n import warnings\n from io import BytesIO\n@@ -2011,6 +2012,12 @@ def _get_request_payload(\n         # in September 2024 release\n         if \"max_tokens\" in payload:\n             payload[\"max_completion_tokens\"] = payload.pop(\"max_tokens\")\n+\n+        # Mutate system message role to \"developer\" for o-series models\n+        if self.model_name and re.match(r\"^o\\d\", self.model_name):\n+            for message in payload.get(\"messages\", []):\n+                if message[\"role\"] == \"system\":\n+                    message[\"role\"] = \"developer\"\n         return payload\n \n     def _should_stream_usage("
      },
      {
        "filename": "libs/partners/openai/tests/unit_tests/chat_models/test_base.py",
        "content_before": "\"\"\"Test OpenAI Chat API wrapper.\"\"\"\n\nimport json\nfrom types import TracebackType\nfrom typing import Any, Dict, List, Literal, Optional, Type, Union\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\nimport pytest\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    FunctionMessage,\n    HumanMessage,\n    InvalidToolCall,\n    SystemMessage,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.ai import UsageMetadata\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai.chat_models.base import (\n    _convert_dict_to_message,\n    _convert_message_to_dict,\n    _convert_to_openai_response_format,\n    _create_usage_metadata,\n    _format_message_content,\n)\n\n\ndef test_openai_model_param() -> None:\n    llm = ChatOpenAI(model=\"foo\")\n    assert llm.model_name == \"foo\"\n    llm = ChatOpenAI(model_name=\"foo\")  # type: ignore[call-arg]\n    assert llm.model_name == \"foo\"\n\n    llm = ChatOpenAI(max_tokens=10)  # type: ignore[call-arg]\n    assert llm.max_tokens == 10\n    llm = ChatOpenAI(max_completion_tokens=10)\n    assert llm.max_tokens == 10\n\n\ndef test_openai_o1_temperature() -> None:\n    llm = ChatOpenAI(model=\"o1-preview\")\n    assert llm.temperature == 1\n    llm = ChatOpenAI(model_name=\"o1-mini\")  # type: ignore[call-arg]\n    assert llm.temperature == 1\n\n\ndef test_function_message_dict_to_function_message() -> None:\n    content = json.dumps({\"result\": \"Example #1\"})\n    name = \"test_function\"\n    result = _convert_dict_to_message(\n        {\"role\": \"function\", \"name\": name, \"content\": content}\n    )\n    assert isinstance(result, FunctionMessage)\n    assert result.name == name\n    assert result.content == content\n\n\ndef test__convert_dict_to_message_human() -> None:\n    message = {\"role\": \"user\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = HumanMessage(content=\"foo\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_human_with_name() -> None:\n    message = {\"role\": \"user\", \"content\": \"foo\", \"name\": \"test\"}\n    result = _convert_dict_to_message(message)\n    expected_output = HumanMessage(content=\"foo\", name=\"test\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_ai() -> None:\n    message = {\"role\": \"assistant\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(content=\"foo\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_ai_with_name() -> None:\n    message = {\"role\": \"assistant\", \"content\": \"foo\", \"name\": \"test\"}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(content=\"foo\", name=\"test\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_system() -> None:\n    message = {\"role\": \"system\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = SystemMessage(content=\"foo\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_developer() -> None:\n    message = {\"role\": \"developer\", \"content\": \"foo\"}\n    result = _convert_dict_to_message(message)\n    expected_output = SystemMessage(\n        content=\"foo\", additional_kwargs={\"__openai_role__\": \"developer\"}\n    )\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_system_with_name() -> None:\n    message = {\"role\": \"system\", \"content\": \"foo\", \"name\": \"test\"}\n    result = _convert_dict_to_message(message)\n    expected_output = SystemMessage(content=\"foo\", name=\"test\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_tool() -> None:\n    message = {\"role\": \"tool\", \"content\": \"foo\", \"tool_call_id\": \"bar\"}\n    result = _convert_dict_to_message(message)\n    expected_output = ToolMessage(content=\"foo\", tool_call_id=\"bar\")\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n\ndef test__convert_dict_to_message_tool_call() -> None:\n    raw_tool_call = {\n        \"id\": \"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n        \"function\": {\n            \"arguments\": '{\"name\": \"Sally\", \"hair_color\": \"green\"}',\n            \"name\": \"GenerateUsername\",\n        },\n        \"type\": \"function\",\n    }\n    message = {\"role\": \"assistant\", \"content\": None, \"tool_calls\": [raw_tool_call]}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(\n        content=\"\",\n        additional_kwargs={\"tool_calls\": [raw_tool_call]},\n        tool_calls=[\n            ToolCall(\n                name=\"GenerateUsername\",\n                args={\"name\": \"Sally\", \"hair_color\": \"green\"},\n                id=\"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n                type=\"tool_call\",\n            )\n        ],\n    )\n    assert result == expected_output\n    assert _convert_message_to_dict(expected_output) == message\n\n    # Test malformed tool call\n    raw_tool_calls: list = [\n        {\n            \"id\": \"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n            \"function\": {\"arguments\": \"oops\", \"name\": \"GenerateUsername\"},\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"call_abc123\",\n            \"function\": {\n                \"arguments\": '{\"name\": \"Sally\", \"hair_color\": \"green\"}',\n                \"name\": \"GenerateUsername\",\n            },\n            \"type\": \"function\",\n        },\n    ]\n    raw_tool_calls = list(sorted(raw_tool_calls, key=lambda x: x[\"id\"]))\n    message = {\"role\": \"assistant\", \"content\": None, \"tool_calls\": raw_tool_calls}\n    result = _convert_dict_to_message(message)\n    expected_output = AIMessage(\n        content=\"\",\n        additional_kwargs={\"tool_calls\": raw_tool_calls},\n        invalid_tool_calls=[\n            InvalidToolCall(\n                name=\"GenerateUsername\",\n                args=\"oops\",\n                id=\"call_wm0JY6CdwOMZ4eTxHWUThDNz\",\n                error=(\n                    \"Function GenerateUsername arguments:\\n\\noops\\n\\nare not \"\n                    \"valid JSON. Received JSONDecodeError Expecting value: line 1 \"\n                    \"column 1 (char 0)\\nFor troubleshooting, visit: https://python\"\n                    \".langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \"\n                ),\n                type=\"invalid_tool_call\",\n            )\n        ],\n        tool_calls=[\n            ToolCall(\n                name=\"GenerateUsername\",\n                args={\"name\": \"Sally\", \"hair_color\": \"green\"},\n                id=\"call_abc123\",\n                type=\"tool_call\",\n            )\n        ],\n    )\n    assert result == expected_output\n    reverted_message_dict = _convert_message_to_dict(expected_output)\n    reverted_message_dict[\"tool_calls\"] = list(\n        sorted(reverted_message_dict[\"tool_calls\"], key=lambda x: x[\"id\"])\n    )\n    assert reverted_message_dict == message\n\n\nclass MockAsyncContextManager:\n    def __init__(self, chunk_list: list):\n        self.current_chunk = 0\n        self.chunk_list = chunk_list\n        self.chunk_num = len(chunk_list)\n\n    async def __aenter__(self) -> \"MockAsyncContextManager\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n    def __aiter__(self) -> \"MockAsyncContextManager\":\n        return self\n\n    async def __anext__(self) -> dict:\n        if self.current_chunk < self.chunk_num:\n            chunk = self.chunk_list[self.current_chunk]\n            self.current_chunk += 1\n            return chunk\n        else:\n            raise StopAsyncIteration\n\n\nclass MockSyncContextManager:\n    def __init__(self, chunk_list: list):\n        self.current_chunk = 0\n        self.chunk_list = chunk_list\n        self.chunk_num = len(chunk_list)\n\n    def __enter__(self) -> \"MockSyncContextManager\":\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        tb: Optional[TracebackType],\n    ) -> None:\n        pass\n\n    def __iter__(self) -> \"MockSyncContextManager\":\n        return self\n\n    def __next__(self) -> dict:\n        if self.current_chunk < self.chunk_num:\n            chunk = self.chunk_list[self.current_chunk]\n            self.current_chunk += 1\n            return chunk\n        else:\n            raise StopIteration\n\n\nGLM4_STREAM_META = \"\"\"{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u4eba\\u5de5\\u667a\\u80fd\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u52a9\\u624b\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\uff0c\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u4f60\\u53ef\\u4ee5\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u53eb\\u6211\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"AI\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\\u52a9\\u624b\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\u3002\"}}]}\n{\"id\":\"20240722102053e7277a4f94e848248ff9588ed37fb6e6\",\"created\":1721614853,\"model\":\"glm-4\",\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"delta\":{\"role\":\"assistant\",\"content\":\"\"}}],\"usage\":{\"prompt_tokens\":13,\"completion_tokens\":10,\"total_tokens\":23}}\n[DONE]\"\"\"  # noqa: E501\n\n\n@pytest.fixture\ndef mock_glm4_completion() -> list:\n    list_chunk_data = GLM4_STREAM_META.split(\"\\n\")\n    result_list = []\n    for msg in list_chunk_data:\n        if msg != \"[DONE]\":\n            result_list.append(json.loads(msg))\n\n    return result_list\n\n\nasync def test_glm4_astream(mock_glm4_completion: list) -> None:\n    llm_name = \"glm-4\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = AsyncMock()\n\n    async def mock_create(*args: Any, **kwargs: Any) -> MockAsyncContextManager:\n        return MockAsyncContextManager(mock_glm4_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_glm4_completion[-1]\n\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"async_client\", mock_client):\n        async for chunk in llm.astream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\ndef test_glm4_stream(mock_glm4_completion: list) -> None:\n    llm_name = \"glm-4\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = MagicMock()\n\n    def mock_create(*args: Any, **kwargs: Any) -> MockSyncContextManager:\n        return MockSyncContextManager(mock_glm4_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_glm4_completion[-1]\n\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"client\", mock_client):\n        for chunk in llm.stream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\nDEEPSEEK_STREAM_DATA = \"\"\"{\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\",\"role\":\"assistant\"},\"finish_reason\":null,\"logprobs\":null}],\"created\":1721630271,\"model\":\"deepseek-chat\",\"system_fingerprint\":\"fp_7e0991cad4\",\"object\":\"chat.completion.chunk\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u6211\u662f\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"Deep\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"Seek\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\" Chat\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\uff0c\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u4e00\u4e2a\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u7531\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u6df1\u5ea6\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u6c42\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u7d22\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u516c\u53f8\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u5f00\u53d1\u7684\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u667a\u80fd\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u52a9\u624b\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\u3002\",\"role\":\"assistant\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":null}\n{\"choices\":[{\"delta\":{\"content\":\"\",\"role\":null},\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null}],\"created\":1721630271,\"id\":\"d3610c24e6b42518a7883ea57c3ea2c3\",\"model\":\"deepseek-chat\",\"object\":\"chat.completion.chunk\",\"system_fingerprint\":\"fp_7e0991cad4\",\"usage\":{\"completion_tokens\":15,\"prompt_tokens\":11,\"total_tokens\":26}}\n[DONE]\"\"\"  # noqa: E501\n\n\n@pytest.fixture\ndef mock_deepseek_completion() -> List[Dict]:\n    list_chunk_data = DEEPSEEK_STREAM_DATA.split(\"\\n\")\n    result_list = []\n    for msg in list_chunk_data:\n        if msg != \"[DONE]\":\n            result_list.append(json.loads(msg))\n\n    return result_list\n\n\nasync def test_deepseek_astream(mock_deepseek_completion: list) -> None:\n    llm_name = \"deepseek-chat\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = AsyncMock()\n\n    async def mock_create(*args: Any, **kwargs: Any) -> MockAsyncContextManager:\n        return MockAsyncContextManager(mock_deepseek_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_deepseek_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"async_client\", mock_client):\n        async for chunk in llm.astream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\ndef test_deepseek_stream(mock_deepseek_completion: list) -> None:\n    llm_name = \"deepseek-chat\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = MagicMock()\n\n    def mock_create(*args: Any, **kwargs: Any) -> MockSyncContextManager:\n        return MockSyncContextManager(mock_deepseek_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_deepseek_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"client\", mock_client):\n        for chunk in llm.stream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\nOPENAI_STREAM_DATA = \"\"\"{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\u6211\u662f\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\u52a9\u624b\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\u3002\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[{\"index\":0,\"delta\":{},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":null}\n{\"id\":\"chatcmpl-9nhARrdUiJWEMd5plwV1Gc9NCjb9M\",\"object\":\"chat.completion.chunk\",\"created\":1721631035,\"model\":\"gpt-4o-2024-05-13\",\"system_fingerprint\":\"fp_18cc0f1fa0\",\"choices\":[],\"usage\":{\"prompt_tokens\":14,\"completion_tokens\":3,\"total_tokens\":17}}\n[DONE]\"\"\"  # noqa: E501\n\n\n@pytest.fixture\ndef mock_openai_completion() -> List[Dict]:\n    list_chunk_data = OPENAI_STREAM_DATA.split(\"\\n\")\n    result_list = []\n    for msg in list_chunk_data:\n        if msg != \"[DONE]\":\n            result_list.append(json.loads(msg))\n\n    return result_list\n\n\nasync def test_openai_astream(mock_openai_completion: list) -> None:\n    llm_name = \"gpt-4o\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = AsyncMock()\n\n    async def mock_create(*args: Any, **kwargs: Any) -> MockAsyncContextManager:\n        return MockAsyncContextManager(mock_openai_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_openai_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"async_client\", mock_client):\n        async for chunk in llm.astream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\ndef test_openai_stream(mock_openai_completion: list) -> None:\n    llm_name = \"gpt-4o\"\n    llm = ChatOpenAI(model=llm_name, stream_usage=True)\n    mock_client = MagicMock()\n\n    def mock_create(*args: Any, **kwargs: Any) -> MockSyncContextManager:\n        return MockSyncContextManager(mock_openai_completion)\n\n    mock_client.create = mock_create\n    usage_chunk = mock_openai_completion[-1]\n    usage_metadata: Optional[UsageMetadata] = None\n    with patch.object(llm, \"client\", mock_client):\n        for chunk in llm.stream(\"\u4f60\u7684\u540d\u5b57\u53eb\u4ec0\u4e48\uff1f\u53ea\u56de\u7b54\u540d\u5b57\"):\n            assert isinstance(chunk, AIMessageChunk)\n            if chunk.usage_metadata is not None:\n                usage_metadata = chunk.usage_metadata\n\n    assert usage_metadata is not None\n\n    assert usage_metadata[\"input_tokens\"] == usage_chunk[\"usage\"][\"prompt_tokens\"]\n    assert usage_metadata[\"output_tokens\"] == usage_chunk[\"usage\"][\"completion_tokens\"]\n    assert usage_metadata[\"total_tokens\"] == usage_chunk[\"usage\"][\"total_tokens\"]\n\n\n@pytest.fixture\ndef mock_completion() -> dict:\n    return {\n        \"id\": \"chatcmpl-7fcZavknQda3SQ\",\n        \"object\": \"chat.completion\",\n        \"created\": 1689989000,\n        \"model\": \"gpt-3.5-turbo-0613\",\n        \"choices\": [\n            {\n                \"index\": 0,\n                \"message\": {\"role\": \"assistant\", \"content\": \"Bar Baz\", \"name\": \"Erick\"},\n                \"finish_reason\": \"stop\",\n            }\n        ],\n    }\n\n\n@pytest.fixture\ndef mock_client(mock_completion: dict) -> MagicMock:\n    rtn = MagicMock()\n\n    mock_create = MagicMock()\n\n    mock_resp = MagicMock()\n    mock_resp.headers = {\"content-type\": \"application/json\"}\n    mock_resp.parse.return_value = mock_completion\n    mock_create.return_value = mock_resp\n\n    rtn.with_raw_response.create = mock_create\n    rtn.create.return_value = mock_completion\n    return rtn\n\n\n@pytest.fixture\ndef mock_async_client(mock_completion: dict) -> AsyncMock:\n    rtn = AsyncMock()\n\n    mock_create = AsyncMock()\n    mock_resp = MagicMock()\n    mock_resp.parse.return_value = mock_completion\n    mock_create.return_value = mock_resp\n\n    rtn.with_raw_response.create = mock_create\n    rtn.create.return_value = mock_completion\n    return rtn\n\n\ndef test_openai_invoke(mock_client: MagicMock) -> None:\n    llm = ChatOpenAI()\n\n    with patch.object(llm, \"client\", mock_client):\n        res = llm.invoke(\"bar\")\n        assert res.content == \"Bar Baz\"\n\n        # headers are not in response_metadata if include_response_headers not set\n        assert \"headers\" not in res.response_metadata\n    assert mock_client.create.called\n\n\nasync def test_openai_ainvoke(mock_async_client: AsyncMock) -> None:\n    llm = ChatOpenAI()\n\n    with patch.object(llm, \"async_client\", mock_async_client):\n        res = await llm.ainvoke(\"bar\")\n        assert res.content == \"Bar Baz\"\n\n        # headers are not in response_metadata if include_response_headers not set\n        assert \"headers\" not in res.response_metadata\n    assert mock_async_client.create.called\n\n\n@pytest.mark.parametrize(\n    \"model\",\n    [\n        \"gpt-3.5-turbo\",\n        \"gpt-4\",\n        \"gpt-3.5-0125\",\n        \"gpt-4-0125-preview\",\n        \"gpt-4-turbo-preview\",\n        \"gpt-4-vision-preview\",\n    ],\n)\ndef test__get_encoding_model(model: str) -> None:\n    ChatOpenAI(model=model)._get_encoding_model()\n    return\n\n\ndef test_openai_invoke_name(mock_client: MagicMock) -> None:\n    llm = ChatOpenAI()\n\n    with patch.object(llm, \"client\", mock_client):\n        messages = [HumanMessage(content=\"Foo\", name=\"Katie\")]\n        res = llm.invoke(messages)\n        call_args, call_kwargs = mock_client.create.call_args\n        assert len(call_args) == 0  # no positional args\n        call_messages = call_kwargs[\"messages\"]\n        assert len(call_messages) == 1\n        assert call_messages[0][\"role\"] == \"user\"\n        assert call_messages[0][\"content\"] == \"Foo\"\n        assert call_messages[0][\"name\"] == \"Katie\"\n\n        # check return type has name\n        assert res.content == \"Bar Baz\"\n        assert res.name == \"Erick\"\n\n\ndef test_custom_token_counting() -> None:\n    def token_encoder(text: str) -> List[int]:\n        return [1, 2, 3]\n\n    llm = ChatOpenAI(custom_get_token_ids=token_encoder)\n    assert llm.get_token_ids(\"foo\") == [1, 2, 3]\n\n\ndef test_format_message_content() -> None:\n    content: Any = \"hello\"\n    assert content == _format_message_content(content)\n\n    content = None\n    assert content == _format_message_content(content)\n\n    content = []\n    assert content == _format_message_content(content)\n\n    content = [\n        {\"type\": \"text\", \"text\": \"What is in this image?\"},\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"url.com\"}},\n    ]\n    assert content == _format_message_content(content)\n\n    content = [\n        {\"type\": \"text\", \"text\": \"hello\"},\n        {\n            \"type\": \"tool_use\",\n            \"id\": \"toolu_01A09q90qw90lq917835lq9\",\n            \"name\": \"get_weather\",\n            \"input\": {\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"},\n        },\n    ]\n    assert [{\"type\": \"text\", \"text\": \"hello\"}] == _format_message_content(content)\n\n\nclass GenerateUsername(BaseModel):\n    \"Get a username based on someone's name and hair color.\"\n\n    name: str\n    hair_color: str\n\n\nclass MakeASandwich(BaseModel):\n    \"Make a sandwich given a list of ingredients.\"\n\n    bread_type: str\n    cheese_type: str\n    condiments: List[str]\n    vegetables: List[str]\n\n\n@pytest.mark.parametrize(\n    \"tool_choice\",\n    [\n        \"any\",\n        \"none\",\n        \"auto\",\n        \"required\",\n        \"GenerateUsername\",\n        {\"type\": \"function\", \"function\": {\"name\": \"MakeASandwich\"}},\n        False,\n        None,\n    ],\n)\n@pytest.mark.parametrize(\"strict\", [True, False, None])\ndef test_bind_tools_tool_choice(tool_choice: Any, strict: Optional[bool]) -> None:\n    \"\"\"Test passing in manually construct tool call message.\"\"\"\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n    llm.bind_tools(\n        tools=[GenerateUsername, MakeASandwich], tool_choice=tool_choice, strict=strict\n    )\n\n\n@pytest.mark.parametrize(\n    \"schema\", [GenerateUsername, GenerateUsername.model_json_schema()]\n)\n@pytest.mark.parametrize(\"method\", [\"json_schema\", \"function_calling\", \"json_mode\"])\n@pytest.mark.parametrize(\"include_raw\", [True, False])\n@pytest.mark.parametrize(\"strict\", [True, False, None])\ndef test_with_structured_output(\n    schema: Union[Type, Dict[str, Any], None],\n    method: Literal[\"function_calling\", \"json_mode\", \"json_schema\"],\n    include_raw: bool,\n    strict: Optional[bool],\n) -> None:\n    \"\"\"Test passing in manually construct tool call message.\"\"\"\n    if method == \"json_mode\":\n        strict = None\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n    llm.with_structured_output(\n        schema, method=method, strict=strict, include_raw=include_raw\n    )\n\n\ndef test_get_num_tokens_from_messages() -> None:\n    llm = ChatOpenAI(model=\"gpt-4o\")\n    messages = [\n        SystemMessage(\"you're a good assistant\"),\n        HumanMessage(\"how are you\"),\n        HumanMessage(\n            [\n                {\"type\": \"text\", \"text\": \"what's in this image\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://foobar.com\"}},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": \"https://foobar.com\", \"detail\": \"low\"},\n                },\n            ]\n        ),\n        AIMessage(\"a nice bird\"),\n        AIMessage(\n            \"\",\n            tool_calls=[\n                ToolCall(id=\"foo\", name=\"bar\", args={\"arg1\": \"arg1\"}, type=\"tool_call\")\n            ],\n        ),\n        AIMessage(\n            \"\",\n            additional_kwargs={\n                \"function_call\": {\n                    \"arguments\": json.dumps({\"arg1\": \"arg1\"}),\n                    \"name\": \"fun\",\n                }\n            },\n        ),\n        AIMessage(\n            \"text\",\n            tool_calls=[\n                ToolCall(id=\"foo\", name=\"bar\", args={\"arg1\": \"arg1\"}, type=\"tool_call\")\n            ],\n        ),\n        ToolMessage(\"foobar\", tool_call_id=\"foo\"),\n    ]\n    expected = 176\n    actual = llm.get_num_tokens_from_messages(messages)\n    assert expected == actual\n\n\nclass Foo(BaseModel):\n    bar: int\n\n\n# class FooV1(BaseModelV1):\n#     bar: int\n\n\n@pytest.mark.parametrize(\n    \"schema\",\n    [\n        Foo\n        # FooV1\n    ],\n)\ndef test_schema_from_with_structured_output(schema: Type) -> None:\n    \"\"\"Test schema from with_structured_output.\"\"\"\n\n    llm = ChatOpenAI(model=\"gpt-4o\")\n\n    structured_llm = llm.with_structured_output(\n        schema, method=\"json_schema\", strict=True\n    )\n\n    expected = {\n        \"properties\": {\"bar\": {\"title\": \"Bar\", \"type\": \"integer\"}},\n        \"required\": [\"bar\"],\n        \"title\": schema.__name__,\n        \"type\": \"object\",\n    }\n    actual = structured_llm.get_output_schema().model_json_schema()\n    assert actual == expected\n\n\ndef test__create_usage_metadata() -> None:\n    usage_metadata = {\n        \"completion_tokens\": 15,\n        \"prompt_tokens_details\": None,\n        \"completion_tokens_details\": None,\n        \"prompt_tokens\": 11,\n        \"total_tokens\": 26,\n    }\n    result = _create_usage_metadata(usage_metadata)\n    assert result == UsageMetadata(\n        output_tokens=15,\n        input_tokens=11,\n        total_tokens=26,\n        input_token_details={},\n        output_token_details={},\n    )\n\n\ndef test__convert_to_openai_response_format() -> None:\n    # Test response formats that aren't tool-like.\n    response_format: dict = {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"math_reasoning\",\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"steps\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"explanation\": {\"type\": \"string\"},\n                                \"output\": {\"type\": \"string\"},\n                            },\n                            \"required\": [\"explanation\", \"output\"],\n                            \"additionalProperties\": False,\n                        },\n                    },\n                    \"final_answer\": {\"type\": \"string\"},\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": False,\n            },\n            \"strict\": True,\n        },\n    }\n\n    actual = _convert_to_openai_response_format(response_format)\n    assert actual == response_format\n\n    actual = _convert_to_openai_response_format(response_format[\"json_schema\"])\n    assert actual == response_format\n\n    actual = _convert_to_openai_response_format(response_format, strict=True)\n    assert actual == response_format\n\n    with pytest.raises(ValueError):\n        _convert_to_openai_response_format(response_format, strict=False)\n\n\n@pytest.mark.parametrize(\"method\", [\"function_calling\", \"json_schema\"])\n@pytest.mark.parametrize(\"strict\", [True, None])\ndef test_structured_output_strict(\n    method: Literal[\"function_calling\", \"json_schema\"], strict: Optional[bool]\n) -> None:\n    \"\"\"Test to verify structured output with strict=True.\"\"\"\n\n    llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n\n    class Joke(BaseModel):\n        \"\"\"Joke to tell user.\"\"\"\n\n        setup: str = Field(description=\"question to set up a joke\")\n        punchline: str = Field(description=\"answer to resolve the joke\")\n\n    llm.with_structured_output(Joke, method=method, strict=strict)\n    # Schema\n    llm.with_structured_output(Joke.model_json_schema(), method=method, strict=strict)\n\n\ndef test_nested_structured_output_strict() -> None:\n    \"\"\"Test to verify structured output with strict=True for nested object.\"\"\"\n\n    llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n\n    class SelfEvaluation(TypedDict):\n        score: int\n        text: str\n\n    class JokeWithEvaluation(TypedDict):\n        \"\"\"Joke to tell user.\"\"\"\n\n        setup: str\n        punchline: str\n        self_evaluation: SelfEvaluation\n\n    llm.with_structured_output(JokeWithEvaluation, method=\"json_schema\")\n\n\ndef test__get_request_payload() -> None:\n    llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n    messages: list = [\n        SystemMessage(\"hello\"),\n        SystemMessage(\"bye\", additional_kwargs={\"__openai_role__\": \"developer\"}),\n        {\"role\": \"human\", \"content\": \"how are you\"},\n    ]\n    expected = {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"hello\"},\n            {\"role\": \"developer\", \"content\": \"bye\"},\n            {\"role\": \"user\", \"content\": \"how are you\"},\n        ],\n        \"model\": \"gpt-4o-2024-08-06\",\n        \"stream\": False,\n    }\n    payload = llm._get_request_payload(messages)\n    assert payload == expected\n\n\ndef test_init_o1() -> None:\n    with pytest.warns(None) as record:  # type: ignore[call-overload]\n        ChatOpenAI(model=\"o1\", reasoning_effort=\"medium\")\n    assert len(record) == 0\n\n\ndef test_structured_output_old_model() -> None:\n    class Output(TypedDict):\n        \"\"\"output.\"\"\"\n\n        foo: str\n\n    with pytest.warns(match=\"Cannot use method='json_schema'\"):\n        llm = ChatOpenAI(model=\"gpt-4\").with_structured_output(Output)\n    # assert tool calling was used instead of json_schema\n    assert \"tools\" in llm.steps[0].kwargs  # type: ignore\n    assert \"response_format\" not in llm.steps[0].kwargs  # type: ignore\n",
        "patch": "@@ -881,6 +881,20 @@ def test__get_request_payload() -> None:\n     payload = llm._get_request_payload(messages)\n     assert payload == expected\n \n+    # Test we coerce to developer role for o-series models\n+    llm = ChatOpenAI(model=\"o3-mini\")\n+    payload = llm._get_request_payload(messages)\n+    expected = {\n+        \"messages\": [\n+            {\"role\": \"developer\", \"content\": \"hello\"},\n+            {\"role\": \"developer\", \"content\": \"bye\"},\n+            {\"role\": \"user\", \"content\": \"how are you\"},\n+        ],\n+        \"model\": \"o3-mini\",\n+        \"stream\": False,\n+    }\n+    assert payload == expected\n+\n \n def test_init_o1() -> None:\n     with pytest.warns(None) as record:  # type: ignore[call-overload]"
      }
    ]
  },
  {
    "number": 29500,
    "title": "partners: Fixed the procedure of initializing pad_token_id",
    "body": "- **Description:** Add to check pad_token_id and eos_token_id of model config. It seems that this is the same bug as the HuggingFace TGI bug.  It's same bug as #29434\r\n- **Issue:** #29431\r\n- **Dependencies:** none\r\n- **Twitter handle:** tell14\r\n\r\nExample code is followings:\r\n```python\r\nfrom langchain_huggingface.llms import HuggingFacePipeline\r\n\r\nhf = HuggingFacePipeline.from_model_id(\r\n    model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\r\n    task=\"text-generation\",\r\n    pipeline_kwargs={\"max_new_tokens\": 10},\r\n)\r\n\r\nfrom langchain_core.prompts import PromptTemplate\r\n\r\ntemplate = \"\"\"Question: {question}\r\n\r\nAnswer: Let's think step by step.\"\"\"\r\nprompt = PromptTemplate.from_template(template)\r\n\r\nchain = prompt | hf\r\n\r\nquestion = \"What is electroencephalography?\"\r\n\r\nprint(chain.invoke({\"question\": question}))\r\n```\r\n",
    "issue_title": "partners: Fixed the procedure of initializing pad_token_id",
    "issue_body": "- **Description:** Add to check pad_token_id and eos_token_id of model config. It seems that this is the same bug as the HuggingFace TGI bug.  It's same bug as #29434\r\n- **Issue:** #29431\r\n- **Dependencies:** none\r\n- **Twitter handle:** tell14\r\n\r\nExample code is followings:\r\n```python\r\nfrom langchain_huggingface.llms import HuggingFacePipeline\r\n\r\nhf = HuggingFacePipeline.from_model_id(\r\n    model_id=\"meta-llama/Llama-3.2-3B-Instruct\",\r\n    task=\"text-generation\",\r\n    pipeline_kwargs={\"max_new_tokens\": 10},\r\n)\r\n\r\nfrom langchain_core.prompts import PromptTemplate\r\n\r\ntemplate = \"\"\"Question: {question}\r\n\r\nAnswer: Let's think step by step.\"\"\"\r\nprompt = PromptTemplate.from_template(template)\r\n\r\nchain = prompt | hf\r\n\r\nquestion = \"What is electroencephalography?\"\r\n\r\nprint(chain.invoke({\"question\": question}))\r\n```\r\n",
    "files": [
      {
        "filename": "libs/partners/huggingface/langchain_huggingface/llms/huggingface_pipeline.py",
        "content_before": "from __future__ import annotations  # type: ignore[import-not-found]\n\nimport importlib.util\nimport logging\nfrom typing import Any, Dict, Iterator, List, Mapping, Optional\n\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import BaseLLM\nfrom langchain_core.outputs import Generation, GenerationChunk, LLMResult\nfrom pydantic import ConfigDict, model_validator\n\nfrom ..utils.import_utils import (\n    IMPORT_ERROR,\n    is_ipex_available,\n    is_openvino_available,\n    is_optimum_intel_available,\n    is_optimum_intel_version,\n)\n\nDEFAULT_MODEL_ID = \"gpt2\"\nDEFAULT_TASK = \"text-generation\"\nVALID_TASKS = (\n    \"text2text-generation\",\n    \"text-generation\",\n    \"summarization\",\n    \"translation\",\n)\nDEFAULT_BATCH_SIZE = 4\n_MIN_OPTIMUM_VERSION = \"1.21\"\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HuggingFacePipeline(BaseLLM):\n    \"\"\"HuggingFace Pipeline API.\n\n    To use, you should have the ``transformers`` python package installed.\n\n    Only supports `text-generation`, `text2text-generation`, `summarization` and\n    `translation`  for now.\n\n    Example using from_model_id:\n        .. code-block:: python\n\n            from langchain_huggingface import HuggingFacePipeline\n            hf = HuggingFacePipeline.from_model_id(\n                model_id=\"gpt2\",\n                task=\"text-generation\",\n                pipeline_kwargs={\"max_new_tokens\": 10},\n            )\n    Example passing pipeline in directly:\n        .. code-block:: python\n\n            from langchain_huggingface import HuggingFacePipeline\n            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n            model_id = \"gpt2\"\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\n            model = AutoModelForCausalLM.from_pretrained(model_id)\n            pipe = pipeline(\n                \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n            )\n            hf = HuggingFacePipeline(pipeline=pipe)\n    \"\"\"\n\n    pipeline: Any = None  #: :meta private:\n    model_id: Optional[str] = None\n    \"\"\"The model name. If not set explicitly by the user,\n    it will be inferred from the provided pipeline (if available).\n    If neither is provided, the DEFAULT_MODEL_ID will be used.\"\"\"\n    model_kwargs: Optional[dict] = None\n    \"\"\"Keyword arguments passed to the model.\"\"\"\n    pipeline_kwargs: Optional[dict] = None\n    \"\"\"Keyword arguments passed to the pipeline.\"\"\"\n    batch_size: int = DEFAULT_BATCH_SIZE\n    \"\"\"Batch size to use when passing multiple documents to generate.\"\"\"\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def pre_init_validator(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Ensure model_id is set either by pipeline or user input.\"\"\"\n        if \"model_id\" not in values:\n            if \"pipeline\" in values and values[\"pipeline\"]:\n                values[\"model_id\"] = values[\"pipeline\"].model.name_or_path\n            else:\n                values[\"model_id\"] = DEFAULT_MODEL_ID\n        return values\n\n    @classmethod\n    def from_model_id(\n        cls,\n        model_id: str,\n        task: str,\n        backend: str = \"default\",\n        device: Optional[int] = None,\n        device_map: Optional[str] = None,\n        model_kwargs: Optional[dict] = None,\n        pipeline_kwargs: Optional[dict] = None,\n        batch_size: int = DEFAULT_BATCH_SIZE,\n        **kwargs: Any,\n    ) -> HuggingFacePipeline:\n        \"\"\"Construct the pipeline object from model_id and task.\"\"\"\n        try:\n            from transformers import (  # type: ignore[import]\n                AutoModelForCausalLM,\n                AutoModelForSeq2SeqLM,\n                AutoTokenizer,\n            )\n            from transformers import pipeline as hf_pipeline  # type: ignore[import]\n\n        except ImportError:\n            raise ValueError(\n                \"Could not import transformers python package. \"\n                \"Please install it with `pip install transformers`.\"\n            )\n\n        _model_kwargs = model_kwargs.copy() if model_kwargs else {}\n        if device_map is not None:\n            if device is not None:\n                raise ValueError(\n                    \"Both `device` and `device_map` are specified. \"\n                    \"`device` will override `device_map`. \"\n                    \"You will most likely encounter unexpected behavior.\"\n                    \"Please remove `device` and keep \"\n                    \"`device_map`.\"\n                )\n\n            if \"device_map\" in _model_kwargs:\n                raise ValueError(\"`device_map` is already specified in `model_kwargs`.\")\n\n            _model_kwargs[\"device_map\"] = device_map\n        tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)\n\n        if backend in {\"openvino\", \"ipex\"}:\n            if task not in VALID_TASKS:\n                raise ValueError(\n                    f\"Got invalid task {task}, \"\n                    f\"currently only {VALID_TASKS} are supported\"\n                )\n\n            err_msg = f'Backend: {backend} {IMPORT_ERROR.format(f\"optimum[{backend}]\")}'\n            if not is_optimum_intel_available():\n                raise ImportError(err_msg)\n\n            # TODO: upgrade _MIN_OPTIMUM_VERSION to 1.22 after release\n            min_optimum_version = (\n                \"1.22\"\n                if backend == \"ipex\" and task != \"text-generation\"\n                else _MIN_OPTIMUM_VERSION\n            )\n            if is_optimum_intel_version(\"<\", min_optimum_version):\n                raise ImportError(\n                    f\"Backend: {backend} requires optimum-intel>=\"\n                    f\"{min_optimum_version}. You can install it with pip: \"\n                    \"`pip install --upgrade --upgrade-strategy eager \"\n                    f\"`optimum[{backend}]`.\"\n                )\n\n            if backend == \"openvino\":\n                if not is_openvino_available():\n                    raise ImportError(err_msg)\n\n                from optimum.intel import (  # type: ignore[import]\n                    OVModelForCausalLM,\n                    OVModelForSeq2SeqLM,\n                )\n\n                model_cls = (\n                    OVModelForCausalLM\n                    if task == \"text-generation\"\n                    else OVModelForSeq2SeqLM\n                )\n            else:\n                if not is_ipex_available():\n                    raise ImportError(err_msg)\n\n                if task == \"text-generation\":\n                    from optimum.intel import (\n                        IPEXModelForCausalLM,  # type: ignore[import]\n                    )\n\n                    model_cls = IPEXModelForCausalLM\n                else:\n                    from optimum.intel import (\n                        IPEXModelForSeq2SeqLM,  # type: ignore[import]\n                    )\n\n                    model_cls = IPEXModelForSeq2SeqLM\n\n        else:\n            model_cls = (\n                AutoModelForCausalLM\n                if task == \"text-generation\"\n                else AutoModelForSeq2SeqLM\n            )\n\n        model = model_cls.from_pretrained(model_id, **_model_kwargs)\n\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token_id = model.config.eos_token_id\n\n        if (\n            (\n                getattr(model, \"is_loaded_in_4bit\", False)\n                or getattr(model, \"is_loaded_in_8bit\", False)\n            )\n            and device is not None\n            and backend == \"default\"\n        ):\n            logger.warning(\n                f\"Setting the `device` argument to None from {device} to avoid \"\n                \"the error caused by attempting to move the model that was already \"\n                \"loaded on the GPU using the Accelerate module to the same or \"\n                \"another device.\"\n            )\n            device = None\n\n        if (\n            device is not None\n            and importlib.util.find_spec(\"torch\") is not None\n            and backend == \"default\"\n        ):\n            import torch\n\n            cuda_device_count = torch.cuda.device_count()\n            if device < -1 or (device >= cuda_device_count):\n                raise ValueError(\n                    f\"Got device=={device}, \"\n                    f\"device is required to be within [-1, {cuda_device_count})\"\n                )\n            if device_map is not None and device < 0:\n                device = None\n            if device is not None and device < 0 and cuda_device_count > 0:\n                logger.warning(\n                    \"Device has %d GPUs available. \"\n                    \"Provide device={deviceId} to `from_model_id` to use available\"\n                    \"GPUs for execution. deviceId is -1 (default) for CPU and \"\n                    \"can be a positive integer associated with CUDA device id.\",\n                    cuda_device_count,\n                )\n        if device is not None and device_map is not None and backend == \"openvino\":\n            logger.warning(\"Please set device for OpenVINO through: `model_kwargs`\")\n        if \"trust_remote_code\" in _model_kwargs:\n            _model_kwargs = {\n                k: v for k, v in _model_kwargs.items() if k != \"trust_remote_code\"\n            }\n        _pipeline_kwargs = pipeline_kwargs or {}\n        pipeline = hf_pipeline(\n            task=task,\n            model=model,\n            tokenizer=tokenizer,\n            device=device,\n            batch_size=batch_size,\n            model_kwargs=_model_kwargs,\n            **_pipeline_kwargs,\n        )\n        if pipeline.task not in VALID_TASKS:\n            raise ValueError(\n                f\"Got invalid task {pipeline.task}, \"\n                f\"currently only {VALID_TASKS} are supported\"\n            )\n        return cls(\n            pipeline=pipeline,\n            model_id=model_id,\n            model_kwargs=_model_kwargs,\n            pipeline_kwargs=_pipeline_kwargs,\n            batch_size=batch_size,\n            **kwargs,\n        )\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            \"model_id\": self.model_id,\n            \"model_kwargs\": self.model_kwargs,\n            \"pipeline_kwargs\": self.pipeline_kwargs,\n        }\n\n    @property\n    def _llm_type(self) -> str:\n        return \"huggingface_pipeline\"\n\n    def _generate(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> LLMResult:\n        # List to hold all results\n        text_generations: List[str] = []\n        pipeline_kwargs = kwargs.get(\"pipeline_kwargs\", {})\n        skip_prompt = kwargs.get(\"skip_prompt\", False)\n\n        for i in range(0, len(prompts), self.batch_size):\n            batch_prompts = prompts[i : i + self.batch_size]\n\n            # Process batch of prompts\n            responses = self.pipeline(\n                batch_prompts,\n                **pipeline_kwargs,\n            )\n\n            # Process each response in the batch\n            for j, response in enumerate(responses):\n                if isinstance(response, list):\n                    # if model returns multiple generations, pick the top one\n                    response = response[0]\n\n                if self.pipeline.task == \"text-generation\":\n                    text = response[\"generated_text\"]\n                elif self.pipeline.task == \"text2text-generation\":\n                    text = response[\"generated_text\"]\n                elif self.pipeline.task == \"summarization\":\n                    text = response[\"summary_text\"]\n                elif self.pipeline.task in \"translation\":\n                    text = response[\"translation_text\"]\n                else:\n                    raise ValueError(\n                        f\"Got invalid task {self.pipeline.task}, \"\n                        f\"currently only {VALID_TASKS} are supported\"\n                    )\n                if skip_prompt:\n                    text = text[len(batch_prompts[j]) :]\n                # Append the processed text to results\n                text_generations.append(text)\n\n        return LLMResult(\n            generations=[[Generation(text=text)] for text in text_generations]\n        )\n\n    def _stream(\n        self,\n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[GenerationChunk]:\n        from threading import Thread\n\n        import torch\n        from transformers import (\n            StoppingCriteria,\n            StoppingCriteriaList,\n            TextIteratorStreamer,\n        )\n\n        pipeline_kwargs = kwargs.get(\"pipeline_kwargs\", {})\n        skip_prompt = kwargs.get(\"skip_prompt\", True)\n\n        if stop is not None:\n            stop = self.pipeline.tokenizer.convert_tokens_to_ids(stop)\n        stopping_ids_list = stop or []\n\n        class StopOnTokens(StoppingCriteria):\n            def __call__(\n                self,\n                input_ids: torch.LongTensor,\n                scores: torch.FloatTensor,\n                **kwargs: Any,\n            ) -> bool:\n                for stop_id in stopping_ids_list:\n                    if input_ids[0][-1] == stop_id:\n                        return True\n                return False\n\n        stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n\n        streamer = TextIteratorStreamer(\n            self.pipeline.tokenizer,\n            timeout=60.0,\n            skip_prompt=skip_prompt,\n            skip_special_tokens=True,\n        )\n        generation_kwargs = dict(\n            text_inputs=prompt,\n            streamer=streamer,\n            stopping_criteria=stopping_criteria,\n            **pipeline_kwargs,\n        )\n        t1 = Thread(target=self.pipeline, kwargs=generation_kwargs)\n        t1.start()\n\n        for char in streamer:\n            chunk = GenerationChunk(text=char)\n            if run_manager:\n                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n\n            yield chunk\n",
        "patch": "@@ -202,7 +202,16 @@ def from_model_id(\n         model = model_cls.from_pretrained(model_id, **_model_kwargs)\n \n         if tokenizer.pad_token is None:\n-            tokenizer.pad_token_id = model.config.eos_token_id\n+            if model.config.pad_token_id is not None:\n+                tokenizer.pad_token_id = model.config.pad_token_id\n+            elif model.config.eos_token_id is not None and isinstance(\n+                model.config.eos_token_id, int\n+            ):\n+                tokenizer.pad_token_id = model.config.eos_token_id\n+            elif tokenizer.eos_token_id is not None:\n+                tokenizer.pad_token_id = tokenizer.eos_token_id\n+            else:\n+                tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n \n         if (\n             ("
      },
      {
        "filename": "libs/partners/huggingface/tests/integration_tests/test_standard.py",
        "content_before": "\"\"\"Standard LangChain interface tests\"\"\"\n\nfrom typing import Type\n\nimport pytest\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.tools import BaseTool\nfrom langchain_tests.integration_tests import ChatModelIntegrationTests\n\nfrom langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n\nclass TestHuggingFaceEndpoint(ChatModelIntegrationTests):\n    @property\n    def chat_model_class(self) -> Type[BaseChatModel]:\n        return ChatHuggingFace\n\n    @property\n    def chat_model_params(self) -> dict:\n        return {}\n\n    @pytest.fixture\n    def model(self) -> BaseChatModel:\n        llm = HuggingFaceEndpoint(  # type: ignore[call-arg]\n            repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n            task=\"text-generation\",\n            max_new_tokens=512,\n            do_sample=False,\n            repetition_penalty=1.03,\n        )\n        return self.chat_model_class(llm=llm)  # type: ignore[call-arg]\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_stream(self, model: BaseChatModel) -> None:\n        super().test_stream(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    async def test_astream(self, model: BaseChatModel) -> None:\n        await super().test_astream(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_usage_metadata(self, model: BaseChatModel) -> None:\n        super().test_usage_metadata(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_usage_metadata_streaming(self, model: BaseChatModel) -> None:\n        super().test_usage_metadata_streaming(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_stop_sequence(self, model: BaseChatModel) -> None:\n        super().test_stop_sequence(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_tool_calling(self, model: BaseChatModel) -> None:\n        super().test_tool_calling(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    async def test_tool_calling_async(self, model: BaseChatModel) -> None:\n        await super().test_tool_calling_async(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_tool_calling_with_no_arguments(self, model: BaseChatModel) -> None:\n        super().test_tool_calling_with_no_arguments(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_bind_runnables_as_tools(self, model: BaseChatModel) -> None:\n        super().test_bind_runnables_as_tools(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_structured_output(self, model: BaseChatModel) -> None:\n        super().test_structured_output(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_structured_output_async(self, model: BaseChatModel) -> None:  # type: ignore[override]\n        super().test_structured_output(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_structured_output_pydantic_2_v1(self, model: BaseChatModel) -> None:\n        super().test_structured_output_pydantic_2_v1(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_structured_output_optional_param(self, model: BaseChatModel) -> None:\n        super().test_structured_output_optional_param(model)\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_tool_message_histories_list_content(\n        self, model: BaseChatModel, my_adder_tool: BaseTool\n    ) -> None:\n        super().test_tool_message_histories_list_content(\n            model, my_adder_tool=my_adder_tool\n        )\n\n    @pytest.mark.xfail(reason=(\"Not implemented\"))\n    def test_structured_few_shot_examples(\n        self, model: BaseChatModel, my_adder_tool: BaseTool\n    ) -> None:\n        super().test_structured_few_shot_examples(model, my_adder_tool=my_adder_tool)\n",
        "patch": "@@ -67,12 +67,14 @@ def test_bind_runnables_as_tools(self, model: BaseChatModel) -> None:\n         super().test_bind_runnables_as_tools(model)\n \n     @pytest.mark.xfail(reason=(\"Not implemented\"))\n-    def test_structured_output(self, model: BaseChatModel) -> None:\n-        super().test_structured_output(model)\n+    def test_structured_output(self, model: BaseChatModel, schema_type: str) -> None:\n+        super().test_structured_output(model, schema_type)\n \n     @pytest.mark.xfail(reason=(\"Not implemented\"))\n-    def test_structured_output_async(self, model: BaseChatModel) -> None:  # type: ignore[override]\n-        super().test_structured_output(model)\n+    async def test_structured_output_async(\n+        self, model: BaseChatModel, schema_type: str\n+    ) -> None:  # type: ignore[override]\n+        super().test_structured_output(model, schema_type)\n \n     @pytest.mark.xfail(reason=(\"Not implemented\"))\n     def test_structured_output_pydantic_2_v1(self, model: BaseChatModel) -> None:"
      },
      {
        "filename": "libs/partners/huggingface/tests/unit_tests/test_chat_models.py",
        "content_before": "from typing import Any, Dict, List  # type: ignore[import-not-found]\nfrom unittest.mock import MagicMock, Mock, patch\n\nimport pytest  # type: ignore[import-not-found]\nfrom langchain_core.messages import (\n    AIMessage,\n    BaseMessage,\n    ChatMessage,\n    HumanMessage,\n    SystemMessage,\n)\nfrom langchain_core.outputs import ChatResult\nfrom langchain_core.tools import BaseTool\n\nfrom langchain_huggingface.chat_models import (  # type: ignore[import]\n    TGI_MESSAGE,\n    ChatHuggingFace,\n    _convert_message_to_chat_message,\n    _convert_TGI_message_to_LC_message,\n)\nfrom langchain_huggingface.llms.huggingface_endpoint import (\n    HuggingFaceEndpoint,\n)\n\n\n@pytest.mark.parametrize(\n    (\"message\", \"expected\"),\n    [\n        (\n            SystemMessage(content=\"Hello\"),\n            dict(role=\"system\", content=\"Hello\"),\n        ),\n        (\n            HumanMessage(content=\"Hello\"),\n            dict(role=\"user\", content=\"Hello\"),\n        ),\n        (\n            AIMessage(content=\"Hello\"),\n            dict(role=\"assistant\", content=\"Hello\", tool_calls=None),\n        ),\n        (\n            ChatMessage(role=\"assistant\", content=\"Hello\"),\n            dict(role=\"assistant\", content=\"Hello\"),\n        ),\n    ],\n)\ndef test_convert_message_to_chat_message(\n    message: BaseMessage, expected: Dict[str, str]\n) -> None:\n    result = _convert_message_to_chat_message(message)\n    assert result == expected\n\n\n@pytest.mark.parametrize(\n    (\"tgi_message\", \"expected\"),\n    [\n        (\n            TGI_MESSAGE(role=\"assistant\", content=\"Hello\", tool_calls=[]),\n            AIMessage(content=\"Hello\"),\n        ),\n        (\n            TGI_MESSAGE(role=\"assistant\", content=\"\", tool_calls=[]),\n            AIMessage(content=\"\"),\n        ),\n        (\n            TGI_MESSAGE(\n                role=\"assistant\",\n                content=\"\",\n                tool_calls=[{\"function\": {\"arguments\": \"function string\"}}],\n            ),\n            AIMessage(\n                content=\"\",\n                additional_kwargs={\n                    \"tool_calls\": [{\"function\": {\"arguments\": '\"function string\"'}}]\n                },\n            ),\n        ),\n        (\n            TGI_MESSAGE(\n                role=\"assistant\",\n                content=\"\",\n                tool_calls=[\n                    {\"function\": {\"arguments\": {\"answer\": \"function's string\"}}}\n                ],\n            ),\n            AIMessage(\n                content=\"\",\n                additional_kwargs={\n                    \"tool_calls\": [\n                        {\"function\": {\"arguments\": '{\"answer\": \"function\\'s string\"}'}}\n                    ]\n                },\n            ),\n        ),\n    ],\n)\ndef test_convert_TGI_message_to_LC_message(\n    tgi_message: TGI_MESSAGE, expected: BaseMessage\n) -> None:\n    result = _convert_TGI_message_to_LC_message(tgi_message)\n    assert result == expected\n\n\n@pytest.fixture\ndef mock_llm() -> Mock:\n    llm = Mock(spec=HuggingFaceEndpoint)\n    llm.inference_server_url = \"test endpoint url\"\n    return llm\n\n\n@pytest.fixture\n@patch(\n    \"langchain_huggingface.chat_models.huggingface.ChatHuggingFace._resolve_model_id\"\n)\ndef chat_hugging_face(mock_resolve_id: Any, mock_llm: Any) -> ChatHuggingFace:\n    chat_hf = ChatHuggingFace(llm=mock_llm, tokenizer=MagicMock())\n    return chat_hf\n\n\ndef test_create_chat_result(chat_hugging_face: Any) -> None:\n    mock_response = MagicMock()\n    mock_response.choices = [\n        MagicMock(\n            message=TGI_MESSAGE(\n                role=\"assistant\", content=\"test message\", tool_calls=[]\n            ),\n            finish_reason=\"test finish reason\",\n        )\n    ]\n    mock_response.usage = {\"tokens\": 420}\n\n    result = chat_hugging_face._create_chat_result(mock_response)\n    assert isinstance(result, ChatResult)\n    assert result.generations[0].message.content == \"test message\"\n    assert (\n        result.generations[0].generation_info[\"finish_reason\"] == \"test finish reason\"  # type: ignore[index]\n    )\n    assert result.llm_output[\"token_usage\"][\"tokens\"] == 420  # type: ignore[index]\n    assert result.llm_output[\"model\"] == chat_hugging_face.llm.inference_server_url  # type: ignore[index]\n\n\n@pytest.mark.parametrize(\n    \"messages, expected_error\",\n    [\n        ([], \"At least one HumanMessage must be provided!\"),\n        (\n            [HumanMessage(content=\"Hi\"), AIMessage(content=\"Hello\")],\n            \"Last message must be a HumanMessage!\",\n        ),\n    ],\n)\ndef test_to_chat_prompt_errors(\n    chat_hugging_face: Any, messages: List[BaseMessage], expected_error: str\n) -> None:\n    with pytest.raises(ValueError) as e:\n        chat_hugging_face._to_chat_prompt(messages)\n    assert expected_error in str(e.value)\n\n\ndef test_to_chat_prompt_valid_messages(chat_hugging_face: Any) -> None:\n    messages = [AIMessage(content=\"Hello\"), HumanMessage(content=\"How are you?\")]\n    expected_prompt = \"Generated chat prompt\"\n\n    chat_hugging_face.tokenizer.apply_chat_template.return_value = expected_prompt\n\n    result = chat_hugging_face._to_chat_prompt(messages)\n\n    assert result == expected_prompt\n    chat_hugging_face.tokenizer.apply_chat_template.assert_called_once_with(\n        [\n            {\"role\": \"assistant\", \"content\": \"Hello\"},\n            {\"role\": \"user\", \"content\": \"How are you?\"},\n        ],\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n\n@pytest.mark.parametrize(\n    (\"message\", \"expected\"),\n    [\n        (\n            SystemMessage(content=\"You are a helpful assistant.\"),\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        ),\n        (\n            AIMessage(content=\"How can I help you?\"),\n            {\"role\": \"assistant\", \"content\": \"How can I help you?\"},\n        ),\n        (\n            HumanMessage(content=\"Hello\"),\n            {\"role\": \"user\", \"content\": \"Hello\"},\n        ),\n    ],\n)\ndef test_to_chatml_format(\n    chat_hugging_face: Any, message: BaseMessage, expected: Dict[str, str]\n) -> None:\n    result = chat_hugging_face._to_chatml_format(message)\n    assert result == expected\n\n\ndef test_to_chatml_format_with_invalid_type(chat_hugging_face: Any) -> None:\n    message = \"Invalid message type\"\n    with pytest.raises(ValueError) as e:\n        chat_hugging_face._to_chatml_format(message)\n    assert \"Unknown message type:\" in str(e.value)\n\n\ndef tool_mock() -> Dict:\n    return {\"function\": {\"name\": \"test_tool\"}}\n\n\n@pytest.mark.parametrize(\n    \"tools, tool_choice, expected_exception, expected_message\",\n    [\n        ([tool_mock()], [\"invalid type\"], ValueError, \"Unrecognized tool_choice type.\"),\n        (\n            [tool_mock(), tool_mock()],\n            \"test_tool\",\n            ValueError,\n            \"must provide exactly one tool.\",\n        ),\n        (\n            [tool_mock()],\n            {\"type\": \"function\", \"function\": {\"name\": \"other_tool\"}},\n            ValueError,\n            \"Tool choice {'type': 'function', 'function': {'name': 'other_tool'}} \"\n            \"was specified, but the only provided tool was test_tool.\",\n        ),\n    ],\n)\ndef test_bind_tools_errors(\n    chat_hugging_face: Any,\n    tools: Dict[str, str],\n    tool_choice: Any,\n    expected_exception: Any,\n    expected_message: str,\n) -> None:\n    with patch(\n        \"langchain_huggingface.chat_models.huggingface.convert_to_openai_tool\",\n        side_effect=lambda x: x,\n    ):\n        with pytest.raises(expected_exception) as excinfo:\n            chat_hugging_face.bind_tools(tools, tool_choice=tool_choice)\n        assert expected_message in str(excinfo.value)\n\n\ndef test_bind_tools(chat_hugging_face: Any) -> None:\n    tools = [MagicMock(spec=BaseTool)]\n    with patch(\n        \"langchain_huggingface.chat_models.huggingface.convert_to_openai_tool\",\n        side_effect=lambda x: x,\n    ), patch(\"langchain_core.runnables.base.Runnable.bind\") as mock_super_bind:\n        chat_hugging_face.bind_tools(tools, tool_choice=\"auto\")\n        mock_super_bind.assert_called_once()\n        _, kwargs = mock_super_bind.call_args\n        assert kwargs[\"tools\"] == tools\n        assert kwargs[\"tool_choice\"] == \"auto\"\n",
        "patch": "@@ -248,10 +248,13 @@ def test_bind_tools_errors(\n \n def test_bind_tools(chat_hugging_face: Any) -> None:\n     tools = [MagicMock(spec=BaseTool)]\n-    with patch(\n-        \"langchain_huggingface.chat_models.huggingface.convert_to_openai_tool\",\n-        side_effect=lambda x: x,\n-    ), patch(\"langchain_core.runnables.base.Runnable.bind\") as mock_super_bind:\n+    with (\n+        patch(\n+            \"langchain_huggingface.chat_models.huggingface.convert_to_openai_tool\",\n+            side_effect=lambda x: x,\n+        ),\n+        patch(\"langchain_core.runnables.base.Runnable.bind\") as mock_super_bind,\n+    ):\n         chat_hugging_face.bind_tools(tools, tool_choice=\"auto\")\n         mock_super_bind.assert_called_once()\n         _, kwargs = mock_super_bind.call_args"
      }
    ]
  },
  {
    "number": 29758,
    "title": "[Exception Handling] DeepSeek JSONDecodeError",
    "body": "For Context please check #29626 \r\n\r\nThe Deepseek is using langchain_openai. The error happens that it show `json decode error`. \r\n\r\nI added a handler for this to give a more sensible error message which is DeepSeek API returned empty/invalid json. \r\n\r\nReproducing the issue is a bit challenging as it is inconsistent, sometimes DeepSeek returns valid data and in other times it returns invalid data which triggers the JSON Decode Error. \r\n\r\nThis PR is an exception handling, but not an ultimate fix for the issue.",
    "issue_title": "[Exception Handling] DeepSeek JSONDecodeError",
    "issue_body": "For Context please check #29626 \r\n\r\nThe Deepseek is using langchain_openai. The error happens that it show `json decode error`. \r\n\r\nI added a handler for this to give a more sensible error message which is DeepSeek API returned empty/invalid json. \r\n\r\nReproducing the issue is a bit challenging as it is inconsistent, sometimes DeepSeek returns valid data and in other times it returns invalid data which triggers the JSON Decode Error. \r\n\r\nThis PR is an exception handling, but not an ultimate fix for the issue.",
    "files": [
      {
        "filename": "libs/partners/deepseek/langchain_deepseek/chat_models.py",
        "content_before": "\"\"\"DeepSeek chat models.\"\"\"\n\nfrom typing import Dict, Optional, Type, Union\n\nimport openai\nfrom langchain_core.messages import AIMessageChunk\nfrom langchain_core.outputs import ChatGenerationChunk, ChatResult\nfrom langchain_core.utils import from_env, secret_from_env\nfrom langchain_openai.chat_models.base import BaseChatOpenAI\nfrom pydantic import ConfigDict, Field, SecretStr, model_validator\nfrom typing_extensions import Self\n\nDEFAULT_API_BASE = \"https://api.deepseek.com/v1\"\n\n\nclass ChatDeepSeek(BaseChatOpenAI):\n    \"\"\"DeepSeek chat model integration to access models hosted in DeepSeek's API.\n\n    Setup:\n        Install ``langchain-deepseek`` and set environment variable ``DEEPSEEK_API_KEY``.\n\n        .. code-block:: bash\n\n            pip install -U langchain-deepseek\n            export DEEPSEEK_API_KEY=\"your-api-key\"\n\n    Key init args \u2014 completion params:\n        model: str\n            Name of DeepSeek model to use, e.g. \"deepseek-chat\".\n        temperature: float\n            Sampling temperature.\n        max_tokens: Optional[int]\n            Max number of tokens to generate.\n\n    Key init args \u2014 client params:\n        timeout: Optional[float]\n            Timeout for requests.\n        max_retries: int\n            Max number of retries.\n        api_key: Optional[str]\n            DeepSeek API key. If not passed in will be read from env var DEEPSEEK_API_KEY.\n\n    See full list of supported init args and their descriptions in the params section.\n\n    Instantiate:\n        .. code-block:: python\n\n            from langchain_deepseek import ChatDeepSeek\n\n            llm = ChatDeepSeek(\n                model=\"...\",\n                temperature=0,\n                max_tokens=None,\n                timeout=None,\n                max_retries=2,\n                # api_key=\"...\",\n                # other params...\n            )\n\n    Invoke:\n        .. code-block:: python\n\n            messages = [\n                (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n                (\"human\", \"I love programming.\"),\n            ]\n            llm.invoke(messages)\n\n    Stream:\n        .. code-block:: python\n\n            for chunk in llm.stream(messages):\n                print(chunk.text(), end=\"\")\n\n        .. code-block:: python\n\n            stream = llm.stream(messages)\n            full = next(stream)\n            for chunk in stream:\n                full += chunk\n            full\n\n    Async:\n        .. code-block:: python\n\n            await llm.ainvoke(messages)\n\n            # stream:\n            # async for chunk in (await llm.astream(messages))\n\n            # batch:\n            # await llm.abatch([messages])\n\n    Tool calling:\n        .. code-block:: python\n\n            from pydantic import BaseModel, Field\n\n            class GetWeather(BaseModel):\n                '''Get the current weather in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            class GetPopulation(BaseModel):\n                '''Get the current population in a given location'''\n\n                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n            llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n            ai_msg = llm_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n            ai_msg.tool_calls\n\n        See ``ChatDeepSeek.bind_tools()`` method for more.\n\n    Structured output:\n        .. code-block:: python\n\n            from typing import Optional\n\n            from pydantic import BaseModel, Field\n\n            class Joke(BaseModel):\n                '''Joke to tell user.'''\n\n                setup: str = Field(description=\"The setup of the joke\")\n                punchline: str = Field(description=\"The punchline to the joke\")\n                rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n\n            structured_llm = llm.with_structured_output(Joke)\n            structured_llm.invoke(\"Tell me a joke about cats\")\n\n        See ``ChatDeepSeek.with_structured_output()`` for more.\n\n    Token usage:\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.usage_metadata\n\n        .. code-block:: python\n\n            {'input_tokens': 28, 'output_tokens': 5, 'total_tokens': 33}\n\n    Response metadata\n        .. code-block:: python\n\n            ai_msg = llm.invoke(messages)\n            ai_msg.response_metadata\n\n    \"\"\"  # noqa: E501\n\n    model_name: str = Field(alias=\"model\")\n    \"\"\"The name of the model\"\"\"\n    api_key: Optional[SecretStr] = Field(\n        default_factory=secret_from_env(\"DEEPSEEK_API_KEY\", default=None)\n    )\n    \"\"\"DeepSeek API key\"\"\"\n    api_base: str = Field(\n        default_factory=from_env(\"DEEPSEEK_API_BASE\", default=DEFAULT_API_BASE)\n    )\n    \"\"\"DeepSeek API base URL\"\"\"\n\n    model_config = ConfigDict(populate_by_name=True)\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"chat-deepseek\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        \"\"\"A map of constructor argument names to secret ids.\"\"\"\n        return {\"api_key\": \"DEEPSEEK_API_KEY\"}\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        if self.api_base == DEFAULT_API_BASE and not (\n            self.api_key and self.api_key.get_secret_value()\n        ):\n            raise ValueError(\"If using default api base, DEEPSEEK_API_KEY must be set.\")\n        client_params: dict = {\n            k: v\n            for k, v in {\n                \"api_key\": self.api_key.get_secret_value() if self.api_key else None,\n                \"base_url\": self.api_base,\n                \"timeout\": self.request_timeout,\n                \"max_retries\": self.max_retries,\n                \"default_headers\": self.default_headers,\n                \"default_query\": self.default_query,\n            }.items()\n            if v is not None\n        }\n\n        if not (self.client or None):\n            sync_specific: dict = {\"http_client\": self.http_client}\n            self.client = openai.OpenAI(\n                **client_params, **sync_specific\n            ).chat.completions\n        if not (self.async_client or None):\n            async_specific: dict = {\"http_client\": self.http_async_client}\n            self.async_client = openai.AsyncOpenAI(\n                **client_params, **async_specific\n            ).chat.completions\n        return self\n\n    def _create_chat_result(\n        self,\n        response: Union[dict, openai.BaseModel],\n        generation_info: Optional[Dict] = None,\n    ) -> ChatResult:\n        rtn = super()._create_chat_result(response, generation_info)\n\n        if not isinstance(response, openai.BaseModel):\n            return rtn\n\n        if hasattr(response.choices[0].message, \"reasoning_content\"):  # type: ignore\n            rtn.generations[0].message.additional_kwargs[\"reasoning_content\"] = (\n                response.choices[0].message.reasoning_content  # type: ignore\n            )\n\n        return rtn\n\n    def _convert_chunk_to_generation_chunk(\n        self,\n        chunk: dict,\n        default_chunk_class: Type,\n        base_generation_info: Optional[Dict],\n    ) -> Optional[ChatGenerationChunk]:\n        generation_chunk = super()._convert_chunk_to_generation_chunk(\n            chunk,\n            default_chunk_class,\n            base_generation_info,\n        )\n        if (choices := chunk.get(\"choices\")) and generation_chunk:\n            top = choices[0]\n            if reasoning_content := top.get(\"delta\", {}).get(\"reasoning_content\"):\n                if isinstance(generation_chunk.message, AIMessageChunk):\n                    generation_chunk.message.additional_kwargs[\"reasoning_content\"] = (\n                        reasoning_content\n                    )\n        return generation_chunk\n",
        "patch": "@@ -1,9 +1,13 @@\n \"\"\"DeepSeek chat models.\"\"\"\n \n-from typing import Dict, Optional, Type, Union\n+from json import JSONDecodeError\n+from typing import Any, Dict, Iterator, List, Optional, Type, Union\n \n import openai\n-from langchain_core.messages import AIMessageChunk\n+from langchain_core.callbacks import (\n+    CallbackManagerForLLMRun,\n+)\n+from langchain_core.messages import AIMessageChunk, BaseMessage\n from langchain_core.outputs import ChatGenerationChunk, ChatResult\n from langchain_core.utils import from_env, secret_from_env\n from langchain_openai.chat_models.base import BaseChatOpenAI\n@@ -239,3 +243,41 @@ def _convert_chunk_to_generation_chunk(\n                         reasoning_content\n                     )\n         return generation_chunk\n+\n+    def _stream(\n+        self,\n+        messages: List[BaseMessage],\n+        stop: Optional[List[str]] = None,\n+        run_manager: Optional[CallbackManagerForLLMRun] = None,\n+        **kwargs: Any,\n+    ) -> Iterator[ChatGenerationChunk]:\n+        try:\n+            yield from super()._stream(\n+                messages, stop=stop, run_manager=run_manager, **kwargs\n+            )\n+        except JSONDecodeError as e:\n+            raise JSONDecodeError(\n+                \"DeepSeek API returned an invalid response. \"\n+                \"Please check the API status and try again.\",\n+                e.doc,\n+                e.pos,\n+            ) from e\n+\n+    def _generate(\n+        self,\n+        messages: List[BaseMessage],\n+        stop: Optional[List[str]] = None,\n+        run_manager: Optional[CallbackManagerForLLMRun] = None,\n+        **kwargs: Any,\n+    ) -> ChatResult:\n+        try:\n+            return super()._generate(\n+                messages, stop=stop, run_manager=run_manager, **kwargs\n+            )\n+        except JSONDecodeError as e:\n+            raise JSONDecodeError(\n+                \"DeepSeek API returned an invalid response. \"\n+                \"Please check the API status and try again.\",\n+                e.doc,\n+                e.pos,\n+            ) from e"
      }
    ]
  },
  {
    "number": 29935,
    "title": "community: Repair embeddings/llamacpp's embed_query method",
    "body": "**Description:** As commented on the commit [41b6a86](https://github.com/langchain-ai/langchain/commit/41b6a86bbe030291cf8ee284ed0cd70dd493152b) it introduced a bug for when we do an embedding request and the model returns a non-nested list. Typically it's the case for model **_nomic-embed-text_**.\r\n\r\n- I added the unit test, and ran `make format`, `make lint` and `make test` from the `community` package.\r\n- No new dependency.\r\n",
    "issue_title": "community: Repair embeddings/llamacpp's embed_query method",
    "issue_body": "**Description:** As commented on the commit [41b6a86](https://github.com/langchain-ai/langchain/commit/41b6a86bbe030291cf8ee284ed0cd70dd493152b) it introduced a bug for when we do an embedding request and the model returns a non-nested list. Typically it's the case for model **_nomic-embed-text_**.\r\n\r\n- I added the unit test, and ran `make format`, `make lint` and `make test` from the `community` package.\r\n- No new dependency.\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/embeddings/llamacpp.py",
        "content_before": "from typing import Any, List, Optional\n\nfrom langchain_core.embeddings import Embeddings\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\nfrom typing_extensions import Self\n\n\nclass LlamaCppEmbeddings(BaseModel, Embeddings):\n    \"\"\"llama.cpp embedding models.\n\n    To use, you should have the llama-cpp-python library installed, and provide the\n    path to the Llama model as a named parameter to the constructor.\n    Check out: https://github.com/abetlen/llama-cpp-python\n\n    Example:\n        .. code-block:: python\n\n            from langchain_community.embeddings import LlamaCppEmbeddings\n            llama = LlamaCppEmbeddings(model_path=\"/path/to/model.bin\")\n    \"\"\"\n\n    client: Any = None  #: :meta private:\n    model_path: str = Field(default=\"\")\n\n    n_ctx: int = Field(512, alias=\"n_ctx\")\n    \"\"\"Token context window.\"\"\"\n\n    n_parts: int = Field(-1, alias=\"n_parts\")\n    \"\"\"Number of parts to split the model into. \n    If -1, the number of parts is automatically determined.\"\"\"\n\n    seed: int = Field(-1, alias=\"seed\")\n    \"\"\"Seed. If -1, a random seed is used.\"\"\"\n\n    f16_kv: bool = Field(False, alias=\"f16_kv\")\n    \"\"\"Use half-precision for key/value cache.\"\"\"\n\n    logits_all: bool = Field(False, alias=\"logits_all\")\n    \"\"\"Return logits for all tokens, not just the last token.\"\"\"\n\n    vocab_only: bool = Field(False, alias=\"vocab_only\")\n    \"\"\"Only load the vocabulary, no weights.\"\"\"\n\n    use_mlock: bool = Field(False, alias=\"use_mlock\")\n    \"\"\"Force system to keep model in RAM.\"\"\"\n\n    n_threads: Optional[int] = Field(None, alias=\"n_threads\")\n    \"\"\"Number of threads to use. If None, the number \n    of threads is automatically determined.\"\"\"\n\n    n_batch: Optional[int] = Field(512, alias=\"n_batch\")\n    \"\"\"Number of tokens to process in parallel.\n    Should be a number between 1 and n_ctx.\"\"\"\n\n    n_gpu_layers: Optional[int] = Field(None, alias=\"n_gpu_layers\")\n    \"\"\"Number of layers to be loaded into gpu memory. Default None.\"\"\"\n\n    verbose: bool = Field(True, alias=\"verbose\")\n    \"\"\"Print verbose output to stderr.\"\"\"\n\n    device: Optional[str] = Field(None, alias=\"device\")\n    \"\"\"Device type to use and pass to the model\"\"\"\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n        protected_namespaces=(),\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        \"\"\"Validate that llama-cpp-python library is installed.\"\"\"\n        model_path = self.model_path\n        model_param_names = [\n            \"n_ctx\",\n            \"n_parts\",\n            \"seed\",\n            \"f16_kv\",\n            \"logits_all\",\n            \"vocab_only\",\n            \"use_mlock\",\n            \"n_threads\",\n            \"n_batch\",\n            \"verbose\",\n            \"device\",\n        ]\n        model_params = {k: getattr(self, k) for k in model_param_names}\n        # For backwards compatibility, only include if non-null.\n        if self.n_gpu_layers is not None:\n            model_params[\"n_gpu_layers\"] = self.n_gpu_layers\n\n        if not self.client:\n            try:\n                from llama_cpp import Llama\n\n                self.client = Llama(model_path, embedding=True, **model_params)\n            except ImportError:\n                raise ImportError(\n                    \"Could not import llama-cpp-python library. \"\n                    \"Please install the llama-cpp-python library to \"\n                    \"use this embedding model: pip install llama-cpp-python\"\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Could not load Llama model from path: {model_path}. \"\n                    f\"Received error {e}\"\n                )\n\n        return self\n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed a list of documents using the Llama model.\n\n        Args:\n            texts: The list of texts to embed.\n\n        Returns:\n            List of embeddings, one for each text.\n        \"\"\"\n        embeddings = self.client.create_embedding(texts)\n        final_embeddings = []\n        for e in embeddings[\"data\"]:\n            try:\n                if isinstance(e[\"embedding\"][0], list):\n                    for data in e[\"embedding\"]:\n                        final_embeddings.append(list(map(float, data)))\n                else:\n                    final_embeddings.append(list(map(float, e[\"embedding\"])))\n            except (IndexError, TypeError):\n                final_embeddings.append(list(map(float, e[\"embedding\"])))\n        return final_embeddings\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a query using the Llama model.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embeddings for the text.\n        \"\"\"\n        embedding = self.client.embed(text)\n        if not isinstance(embedding, list):\n            return list(map(float, embedding))\n        else:\n            return list(map(float, embedding[0]))\n",
        "patch": "@@ -139,7 +139,7 @@ def embed_query(self, text: str) -> List[float]:\n             Embeddings for the text.\n         \"\"\"\n         embedding = self.client.embed(text)\n-        if not isinstance(embedding, list):\n-            return list(map(float, embedding))\n-        else:\n+        if embedding and isinstance(embedding, list) and isinstance(embedding[0], list):\n             return list(map(float, embedding[0]))\n+        else:\n+            return list(map(float, embedding))"
      },
      {
        "filename": "libs/community/tests/unit_tests/embeddings/test_llamacpp.py",
        "content_before": "from typing import Generator\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\n\n\n@pytest.fixture\ndef mock_llama_client() -> Generator[MagicMock, None, None]:\n    with patch(\n        \"langchain_community.embeddings.llamacpp.LlamaCppEmbeddings\"\n    ) as MockLlama:\n        mock_client = MagicMock()\n        MockLlama.return_value = mock_client\n        yield mock_client\n\n\ndef test_initialization(mock_llama_client: MagicMock) -> None:\n    embeddings = LlamaCppEmbeddings(client=mock_llama_client)  # type: ignore[call-arg]\n    assert embeddings.client is not None\n\n\ndef test_embed_documents(mock_llama_client: MagicMock) -> None:\n    mock_llama_client.create_embedding.return_value = {\n        \"data\": [{\"embedding\": [[0.1, 0.2, 0.3]]}, {\"embedding\": [[0.4, 0.5, 0.6]]}]\n    }\n    embeddings = LlamaCppEmbeddings(client=mock_llama_client)  # type: ignore[call-arg]\n    texts = [\"Hello world\", \"Test document\"]\n    result = embeddings.embed_documents(texts)\n    expected = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n    assert result == expected\n\n\ndef test_embed_query(mock_llama_client: MagicMock) -> None:\n    mock_llama_client.embed.return_value = [[0.1, 0.2, 0.3]]\n    embeddings = LlamaCppEmbeddings(client=mock_llama_client)  # type: ignore[call-arg]\n    result = embeddings.embed_query(\"Sample query\")\n    expected = [0.1, 0.2, 0.3]\n    assert result == expected\n",
        "patch": "@@ -38,3 +38,11 @@ def test_embed_query(mock_llama_client: MagicMock) -> None:\n     result = embeddings.embed_query(\"Sample query\")\n     expected = [0.1, 0.2, 0.3]\n     assert result == expected\n+\n+\n+def test_embed_query_not_nested(mock_llama_client: MagicMock) -> None:\n+    mock_llama_client.embed.return_value = [0.1, 0.2, 0.3]\n+    embeddings = LlamaCppEmbeddings(client=mock_llama_client)  # type: ignore[call-arg]\n+    result = embeddings.embed_query(\"Sample query\")\n+    expected = [0.1, 0.2, 0.3]\n+    assert result == expected"
      }
    ]
  },
  {
    "number": 29937,
    "title": "docs: updated ChatLiteLLM model_kwargs description",
    "body": "\r\n- [x] **PR title**: docs: (community) update ChatLiteLLM\r\n\r\n- [x] **PR message**:\r\n    - **Description:** updated description of model_kwargs parameter which was wrongly describing for temperature.\r\n    - **Issue:** #29862 \r\n    - **Dependencies:** N/A\r\n    \r\n- [x] **Add tests and docs**: N/A\r\n\r\n- [x] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n",
    "issue_title": "docs: updated ChatLiteLLM model_kwargs description",
    "issue_body": "\r\n- [x] **PR title**: docs: (community) update ChatLiteLLM\r\n\r\n- [x] **PR message**:\r\n    - **Description:** updated description of model_kwargs parameter which was wrongly describing for temperature.\r\n    - **Issue:** #29862 \r\n    - **Dependencies:** N/A\r\n    \r\n- [x] **Add tests and docs**: N/A\r\n\r\n- [x] **Lint and test**: Run `make format`, `make lint` and `make test` from the root of the package(s) you've modified. See contribution guidelines for more: https://python.langchain.com/docs/contributing/\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/chat_models/deepinfra.py",
        "content_before": "\"\"\"deepinfra.com chat models wrapper\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom json import JSONDecodeError\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n)\n\nimport aiohttp\nimport requests\nfrom langchain_core.callbacks.manager import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.language_models.llms import create_base_retry_decorator\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    ChatMessage,\n    ChatMessageChunk,\n    FunctionMessage,\n    FunctionMessageChunk,\n    HumanMessage,\n    HumanMessageChunk,\n    SystemMessage,\n    SystemMessageChunk,\n    ToolMessage,\n)\nfrom langchain_core.messages.tool import ToolCall\nfrom langchain_core.messages.tool import tool_call as create_tool_call\nfrom langchain_core.outputs import (\n    ChatGeneration,\n    ChatGenerationChunk,\n    ChatResult,\n)\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import get_from_dict_or_env\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\nfrom typing_extensions import Self\n\nfrom langchain_community.utilities.requests import Requests\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChatDeepInfraException(Exception):\n    \"\"\"Exception raised when the DeepInfra API returns an error.\"\"\"\n\n    pass\n\n\ndef _create_retry_decorator(\n    llm: ChatDeepInfra,\n    run_manager: Optional[\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n    ] = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Returns a tenacity retry decorator, preconfigured to handle PaLM exceptions.\"\"\"\n    return create_base_retry_decorator(\n        error_types=[requests.exceptions.ConnectTimeout, ChatDeepInfraException],\n        max_retries=llm.max_retries,\n        run_manager=run_manager,\n    )\n\n\ndef _parse_tool_calling(tool_call: dict) -> ToolCall:\n    \"\"\"\n    Convert a tool calling response from server to a ToolCall object.\n    Args:\n        tool_call:\n\n    Returns:\n\n    \"\"\"\n    name = tool_call[\"function\"].get(\"name\", \"\")\n    try:\n        args = json.loads(tool_call[\"function\"][\"arguments\"])\n    except (JSONDecodeError, TypeError):\n        args = {}\n    id = tool_call.get(\"id\")\n    return create_tool_call(name=name, args=args, id=id)\n\n\ndef _convert_to_tool_calling(tool_call: ToolCall) -> Dict[str, Any]:\n    \"\"\"\n    Convert a ToolCall object to a tool calling request for server.\n    Args:\n        tool_call:\n\n    Returns:\n\n    \"\"\"\n    return {\n        \"type\": \"function\",\n        \"function\": {\n            \"arguments\": json.dumps(tool_call[\"args\"]),\n            \"name\": tool_call[\"name\"],\n        },\n        \"id\": tool_call.get(\"id\"),\n    }\n\n\ndef _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n    role = _dict[\"role\"]\n    if role == \"user\":\n        return HumanMessage(content=_dict[\"content\"])\n    elif role == \"assistant\":\n        content = _dict.get(\"content\", \"\") or \"\"\n        tool_calls_content = _dict.get(\"tool_calls\", []) or []\n        tool_calls = [\n            _parse_tool_calling(tool_call) for tool_call in tool_calls_content\n        ]\n        return AIMessage(content=content, tool_calls=tool_calls)\n    elif role == \"system\":\n        return SystemMessage(content=_dict[\"content\"])\n    elif role == \"function\":\n        return FunctionMessage(content=_dict[\"content\"], name=_dict[\"name\"])\n    else:\n        return ChatMessage(content=_dict[\"content\"], role=role)\n\n\ndef _convert_delta_to_message_chunk(\n    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n) -> BaseMessageChunk:\n    role = _dict.get(\"role\")\n    content = _dict.get(\"content\") or \"\"\n    tool_calls = _dict.get(\"tool_calls\") or []\n\n    if role == \"user\" or default_class == HumanMessageChunk:\n        return HumanMessageChunk(content=content)\n    elif role == \"assistant\" or default_class == AIMessageChunk:\n        tool_calls = [_parse_tool_calling(tool_call) for tool_call in tool_calls]\n        return AIMessageChunk(content=content, tool_calls=tool_calls)\n    elif role == \"system\" or default_class == SystemMessageChunk:\n        return SystemMessageChunk(content=content)\n    elif role == \"function\" or default_class == FunctionMessageChunk:\n        return FunctionMessageChunk(content=content, name=_dict[\"name\"])\n    elif role or default_class == ChatMessageChunk:\n        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]\n    else:\n        return default_class(content=content)  # type: ignore[call-arg]\n\n\ndef _convert_message_to_dict(message: BaseMessage) -> dict:\n    if isinstance(message, ChatMessage):\n        message_dict = {\"role\": message.role, \"content\": message.content}\n    elif isinstance(message, HumanMessage):\n        message_dict = {\"role\": \"user\", \"content\": message.content}\n    elif isinstance(message, AIMessage):\n        tool_calls = [\n            _convert_to_tool_calling(tool_call) for tool_call in message.tool_calls\n        ]\n        message_dict = {\n            \"role\": \"assistant\",\n            \"content\": message.content,\n            \"tool_calls\": tool_calls,  # type: ignore[dict-item]\n        }\n    elif isinstance(message, SystemMessage):\n        message_dict = {\"role\": \"system\", \"content\": message.content}\n    elif isinstance(message, FunctionMessage):\n        message_dict = {\n            \"role\": \"function\",\n            \"content\": message.content,\n            \"name\": message.name,\n        }\n    elif isinstance(message, ToolMessage):\n        message_dict = {\n            \"role\": \"tool\",\n            \"content\": message.content,\n            \"name\": message.name,  # type: ignore[dict-item]\n            \"tool_call_id\": message.tool_call_id,\n        }\n    else:\n        raise ValueError(f\"Got unknown type {message}\")\n    if \"name\" in message.additional_kwargs:\n        message_dict[\"name\"] = message.additional_kwargs[\"name\"]\n    return message_dict\n\n\nclass ChatDeepInfra(BaseChatModel):\n    \"\"\"A chat model that uses the DeepInfra API.\"\"\"\n\n    # client: Any  #: :meta private:\n    model_name: str = Field(default=\"meta-llama/Llama-2-70b-chat-hf\", alias=\"model\")\n    \"\"\"Model name to use.\"\"\"\n\n    url: str = \"https://api.deepinfra.com/v1/openai/chat/completions\"\n    \"\"\"URL to use for the API call.\"\"\"\n\n    deepinfra_api_token: Optional[str] = None\n    request_timeout: Optional[float] = Field(default=None, alias=\"timeout\")\n    temperature: Optional[float] = 1\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Run inference with this temperature. Must be in the closed\n       interval [0.0, 1.0].\"\"\"\n    top_p: Optional[float] = None\n    \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose\n       probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\"\n    top_k: Optional[int] = None\n    \"\"\"Decode using top-k sampling: consider the set of top_k most probable tokens.\n       Must be positive.\"\"\"\n    n: int = 1\n    \"\"\"Number of chat completions to generate for each prompt. Note that the API may\n       not return the full n completions if duplicates are generated.\"\"\"\n    max_tokens: int = 256\n    streaming: bool = False\n    max_retries: int = 1\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n    )\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        return {\n            \"model\": self.model_name,\n            \"max_tokens\": self.max_tokens,\n            \"stream\": self.streaming,\n            \"n\": self.n,\n            \"temperature\": self.temperature,\n            \"request_timeout\": self.request_timeout,\n            **self.model_kwargs,\n        }\n\n    @property\n    def _client_params(self) -> Dict[str, Any]:\n        \"\"\"Get the parameters used for the openai client.\"\"\"\n        return {**self._default_params}\n\n    def completion_with_retry(\n        self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Use tenacity to retry the completion call.\"\"\"\n        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)\n\n        @retry_decorator\n        def _completion_with_retry(**kwargs: Any) -> Any:\n            try:\n                request_timeout = kwargs.pop(\"request_timeout\")\n                request = Requests(headers=self._headers())\n                response = request.post(\n                    url=self._url(), data=self._body(kwargs), timeout=request_timeout\n                )\n                self._handle_status(response.status_code, response.text)\n                return response\n            except Exception as e:\n                print(\"EX\", e)  # noqa: T201\n                raise\n\n        return _completion_with_retry(**kwargs)\n\n    async def acompletion_with_retry(\n        self,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Use tenacity to retry the async completion call.\"\"\"\n        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)\n\n        @retry_decorator\n        async def _completion_with_retry(**kwargs: Any) -> Any:\n            try:\n                request_timeout = kwargs.pop(\"request_timeout\")\n                request = Requests(headers=self._headers())\n                async with request.apost(\n                    url=self._url(), data=self._body(kwargs), timeout=request_timeout\n                ) as response:\n                    self._handle_status(response.status, await response.text())\n                    return await response.json()\n            except Exception as e:\n                print(\"EX\", e)  # noqa: T201\n                raise\n\n        return await _completion_with_retry(**kwargs)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def init_defaults(cls, values: Dict) -> Any:\n        \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\"\n        # For compatibility with LiteLLM\n        api_key = get_from_dict_or_env(\n            values,\n            \"deepinfra_api_key\",\n            \"DEEPINFRA_API_KEY\",\n            default=\"\",\n        )\n        values[\"deepinfra_api_token\"] = get_from_dict_or_env(\n            values,\n            \"deepinfra_api_token\",\n            \"DEEPINFRA_API_TOKEN\",\n            default=api_key,\n        )\n        return values\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        if self.temperature is not None and not 0 <= self.temperature <= 1:\n            raise ValueError(\"temperature must be in the range [0.0, 1.0]\")\n\n        if self.top_p is not None and not 0 <= self.top_p <= 1:\n            raise ValueError(\"top_p must be in the range [0.0, 1.0]\")\n\n        if self.top_k is not None and self.top_k <= 0:\n            raise ValueError(\"top_k must be positive\")\n\n        return self\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs}\n        response = self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        )\n        return self._create_chat_result(response.json())\n\n    def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult:\n        generations = []\n        for res in response[\"choices\"]:\n            message = _convert_dict_to_message(res[\"message\"])\n            gen = ChatGeneration(\n                message=message,\n                generation_info=dict(finish_reason=res.get(\"finish_reason\")),\n            )\n            generations.append(gen)\n        token_usage = response.get(\"usage\", {})\n        llm_output = {\"token_usage\": token_usage, \"model\": self.model_name}\n        res = ChatResult(generations=generations, llm_output=llm_output)\n        return res\n\n    def _create_message_dicts(\n        self, messages: List[BaseMessage], stop: Optional[List[str]]\n    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n        params = self._client_params\n        if stop is not None:\n            if \"stop\" in params:\n                raise ValueError(\"`stop` found in both the input and default params.\")\n            params[\"stop\"] = stop\n        message_dicts = [_convert_message_to_dict(m) for m in messages]\n        return message_dicts, params\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs, \"stream\": True}\n\n        response = self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        )\n        for line in _parse_stream(response.iter_lines()):\n            chunk = _handle_sse_line(line)\n            if chunk:\n                cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)\n                if run_manager:\n                    run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk)\n                yield cg_chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {\"messages\": message_dicts, \"stream\": True, **params, **kwargs}\n\n        request_timeout = params.pop(\"request_timeout\")\n        request = Requests(headers=self._headers())\n        async with request.apost(\n            url=self._url(), data=self._body(params), timeout=request_timeout\n        ) as response:\n            async for line in _parse_stream_async(response.content):\n                chunk = _handle_sse_line(line)\n                if chunk:\n                    cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)\n                    if run_manager:\n                        await run_manager.on_llm_new_token(\n                            str(chunk.content), chunk=cg_chunk\n                        )\n                    yield cg_chunk\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._astream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {\"messages\": message_dicts, **params, **kwargs}\n\n        res = await self.acompletion_with_retry(run_manager=run_manager, **params)\n        return self._create_chat_result(res)\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            \"model\": self.model_name,\n            \"temperature\": self.temperature,\n            \"top_p\": self.top_p,\n            \"top_k\": self.top_k,\n            \"n\": self.n,\n        }\n\n    @property\n    def _llm_type(self) -> str:\n        return \"deepinfra-chat\"\n\n    def _handle_status(self, code: int, text: Any) -> None:\n        if code >= 500:\n            raise ChatDeepInfraException(\n                f\"DeepInfra Server error status {code}: {text}\"\n            )\n        elif code >= 400:\n            raise ValueError(f\"DeepInfra received an invalid payload: {text}\")\n        elif code != 200:\n            raise Exception(\n                f\"DeepInfra returned an unexpected response with status {code}: {text}\"\n            )\n\n    def _url(self) -> str:\n        return self.url\n\n    def _headers(self) -> Dict:\n        return {\n            \"Authorization\": f\"bearer {self.deepinfra_api_token}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def _body(self, kwargs: Any) -> Dict:\n        return kwargs\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        Assumes model is compatible with OpenAI tool-calling API.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n                models, callables, and BaseTools will be automatically converted to\n                their schema dictionary representation.\n            **kwargs: Any additional parameters to pass to the\n                :class:`~langchain.runnable.Runnable` constructor.\n        \"\"\"\n\n        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n        return super().bind(tools=formatted_tools, **kwargs)\n\n\ndef _parse_stream(rbody: Iterator[bytes]) -> Iterator[str]:\n    for line in rbody:\n        _line = _parse_stream_helper(line)\n        if _line is not None:\n            yield _line\n\n\nasync def _parse_stream_async(rbody: aiohttp.StreamReader) -> AsyncIterator[str]:\n    async for line in rbody:\n        _line = _parse_stream_helper(line)\n        if _line is not None:\n            yield _line\n\n\ndef _parse_stream_helper(line: bytes) -> Optional[str]:\n    if line and line.startswith(b\"data:\"):\n        if line.startswith(b\"data: \"):\n            # SSE event may be valid when it contain whitespace\n            line = line[len(b\"data: \") :]\n        else:\n            line = line[len(b\"data:\") :]\n        if line.strip() == b\"[DONE]\":\n            # return here will cause GeneratorExit exception in urllib3\n            # and it will close http connection with TCP Reset\n            return None\n        else:\n            return line.decode(\"utf-8\")\n    return None\n\n\ndef _handle_sse_line(line: str) -> Optional[BaseMessageChunk]:\n    try:\n        obj = json.loads(line)\n        default_chunk_class = AIMessageChunk\n        delta = obj.get(\"choices\", [{}])[0].get(\"delta\", {})\n        return _convert_delta_to_message_chunk(delta, default_chunk_class)\n    except Exception:\n        return None\n",
        "patch": "@@ -214,9 +214,10 @@ class ChatDeepInfra(BaseChatModel):\n     deepinfra_api_token: Optional[str] = None\n     request_timeout: Optional[float] = Field(default=None, alias=\"timeout\")\n     temperature: Optional[float] = 1\n-    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n     \"\"\"Run inference with this temperature. Must be in the closed\n        interval [0.0, 1.0].\"\"\"\n+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n+    \"\"\"Holds any model parameters valid for API call not explicitly specified.\"\"\"\n     top_p: Optional[float] = None\n     \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose\n        probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\""
      },
      {
        "filename": "libs/community/langchain_community/chat_models/litellm.py",
        "content_before": "\"\"\"Wrapper around LiteLLM's model I/O library.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.language_models.llms import create_base_retry_decorator\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    ChatMessage,\n    ChatMessageChunk,\n    FunctionMessage,\n    FunctionMessageChunk,\n    HumanMessage,\n    HumanMessageChunk,\n    SystemMessage,\n    SystemMessageChunk,\n    ToolCall,\n    ToolCallChunk,\n    ToolMessage,\n)\nfrom langchain_core.outputs import (\n    ChatGeneration,\n    ChatGenerationChunk,\n    ChatResult,\n)\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import get_from_dict_or_env, pre_init\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom pydantic import BaseModel, Field\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChatLiteLLMException(Exception):\n    \"\"\"Error with the `LiteLLM I/O` library\"\"\"\n\n\ndef _create_retry_decorator(\n    llm: ChatLiteLLM,\n    run_manager: Optional[\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n    ] = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Returns a tenacity retry decorator, preconfigured to handle PaLM exceptions\"\"\"\n    import litellm\n\n    errors = [\n        litellm.Timeout,\n        litellm.APIError,\n        litellm.APIConnectionError,\n        litellm.RateLimitError,\n    ]\n    return create_base_retry_decorator(\n        error_types=errors, max_retries=llm.max_retries, run_manager=run_manager\n    )\n\n\ndef _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n    role = _dict[\"role\"]\n    if role == \"user\":\n        return HumanMessage(content=_dict[\"content\"])\n    elif role == \"assistant\":\n        # Fix for azure\n        # Also OpenAI returns None for tool invocations\n        content = _dict.get(\"content\", \"\") or \"\"\n\n        additional_kwargs = {}\n        if _dict.get(\"function_call\"):\n            additional_kwargs[\"function_call\"] = dict(_dict[\"function_call\"])\n\n        if _dict.get(\"tool_calls\"):\n            additional_kwargs[\"tool_calls\"] = _dict[\"tool_calls\"]\n\n        return AIMessage(content=content, additional_kwargs=additional_kwargs)\n    elif role == \"system\":\n        return SystemMessage(content=_dict[\"content\"])\n    elif role == \"function\":\n        return FunctionMessage(content=_dict[\"content\"], name=_dict[\"name\"])\n    else:\n        return ChatMessage(content=_dict[\"content\"], role=role)\n\n\nasync def acompletion_with_retry(\n    llm: ChatLiteLLM,\n    run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n    **kwargs: Any,\n) -> Any:\n    \"\"\"Use tenacity to retry the async completion call.\"\"\"\n    retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)\n\n    @retry_decorator\n    async def _completion_with_retry(**kwargs: Any) -> Any:\n        # Use OpenAI's async api https://github.com/openai/openai-python#async-api\n        return await llm.client.acreate(**kwargs)\n\n    return await _completion_with_retry(**kwargs)\n\n\ndef _convert_delta_to_message_chunk(\n    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]\n) -> BaseMessageChunk:\n    role = _dict.get(\"role\")\n    content = _dict.get(\"content\") or \"\"\n    if _dict.get(\"function_call\"):\n        additional_kwargs = {\"function_call\": dict(_dict[\"function_call\"])}\n    else:\n        additional_kwargs = {}\n\n    tool_call_chunks = []\n    if raw_tool_calls := _dict.get(\"tool_calls\"):\n        additional_kwargs[\"tool_calls\"] = raw_tool_calls\n        try:\n            tool_call_chunks = [\n                ToolCallChunk(\n                    name=rtc[\"function\"].get(\"name\"),\n                    args=rtc[\"function\"].get(\"arguments\"),\n                    id=rtc.get(\"id\"),\n                    index=rtc[\"index\"],\n                )\n                for rtc in raw_tool_calls\n            ]\n        except KeyError:\n            pass\n\n    if role == \"user\" or default_class == HumanMessageChunk:\n        return HumanMessageChunk(content=content)\n    elif role == \"assistant\" or default_class == AIMessageChunk:\n        return AIMessageChunk(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            tool_call_chunks=tool_call_chunks,\n        )\n    elif role == \"system\" or default_class == SystemMessageChunk:\n        return SystemMessageChunk(content=content)\n    elif role == \"function\" or default_class == FunctionMessageChunk:\n        return FunctionMessageChunk(content=content, name=_dict[\"name\"])\n    elif role or default_class == ChatMessageChunk:\n        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]\n    else:\n        return default_class(content=content)  # type: ignore[call-arg]\n\n\ndef _lc_tool_call_to_openai_tool_call(tool_call: ToolCall) -> dict:\n    return {\n        \"type\": \"function\",\n        \"id\": tool_call[\"id\"],\n        \"function\": {\n            \"name\": tool_call[\"name\"],\n            \"arguments\": json.dumps(tool_call[\"args\"]),\n        },\n    }\n\n\ndef _convert_message_to_dict(message: BaseMessage) -> dict:\n    message_dict: Dict[str, Any] = {\"content\": message.content}\n    if isinstance(message, ChatMessage):\n        message_dict[\"role\"] = message.role\n    elif isinstance(message, HumanMessage):\n        message_dict[\"role\"] = \"user\"\n    elif isinstance(message, AIMessage):\n        message_dict[\"role\"] = \"assistant\"\n        if \"function_call\" in message.additional_kwargs:\n            message_dict[\"function_call\"] = message.additional_kwargs[\"function_call\"]\n        if message.tool_calls:\n            message_dict[\"tool_calls\"] = [\n                _lc_tool_call_to_openai_tool_call(tc) for tc in message.tool_calls\n            ]\n        elif \"tool_calls\" in message.additional_kwargs:\n            message_dict[\"tool_calls\"] = message.additional_kwargs[\"tool_calls\"]\n    elif isinstance(message, SystemMessage):\n        message_dict[\"role\"] = \"system\"\n    elif isinstance(message, FunctionMessage):\n        message_dict[\"role\"] = \"function\"\n        message_dict[\"name\"] = message.name\n    elif isinstance(message, ToolMessage):\n        message_dict[\"role\"] = \"tool\"\n        message_dict[\"tool_call_id\"] = message.tool_call_id\n    else:\n        raise ValueError(f\"Got unknown type {message}\")\n    if \"name\" in message.additional_kwargs:\n        message_dict[\"name\"] = message.additional_kwargs[\"name\"]\n    return message_dict\n\n\n_OPENAI_MODELS = [\n    \"o1-mini\",\n    \"o1-preview\",\n    \"gpt-4o-mini\",\n    \"gpt-4o-mini-2024-07-18\",\n    \"gpt-4o\",\n    \"gpt-4o-2024-08-06\",\n    \"gpt-4o-2024-05-13\",\n    \"gpt-4-turbo\",\n    \"gpt-4-turbo-preview\",\n    \"gpt-4-0125-preview\",\n    \"gpt-4-1106-preview\",\n    \"gpt-3.5-turbo-1106\",\n    \"gpt-3.5-turbo\",\n    \"gpt-3.5-turbo-0301\",\n    \"gpt-3.5-turbo-0613\",\n    \"gpt-3.5-turbo-16k\",\n    \"gpt-3.5-turbo-16k-0613\",\n    \"gpt-4\",\n    \"gpt-4-0314\",\n    \"gpt-4-0613\",\n    \"gpt-4-32k\",\n    \"gpt-4-32k-0314\",\n    \"gpt-4-32k-0613\",\n]\n\n\nclass ChatLiteLLM(BaseChatModel):\n    \"\"\"Chat model that uses the LiteLLM API.\"\"\"\n\n    client: Any = None  #: :meta private:\n    model: str = \"gpt-3.5-turbo\"\n    model_name: Optional[str] = None\n    \"\"\"Model name to use.\"\"\"\n    openai_api_key: Optional[str] = None\n    azure_api_key: Optional[str] = None\n    anthropic_api_key: Optional[str] = None\n    replicate_api_key: Optional[str] = None\n    cohere_api_key: Optional[str] = None\n    openrouter_api_key: Optional[str] = None\n    api_key: Optional[str] = None\n    streaming: bool = False\n    api_base: Optional[str] = None\n    organization: Optional[str] = None\n    custom_llm_provider: Optional[str] = None\n    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n    temperature: Optional[float] = 1\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Run inference with this temperature. Must be in the closed\n       interval [0.0, 1.0].\"\"\"\n    top_p: Optional[float] = None\n    \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose\n       probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\"\n    top_k: Optional[int] = None\n    \"\"\"Decode using top-k sampling: consider the set of top_k most probable tokens.\n       Must be positive.\"\"\"\n    n: int = 1\n    \"\"\"Number of chat completions to generate for each prompt. Note that the API may\n       not return the full n completions if duplicates are generated.\"\"\"\n    max_tokens: Optional[int] = None\n\n    max_retries: int = 6\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling OpenAI API.\"\"\"\n        set_model_value = self.model\n        if self.model_name is not None:\n            set_model_value = self.model_name\n        return {\n            \"model\": set_model_value,\n            \"force_timeout\": self.request_timeout,\n            \"max_tokens\": self.max_tokens,\n            \"stream\": self.streaming,\n            \"n\": self.n,\n            \"temperature\": self.temperature,\n            \"custom_llm_provider\": self.custom_llm_provider,\n            **self.model_kwargs,\n        }\n\n    @property\n    def _client_params(self) -> Dict[str, Any]:\n        \"\"\"Get the parameters used for the openai client.\"\"\"\n        set_model_value = self.model\n        if self.model_name is not None:\n            set_model_value = self.model_name\n        self.client.api_base = self.api_base\n        self.client.api_key = self.api_key\n        for named_api_key in [\n            \"openai_api_key\",\n            \"azure_api_key\",\n            \"anthropic_api_key\",\n            \"replicate_api_key\",\n            \"cohere_api_key\",\n            \"openrouter_api_key\",\n        ]:\n            if api_key_value := getattr(self, named_api_key):\n                setattr(\n                    self.client,\n                    named_api_key.replace(\"_api_key\", \"_key\"),\n                    api_key_value,\n                )\n        self.client.organization = self.organization\n        creds: Dict[str, Any] = {\n            \"model\": set_model_value,\n            \"force_timeout\": self.request_timeout,\n            \"api_base\": self.api_base,\n        }\n        return {**self._default_params, **creds}\n\n    def completion_with_retry(\n        self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Use tenacity to retry the completion call.\"\"\"\n        retry_decorator = _create_retry_decorator(self, run_manager=run_manager)\n\n        @retry_decorator\n        def _completion_with_retry(**kwargs: Any) -> Any:\n            return self.client.completion(**kwargs)\n\n        return _completion_with_retry(**kwargs)\n\n    @pre_init\n    def validate_environment(cls, values: Dict) -> Dict:\n        \"\"\"Validate api key, python package exists, temperature, top_p, and top_k.\"\"\"\n        try:\n            import litellm\n        except ImportError:\n            raise ChatLiteLLMException(\n                \"Could not import litellm python package. \"\n                \"Please install it with `pip install litellm`\"\n            )\n\n        values[\"openai_api_key\"] = get_from_dict_or_env(\n            values, \"openai_api_key\", \"OPENAI_API_KEY\", default=\"\"\n        )\n        values[\"azure_api_key\"] = get_from_dict_or_env(\n            values, \"azure_api_key\", \"AZURE_API_KEY\", default=\"\"\n        )\n        values[\"anthropic_api_key\"] = get_from_dict_or_env(\n            values, \"anthropic_api_key\", \"ANTHROPIC_API_KEY\", default=\"\"\n        )\n        values[\"replicate_api_key\"] = get_from_dict_or_env(\n            values, \"replicate_api_key\", \"REPLICATE_API_KEY\", default=\"\"\n        )\n        values[\"openrouter_api_key\"] = get_from_dict_or_env(\n            values, \"openrouter_api_key\", \"OPENROUTER_API_KEY\", default=\"\"\n        )\n        values[\"cohere_api_key\"] = get_from_dict_or_env(\n            values, \"cohere_api_key\", \"COHERE_API_KEY\", default=\"\"\n        )\n        values[\"huggingface_api_key\"] = get_from_dict_or_env(\n            values, \"huggingface_api_key\", \"HUGGINGFACE_API_KEY\", default=\"\"\n        )\n        values[\"together_ai_api_key\"] = get_from_dict_or_env(\n            values, \"together_ai_api_key\", \"TOGETHERAI_API_KEY\", default=\"\"\n        )\n        values[\"client\"] = litellm\n\n        if values[\"temperature\"] is not None and not 0 <= values[\"temperature\"] <= 1:\n            raise ValueError(\"temperature must be in the range [0.0, 1.0]\")\n\n        if values[\"top_p\"] is not None and not 0 <= values[\"top_p\"] <= 1:\n            raise ValueError(\"top_p must be in the range [0.0, 1.0]\")\n\n        if values[\"top_k\"] is not None and values[\"top_k\"] <= 0:\n            raise ValueError(\"top_k must be positive\")\n\n        return values\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs}\n        response = self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        )\n        return self._create_chat_result(response)\n\n    def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult:\n        generations = []\n        for res in response[\"choices\"]:\n            message = _convert_dict_to_message(res[\"message\"])\n            gen = ChatGeneration(\n                message=message,\n                generation_info=dict(finish_reason=res.get(\"finish_reason\")),\n            )\n            generations.append(gen)\n        token_usage = response.get(\"usage\", {})\n        set_model_value = self.model\n        if self.model_name is not None:\n            set_model_value = self.model_name\n        llm_output = {\"token_usage\": token_usage, \"model\": set_model_value}\n        return ChatResult(generations=generations, llm_output=llm_output)\n\n    def _create_message_dicts(\n        self, messages: List[BaseMessage], stop: Optional[List[str]]\n    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n        params = self._client_params\n        if stop is not None:\n            if \"stop\" in params:\n                raise ValueError(\"`stop` found in both the input and default params.\")\n            params[\"stop\"] = stop\n        message_dicts = [_convert_message_to_dict(m) for m in messages]\n        return message_dicts, params\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs, \"stream\": True}\n\n        default_chunk_class = AIMessageChunk\n        for chunk in self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        ):\n            if not isinstance(chunk, dict):\n                chunk = chunk.model_dump()\n            if len(chunk[\"choices\"]) == 0:\n                continue\n            delta = chunk[\"choices\"][0][\"delta\"]\n            chunk = _convert_delta_to_message_chunk(delta, default_chunk_class)\n            default_chunk_class = chunk.__class__\n            cg_chunk = ChatGenerationChunk(message=chunk)\n            if run_manager:\n                run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)\n            yield cg_chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs, \"stream\": True}\n\n        default_chunk_class = AIMessageChunk\n        async for chunk in await acompletion_with_retry(\n            self, messages=message_dicts, run_manager=run_manager, **params\n        ):\n            if not isinstance(chunk, dict):\n                chunk = chunk.model_dump()\n            if len(chunk[\"choices\"]) == 0:\n                continue\n            delta = chunk[\"choices\"][0][\"delta\"]\n            chunk = _convert_delta_to_message_chunk(delta, default_chunk_class)\n            default_chunk_class = chunk.__class__\n            cg_chunk = ChatGenerationChunk(message=chunk)\n            if run_manager:\n                await run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)\n            yield cg_chunk\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._astream(\n                messages=messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs}\n        response = await acompletion_with_retry(\n            self, messages=message_dicts, run_manager=run_manager, **params\n        )\n        return self._create_chat_result(response)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n        tool_choice: Optional[\n            Union[dict, str, Literal[\"auto\", \"none\", \"required\", \"any\"], bool]\n        ] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        LiteLLM expects tools argument in OpenAI format.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n                models, callables, and BaseTools will be automatically converted to\n                their schema dictionary representation.\n            tool_choice: Which tool to require the model to call. Options are:\n                - str of the form ``\"<<tool_name>>\"``: calls <<tool_name>> tool.\n                - ``\"auto\"``:\n                    automatically selects a tool (including no tool).\n                - ``\"none\"``:\n                    does not call a tool.\n                - ``\"any\"`` or ``\"required\"`` or ``True``:\n                    forces least one tool to be called.\n                - dict of the form:\n                ``{\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}``\n                - ``False`` or ``None``: no effect\n            **kwargs: Any additional parameters to pass to the\n                :class:`~langchain.runnable.Runnable` constructor.\n        \"\"\"\n\n        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n\n        # In case of openai if tool_choice is `any` or if bool has been provided we\n        # change it to `required` as that is suppored by openai.\n        if (\n            (self.model is not None and \"azure\" in self.model)\n            or (self.model_name is not None and \"azure\" in self.model_name)\n            or (self.model is not None and self.model in _OPENAI_MODELS)\n            or (self.model_name is not None and self.model_name in _OPENAI_MODELS)\n        ) and (tool_choice == \"any\" or isinstance(tool_choice, bool)):\n            tool_choice = \"required\"\n        # If tool_choice is bool apart from openai we make it `any`\n        elif isinstance(tool_choice, bool):\n            tool_choice = \"any\"\n        elif isinstance(tool_choice, dict):\n            tool_names = [\n                formatted_tool[\"function\"][\"name\"] for formatted_tool in formatted_tools\n            ]\n            if not any(\n                tool_name == tool_choice[\"function\"][\"name\"] for tool_name in tool_names\n            ):\n                raise ValueError(\n                    f\"Tool choice {tool_choice} was specified, but the only \"\n                    f\"provided tools were {tool_names}.\"\n                )\n        return super().bind(tools=formatted_tools, tool_choice=tool_choice, **kwargs)\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        set_model_value = self.model\n        if self.model_name is not None:\n            set_model_value = self.model_name\n        return {\n            \"model\": set_model_value,\n            \"temperature\": self.temperature,\n            \"top_p\": self.top_p,\n            \"top_k\": self.top_k,\n            \"n\": self.n,\n        }\n\n    @property\n    def _llm_type(self) -> str:\n        return \"litellm-chat\"\n",
        "patch": "@@ -260,9 +260,10 @@ class ChatLiteLLM(BaseChatModel):\n     custom_llm_provider: Optional[str] = None\n     request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n     temperature: Optional[float] = 1\n-    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n     \"\"\"Run inference with this temperature. Must be in the closed\n        interval [0.0, 1.0].\"\"\"\n+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n+    \"\"\"Holds any model parameters valid for API call not explicitly specified.\"\"\"\n     top_p: Optional[float] = None\n     \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose\n        probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\""
      }
    ]
  },
  {
    "number": 29938,
    "title": "anthropic[patch]: update models for integration tests",
    "body": null,
    "issue_title": "anthropic[patch]: update models for integration tests",
    "issue_body": null,
    "files": [
      {
        "filename": "libs/partners/anthropic/tests/integration_tests/test_chat_models.py",
        "content_before": "\"\"\"Test ChatAnthropic chat model.\"\"\"\n\nimport json\nfrom base64 import b64encode\nfrom typing import List, Optional\n\nimport pytest\nimport requests\nfrom langchain_core.callbacks import CallbackManager\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    HumanMessage,\n    SystemMessage,\n    ToolMessage,\n)\nfrom langchain_core.outputs import ChatGeneration, LLMResult\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\nfrom pydantic import BaseModel, Field\n\nfrom langchain_anthropic import ChatAnthropic, ChatAnthropicMessages\nfrom tests.unit_tests._utils import FakeCallbackHandler\n\nMODEL_NAME = \"claude-3-5-sonnet-20240620\"\n\n\ndef test_stream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    for token in llm.stream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n\nasync def test_astream() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    full: Optional[BaseMessageChunk] = None\n    chunks_with_input_token_counts = 0\n    chunks_with_output_token_counts = 0\n    async for token in llm.astream(\"I'm Pickle Rick\"):\n        assert isinstance(token.content, str)\n        full = token if full is None else full + token\n        assert isinstance(token, AIMessageChunk)\n        if token.usage_metadata is not None:\n            if token.usage_metadata.get(\"input_tokens\"):\n                chunks_with_input_token_counts += 1\n            elif token.usage_metadata.get(\"output_tokens\"):\n                chunks_with_output_token_counts += 1\n    if chunks_with_input_token_counts != 1 or chunks_with_output_token_counts != 1:\n        raise AssertionError(\n            \"Expected exactly one chunk with input or output token counts. \"\n            \"AIMessageChunk aggregation adds counts. Check that \"\n            \"this is behaving properly.\"\n        )\n    # check token usage is populated\n    assert isinstance(full, AIMessageChunk)\n    assert full.usage_metadata is not None\n    assert full.usage_metadata[\"input_tokens\"] > 0\n    assert full.usage_metadata[\"output_tokens\"] > 0\n    assert full.usage_metadata[\"total_tokens\"] > 0\n    assert (\n        full.usage_metadata[\"input_tokens\"] + full.usage_metadata[\"output_tokens\"]\n        == full.usage_metadata[\"total_tokens\"]\n    )\n    assert \"stop_reason\" in full.response_metadata\n    assert \"stop_sequence\" in full.response_metadata\n\n    # test usage metadata can be excluded\n    model = ChatAnthropic(model_name=MODEL_NAME, stream_usage=False)  # type: ignore[call-arg]\n    async for token in model.astream(\"hi\"):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n    # check we override with kwarg\n    model = ChatAnthropic(model_name=MODEL_NAME)  # type: ignore[call-arg]\n    assert model.stream_usage\n    async for token in model.astream(\"hi\", stream_usage=False):\n        assert isinstance(token, AIMessageChunk)\n        assert token.usage_metadata is None\n\n    # Check expected raw API output\n    async_client = model._async_client\n    params: dict = {\n        \"model\": \"claude-3-haiku-20240307\",\n        \"max_tokens\": 1024,\n        \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}],\n        \"temperature\": 0.0,\n    }\n    stream = await async_client.messages.create(**params, stream=True)\n    async for event in stream:\n        if event.type == \"message_start\":\n            assert event.message.usage.input_tokens > 1\n            # Note: this single output token included in message start event\n            # does not appear to contribute to overall output token counts. It\n            # is excluded from the total token count.\n            assert event.message.usage.output_tokens == 1\n        elif event.type == \"message_delta\":\n            assert event.usage.output_tokens > 1\n        else:\n            pass\n\n\nasync def test_abatch() -> None:\n    \"\"\"Test streaming tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_abatch_tags() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.abatch(\n        [\"I'm Pickle Rick\", \"I'm not Pickle Rick\"], config={\"tags\": [\"foo\"]}\n    )\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_async_tool_use() -> None:\n    llm = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = await llm_with_tools.ainvoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    first = True\n    chunks = []  # type: ignore\n    async for chunk in llm_with_tools.astream(\n        \"what's the weather in san francisco, ca\"\n    ):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_call_chunks, list)\n    assert len(gathered.tool_call_chunks) == 1\n    tool_call_chunk = gathered.tool_call_chunks[0]\n    assert tool_call_chunk[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call_chunk[\"args\"], str)\n    assert \"location\" in json.loads(tool_call_chunk[\"args\"])\n\n\ndef test_batch() -> None:\n    \"\"\"Test batch tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.batch([\"I'm Pickle Rick\", \"I'm not Pickle Rick\"])\n    for token in result:\n        assert isinstance(token.content, str)\n\n\nasync def test_ainvoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = await llm.ainvoke(\"I'm Pickle Rick\", config={\"tags\": [\"foo\"]})\n    assert isinstance(result.content, str)\n\n\ndef test_invoke() -> None:\n    \"\"\"Test invoke tokens from ChatAnthropicMessages.\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    result = llm.invoke(\"I'm Pickle Rick\", config=dict(tags=[\"foo\"]))\n    assert isinstance(result.content, str)\n\n\ndef test_system_invoke() -> None:\n    \"\"\"Test invoke tokens with a system message\"\"\"\n    llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are an expert cartographer. If asked, you are a cartographer. \"\n                \"STAY IN CHARACTER\",\n            ),\n            (\"human\", \"Are you a mathematician?\"),\n        ]\n    )\n\n    chain = prompt | llm\n\n    result = chain.invoke({})\n    assert isinstance(result.content, str)\n\n\ndef test_anthropic_call() -> None:\n    \"\"\"Test valid call to anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.invoke([message])\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n\n\ndef test_anthropic_generate() -> None:\n    \"\"\"Test generate method of anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    chat_messages: List[List[BaseMessage]] = [\n        [HumanMessage(content=\"How many toes do dogs have?\")]\n    ]\n    messages_copy = [messages.copy() for messages in chat_messages]\n    result: LLMResult = chat.generate(chat_messages)\n    assert isinstance(result, LLMResult)\n    for response in result.generations[0]:\n        assert isinstance(response, ChatGeneration)\n        assert isinstance(response.text, str)\n        assert response.text == response.message.content\n    assert chat_messages == messages_copy\n\n\ndef test_anthropic_streaming() -> None:\n    \"\"\"Test streaming tokens from anthropic.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    message = HumanMessage(content=\"Hello\")\n    response = chat.stream([message])\n    for token in response:\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n\n\ndef test_anthropic_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    message = HumanMessage(content=\"Write me a sentence with 10 words.\")\n    for token in chat.stream([message]):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\nasync def test_anthropic_async_streaming_callback() -> None:\n    \"\"\"Test that streaming correctly invokes on_llm_new_token callback.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n    chat = ChatAnthropic(\n        model=MODEL_NAME,\n        callback_manager=callback_manager,\n        verbose=True,\n    )\n    chat_messages: List[BaseMessage] = [\n        HumanMessage(content=\"How many toes do dogs have?\")\n    ]\n    async for token in chat.astream(chat_messages):\n        assert isinstance(token, AIMessageChunk)\n        assert isinstance(token.content, str)\n    assert callback_handler.llm_streams > 1\n\n\ndef test_anthropic_multimodal() -> None:\n    \"\"\"Test that multimodal inputs are handled correctly.\"\"\"\n    chat = ChatAnthropic(model=MODEL_NAME)\n    messages: list[BaseMessage] = [\n        HumanMessage(\n            content=[\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        # langchain logo\n                        \"url\": \"data:image/jpeg;base64,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\",  # noqa: E501\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is this a logo for?\"},\n            ]\n        )\n    ]\n    response = chat.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, str)\n    num_tokens = chat.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n\ndef test_streaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = llm.generate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\nasync def test_astreaming() -> None:\n    \"\"\"Test streaming tokens from Anthropic.\"\"\"\n    callback_handler = FakeCallbackHandler()\n    callback_manager = CallbackManager([callback_handler])\n\n    llm = ChatAnthropicMessages(  # type: ignore[call-arg, call-arg]\n        model_name=MODEL_NAME, streaming=True, callback_manager=callback_manager\n    )\n\n    response = await llm.agenerate([[HumanMessage(content=\"I'm Pickle Rick\")]])\n    assert callback_handler.llm_streams > 0\n    assert isinstance(response, LLMResult)\n\n\ndef test_tool_use() -> None:\n    llm = ChatAnthropic(model=MODEL_NAME)\n    llm_with_tools = llm.bind_tools(\n        [\n            {\n                \"name\": \"get_weather\",\n                \"description\": \"Get weather report for a city\",\n                \"input_schema\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"location\": {\"type\": \"string\"}},\n                },\n            }\n        ]\n    )\n    response = llm_with_tools.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert isinstance(response.tool_calls, list)\n    assert len(response.tool_calls) == 1\n    tool_call = response.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n\n    # Test streaming\n    input = \"how are you? what's the weather in san francisco, ca\"\n    first = True\n    chunks = []  # type: ignore\n    for chunk in llm_with_tools.stream(input):\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n    assert isinstance(gathered.content, list)\n    assert len(gathered.content) == 2\n    tool_use_block = None\n    for content_block in gathered.content:\n        assert isinstance(content_block, dict)\n        if content_block[\"type\"] == \"tool_use\":\n            tool_use_block = content_block\n            break\n    assert tool_use_block is not None\n    assert tool_use_block[\"name\"] == \"get_weather\"\n    assert \"location\" in json.loads(tool_use_block[\"partial_json\"])\n    assert isinstance(gathered, AIMessageChunk)\n    assert isinstance(gathered.tool_calls, list)\n    assert len(gathered.tool_calls) == 1\n    tool_call = gathered.tool_calls[0]\n    assert tool_call[\"name\"] == \"get_weather\"\n    assert isinstance(tool_call[\"args\"], dict)\n    assert \"location\" in tool_call[\"args\"]\n    assert tool_call[\"id\"] is not None\n\n    # Test passing response back to model\n    stream = llm_with_tools.stream(\n        [\n            input,\n            gathered,\n            ToolMessage(content=\"sunny and warm\", tool_call_id=tool_call[\"id\"]),\n        ]\n    )\n    chunks = []  # type: ignore\n    first = True\n    for chunk in stream:\n        chunks = chunks + [chunk]\n        if first:\n            gathered = chunk\n            first = False\n        else:\n            gathered = gathered + chunk  # type: ignore\n    assert len(chunks) > 1\n\n\nclass GenerateUsername(BaseModel):\n    \"Get a username based on someone's name and hair color.\"\n\n    name: str\n    hair_color: str\n\n\ndef test_disable_parallel_tool_calling() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n    llm_with_tools = llm.bind_tools([GenerateUsername], parallel_tool_calls=False)\n    result = llm_with_tools.invoke(\n        \"Use the GenerateUsername tool to generate user names for:\\n\\n\"\n        \"Sally with green hair\\n\"\n        \"Bob with blue hair\"\n    )\n    assert isinstance(result, AIMessage)\n    assert len(result.tool_calls) == 1\n\n\ndef test_anthropic_with_empty_text_block() -> None:\n    \"\"\"Anthropic SDK can return an empty text block.\"\"\"\n\n    @tool\n    def type_letter(letter: str) -> str:\n        \"\"\"Type the given letter.\"\"\"\n        return \"OK\"\n\n    model = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0).bind_tools(\n        [type_letter]\n    )\n\n    messages = [\n        SystemMessage(\n            content=\"Repeat the given string using the provided tools. Do not write \"\n            \"anything else or provide any explanations. For example, \"\n            \"if the string is 'abc', you must print the \"\n            \"letters 'a', 'b', and 'c' one at a time and in that order. \"\n        ),\n        HumanMessage(content=\"dog\"),\n        AIMessage(\n            content=[\n                {\"text\": \"\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"letter\": \"d\"},\n                    \"name\": \"type_letter\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"type_letter\",\n                    \"args\": {\"letter\": \"d\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"OK\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n\n    model.invoke(messages)\n\n\ndef test_with_structured_output() -> None:\n    llm = ChatAnthropic(\n        model=\"claude-3-opus-20240229\",\n    )\n\n    structured_llm = llm.with_structured_output(\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather report for a city\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\"location\": {\"type\": \"string\"}},\n            },\n        }\n    )\n    response = structured_llm.invoke(\"what's the weather in san francisco, ca\")\n    assert isinstance(response, dict)\n    assert response[\"location\"]\n\n\ndef test_get_num_tokens_from_messages() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n\n    # Test simple case\n    messages = [\n        SystemMessage(content=\"You are a scientist\"),\n        HumanMessage(content=\"Hello, Claude\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages)\n    assert num_tokens > 0\n\n    # Test tool use\n    @tool(parse_docstring=True)\n    def get_weather(location: str) -> str:\n        \"\"\"Get the current weather in a given location\n\n        Args:\n            location: The city and state, e.g. San Francisco, CA\n        \"\"\"\n        return \"Sunny\"\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n    messages = [\n        HumanMessage(content=\"What's the weather like in San Francisco?\"),\n        AIMessage(\n            content=[\n                {\"text\": \"Let's see.\", \"type\": \"text\"},\n                {\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"input\": {\"location\": \"SF\"},\n                    \"name\": \"get_weather\",\n                    \"type\": \"tool_use\",\n                },\n            ],\n            tool_calls=[\n                {\n                    \"name\": \"get_weather\",\n                    \"args\": {\"location\": \"SF\"},\n                    \"id\": \"toolu_01V6d6W32QGGSmQm4BT98EKk\",\n                    \"type\": \"tool_call\",\n                },\n            ],\n        ),\n        ToolMessage(content=\"Sunny\", tool_call_id=\"toolu_01V6d6W32QGGSmQm4BT98EKk\"),\n    ]\n    num_tokens = llm.get_num_tokens_from_messages(messages, tools=[get_weather])\n    assert num_tokens > 0\n\n\nclass GetWeather(BaseModel):\n    \"\"\"Get the current weather in a given location\"\"\"\n\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n\n@pytest.mark.parametrize(\"tool_choice\", [\"GetWeather\", \"auto\", \"any\"])\ndef test_anthropic_bind_tools_tool_choice(tool_choice: str) -> None:\n    chat_model = ChatAnthropic(\n        model=MODEL_NAME,\n    )\n    chat_model_with_tools = chat_model.bind_tools([GetWeather], tool_choice=tool_choice)\n    response = chat_model_with_tools.invoke(\"what's the weather in ny and la\")\n    assert isinstance(response, AIMessage)\n\n\ndef test_pdf_document_input() -> None:\n    url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n    data = b64encode(requests.get(url).content).decode()\n\n    result = ChatAnthropic(model=MODEL_NAME).invoke(\n        [\n            HumanMessage(\n                [\n                    \"summarize this document\",\n                    {\n                        \"type\": \"document\",\n                        \"source\": {\n                            \"type\": \"base64\",\n                            \"data\": data,\n                            \"media_type\": \"application/pdf\",\n                        },\n                    },\n                ]\n            )\n        ]\n    )\n    assert isinstance(result, AIMessage)\n    assert isinstance(result.content, str)\n    assert len(result.content) > 0\n\n\ndef test_citations() -> None:\n    llm = ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"content\",\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": \"The grass is green\"},\n                            {\"type\": \"text\", \"text\": \"The sky is blue\"},\n                        ],\n                    },\n                    \"citations\": {\"enabled\": True},\n                },\n                {\"type\": \"text\", \"text\": \"What color is the grass and sky?\"},\n            ],\n        }\n    ]\n    response = llm.invoke(messages)\n    assert isinstance(response, AIMessage)\n    assert isinstance(response.content, list)\n    assert any(\"citations\" in block for block in response.content)\n\n    # Test streaming\n    full: Optional[BaseMessageChunk] = None\n    for chunk in llm.stream(messages):\n        full = chunk if full is None else full + chunk\n    assert isinstance(full, AIMessageChunk)\n    assert isinstance(full.content, list)\n    assert any(\"citations\" in block for block in full.content)\n    assert not any(\"citation\" in block for block in full.content)\n",
        "patch": "@@ -24,7 +24,8 @@\n from langchain_anthropic import ChatAnthropic, ChatAnthropicMessages\n from tests.unit_tests._utils import FakeCallbackHandler\n \n-MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n+MODEL_NAME = \"claude-3-5-haiku-latest\"\n+IMAGE_MODEL_NAME = \"claude-3-5-sonnet-latest\"\n \n \n def test_stream() -> None:\n@@ -98,22 +99,10 @@ async def test_astream() -> None:\n     assert \"stop_reason\" in full.response_metadata\n     assert \"stop_sequence\" in full.response_metadata\n \n-    # test usage metadata can be excluded\n-    model = ChatAnthropic(model_name=MODEL_NAME, stream_usage=False)  # type: ignore[call-arg]\n-    async for token in model.astream(\"hi\"):\n-        assert isinstance(token, AIMessageChunk)\n-        assert token.usage_metadata is None\n-    # check we override with kwarg\n-    model = ChatAnthropic(model_name=MODEL_NAME)  # type: ignore[call-arg]\n-    assert model.stream_usage\n-    async for token in model.astream(\"hi\", stream_usage=False):\n-        assert isinstance(token, AIMessageChunk)\n-        assert token.usage_metadata is None\n-\n     # Check expected raw API output\n-    async_client = model._async_client\n+    async_client = llm._async_client\n     params: dict = {\n-        \"model\": \"claude-3-haiku-20240307\",\n+        \"model\": MODEL_NAME,\n         \"max_tokens\": 1024,\n         \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}],\n         \"temperature\": 0.0,\n@@ -132,6 +121,20 @@ async def test_astream() -> None:\n             pass\n \n \n+async def test_stream_usage() -> None:\n+    \"\"\"Test usage metadata can be excluded.\"\"\"\n+    model = ChatAnthropic(model_name=MODEL_NAME, stream_usage=False)  # type: ignore[call-arg]\n+    async for token in model.astream(\"hi\"):\n+        assert isinstance(token, AIMessageChunk)\n+        assert token.usage_metadata is None\n+    # check we override with kwarg\n+    model = ChatAnthropic(model_name=MODEL_NAME)  # type: ignore[call-arg]\n+    assert model.stream_usage\n+    async for token in model.astream(\"hi\", stream_usage=False):\n+        assert isinstance(token, AIMessageChunk)\n+        assert token.usage_metadata is None\n+\n+\n async def test_abatch() -> None:\n     \"\"\"Test streaming tokens from ChatAnthropicMessages.\"\"\"\n     llm = ChatAnthropicMessages(model_name=MODEL_NAME)  # type: ignore[call-arg, call-arg]\n@@ -318,7 +321,7 @@ async def test_anthropic_async_streaming_callback() -> None:\n \n def test_anthropic_multimodal() -> None:\n     \"\"\"Test that multimodal inputs are handled correctly.\"\"\"\n-    chat = ChatAnthropic(model=MODEL_NAME)\n+    chat = ChatAnthropic(model=IMAGE_MODEL_NAME)\n     messages: list[BaseMessage] = [\n         HumanMessage(\n             content=[\n@@ -602,7 +605,7 @@ def test_pdf_document_input() -> None:\n     url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n     data = b64encode(requests.get(url).content).decode()\n \n-    result = ChatAnthropic(model=MODEL_NAME).invoke(\n+    result = ChatAnthropic(model=IMAGE_MODEL_NAME).invoke(\n         [\n             HumanMessage(\n                 ["
      },
      {
        "filename": "libs/partners/anthropic/tests/integration_tests/test_standard.py",
        "content_before": "\"\"\"Standard LangChain interface tests\"\"\"\n\nfrom pathlib import Path\nfrom typing import Dict, List, Literal, Type, cast\n\nfrom langchain_core.language_models import BaseChatModel\nfrom langchain_core.messages import AIMessage\nfrom langchain_tests.integration_tests import ChatModelIntegrationTests\n\nfrom langchain_anthropic import ChatAnthropic\n\nREPO_ROOT_DIR = Path(__file__).parents[5]\n\n\nclass TestAnthropicStandard(ChatModelIntegrationTests):\n    @property\n    def chat_model_class(self) -> Type[BaseChatModel]:\n        return ChatAnthropic\n\n    @property\n    def chat_model_params(self) -> dict:\n        return {\"model\": \"claude-3-haiku-20240307\"}\n\n    @property\n    def supports_image_inputs(self) -> bool:\n        return True\n\n    @property\n    def supports_image_tool_message(self) -> bool:\n        return True\n\n    @property\n    def supports_anthropic_inputs(self) -> bool:\n        return True\n\n    @property\n    def supported_usage_metadata_details(\n        self,\n    ) -> Dict[\n        Literal[\"invoke\", \"stream\"],\n        List[\n            Literal[\n                \"audio_input\",\n                \"audio_output\",\n                \"reasoning_output\",\n                \"cache_read_input\",\n                \"cache_creation_input\",\n            ]\n        ],\n    ]:\n        return {\n            \"invoke\": [\"cache_read_input\", \"cache_creation_input\"],\n            \"stream\": [\"cache_read_input\", \"cache_creation_input\"],\n        }\n\n    def invoke_with_cache_creation_input(self, *, stream: bool = False) -> AIMessage:\n        llm = ChatAnthropic(\n            model=\"claude-3-5-sonnet-20240620\",  # type: ignore[call-arg]\n            extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},  # type: ignore[call-arg]\n        )\n        with open(REPO_ROOT_DIR / \"README.md\", \"r\") as f:\n            readme = f.read()\n\n        input_ = f\"\"\"What's langchain? Here's the langchain README:\n\n        {readme}\n        \"\"\"\n        return _invoke(\n            llm,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": input_,\n                            \"cache_control\": {\"type\": \"ephemeral\"},\n                        }\n                    ],\n                }\n            ],\n            stream,\n        )\n\n    def invoke_with_cache_read_input(self, *, stream: bool = False) -> AIMessage:\n        llm = ChatAnthropic(\n            model=\"claude-3-5-sonnet-20240620\",  # type: ignore[call-arg]\n            extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},  # type: ignore[call-arg]\n        )\n        with open(REPO_ROOT_DIR / \"README.md\", \"r\") as f:\n            readme = f.read()\n\n        input_ = f\"\"\"What's langchain? Here's the langchain README:\n\n        {readme}\n        \"\"\"\n\n        # invoke twice so first invocation is cached\n        _invoke(\n            llm,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": input_,\n                            \"cache_control\": {\"type\": \"ephemeral\"},\n                        }\n                    ],\n                }\n            ],\n            stream,\n        )\n        return _invoke(\n            llm,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": input_,\n                            \"cache_control\": {\"type\": \"ephemeral\"},\n                        }\n                    ],\n                }\n            ],\n            stream,\n        )\n\n\ndef _invoke(llm: ChatAnthropic, input_: list, stream: bool) -> AIMessage:\n    if stream:\n        full = None\n        for chunk in llm.stream(input_):\n            full = full + chunk if full else chunk  # type: ignore[operator]\n        return cast(AIMessage, full)\n    else:\n        return cast(AIMessage, llm.invoke(input_))\n",
        "patch": "@@ -19,7 +19,7 @@ def chat_model_class(self) -> Type[BaseChatModel]:\n \n     @property\n     def chat_model_params(self) -> dict:\n-        return {\"model\": \"claude-3-haiku-20240307\"}\n+        return {\"model\": \"claude-3-5-sonnet-latest\"}\n \n     @property\n     def supports_image_inputs(self) -> bool:"
      }
    ]
  },
  {
    "number": 29271,
    "title": "core: Add ruff rules ANN (type annotations)",
    "body": "See https://docs.astral.sh/ruff/rules/#flake8-annotations-ann\r\nThe interest compared to only mypy is that ruff is very fast at detecting missing annotations.\r\n\r\nANN101 and ANN102 are deprecated so we ignore them \r\nANN401 (no Any type) ignored to be in sync with mypy config",
    "issue_title": "core: Add ruff rules ANN (type annotations)",
    "issue_body": "See https://docs.astral.sh/ruff/rules/#flake8-annotations-ann\r\nThe interest compared to only mypy is that ruff is very fast at detecting missing annotations.\r\n\r\nANN101 and ANN102 are deprecated so we ignore them \r\nANN401 (no Any type) ignored to be in sync with mypy config",
    "files": [
      {
        "filename": "libs/core/langchain_core/_api/beta_decorator.py",
        "content_before": "\"\"\"Helper functions for marking parts of the LangChain API as beta.\n\nThis module was loosely adapted from matplotlibs _api/deprecation.py module:\n\nhttps://github.com/matplotlib/matplotlib/blob/main/lib/matplotlib/_api/deprecation.py\n\n.. warning::\n\n    This module is for internal use only.  Do not use it in your own code.\n    We may change the API at any time with no warning.\n\"\"\"\n\nimport contextlib\nimport functools\nimport inspect\nimport warnings\nfrom collections.abc import Generator\nfrom typing import Any, Callable, TypeVar, Union, cast\n\nfrom langchain_core._api.internal import is_caller_internal\n\n\nclass LangChainBetaWarning(DeprecationWarning):\n    \"\"\"A class for issuing beta warnings for LangChain users.\"\"\"\n\n\n# PUBLIC API\n\n\nT = TypeVar(\"T\", bound=Union[Callable[..., Any], type])\n\n\ndef beta(\n    *,\n    message: str = \"\",\n    name: str = \"\",\n    obj_type: str = \"\",\n    addendum: str = \"\",\n) -> Callable[[T], T]:\n    \"\"\"Decorator to mark a function, a class, or a property as beta.\n\n    When marking a classmethod, a staticmethod, or a property, the\n    ``@beta`` decorator should go *under* ``@classmethod`` and\n    ``@staticmethod`` (i.e., `beta` should directly decorate the\n    underlying callable), but *over* ``@property``.\n\n    When marking a class ``C`` intended to be used as a base class in a\n    multiple inheritance hierarchy, ``C`` *must* define an ``__init__`` method\n    (if ``C`` instead inherited its ``__init__`` from its own base class, then\n    ``@beta`` would mess up ``__init__`` inheritance when installing its\n    own (annotation-emitting) ``C.__init__``).\n\n    Args:\n        message : str, optional\n            Override the default beta message. The %(since)s,\n            %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,\n            and %(removal)s format specifiers will be replaced by the\n            values of the respective arguments passed to this function.\n        name : str, optional\n            The name of the beta object.\n        obj_type : str, optional\n            The object type being beta.\n        addendum : str, optional\n            Additional text appended directly to the final message.\n\n    Examples:\n\n        .. code-block:: python\n\n            @beta\n            def the_function_to_annotate():\n                pass\n    \"\"\"\n\n    def beta(\n        obj: T,\n        *,\n        _obj_type: str = obj_type,\n        _name: str = name,\n        _message: str = message,\n        _addendum: str = addendum,\n    ) -> T:\n        \"\"\"Implementation of the decorator returned by `beta`.\"\"\"\n\n        def emit_warning() -> None:\n            \"\"\"Emit the warning.\"\"\"\n            warn_beta(\n                message=_message,\n                name=_name,\n                obj_type=_obj_type,\n                addendum=_addendum,\n            )\n\n        warned = False\n\n        def warning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:\n            \"\"\"Wrapper for the original wrapped callable that emits a warning.\n\n            Args:\n                *args: The positional arguments to the function.\n                **kwargs: The keyword arguments to the function.\n\n            Returns:\n                The return value of the function being wrapped.\n            \"\"\"\n            nonlocal warned\n            if not warned and not is_caller_internal():\n                warned = True\n                emit_warning()\n            return wrapped(*args, **kwargs)\n\n        async def awarning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:\n            \"\"\"Same as warning_emitting_wrapper, but for async functions.\"\"\"\n            nonlocal warned\n            if not warned and not is_caller_internal():\n                warned = True\n                emit_warning()\n            return await wrapped(*args, **kwargs)\n\n        if isinstance(obj, type):\n            if not _obj_type:\n                _obj_type = \"class\"\n            wrapped = obj.__init__  # type: ignore\n            _name = _name or obj.__qualname__\n            old_doc = obj.__doc__\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n                \"\"\"Finalize the annotation of a class.\"\"\"\n                # Can't set new_doc on some extension objects.\n                with contextlib.suppress(AttributeError):\n                    obj.__doc__ = new_doc\n\n                def warn_if_direct_instance(\n                    self: Any, *args: Any, **kwargs: Any\n                ) -> Any:\n                    \"\"\"Warn that the class is in beta.\"\"\"\n                    nonlocal warned\n                    if not warned and type(self) is obj and not is_caller_internal():\n                        warned = True\n                        emit_warning()\n                    return wrapped(self, *args, **kwargs)\n\n                obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n                    warn_if_direct_instance\n                )\n                return cast(T, obj)\n\n        elif isinstance(obj, property):\n            # note(erick): this block doesn't seem to be used?\n            if not _obj_type:\n                _obj_type = \"attribute\"\n            wrapped = None\n            _name = _name or obj.fget.__qualname__\n            old_doc = obj.__doc__\n\n            class _BetaProperty(property):\n                \"\"\"A beta property.\"\"\"\n\n                def __init__(self, fget=None, fset=None, fdel=None, doc=None):\n                    super().__init__(fget, fset, fdel, doc)\n                    self.__orig_fget = fget\n                    self.__orig_fset = fset\n                    self.__orig_fdel = fdel\n\n                def __get__(self, instance, owner=None):\n                    if instance is not None or owner is not None:\n                        emit_warning()\n                    return self.fget(instance)\n\n                def __set__(self, instance, value):\n                    if instance is not None:\n                        emit_warning()\n                    return self.fset(instance, value)\n\n                def __delete__(self, instance):\n                    if instance is not None:\n                        emit_warning()\n                    return self.fdel(instance)\n\n                def __set_name__(self, owner, set_name):\n                    nonlocal _name\n                    if _name == \"<lambda>\":\n                        _name = set_name\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> Any:\n                \"\"\"Finalize the property.\"\"\"\n                return _BetaProperty(\n                    fget=obj.fget, fset=obj.fset, fdel=obj.fdel, doc=new_doc\n                )\n\n        else:\n            _name = _name or obj.__qualname__\n            if not _obj_type:\n                # edge case: when a function is within another function\n                # within a test, this will call it a \"method\" not a \"function\"\n                _obj_type = \"function\" if \".\" not in _name else \"method\"\n            wrapped = obj\n            old_doc = wrapped.__doc__\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n                \"\"\"Wrap the wrapped function using the wrapper and update the docstring.\n\n                Args:\n                    wrapper: The wrapper function.\n                    new_doc: The new docstring.\n\n                Returns:\n                    The wrapped function.\n                \"\"\"\n                wrapper = functools.wraps(wrapped)(wrapper)\n                wrapper.__doc__ = new_doc\n                return cast(T, wrapper)\n\n        old_doc = inspect.cleandoc(old_doc or \"\").strip(\"\\n\") or \"\"\n        components = [message, addendum]\n        details = \" \".join([component.strip() for component in components if component])\n        new_doc = f\".. beta::\\n   {details}\\n\\n{old_doc}\\n\"\n\n        if inspect.iscoroutinefunction(obj):\n            finalized = finalize(awarning_emitting_wrapper, new_doc)\n        else:\n            finalized = finalize(warning_emitting_wrapper, new_doc)\n        return cast(T, finalized)\n\n    return beta\n\n\n@contextlib.contextmanager\ndef suppress_langchain_beta_warning() -> Generator[None, None, None]:\n    \"\"\"Context manager to suppress LangChainDeprecationWarning.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", LangChainBetaWarning)\n        yield\n\n\ndef warn_beta(\n    *,\n    message: str = \"\",\n    name: str = \"\",\n    obj_type: str = \"\",\n    addendum: str = \"\",\n) -> None:\n    \"\"\"Display a standardized beta annotation.\n\n    Arguments:\n        message : str, optional\n            Override the default beta message. The\n            %(name)s, %(obj_type)s, %(addendum)s\n            format specifiers will be replaced by the\n            values of the respective arguments passed to this function.\n        name : str, optional\n            The name of the annotated object.\n        obj_type : str, optional\n            The object type being annotated.\n        addendum : str, optional\n            Additional text appended directly to the final message.\n    \"\"\"\n    if not message:\n        message = \"\"\n\n        if obj_type:\n            message += f\"The {obj_type} `{name}`\"\n        else:\n            message += f\"`{name}`\"\n\n        message += \" is in beta. It is actively being worked on, so the API may change.\"\n\n        if addendum:\n            message += f\" {addendum}\"\n\n    warning = LangChainBetaWarning(message)\n    warnings.warn(warning, category=LangChainBetaWarning, stacklevel=4)\n\n\ndef surface_langchain_beta_warnings() -> None:\n    \"\"\"Unmute LangChain beta warnings.\"\"\"\n    warnings.filterwarnings(\n        \"default\",\n        category=LangChainBetaWarning,\n    )\n",
        "patch": "@@ -156,28 +156,36 @@ def warn_if_direct_instance(\n             class _BetaProperty(property):\n                 \"\"\"A beta property.\"\"\"\n \n-                def __init__(self, fget=None, fset=None, fdel=None, doc=None):\n+                def __init__(\n+                    self,\n+                    fget: Union[Callable[[Any], Any], None] = None,\n+                    fset: Union[Callable[[Any, Any], None], None] = None,\n+                    fdel: Union[Callable[[Any], None], None] = None,\n+                    doc: Union[str, None] = None,\n+                ) -> None:\n                     super().__init__(fget, fset, fdel, doc)\n                     self.__orig_fget = fget\n                     self.__orig_fset = fset\n                     self.__orig_fdel = fdel\n \n-                def __get__(self, instance, owner=None):\n+                def __get__(\n+                    self, instance: Any, owner: Union[type, None] = None\n+                ) -> Any:\n                     if instance is not None or owner is not None:\n                         emit_warning()\n                     return self.fget(instance)\n \n-                def __set__(self, instance, value):\n+                def __set__(self, instance: Any, value: Any) -> None:\n                     if instance is not None:\n                         emit_warning()\n                     return self.fset(instance, value)\n \n-                def __delete__(self, instance):\n+                def __delete__(self, instance: Any) -> None:\n                     if instance is not None:\n                         emit_warning()\n                     return self.fdel(instance)\n \n-                def __set_name__(self, owner, set_name):\n+                def __set_name__(self, owner: Union[type, None], set_name: str) -> None:\n                     nonlocal _name\n                     if _name == \"<lambda>\":\n                         _name = set_name"
      },
      {
        "filename": "libs/core/langchain_core/_api/deprecation.py",
        "content_before": "\"\"\"Helper functions for deprecating parts of the LangChain API.\n\nThis module was adapted from matplotlibs _api/deprecation.py module:\n\nhttps://github.com/matplotlib/matplotlib/blob/main/lib/matplotlib/_api/deprecation.py\n\n.. warning::\n\n    This module is for internal use only.  Do not use it in your own code.\n    We may change the API at any time with no warning.\n\"\"\"\n\nimport contextlib\nimport functools\nimport inspect\nimport warnings\nfrom collections.abc import Generator\nfrom typing import (\n    Any,\n    Callable,\n    TypeVar,\n    Union,\n    cast,\n)\n\nfrom typing_extensions import ParamSpec\n\nfrom langchain_core._api.internal import is_caller_internal\n\n\nclass LangChainDeprecationWarning(DeprecationWarning):\n    \"\"\"A class for issuing deprecation warnings for LangChain users.\"\"\"\n\n\nclass LangChainPendingDeprecationWarning(PendingDeprecationWarning):\n    \"\"\"A class for issuing deprecation warnings for LangChain users.\"\"\"\n\n\n# PUBLIC API\n\n\n# Last Any should be FieldInfoV1 but this leads to circular imports\nT = TypeVar(\"T\", bound=Union[type, Callable[..., Any], Any])\n\n\ndef _validate_deprecation_params(\n    pending: bool,\n    removal: str,\n    alternative: str,\n    alternative_import: str,\n) -> None:\n    \"\"\"Validate the deprecation parameters.\"\"\"\n    if pending and removal:\n        msg = \"A pending deprecation cannot have a scheduled removal\"\n        raise ValueError(msg)\n    if alternative and alternative_import:\n        msg = \"Cannot specify both alternative and alternative_import\"\n        raise ValueError(msg)\n\n    if alternative_import and \".\" not in alternative_import:\n        msg = (\n            \"alternative_import must be a fully qualified module path. Got \"\n            f\" {alternative_import}\"\n        )\n        raise ValueError(msg)\n\n\ndef deprecated(\n    since: str,\n    *,\n    message: str = \"\",\n    name: str = \"\",\n    alternative: str = \"\",\n    alternative_import: str = \"\",\n    pending: bool = False,\n    obj_type: str = \"\",\n    addendum: str = \"\",\n    removal: str = \"\",\n    package: str = \"\",\n) -> Callable[[T], T]:\n    \"\"\"Decorator to mark a function, a class, or a property as deprecated.\n\n    When deprecating a classmethod, a staticmethod, or a property, the\n    ``@deprecated`` decorator should go *under* ``@classmethod`` and\n    ``@staticmethod`` (i.e., `deprecated` should directly decorate the\n    underlying callable), but *over* ``@property``.\n\n    When deprecating a class ``C`` intended to be used as a base class in a\n    multiple inheritance hierarchy, ``C`` *must* define an ``__init__`` method\n    (if ``C`` instead inherited its ``__init__`` from its own base class, then\n    ``@deprecated`` would mess up ``__init__`` inheritance when installing its\n    own (deprecation-emitting) ``C.__init__``).\n\n    Parameters are the same as for `warn_deprecated`, except that *obj_type*\n    defaults to 'class' if decorating a class, 'attribute' if decorating a\n    property, and 'function' otherwise.\n\n    Args:\n        since : str\n            The release at which this API became deprecated.\n        message : str, optional\n            Override the default deprecation message. The %(since)s,\n            %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,\n            and %(removal)s format specifiers will be replaced by the\n            values of the respective arguments passed to this function.\n        name : str, optional\n            The name of the deprecated object.\n        alternative : str, optional\n            An alternative API that the user may use in place of the\n            deprecated API. The deprecation warning will tell the user\n            about this alternative if provided.\n        pending : bool, optional\n            If True, uses a PendingDeprecationWarning instead of a\n            DeprecationWarning. Cannot be used together with removal.\n        obj_type : str, optional\n            The object type being deprecated.\n        addendum : str, optional\n            Additional text appended directly to the final message.\n        removal : str, optional\n            The expected removal version. With the default (an empty\n            string), a removal version is automatically computed from\n            since. Set to other Falsy values to not schedule a removal\n            date. Cannot be used together with pending.\n\n    Examples:\n\n        .. code-block:: python\n\n            @deprecated('1.4.0')\n            def the_function_to_deprecate():\n                pass\n    \"\"\"\n    _validate_deprecation_params(pending, removal, alternative, alternative_import)\n\n    def deprecate(\n        obj: T,\n        *,\n        _obj_type: str = obj_type,\n        _name: str = name,\n        _message: str = message,\n        _alternative: str = alternative,\n        _alternative_import: str = alternative_import,\n        _pending: bool = pending,\n        _addendum: str = addendum,\n        _package: str = package,\n    ) -> T:\n        \"\"\"Implementation of the decorator returned by `deprecated`.\"\"\"\n        from langchain_core.utils.pydantic import FieldInfoV1, FieldInfoV2\n\n        def emit_warning() -> None:\n            \"\"\"Emit the warning.\"\"\"\n            warn_deprecated(\n                since,\n                message=_message,\n                name=_name,\n                alternative=_alternative,\n                alternative_import=_alternative_import,\n                pending=_pending,\n                obj_type=_obj_type,\n                addendum=_addendum,\n                removal=removal,\n                package=_package,\n            )\n\n        warned = False\n\n        def warning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:\n            \"\"\"Wrapper for the original wrapped callable that emits a warning.\n\n            Args:\n                *args: The positional arguments to the function.\n                **kwargs: The keyword arguments to the function.\n\n            Returns:\n                The return value of the function being wrapped.\n            \"\"\"\n            nonlocal warned\n            if not warned and not is_caller_internal():\n                warned = True\n                emit_warning()\n            return wrapped(*args, **kwargs)\n\n        async def awarning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:\n            \"\"\"Same as warning_emitting_wrapper, but for async functions.\"\"\"\n            nonlocal warned\n            if not warned and not is_caller_internal():\n                warned = True\n                emit_warning()\n            return await wrapped(*args, **kwargs)\n\n        _package = _package or obj.__module__.split(\".\")[0].replace(\"_\", \"-\")\n\n        if isinstance(obj, type):\n            if not _obj_type:\n                _obj_type = \"class\"\n            wrapped = obj.__init__  # type: ignore\n            _name = _name or obj.__qualname__\n            old_doc = obj.__doc__\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n                \"\"\"Finalize the deprecation of a class.\"\"\"\n                # Can't set new_doc on some extension objects.\n                with contextlib.suppress(AttributeError):\n                    obj.__doc__ = new_doc\n\n                def warn_if_direct_instance(\n                    self: Any, *args: Any, **kwargs: Any\n                ) -> Any:\n                    \"\"\"Warn that the class is in beta.\"\"\"\n                    nonlocal warned\n                    if not warned and type(self) is obj and not is_caller_internal():\n                        warned = True\n                        emit_warning()\n                    return wrapped(self, *args, **kwargs)\n\n                obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n                    warn_if_direct_instance\n                )\n                return cast(T, obj)\n\n        elif isinstance(obj, FieldInfoV1):\n            wrapped = None\n            if not _obj_type:\n                _obj_type = \"attribute\"\n            if not _name:\n                msg = f\"Field {obj} must have a name to be deprecated.\"\n                raise ValueError(msg)\n            old_doc = obj.description\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n                return cast(\n                    T,\n                    FieldInfoV1(\n                        default=obj.default,\n                        default_factory=obj.default_factory,\n                        description=new_doc,\n                        alias=obj.alias,\n                        exclude=obj.exclude,\n                    ),\n                )\n\n        elif isinstance(obj, FieldInfoV2):\n            wrapped = None\n            if not _obj_type:\n                _obj_type = \"attribute\"\n            if not _name:\n                msg = f\"Field {obj} must have a name to be deprecated.\"\n                raise ValueError(msg)\n            old_doc = obj.description\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n                return cast(\n                    T,\n                    FieldInfoV2(\n                        default=obj.default,\n                        default_factory=obj.default_factory,\n                        description=new_doc,\n                        alias=obj.alias,\n                        exclude=obj.exclude,\n                    ),\n                )\n\n        elif isinstance(obj, property):\n            if not _obj_type:\n                _obj_type = \"attribute\"\n            wrapped = None\n            _name = _name or cast(Union[type, Callable], obj.fget).__qualname__\n            old_doc = obj.__doc__\n\n            class _DeprecatedProperty(property):\n                \"\"\"A deprecated property.\"\"\"\n\n                def __init__(self, fget=None, fset=None, fdel=None, doc=None):  # type: ignore[no-untyped-def]\n                    super().__init__(fget, fset, fdel, doc)\n                    self.__orig_fget = fget\n                    self.__orig_fset = fset\n                    self.__orig_fdel = fdel\n\n                def __get__(self, instance, owner=None):  # type: ignore[no-untyped-def]\n                    if instance is not None or owner is not None:\n                        emit_warning()\n                    return self.fget(instance)\n\n                def __set__(self, instance, value):  # type: ignore[no-untyped-def]\n                    if instance is not None:\n                        emit_warning()\n                    return self.fset(instance, value)\n\n                def __delete__(self, instance):  # type: ignore[no-untyped-def]\n                    if instance is not None:\n                        emit_warning()\n                    return self.fdel(instance)\n\n                def __set_name__(self, owner, set_name):  # type: ignore[no-untyped-def]\n                    nonlocal _name\n                    if _name == \"<lambda>\":\n                        _name = set_name\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n                \"\"\"Finalize the property.\"\"\"\n                return cast(\n                    T,\n                    _DeprecatedProperty(\n                        fget=obj.fget, fset=obj.fset, fdel=obj.fdel, doc=new_doc\n                    ),\n                )\n\n        else:\n            _name = _name or cast(Union[type, Callable], obj).__qualname__\n            if not _obj_type:\n                # edge case: when a function is within another function\n                # within a test, this will call it a \"method\" not a \"function\"\n                _obj_type = \"function\" if \".\" not in _name else \"method\"\n            wrapped = obj\n            old_doc = wrapped.__doc__\n\n            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n                \"\"\"Wrap the wrapped function using the wrapper and update the docstring.\n\n                Args:\n                    wrapper: The wrapper function.\n                    new_doc: The new docstring.\n\n                Returns:\n                    The wrapped function.\n                \"\"\"\n                wrapper = functools.wraps(wrapped)(wrapper)\n                wrapper.__doc__ = new_doc\n                return cast(T, wrapper)\n\n        old_doc = inspect.cleandoc(old_doc or \"\").strip(\"\\n\")\n\n        # old_doc can be None\n        if not old_doc:\n            old_doc = \"\"\n\n        # Modify the docstring to include a deprecation notice.\n        if (\n            _alternative\n            and _alternative.split(\".\")[-1].lower() == _alternative.split(\".\")[-1]\n        ):\n            _alternative = f\":meth:`~{_alternative}`\"\n        elif _alternative:\n            _alternative = f\":class:`~{_alternative}`\"\n\n        if (\n            _alternative_import\n            and _alternative_import.split(\".\")[-1].lower()\n            == _alternative_import.split(\".\")[-1]\n        ):\n            _alternative_import = f\":meth:`~{_alternative_import}`\"\n        elif _alternative_import:\n            _alternative_import = f\":class:`~{_alternative_import}`\"\n\n        components = [\n            _message,\n            f\"Use {_alternative} instead.\" if _alternative else \"\",\n            f\"Use ``{_alternative_import}`` instead.\" if _alternative_import else \"\",\n            _addendum,\n        ]\n        details = \" \".join([component.strip() for component in components if component])\n        package = _package or (\n            _name.split(\".\")[0].replace(\"_\", \"-\") if \".\" in _name else None\n        )\n        if removal:\n            if removal.startswith(\"1.\") and package and package.startswith(\"langchain\"):\n                removal_str = f\"It will not be removed until {package}=={removal}.\"\n            else:\n                removal_str = f\"It will be removed in {package}=={removal}.\"\n        else:\n            removal_str = \"\"\n        new_doc = f\"\"\"\\\n.. deprecated:: {since} {details} {removal_str}\n\n{old_doc}\\\n\"\"\"\n\n        if inspect.iscoroutinefunction(obj):\n            finalized = finalize(awarning_emitting_wrapper, new_doc)\n        else:\n            finalized = finalize(warning_emitting_wrapper, new_doc)\n        return cast(T, finalized)\n\n    return deprecate\n\n\n@contextlib.contextmanager\ndef suppress_langchain_deprecation_warning() -> Generator[None, None, None]:\n    \"\"\"Context manager to suppress LangChainDeprecationWarning.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", LangChainDeprecationWarning)\n        warnings.simplefilter(\"ignore\", LangChainPendingDeprecationWarning)\n        yield\n\n\ndef warn_deprecated(\n    since: str,\n    *,\n    message: str = \"\",\n    name: str = \"\",\n    alternative: str = \"\",\n    alternative_import: str = \"\",\n    pending: bool = False,\n    obj_type: str = \"\",\n    addendum: str = \"\",\n    removal: str = \"\",\n    package: str = \"\",\n) -> None:\n    \"\"\"Display a standardized deprecation.\n\n    Arguments:\n        since : str\n            The release at which this API became deprecated.\n        message : str, optional\n            Override the default deprecation message. The %(since)s,\n            %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,\n            and %(removal)s format specifiers will be replaced by the\n            values of the respective arguments passed to this function.\n        name : str, optional\n            The name of the deprecated object.\n        alternative : str, optional\n            An alternative API that the user may use in place of the\n            deprecated API. The deprecation warning will tell the user\n            about this alternative if provided.\n        pending : bool, optional\n            If True, uses a PendingDeprecationWarning instead of a\n            DeprecationWarning. Cannot be used together with removal.\n        obj_type : str, optional\n            The object type being deprecated.\n        addendum : str, optional\n            Additional text appended directly to the final message.\n        removal : str, optional\n            The expected removal version. With the default (an empty\n            string), a removal version is automatically computed from\n            since. Set to other Falsy values to not schedule a removal\n            date. Cannot be used together with pending.\n    \"\"\"\n    if not pending:\n        if not removal:\n            removal = f\"in {removal}\" if removal else \"within ?? minor releases\"\n            msg = (\n                f\"Need to determine which default deprecation schedule to use. \"\n                f\"{removal}\"\n            )\n            raise NotImplementedError(msg)\n        else:\n            removal = f\"in {removal}\"\n\n    if not message:\n        message = \"\"\n        _package = (\n            package or name.split(\".\")[0].replace(\"_\", \"-\")\n            if \".\" in name\n            else \"LangChain\"\n        )\n\n        if obj_type:\n            message += f\"The {obj_type} `{name}`\"\n        else:\n            message += f\"`{name}`\"\n\n        if pending:\n            message += \" will be deprecated in a future version\"\n        else:\n            message += f\" was deprecated in {_package} {since}\"\n\n            if removal:\n                message += f\" and will be removed {removal}\"\n\n        if alternative_import:\n            alt_package = alternative_import.split(\".\")[0].replace(\"_\", \"-\")\n            if alt_package == _package:\n                message += f\". Use {alternative_import} instead.\"\n            else:\n                alt_module, alt_name = alternative_import.rsplit(\".\", 1)\n                message += (\n                    f\". An updated version of the {obj_type} exists in the \"\n                    f\"{alt_package} package and should be used instead. To use it run \"\n                    f\"`pip install -U {alt_package}` and import as \"\n                    f\"`from {alt_module} import {alt_name}`.\"\n                )\n        elif alternative:\n            message += f\". Use {alternative} instead.\"\n\n        if addendum:\n            message += f\" {addendum}\"\n\n    warning_cls = (\n        LangChainPendingDeprecationWarning if pending else LangChainDeprecationWarning\n    )\n    warning = warning_cls(message)\n    warnings.warn(warning, category=LangChainDeprecationWarning, stacklevel=4)\n\n\ndef surface_langchain_deprecation_warnings() -> None:\n    \"\"\"Unmute LangChain deprecation warnings.\"\"\"\n    warnings.filterwarnings(\n        \"default\",\n        category=LangChainPendingDeprecationWarning,\n    )\n\n    warnings.filterwarnings(\n        \"default\",\n        category=LangChainDeprecationWarning,\n    )\n\n\n_P = ParamSpec(\"_P\")\n_R = TypeVar(\"_R\")\n\n\ndef rename_parameter(\n    *,\n    since: str,\n    removal: str,\n    old: str,\n    new: str,\n) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:\n    \"\"\"Decorator indicating that parameter *old* of *func* is renamed to *new*.\n\n    The actual implementation of *func* should use *new*, not *old*.  If *old*\n    is passed to *func*, a DeprecationWarning is emitted, and its value is\n    used, even if *new* is also passed by keyword.\n\n    Example:\n\n        .. code-block:: python\n\n            @_api.rename_parameter(\"3.1\", \"bad_name\", \"good_name\")\n            def func(good_name): ...\n    \"\"\"\n\n    def decorator(f: Callable[_P, _R]) -> Callable[_P, _R]:\n        @functools.wraps(f)\n        def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n            if new in kwargs and old in kwargs:\n                msg = f\"{f.__name__}() got multiple values for argument {new!r}\"\n                raise TypeError(msg)\n            if old in kwargs:\n                warn_deprecated(\n                    since,\n                    removal=removal,\n                    message=f\"The parameter `{old}` of `{f.__name__}` was \"\n                    f\"deprecated in {since} and will be removed \"\n                    f\"in {removal} Use `{new}` instead.\",\n                )\n                kwargs[new] = kwargs.pop(old)\n            return f(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n",
        "patch": "@@ -270,28 +270,40 @@ def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:\n             class _DeprecatedProperty(property):\n                 \"\"\"A deprecated property.\"\"\"\n \n-                def __init__(self, fget=None, fset=None, fdel=None, doc=None):  # type: ignore[no-untyped-def]\n+                def __init__(\n+                    self,\n+                    fget: Union[Callable[[Any], Any], None] = None,\n+                    fset: Union[Callable[[Any, Any], None], None] = None,\n+                    fdel: Union[Callable[[Any], None], None] = None,\n+                    doc: Union[str, None] = None,\n+                ) -> None:\n                     super().__init__(fget, fset, fdel, doc)\n                     self.__orig_fget = fget\n                     self.__orig_fset = fset\n                     self.__orig_fdel = fdel\n \n-                def __get__(self, instance, owner=None):  # type: ignore[no-untyped-def]\n+                def __get__(\n+                    self, instance: Any, owner: Union[type, None] = None\n+                ) -> Any:\n                     if instance is not None or owner is not None:\n                         emit_warning()\n+                    if self.fget is None:\n+                        return None\n                     return self.fget(instance)\n \n-                def __set__(self, instance, value):  # type: ignore[no-untyped-def]\n+                def __set__(self, instance: Any, value: Any) -> None:\n                     if instance is not None:\n                         emit_warning()\n-                    return self.fset(instance, value)\n+                    if self.fset is not None:\n+                        self.fset(instance, value)\n \n-                def __delete__(self, instance):  # type: ignore[no-untyped-def]\n+                def __delete__(self, instance: Any) -> None:\n                     if instance is not None:\n                         emit_warning()\n-                    return self.fdel(instance)\n+                    if self.fdel is not None:\n+                        self.fdel(instance)\n \n-                def __set_name__(self, owner, set_name):  # type: ignore[no-untyped-def]\n+                def __set_name__(self, owner: Union[type, None], set_name: str) -> None:\n                     nonlocal _name\n                     if _name == \"<lambda>\":\n                         _name = set_name"
      },
      {
        "filename": "libs/core/langchain_core/runnables/base.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport collections\nimport contextlib\nimport functools\nimport inspect\nimport threading\nfrom abc import ABC, abstractmethod\nfrom collections.abc import (\n    AsyncGenerator,\n    AsyncIterator,\n    Awaitable,\n    Coroutine,\n    Iterator,\n    Mapping,\n    Sequence,\n)\nfrom concurrent.futures import FIRST_COMPLETED, wait\nfrom contextvars import copy_context\nfrom functools import wraps\nfrom itertools import groupby, tee\nfrom operator import itemgetter\nfrom types import GenericAlias\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    Optional,\n    Protocol,\n    TypeVar,\n    Union,\n    cast,\n    get_type_hints,\n    overload,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, RootModel\nfrom typing_extensions import Literal, get_args, override\n\nfrom langchain_core._api import beta_decorator\nfrom langchain_core.load.serializable import (\n    Serializable,\n    SerializedConstructor,\n    SerializedNotImplemented,\n)\nfrom langchain_core.runnables.config import (\n    RunnableConfig,\n    _set_config_context,\n    acall_func_with_variable_args,\n    call_func_with_variable_args,\n    ensure_config,\n    get_async_callback_manager_for_config,\n    get_callback_manager_for_config,\n    get_config_list,\n    get_executor_for_config,\n    merge_configs,\n    patch_config,\n    run_in_executor,\n)\nfrom langchain_core.runnables.graph import Graph\nfrom langchain_core.runnables.schema import StreamEvent\nfrom langchain_core.runnables.utils import (\n    AddableDict,\n    AnyConfigurableField,\n    ConfigurableField,\n    ConfigurableFieldSpec,\n    Input,\n    Output,\n    accepts_config,\n    accepts_run_manager,\n    asyncio_accepts_context,\n    gated_coro,\n    gather_with_concurrency,\n    get_function_first_arg_dict_keys,\n    get_function_nonlocals,\n    get_lambda_source,\n    get_unique_config_specs,\n    indent_lines_after_first,\n    is_async_callable,\n    is_async_generator,\n)\nfrom langchain_core.utils.aiter import aclosing, atee, py_anext\nfrom langchain_core.utils.iter import safetee\nfrom langchain_core.utils.pydantic import create_model_v2\n\nif TYPE_CHECKING:\n    from langchain_core.callbacks.manager import (\n        AsyncCallbackManagerForChainRun,\n        CallbackManagerForChainRun,\n    )\n    from langchain_core.prompts.base import BasePromptTemplate\n    from langchain_core.runnables.fallbacks import (\n        RunnableWithFallbacks as RunnableWithFallbacksT,\n    )\n    from langchain_core.tools import BaseTool\n    from langchain_core.tracers.log_stream import (\n        RunLog,\n        RunLogPatch,\n    )\n    from langchain_core.tracers.root_listeners import AsyncListener\n    from langchain_core.tracers.schemas import Run\n\n\nOther = TypeVar(\"Other\")\n\n\nclass Runnable(Generic[Input, Output], ABC):\n    \"\"\"A unit of work that can be invoked, batched, streamed, transformed and composed.\n\n    Key Methods\n    ===========\n\n    - **invoke/ainvoke**: Transforms a single input into an output.\n    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.\n    - **stream/astream**: Streams output from a single input as it's produced.\n    - **astream_log**: Streams output and selected intermediate results from an input.\n\n    Built-in optimizations:\n\n    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.\n      Override to optimize batching.\n\n    - **Async**: Methods with \"a\" suffix are asynchronous. By default, they execute\n      the sync counterpart using asyncio's thread pool.\n      Override for native async.\n\n    All methods accept an optional config argument, which can be used to configure\n    execution, add tags and metadata for tracing and debugging etc.\n\n    Runnables expose schematic information about their input, output and config via\n    the input_schema property, the output_schema property and config_schema method.\n\n    LCEL and Composition\n    ====================\n\n    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables\n    into chains. Any chain constructed this way will automatically have sync, async,\n    batch, and streaming support.\n\n    The main composition primitives are RunnableSequence and RunnableParallel.\n\n    **RunnableSequence** invokes a series of runnables sequentially, with\n    one Runnable's output serving as the next's input. Construct using\n    the `|` operator or by passing a list of runnables to RunnableSequence.\n\n    **RunnableParallel** invokes runnables concurrently, providing the same input\n    to each. Construct it using a dict literal within a sequence or by passing a\n    dict to RunnableParallel.\n\n\n    For example,\n\n    .. code-block:: python\n\n        from langchain_core.runnables import RunnableLambda\n\n        # A RunnableSequence constructed using the `|` operator\n        sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n        sequence.invoke(1) # 4\n        sequence.batch([1, 2, 3]) # [4, 6, 8]\n\n\n        # A sequence that contains a RunnableParallel constructed using a dict literal\n        sequence = RunnableLambda(lambda x: x + 1) | {\n            'mul_2': RunnableLambda(lambda x: x * 2),\n            'mul_5': RunnableLambda(lambda x: x * 5)\n        }\n        sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}\n\n    Standard Methods\n    ================\n\n    All Runnables expose additional methods that can be used to modify their behavior\n    (e.g., add a retry policy, add lifecycle listeners, make them configurable, etc.).\n\n    These methods will work on any Runnable, including Runnable chains constructed\n    by composing other Runnables. See the individual methods for details.\n\n    For example,\n\n    .. code-block:: python\n\n        from langchain_core.runnables import RunnableLambda\n\n        import random\n\n        def add_one(x: int) -> int:\n            return x + 1\n\n\n        def buggy_double(y: int) -> int:\n            \\\"\\\"\\\"Buggy code that will fail 70% of the time\\\"\\\"\\\"\n            if random.random() > 0.3:\n                print('This code failed, and will probably be retried!')  # noqa: T201\n                raise ValueError('Triggered buggy code')\n            return y * 2\n\n        sequence = (\n            RunnableLambda(add_one) |\n            RunnableLambda(buggy_double).with_retry( # Retry on failure\n                stop_after_attempt=10,\n                wait_exponential_jitter=False\n            )\n        )\n\n        print(sequence.input_schema.model_json_schema()) # Show inferred input schema\n        print(sequence.output_schema.model_json_schema()) # Show inferred output schema\n        print(sequence.invoke(2)) # invoke the sequence (note the retry above!!)\n\n    Debugging and tracing\n    =====================\n\n    As the chains get longer, it can be useful to be able to see intermediate results\n    to debug and trace the chain.\n\n    You can set the global debug flag to True to enable debug output for all chains:\n\n        .. code-block:: python\n\n            from langchain_core.globals import set_debug\n            set_debug(True)\n\n    Alternatively, you can pass existing or custom callbacks to any given chain:\n\n        .. code-block:: python\n\n            from langchain_core.tracers import ConsoleCallbackHandler\n\n            chain.invoke(\n                ...,\n                config={'callbacks': [ConsoleCallbackHandler()]}\n            )\n\n    For a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/\n    \"\"\"  # noqa: E501\n\n    name: Optional[str]\n    \"\"\"The name of the Runnable. Used for debugging and tracing.\"\"\"\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        \"\"\"Get the name of the Runnable.\"\"\"\n        if name:\n            name_ = name\n        elif hasattr(self, \"name\") and self.name:\n            name_ = self.name\n        else:\n            # Here we handle a case where the runnable subclass is also a pydantic\n            # model.\n            cls = self.__class__\n            # Then it's a pydantic sub-class, and we have to check\n            # whether it's a generic, and if so recover the original name.\n            if (\n                hasattr(\n                    cls,\n                    \"__pydantic_generic_metadata__\",\n                )\n                and \"origin\" in cls.__pydantic_generic_metadata__\n                and cls.__pydantic_generic_metadata__[\"origin\"] is not None\n            ):\n                name_ = cls.__pydantic_generic_metadata__[\"origin\"].__name__\n            else:\n                name_ = cls.__name__\n\n        if suffix:\n            if name_[0].isupper():\n                return name_ + suffix.title()\n            else:\n                return name_ + \"_\" + suffix.lower()\n        else:\n            return name_\n\n    @property\n    def InputType(self) -> type[Input]:  # noqa: N802\n        \"\"\"The type of input this Runnable accepts specified as a type annotation.\"\"\"\n        # First loop through all parent classes and if any of them is\n        # a pydantic model, we will pick up the generic parameterization\n        # from that model via the __pydantic_generic_metadata__ attribute.\n        for base in self.__class__.mro():\n            if hasattr(base, \"__pydantic_generic_metadata__\"):\n                metadata = base.__pydantic_generic_metadata__\n                if \"args\" in metadata and len(metadata[\"args\"]) == 2:\n                    return metadata[\"args\"][0]\n\n        # If we didn't find a pydantic model in the parent classes,\n        # then loop through __orig_bases__. This corresponds to\n        # Runnables that are not pydantic models.\n        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]\n            type_args = get_args(cls)\n            if type_args and len(type_args) == 2:\n                return type_args[0]\n\n        msg = (\n            f\"Runnable {self.get_name()} doesn't have an inferable InputType. \"\n            \"Override the InputType property to specify the input type.\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def OutputType(self) -> type[Output]:  # noqa: N802\n        \"\"\"The type of output this Runnable produces specified as a type annotation.\"\"\"\n        # First loop through bases -- this will help generic\n        # any pydantic models.\n        for base in self.__class__.mro():\n            if hasattr(base, \"__pydantic_generic_metadata__\"):\n                metadata = base.__pydantic_generic_metadata__\n                if \"args\" in metadata and len(metadata[\"args\"]) == 2:\n                    return metadata[\"args\"][1]\n\n        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]\n            type_args = get_args(cls)\n            if type_args and len(type_args) == 2:\n                return type_args[1]\n\n        msg = (\n            f\"Runnable {self.get_name()} doesn't have an inferable OutputType. \"\n            \"Override the OutputType property to specify the output type.\"\n        )\n        raise TypeError(msg)\n\n    @property\n    def input_schema(self) -> type[BaseModel]:\n        \"\"\"The type of input this Runnable accepts specified as a pydantic model.\"\"\"\n        return self.get_input_schema()\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get a pydantic model that can be used to validate input to the Runnable.\n\n        Runnables that leverage the configurable_fields and configurable_alternatives\n        methods will have a dynamic input schema that depends on which\n        configuration the Runnable is invoked with.\n\n        This method allows to get an input schema for a specific configuration.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A pydantic model that can be used to validate input.\n        \"\"\"\n        root_type = self.InputType\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=root_type,\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    def get_input_jsonschema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the input to the Runnable.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A JSON schema that represents the input to the Runnable.\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                runnable = RunnableLambda(add_one)\n\n                print(runnable.get_input_jsonschema())\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.get_input_schema(config).model_json_schema()\n\n    @property\n    def output_schema(self) -> type[BaseModel]:\n        \"\"\"The type of output this Runnable produces specified as a pydantic model.\"\"\"\n        return self.get_output_schema()\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get a pydantic model that can be used to validate output to the Runnable.\n\n        Runnables that leverage the configurable_fields and configurable_alternatives\n        methods will have a dynamic output schema that depends on which\n        configuration the Runnable is invoked with.\n\n        This method allows to get an output schema for a specific configuration.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A pydantic model that can be used to validate output.\n        \"\"\"\n        root_type = self.OutputType\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    def get_output_jsonschema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the output of the Runnable.\n\n        Args:\n            config: A config to use when generating the schema.\n\n        Returns:\n            A JSON schema that represents the output of the Runnable.\n\n        Example:\n\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                runnable = RunnableLambda(add_one)\n\n                print(runnable.get_output_jsonschema())\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.get_output_schema(config).model_json_schema()\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"List configurable fields for this Runnable.\"\"\"\n        return []\n\n    def config_schema(\n        self, *, include: Optional[Sequence[str]] = None\n    ) -> type[BaseModel]:\n        \"\"\"The type of config this Runnable accepts specified as a pydantic model.\n\n        To mark a field as configurable, see the `configurable_fields`\n        and `configurable_alternatives` methods.\n\n        Args:\n            include: A list of fields to include in the config schema.\n\n        Returns:\n            A pydantic model that can be used to validate config.\n        \"\"\"\n        include = include or []\n        config_specs = self.config_specs\n        configurable = (\n            create_model_v2(  # type: ignore[call-overload]\n                \"Configurable\",\n                field_definitions={\n                    spec.id: (\n                        spec.annotation,\n                        Field(\n                            spec.default, title=spec.name, description=spec.description\n                        ),\n                    )\n                    for spec in config_specs\n                },\n            )\n            if config_specs\n            else None\n        )\n\n        # Many need to create a typed dict instead to implement NotRequired!\n        all_fields = {\n            **({\"configurable\": (configurable, None)} if configurable else {}),\n            **{\n                field_name: (field_type, None)\n                for field_name, field_type in get_type_hints(RunnableConfig).items()\n                if field_name in [i for i in include if i != \"configurable\"]\n            },\n        }\n        model = create_model_v2(  # type: ignore[call-overload]\n            self.get_name(\"Config\"), field_definitions=all_fields\n        )\n        return model\n\n    def get_config_jsonschema(\n        self, *, include: Optional[Sequence[str]] = None\n    ) -> dict[str, Any]:\n        \"\"\"Get a JSON schema that represents the config of the Runnable.\n\n        Args:\n            include: A list of fields to include in the config schema.\n\n        Returns:\n            A JSON schema that represents the config of the Runnable.\n\n        .. versionadded:: 0.3.0\n        \"\"\"\n        return self.config_schema(include=include).model_json_schema()\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Return a graph representation of this Runnable.\"\"\"\n        graph = Graph()\n        try:\n            input_node = graph.add_node(self.get_input_schema(config))\n        except TypeError:\n            input_node = graph.add_node(create_model_v2(self.get_name(\"Input\")))\n        runnable_node = graph.add_node(\n            self, metadata=config.get(\"metadata\") if config else None\n        )\n        try:\n            output_node = graph.add_node(self.get_output_schema(config))\n        except TypeError:\n            output_node = graph.add_node(create_model_v2(self.get_name(\"Output\")))\n        graph.add_edge(input_node, runnable_node)\n        graph.add_edge(runnable_node, output_node)\n        return graph\n\n    def get_prompts(\n        self, config: Optional[RunnableConfig] = None\n    ) -> list[BasePromptTemplate]:\n        \"\"\"Return a list of prompts used by this Runnable.\"\"\"\n        from langchain_core.prompts.base import BasePromptTemplate\n\n        prompts = []\n        for _, node in self.get_graph(config=config).nodes.items():\n            if isinstance(node.data, BasePromptTemplate):\n                prompts.append(node.data)\n        return prompts\n\n    def __or__(\n        self,\n        other: Union[\n            Runnable[Any, Other],\n            Callable[[Any], Other],\n            Callable[[Iterator[Any]], Iterator[Other]],\n            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],\n        ],\n    ) -> RunnableSerializable[Input, Other]:\n        \"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\n        return RunnableSequence(self, coerce_to_runnable(other))\n\n    def __ror__(\n        self,\n        other: Union[\n            Runnable[Other, Any],\n            Callable[[Other], Any],\n            Callable[[Iterator[Other]], Iterator[Any]],\n            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],\n        ],\n    ) -> RunnableSerializable[Other, Output]:\n        \"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\n        return RunnableSequence(coerce_to_runnable(other), self)\n\n    def pipe(\n        self,\n        *others: Union[Runnable[Any, Other], Callable[[Any], Other]],\n        name: Optional[str] = None,\n    ) -> RunnableSerializable[Input, Other]:\n        \"\"\"Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n\n        Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n\n        Example:\n            .. code-block:: python\n\n                from langchain_core.runnables import RunnableLambda\n\n                def add_one(x: int) -> int:\n                    return x + 1\n\n                def mul_two(x: int) -> int:\n                    return x * 2\n\n                runnable_1 = RunnableLambda(add_one)\n                runnable_2 = RunnableLambda(mul_two)\n                sequence = runnable_1.pipe(runnable_2)\n                # Or equivalently:\n                # sequence = runnable_1 | runnable_2\n                # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n                sequence.invoke(1)\n                await sequence.ainvoke(1)\n                # -> 4\n\n                sequence.batch([1, 2, 3])\n                await sequence.abatch([1, 2, 3])\n                # -> [4, 6, 8]\n        \"\"\"\n        return RunnableSequence(self, *others, name=name)\n\n    def pick(self, keys: Union[str, list[str]]) -> RunnableSerializable[Any, Any]:\n        \"\"\"Pick keys from the output dict of this Runnable.\n\n        Pick single key:\n            .. code-block:: python\n\n                import json\n\n                from langchain_core.runnables import RunnableLambda, RunnableMap\n\n                as_str = RunnableLambda(str)\n                as_json = RunnableLambda(json.loads)\n                chain = RunnableMap(str=as_str, json=as_json)\n\n                chain.invoke(\"[1, 2, 3]\")\n                # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n\n                json_only_chain = chain.pick(\"json\")\n                json_only_chain.invoke(\"[1, 2, 3]\")\n                # -> [1, 2, 3]\n\n        Pick list of keys:\n            .. code-block:: python\n\n                from typing import Any\n\n                import json\n\n                from langchain_core.runnables import RunnableLambda, RunnableMap\n\n                as_str = RunnableLambda(str)\n                as_json = RunnableLambda(json.loads)\n                def as_bytes(x: Any) -> bytes:\n                    return bytes(x, \"utf-8\")\n\n                chain = RunnableMap(\n                    str=as_str,\n                    json=as_json,\n                    bytes=RunnableLambda(as_bytes)\n                )\n\n                chain.invoke(\"[1, 2, 3]\")\n                # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n\n                json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n                json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n                # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n\n        \"\"\"\n        from langchain_core.runnables.passthrough import RunnablePick\n\n        return self | RunnablePick(keys)\n\n    def assign(\n        self,\n        **kwargs: Union[\n            Runnable[dict[str, Any], Any],\n            Callable[[dict[str, Any]], Any],\n            Mapping[\n                str,\n                Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]],\n            ],\n        ],\n    ) -> RunnableSerializable[Any, Any]:\n        \"\"\"Assigns new fields to the dict output of this Runnable.\n        Returns a new Runnable.\n\n        .. code-block:: python\n\n            from langchain_community.llms.fake import FakeStreamingListLLM\n            from langchain_core.output_parsers import StrOutputParser\n            from langchain_core.prompts import SystemMessagePromptTemplate\n            from langchain_core.runnables import Runnable\n            from operator import itemgetter\n\n            prompt = (\n                SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n                + \"{question}\"\n            )\n            llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n            chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n\n            chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n\n            print(chain_with_assign.input_schema.model_json_schema())\n            # {'title': 'PromptInput', 'type': 'object', 'properties':\n            {'question': {'title': 'Question', 'type': 'string'}}}\n            print(chain_with_assign.output_schema.model_json_schema())\n            # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n            {'str': {'title': 'Str',\n            'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n\n        \"\"\"\n        from langchain_core.runnables.passthrough import RunnableAssign\n\n        return self | RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))\n\n    \"\"\" --- Public API --- \"\"\"\n\n    @abstractmethod\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        \"\"\"Transform a single input into an output. Override to implement.\n\n        Args:\n            input: The input to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details.\n\n        Returns:\n            The output of the Runnable.\n        \"\"\"\n\n    async def ainvoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        \"\"\"Default implementation of ainvoke, calls invoke from a thread.\n\n        The default implementation allows usage of async code even if\n        the Runnable did not implement a native async version of invoke.\n\n        Subclasses should override this method if they can run asynchronously.\n        \"\"\"\n        return await run_in_executor(config, self.invoke, input, config, **kwargs)\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Default implementation runs invoke in parallel using a thread pool executor.\n\n        The default implementation of batch works well for IO bound runnables.\n\n        Subclasses should override this method if they can batch more efficiently;\n        e.g., if the underlying Runnable uses an API which supports a batch mode.\n        \"\"\"\n        if not inputs:\n            return []\n\n        configs = get_config_list(config, len(inputs))\n\n        def invoke(input: Input, config: RunnableConfig) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return self.invoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return self.invoke(input, config, **kwargs)\n\n        # If there's only one input, don't bother with the executor\n        if len(inputs) == 1:\n            return cast(list[Output], [invoke(inputs[0], configs[0])])\n\n        with get_executor_for_config(configs[0]) as executor:\n            return cast(list[Output], list(executor.map(invoke, inputs, configs)))\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Output]]: ...\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...\n\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]:\n        \"\"\"Run invoke in parallel on a list of inputs,\n        yielding results as they complete.\n        \"\"\"\n        if not inputs:\n            return\n\n        configs = get_config_list(config, len(inputs))\n\n        def invoke(\n            i: int, input: Input, config: RunnableConfig\n        ) -> tuple[int, Union[Output, Exception]]:\n            if return_exceptions:\n                try:\n                    out: Union[Output, Exception] = self.invoke(input, config, **kwargs)\n                except Exception as e:\n                    out = e\n            else:\n                out = self.invoke(input, config, **kwargs)\n\n            return (i, out)\n\n        if len(inputs) == 1:\n            yield invoke(0, inputs[0], configs[0])\n            return\n\n        with get_executor_for_config(configs[0]) as executor:\n            futures = {\n                executor.submit(invoke, i, input, config)\n                for i, (input, config) in enumerate(zip(inputs, configs))\n            }\n\n            try:\n                while futures:\n                    done, futures = wait(futures, return_when=FIRST_COMPLETED)\n                    while done:\n                        yield done.pop().result()\n            finally:\n                for future in futures:\n                    future.cancel()\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Default implementation runs ainvoke in parallel using asyncio.gather.\n\n        The default implementation of batch works well for IO bound runnables.\n\n        Subclasses should override this method if they can batch more efficiently;\n        e.g., if the underlying Runnable uses an API which supports a batch mode.\n\n        Args:\n            inputs: A list of inputs to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details. Defaults to None.\n            return_exceptions: Whether to return exceptions instead of raising them.\n                Defaults to False.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Returns:\n            A list of outputs from the Runnable.\n        \"\"\"\n        if not inputs:\n            return []\n\n        configs = get_config_list(config, len(inputs))\n\n        async def ainvoke(\n            input: Input, config: RunnableConfig\n        ) -> Union[Output, Exception]:\n            if return_exceptions:\n                try:\n                    return await self.ainvoke(input, config, **kwargs)\n                except Exception as e:\n                    return e\n            else:\n                return await self.ainvoke(input, config, **kwargs)\n\n        coros = map(ainvoke, inputs, configs)\n        return await gather_with_concurrency(configs[0].get(\"max_concurrency\"), *coros)\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Output]]: ...\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...\n\n    async def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:\n        \"\"\"Run ainvoke in parallel on a list of inputs,\n        yielding results as they complete.\n\n        Args:\n            inputs: A list of inputs to the Runnable.\n            config: A config to use when invoking the Runnable.\n               The config supports standard keys like 'tags', 'metadata' for tracing\n               purposes, 'max_concurrency' for controlling how much work to do\n               in parallel, and other keys. Please refer to the RunnableConfig\n               for more details. Defaults to None. Defaults to None.\n            return_exceptions: Whether to return exceptions instead of raising them.\n                Defaults to False.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            A tuple of the index of the input and the output from the Runnable.\n        \"\"\"\n        if not inputs:\n            return\n\n        configs = get_config_list(config, len(inputs))\n        # Get max_concurrency from first config, defaulting to None (unlimited)\n        max_concurrency = configs[0].get(\"max_concurrency\") if configs else None\n        semaphore = asyncio.Semaphore(max_concurrency) if max_concurrency else None\n\n        async def ainvoke_task(\n            i: int, input: Input, config: RunnableConfig\n        ) -> tuple[int, Union[Output, Exception]]:\n            if return_exceptions:\n                try:\n                    out: Union[Output, Exception] = await self.ainvoke(\n                        input, config, **kwargs\n                    )\n                except Exception as e:\n                    out = e\n            else:\n                out = await self.ainvoke(input, config, **kwargs)\n            return (i, out)\n\n        coros = [\n            gated_coro(semaphore, ainvoke_task(i, input, config))\n            if semaphore\n            else ainvoke_task(i, input, config)\n            for i, (input, config) in enumerate(zip(inputs, configs))\n        ]\n\n        for coro in asyncio.as_completed(coros):\n            yield await coro\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Default implementation of stream, which calls invoke.\n        Subclasses should override this method if they support streaming output.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        yield self.invoke(input, config, **kwargs)\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Default implementation of astream, which calls ainvoke.\n        Subclasses should override this method if they support streaming output.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        yield await self.ainvoke(input, config, **kwargs)\n\n    @overload\n    def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: Literal[True] = True,\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[RunLogPatch]: ...\n\n    @overload\n    def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: Literal[False],\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[RunLog]: ...\n\n    async def astream_log(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        diff: bool = True,\n        with_streamed_output_list: bool = True,\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]:\n        \"\"\"Stream all output from a Runnable, as reported to the callback system.\n        This includes all inner runs of LLMs, Retrievers, Tools, etc.\n\n        Output is streamed as Log objects, which include a list of\n        Jsonpatch ops that describe how the state of the run has changed in each\n        step, and the final state of the run.\n\n        The Jsonpatch ops can be applied in order to construct state.\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable.\n            diff: Whether to yield diffs between each step or the current state.\n            with_streamed_output_list: Whether to yield the streamed_output list.\n            include_names: Only include logs with these names.\n            include_types: Only include logs with these types.\n            include_tags: Only include logs with these tags.\n            exclude_names: Exclude logs with these names.\n            exclude_types: Exclude logs with these types.\n            exclude_tags: Exclude logs with these tags.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            A RunLogPatch or RunLog object.\n        \"\"\"\n        from langchain_core.tracers.log_stream import (\n            LogStreamCallbackHandler,\n            _astream_log_implementation,\n        )\n\n        stream = LogStreamCallbackHandler(\n            auto_close=False,\n            include_names=include_names,\n            include_types=include_types,\n            include_tags=include_tags,\n            exclude_names=exclude_names,\n            exclude_types=exclude_types,\n            exclude_tags=exclude_tags,\n            _schema_format=\"original\",\n        )\n\n        # Mypy isn't resolving the overloads here\n        # Likely an issue b/c `self` is being passed through\n        # and it's can't map it to Runnable[Input,Output]?\n        async for item in _astream_log_implementation(  # type: ignore\n            self,\n            input,\n            config,\n            diff=diff,\n            stream=stream,\n            with_streamed_output_list=with_streamed_output_list,\n            **kwargs,\n        ):\n            yield item\n\n    async def astream_events(\n        self,\n        input: Any,\n        config: Optional[RunnableConfig] = None,\n        *,\n        version: Literal[\"v1\", \"v2\"],\n        include_names: Optional[Sequence[str]] = None,\n        include_types: Optional[Sequence[str]] = None,\n        include_tags: Optional[Sequence[str]] = None,\n        exclude_names: Optional[Sequence[str]] = None,\n        exclude_types: Optional[Sequence[str]] = None,\n        exclude_tags: Optional[Sequence[str]] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[StreamEvent]:\n        \"\"\"Generate a stream of events.\n\n        Use to create an iterator over StreamEvents that provide real-time information\n        about the progress of the Runnable, including StreamEvents from intermediate\n        results.\n\n        A StreamEvent is a dictionary with the following schema:\n\n        - ``event``: **str** - Event names are of the\n            format: on_[runnable_type]_(start|stream|end).\n        - ``name``: **str** - The name of the Runnable that generated the event.\n        - ``run_id``: **str** - randomly generated ID associated with the given execution of\n            the Runnable that emitted the event.\n            A child Runnable that gets invoked as part of the execution of a\n            parent Runnable is assigned its own unique ID.\n        - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n            generated the event. The root Runnable will have an empty list.\n            The order of the parent IDs is from the root to the immediate parent.\n            Only available for v2 version of the API. The v1 version of the API\n            will return an empty list.\n        - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n            the event.\n        - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n            that generated the event.\n        - ``data``: **Dict[str, Any]**\n\n\n        Below is a table that illustrates some events that might be emitted by various\n        chains. Metadata fields have been omitted from the table for brevity.\n        Chain definitions have been included after the table.\n\n        **ATTENTION** This reference table is for the V2 version of the schema.\n\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | event                | name             | chunk                           | input                                         | output                                          |\n        +======================+==================+=================================+===============================================+=================================================+\n        | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n        | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n\n        In addition to the standard events, users can also dispatch custom events (see example below).\n\n        Custom events will be only be surfaced with in the `v2` version of the API!\n\n        A custom event has following format:\n\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n        | Attribute | Type | Description                                                                                               |\n        +===========+======+===========================================================================================================+\n        | name      | str  | A user defined name for the event.                                                                        |\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n        | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n        +-----------+------+-----------------------------------------------------------------------------------------------------------+\n\n        Here are declarations associated with the standard events shown above:\n\n        `format_docs`:\n\n        .. code-block:: python\n\n            def format_docs(docs: List[Document]) -> str:\n                '''Format the docs.'''\n                return \", \".join([doc.page_content for doc in docs])\n\n            format_docs = RunnableLambda(format_docs)\n\n        `some_tool`:\n\n        .. code-block:: python\n\n            @tool\n            def some_tool(x: int, y: str) -> dict:\n                '''Some_tool.'''\n                return {\"x\": x, \"y\": y}\n\n        `prompt`:\n\n        .. code-block:: python\n\n            template = ChatPromptTemplate.from_messages(\n                [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n            ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            async def reverse(s: str) -> str:\n                return s[::-1]\n\n            chain = RunnableLambda(func=reverse)\n\n            events = [\n                event async for event in chain.astream_events(\"hello\", version=\"v2\")\n            ]\n\n            # will produce the following events (run_id, and parent_ids\n            # has been omitted for brevity):\n            [\n                {\n                    \"data\": {\"input\": \"hello\"},\n                    \"event\": \"on_chain_start\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"chunk\": \"olleh\"},\n                    \"event\": \"on_chain_stream\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n                {\n                    \"data\": {\"output\": \"olleh\"},\n                    \"event\": \"on_chain_end\",\n                    \"metadata\": {},\n                    \"name\": \"reverse\",\n                    \"tags\": [],\n                },\n            ]\n\n\n        Example: Dispatch Custom Event\n\n        .. code-block:: python\n\n            from langchain_core.callbacks.manager import (\n                adispatch_custom_event,\n            )\n            from langchain_core.runnables import RunnableLambda, RunnableConfig\n            import asyncio\n\n\n            async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n                \\\"\\\"\\\"Do something that takes a long time.\\\"\\\"\\\"\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                await adispatch_custom_event(\n                    \"progress_event\",\n                    {\"message\": \"Finished step 1 of 3\"},\n                    config=config # Must be included for python < 3.10\n                )\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                await adispatch_custom_event(\n                    \"progress_event\",\n                    {\"message\": \"Finished step 2 of 3\"},\n                    config=config # Must be included for python < 3.10\n                )\n                await asyncio.sleep(1) # Placeholder for some slow operation\n                return \"Done\"\n\n            slow_thing = RunnableLambda(slow_thing)\n\n            async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n                print(event)\n\n        Args:\n            input: The input to the Runnable.\n            config: The config to use for the Runnable.\n            version: The version of the schema to use either `v2` or `v1`.\n                     Users should use `v2`.\n                     `v1` is for backwards compatibility and will be deprecated\n                     in 0.4.0.\n                     No default will be assigned until the API is stabilized.\n                     custom events will only be surfaced in `v2`.\n            include_names: Only include events from runnables with matching names.\n            include_types: Only include events from runnables with matching types.\n            include_tags: Only include events from runnables with matching tags.\n            exclude_names: Exclude events from runnables with matching names.\n            exclude_types: Exclude events from runnables with matching types.\n            exclude_tags: Exclude events from runnables with matching tags.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n                These will be passed to astream_log as this implementation\n                of astream_events is built on top of astream_log.\n\n        Yields:\n            An async stream of StreamEvents.\n\n        Raises:\n            NotImplementedError: If the version is not `v1` or `v2`.\n        \"\"\"  # noqa: E501\n        from langchain_core.tracers.event_stream import (\n            _astream_events_implementation_v1,\n            _astream_events_implementation_v2,\n        )\n\n        if version == \"v2\":\n            event_stream = _astream_events_implementation_v2(\n                self,\n                input,\n                config=config,\n                include_names=include_names,\n                include_types=include_types,\n                include_tags=include_tags,\n                exclude_names=exclude_names,\n                exclude_types=exclude_types,\n                exclude_tags=exclude_tags,\n                **kwargs,\n            )\n        elif version == \"v1\":\n            # First implementation, built on top of astream_log API\n            # This implementation will be deprecated as of 0.2.0\n            event_stream = _astream_events_implementation_v1(\n                self,\n                input,\n                config=config,\n                include_names=include_names,\n                include_types=include_types,\n                include_tags=include_tags,\n                exclude_names=exclude_names,\n                exclude_types=exclude_types,\n                exclude_tags=exclude_tags,\n                **kwargs,\n            )\n        else:\n            msg = 'Only versions \"v1\" and \"v2\" of the schema is currently supported.'\n            raise NotImplementedError(msg)\n\n        async with aclosing(event_stream):\n            async for event in event_stream:\n                yield event\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Default implementation of transform, which buffers input and calls astream.\n\n        Subclasses should override this method if they can start producing output while\n        input is still being generated.\n\n        Args:\n            input: An iterator of inputs to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        final: Input\n        got_first_val = False\n\n        for ichunk in input:\n            # The default implementation of transform is to buffer input and\n            # then call stream.\n            # It'll attempt to gather all input into a single chunk using\n            # the `+` operator.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk,\n            # and we'll iterate until we get to the last chunk.\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if got_first_val:\n            yield from self.stream(final, config, **kwargs)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Default implementation of atransform, which buffers input and calls astream.\n        Subclasses should override this method if they can start producing output while\n        input is still being generated.\n\n        Args:\n            input: An async iterator of inputs to the Runnable.\n            config: The config to use for the Runnable. Defaults to None.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Yields:\n            The output of the Runnable.\n        \"\"\"\n        final: Input\n        got_first_val = False\n\n        async for ichunk in input:\n            # The default implementation of transform is to buffer input and\n            # then call stream.\n            # It'll attempt to gather all input into a single chunk using\n            # the `+` operator.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk,\n            # and we'll iterate until we get to the last chunk.\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if got_first_val:\n            async for output in self.astream(final, config, **kwargs):\n                yield output\n\n    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:\n        \"\"\"Bind arguments to a Runnable, returning a new Runnable.\n\n        Useful when a Runnable in a chain requires an argument that is not\n        in the output of the previous Runnable or included in the user input.\n\n        Args:\n            kwargs: The arguments to bind to the Runnable.\n\n        Returns:\n            A new Runnable with the arguments bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_community.chat_models import ChatOllama\n            from langchain_core.output_parsers import StrOutputParser\n\n            llm = ChatOllama(model='llama2')\n\n            # Without bind.\n            chain = (\n                llm\n                | StrOutputParser()\n            )\n\n            chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n            # Output is 'One two three four five.'\n\n            # With bind.\n            chain = (\n                llm.bind(stop=[\"three\"])\n                | StrOutputParser()\n            )\n\n            chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n            # Output is 'One two'\n\n        \"\"\"\n        return RunnableBinding(bound=self, kwargs=kwargs, config={})\n\n    def with_config(\n        self,\n        config: Optional[RunnableConfig] = None,\n        # Sadly Unpack is not well-supported by mypy so this will have to be untyped\n        **kwargs: Any,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind config to a Runnable, returning a new Runnable.\n\n        Args:\n            config: The config to bind to the Runnable.\n            kwargs: Additional keyword arguments to pass to the Runnable.\n\n        Returns:\n            A new Runnable with the config bound.\n        \"\"\"\n        return RunnableBinding(\n            bound=self,\n            config=cast(\n                RunnableConfig,\n                {**(config or {}), **kwargs},\n            ),  # type: ignore[misc]\n            kwargs={},\n        )\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        on_start: Called before the Runnable starts running, with the Run object.\n        on_end: Called after the Runnable finishes running, with the Run object.\n        on_error: Called if the Runnable throws an error, with the Run object.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n\n        Args:\n            on_start: Called before the Runnable starts running. Defaults to None.\n            on_end: Called after the Runnable finishes running. Defaults to None.\n            on_error: Called if the Runnable throws an error. Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n            from langchain_core.tracers.schemas import Run\n\n            import time\n\n            def test_runnable(time_to_sleep : int):\n                time.sleep(time_to_sleep)\n\n            def fn_start(run_obj: Run):\n                print(\"start_time:\", run_obj.start_time)\n\n            def fn_end(run_obj: Run):\n                print(\"end_time:\", run_obj.end_time)\n\n            chain = RunnableLambda(test_runnable).with_listeners(\n                on_start=fn_start,\n                on_end=fn_end\n            )\n            chain.invoke(2)\n        \"\"\"\n        from langchain_core.tracers.root_listeners import RootListenersTracer\n\n        return RunnableBinding(\n            bound=self,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        RootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n        )\n\n    def with_alisteners(\n        self,\n        *,\n        on_start: Optional[AsyncListener] = None,\n        on_end: Optional[AsyncListener] = None,\n        on_error: Optional[AsyncListener] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n\n        on_start: Asynchronously called before the Runnable starts running.\n        on_end: Asynchronously called after the Runnable finishes running.\n        on_error: Asynchronously called if the Runnable throws an error.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n\n        Args:\n            on_start: Asynchronously called before the Runnable starts running.\n                Defaults to None.\n            on_end: Asynchronously called after the Runnable finishes running.\n                Defaults to None.\n            on_error: Asynchronously called if the Runnable throws an error.\n                Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n            import time\n\n            async def test_runnable(time_to_sleep : int):\n                print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n                await asyncio.sleep(time_to_sleep)\n                print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n\n            async def fn_start(run_obj : Runnable):\n                print(f\"on start callback starts at {format_t(time.time())}\n                await asyncio.sleep(3)\n                print(f\"on start callback ends at {format_t(time.time())}\")\n\n            async def fn_end(run_obj : Runnable):\n                print(f\"on end callback starts at {format_t(time.time())}\n                await asyncio.sleep(2)\n                print(f\"on end callback ends at {format_t(time.time())}\")\n\n            runnable = RunnableLambda(test_runnable).with_alisteners(\n                on_start=fn_start,\n                on_end=fn_end\n            )\n            async def concurrent_runs():\n                await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n\n            asyncio.run(concurrent_runs())\n            Result:\n            on start callback starts at 2024-05-16T14:20:29.637053+00:00\n            on start callback starts at 2024-05-16T14:20:29.637150+00:00\n            on start callback ends at 2024-05-16T14:20:32.638305+00:00\n            on start callback ends at 2024-05-16T14:20:32.638383+00:00\n            Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n            Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n            Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n            on end callback starts at 2024-05-16T14:20:35.640534+00:00\n            Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n            on end callback starts at 2024-05-16T14:20:37.640574+00:00\n            on end callback ends at 2024-05-16T14:20:37.640654+00:00\n            on end callback ends at 2024-05-16T14:20:39.641751+00:00\n\n        \"\"\"\n        from langchain_core.tracers.root_listeners import AsyncRootListenersTracer\n\n        return RunnableBinding(\n            bound=self,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        AsyncRootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n        )\n\n    def with_types(\n        self,\n        *,\n        input_type: Optional[type[Input]] = None,\n        output_type: Optional[type[Output]] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind input and output types to a Runnable, returning a new Runnable.\n\n        Args:\n            input_type: The input type to bind to the Runnable. Defaults to None.\n            output_type: The output type to bind to the Runnable. Defaults to None.\n\n        Returns:\n            A new Runnable with the types bound.\n        \"\"\"\n        return RunnableBinding(\n            bound=self,\n            custom_input_type=input_type,\n            custom_output_type=output_type,\n            kwargs={},\n        )\n\n    def with_retry(\n        self,\n        *,\n        retry_if_exception_type: tuple[type[BaseException], ...] = (Exception,),\n        wait_exponential_jitter: bool = True,\n        stop_after_attempt: int = 3,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Create a new Runnable that retries the original Runnable on exceptions.\n\n        Args:\n            retry_if_exception_type: A tuple of exception types to retry on.\n                Defaults to (Exception,).\n            wait_exponential_jitter: Whether to add jitter to the wait\n                time between retries. Defaults to True.\n            stop_after_attempt: The maximum number of attempts to make before\n                giving up. Defaults to 3.\n\n        Returns:\n            A new Runnable that retries the original Runnable on exceptions.\n\n        Example:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            count = 0\n\n\n            def _lambda(x: int) -> None:\n                global count\n                count = count + 1\n                if x == 1:\n                    raise ValueError(\"x is 1\")\n                else:\n                     pass\n\n\n            runnable = RunnableLambda(_lambda)\n            try:\n                runnable.with_retry(\n                    stop_after_attempt=2,\n                    retry_if_exception_type=(ValueError,),\n                ).invoke(1)\n            except ValueError:\n                pass\n\n            assert (count == 2)\n\n\n        Args:\n            retry_if_exception_type: A tuple of exception types to retry on\n            wait_exponential_jitter: Whether to add jitter to the wait time\n                                     between retries\n            stop_after_attempt: The maximum number of attempts to make before giving up\n\n        Returns:\n            A new Runnable that retries the original Runnable on exceptions.\n        \"\"\"\n        from langchain_core.runnables.retry import RunnableRetry\n\n        return RunnableRetry(\n            bound=self,\n            kwargs={},\n            config={},\n            retry_exception_types=retry_if_exception_type,\n            wait_exponential_jitter=wait_exponential_jitter,\n            max_attempt_number=stop_after_attempt,\n        )\n\n    def map(self) -> Runnable[list[Input], list[Output]]:\n        \"\"\"Return a new Runnable that maps a list of inputs to a list of outputs,\n        by calling invoke() with each input.\n\n        Returns:\n            A new Runnable that maps a list of inputs to a list of outputs.\n\n        Example:\n\n            .. code-block:: python\n\n                    from langchain_core.runnables import RunnableLambda\n\n                    def _lambda(x: int) -> int:\n                        return x + 1\n\n                    runnable = RunnableLambda(_lambda)\n                    print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n        \"\"\"\n        return RunnableEach(bound=self)\n\n    def with_fallbacks(\n        self,\n        fallbacks: Sequence[Runnable[Input, Output]],\n        *,\n        exceptions_to_handle: tuple[type[BaseException], ...] = (Exception,),\n        exception_key: Optional[str] = None,\n    ) -> RunnableWithFallbacksT[Input, Output]:\n        \"\"\"Add fallbacks to a Runnable, returning a new Runnable.\n\n        The new Runnable will try the original Runnable, and then each fallback\n        in order, upon failures.\n\n        Args:\n            fallbacks: A sequence of runnables to try if the original Runnable fails.\n            exceptions_to_handle: A tuple of exception types to handle.\n                Defaults to (Exception,).\n            exception_key: If string is specified then handled exceptions will be passed\n                to fallbacks as part of the input under the specified key. If None,\n                exceptions will not be passed to fallbacks. If used, the base Runnable\n                and its fallbacks must accept a dictionary as input. Defaults to None.\n\n        Returns:\n            A new Runnable that will try the original Runnable, and then each\n            fallback in order, upon failures.\n\n        Example:\n\n            .. code-block:: python\n\n                from typing import Iterator\n\n                from langchain_core.runnables import RunnableGenerator\n\n\n                def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n                    raise ValueError()\n                    yield \"\"\n\n\n                def _generate(input: Iterator) -> Iterator[str]:\n                    yield from \"foo bar\"\n\n\n                runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n                    [RunnableGenerator(_generate)]\n                    )\n                print(''.join(runnable.stream({}))) #foo bar\n\n        Args:\n            fallbacks: A sequence of runnables to try if the original Runnable fails.\n            exceptions_to_handle: A tuple of exception types to handle.\n            exception_key: If string is specified then handled exceptions will be passed\n                to fallbacks as part of the input under the specified key. If None,\n                exceptions will not be passed to fallbacks. If used, the base Runnable\n                and its fallbacks must accept a dictionary as input.\n\n        Returns:\n            A new Runnable that will try the original Runnable, and then each\n            fallback in order, upon failures.\n\n        \"\"\"\n        from langchain_core.runnables.fallbacks import RunnableWithFallbacks\n\n        return RunnableWithFallbacks(\n            runnable=self,\n            fallbacks=fallbacks,\n            exceptions_to_handle=exceptions_to_handle,\n            exception_key=exception_key,\n        )\n\n    \"\"\" --- Helper methods for Subclasses --- \"\"\"\n\n    def _call_with_config(\n        self,\n        func: Union[\n            Callable[[Input], Output],\n            Callable[[Input, CallbackManagerForChainRun], Output],\n            Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\n        ],\n        input: Input,\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        serialized: Optional[dict[str, Any]] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        config = ensure_config(config)\n        callback_manager = get_callback_manager_for_config(config)\n        run_manager = callback_manager.on_chain_start(\n            serialized,\n            input,\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            output = cast(\n                Output,\n                context.run(\n                    call_func_with_variable_args,  # type: ignore[arg-type]\n                    func,  # type: ignore[arg-type]\n                    input,  # type: ignore[arg-type]\n                    config,\n                    run_manager,\n                    **kwargs,\n                ),\n            )\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(output)\n            return output\n\n    async def _acall_with_config(\n        self,\n        func: Union[\n            Callable[[Input], Awaitable[Output]],\n            Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n            Callable[\n                [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                Awaitable[Output],\n            ],\n        ],\n        input: Input,\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        serialized: Optional[dict[str, Any]] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement ainvoke() in subclasses.\n        \"\"\"\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        run_manager = await callback_manager.on_chain_start(\n            serialized,\n            input,\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            coro = acall_func_with_variable_args(\n                func, input, config, run_manager, **kwargs\n            )\n            if asyncio_accepts_context():\n                output: Output = await asyncio.create_task(coro, context=context)  # type: ignore\n            else:\n                output = await coro\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(output)\n            return output\n\n    def _batch_with_config(\n        self,\n        func: Union[\n            Callable[[list[Input]], list[Union[Exception, Output]]],\n            Callable[\n                [list[Input], list[CallbackManagerForChainRun]],\n                list[Union[Exception, Output]],\n            ],\n            Callable[\n                [list[Input], list[CallbackManagerForChainRun], list[RunnableConfig]],\n                list[Union[Exception, Output]],\n            ],\n        ],\n        input: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        if not input:\n            return []\n\n        configs = get_config_list(config, len(input))\n        callback_managers = [get_callback_manager_for_config(c) for c in configs]\n        run_managers = [\n            callback_manager.on_chain_start(\n                None,\n                input,\n                run_type=run_type,\n                name=config.get(\"run_name\") or self.get_name(),\n                run_id=config.pop(\"run_id\", None),\n            )\n            for callback_manager, input, config in zip(\n                callback_managers, input, configs\n            )\n        ]\n        try:\n            if accepts_config(func):\n                kwargs[\"config\"] = [\n                    patch_config(c, callbacks=rm.get_child())\n                    for c, rm in zip(configs, run_managers)\n                ]\n            if accepts_run_manager(func):\n                kwargs[\"run_manager\"] = run_managers\n            output = func(input, **kwargs)  # type: ignore[call-arg]\n        except BaseException as e:\n            for run_manager in run_managers:\n                run_manager.on_chain_error(e)\n            if return_exceptions:\n                return cast(list[Output], [e for _ in input])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            for run_manager, out in zip(run_managers, output):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    run_manager.on_chain_error(out)\n                else:\n                    run_manager.on_chain_end(out)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], output)\n            else:\n                raise first_exception\n\n    async def _abatch_with_config(\n        self,\n        func: Union[\n            Callable[[list[Input]], Awaitable[list[Union[Exception, Output]]]],\n            Callable[\n                [list[Input], list[AsyncCallbackManagerForChainRun]],\n                Awaitable[list[Union[Exception, Output]]],\n            ],\n            Callable[\n                [\n                    list[Input],\n                    list[AsyncCallbackManagerForChainRun],\n                    list[RunnableConfig],\n                ],\n                Awaitable[list[Union[Exception, Output]]],\n            ],\n        ],\n        input: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        \"\"\"Helper method to transform an Input value to an Output value,\n        with callbacks. Use this method to implement invoke() in subclasses.\n        \"\"\"\n        if not input:\n            return []\n\n        configs = get_config_list(config, len(input))\n        callback_managers = [get_async_callback_manager_for_config(c) for c in configs]\n        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(\n            *(\n                callback_manager.on_chain_start(\n                    None,\n                    input,\n                    run_type=run_type,\n                    name=config.get(\"run_name\") or self.get_name(),\n                    run_id=config.pop(\"run_id\", None),\n                )\n                for callback_manager, input, config in zip(\n                    callback_managers, input, configs\n                )\n            )\n        )\n        try:\n            if accepts_config(func):\n                kwargs[\"config\"] = [\n                    patch_config(c, callbacks=rm.get_child())\n                    for c, rm in zip(configs, run_managers)\n                ]\n            if accepts_run_manager(func):\n                kwargs[\"run_manager\"] = run_managers\n            output = await func(input, **kwargs)  # type: ignore[call-arg]\n        except BaseException as e:\n            await asyncio.gather(\n                *(run_manager.on_chain_error(e) for run_manager in run_managers)\n            )\n            if return_exceptions:\n                return cast(list[Output], [e for _ in input])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            coros: list[Awaitable[None]] = []\n            for run_manager, out in zip(run_managers, output):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    coros.append(run_manager.on_chain_error(out))\n                else:\n                    coros.append(run_manager.on_chain_end(out))\n            await asyncio.gather(*coros)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], output)\n            else:\n                raise first_exception\n\n    def _transform_stream_with_config(\n        self,\n        input: Iterator[Input],\n        transformer: Union[\n            Callable[[Iterator[Input]], Iterator[Output]],\n            Callable[[Iterator[Input], CallbackManagerForChainRun], Iterator[Output]],\n            Callable[\n                [\n                    Iterator[Input],\n                    CallbackManagerForChainRun,\n                    RunnableConfig,\n                ],\n                Iterator[Output],\n            ],\n        ],\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        \"\"\"Helper method to transform an Iterator of Input values into an Iterator of\n        Output values, with callbacks.\n        Use this to implement `stream()` or `transform()` in Runnable subclasses.\n        \"\"\"\n        # Mixin that is used by both astream log and astream events implementation\n        from langchain_core.tracers._streaming import _StreamingCallbackHandler\n\n        # tee the input so we can iterate over it twice\n        input_for_tracing, input_for_transform = tee(input, 2)\n        # Start the input iterator to ensure the input Runnable starts before this one\n        final_input: Optional[Input] = next(input_for_tracing, None)\n        final_input_supported = True\n        final_output: Optional[Output] = None\n        final_output_supported = True\n\n        config = ensure_config(config)\n        callback_manager = get_callback_manager_for_config(config)\n        run_manager = callback_manager.on_chain_start(\n            None,\n            {\"input\": \"\"},\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            if accepts_config(transformer):\n                kwargs[\"config\"] = child_config\n            if accepts_run_manager(transformer):\n                kwargs[\"run_manager\"] = run_manager\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            iterator = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]\n            if stream_handler := next(\n                (\n                    cast(_StreamingCallbackHandler, h)\n                    for h in run_manager.handlers\n                    # instance check OK here, it's a mixin\n                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]\n                ),\n                None,\n            ):\n                # populates streamed_output in astream_log() output if needed\n                iterator = stream_handler.tap_output_iter(run_manager.run_id, iterator)\n            try:\n                while True:\n                    chunk: Output = context.run(next, iterator)  # type: ignore\n                    yield chunk\n                    if final_output_supported:\n                        if final_output is None:\n                            final_output = chunk\n                        else:\n                            try:\n                                final_output = final_output + chunk  # type: ignore\n                            except TypeError:\n                                final_output = chunk\n                                final_output_supported = False\n                    else:\n                        final_output = chunk\n            except (StopIteration, GeneratorExit):\n                pass\n            for ichunk in input_for_tracing:\n                if final_input_supported:\n                    if final_input is None:\n                        final_input = ichunk\n                    else:\n                        try:\n                            final_input = final_input + ichunk  # type: ignore\n                        except TypeError:\n                            final_input = ichunk\n                            final_input_supported = False\n                else:\n                    final_input = ichunk\n        except BaseException as e:\n            run_manager.on_chain_error(e, inputs=final_input)\n            raise\n        else:\n            run_manager.on_chain_end(final_output, inputs=final_input)\n\n    async def _atransform_stream_with_config(\n        self,\n        input: AsyncIterator[Input],\n        transformer: Union[\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n            Callable[\n                [AsyncIterator[Input], AsyncCallbackManagerForChainRun],\n                AsyncIterator[Output],\n            ],\n            Callable[\n                [\n                    AsyncIterator[Input],\n                    AsyncCallbackManagerForChainRun,\n                    RunnableConfig,\n                ],\n                AsyncIterator[Output],\n            ],\n        ],\n        config: Optional[RunnableConfig],\n        run_type: Optional[str] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        \"\"\"Helper method to transform an Async Iterator of Input values into an Async\n        Iterator of Output values, with callbacks.\n        Use this to implement `astream()` or `atransform()` in Runnable subclasses.\n        \"\"\"\n        # Mixin that is used by both astream log and astream events implementation\n        from langchain_core.tracers._streaming import _StreamingCallbackHandler\n\n        # tee the input so we can iterate over it twice\n        input_for_tracing, input_for_transform = atee(input, 2)\n        # Start the input iterator to ensure the input Runnable starts before this one\n        final_input: Optional[Input] = await py_anext(input_for_tracing, None)\n        final_input_supported = True\n        final_output: Optional[Output] = None\n        final_output_supported = True\n\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            {\"input\": \"\"},\n            run_type=run_type,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            if accepts_config(transformer):\n                kwargs[\"config\"] = child_config\n            if accepts_run_manager(transformer):\n                kwargs[\"run_manager\"] = run_manager\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            iterator_ = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]\n\n            if stream_handler := next(\n                (\n                    cast(_StreamingCallbackHandler, h)\n                    for h in run_manager.handlers\n                    # instance check OK here, it's a mixin\n                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]\n                ),\n                None,\n            ):\n                # populates streamed_output in astream_log() output if needed\n                iterator = stream_handler.tap_output_aiter(\n                    run_manager.run_id, iterator_\n                )\n            else:\n                iterator = iterator_\n            try:\n                while True:\n                    if asyncio_accepts_context():\n                        chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]\n                            py_anext(iterator),  # type: ignore[arg-type]\n                            context=context,\n                        )\n                    else:\n                        chunk = cast(Output, await py_anext(iterator))\n                    yield chunk\n                    if final_output_supported:\n                        if final_output is None:\n                            final_output = chunk\n                        else:\n                            try:\n                                final_output = final_output + chunk  # type: ignore\n                            except TypeError:\n                                final_output = chunk\n                                final_output_supported = False\n                    else:\n                        final_output = chunk\n            except StopAsyncIteration:\n                pass\n            async for ichunk in input_for_tracing:\n                if final_input_supported:\n                    if final_input is None:\n                        final_input = ichunk\n                    else:\n                        try:\n                            final_input = final_input + ichunk  # type: ignore[operator]\n                        except TypeError:\n                            final_input = ichunk\n                            final_input_supported = False\n                else:\n                    final_input = ichunk\n        except BaseException as e:\n            await run_manager.on_chain_error(e, inputs=final_input)\n            raise\n        else:\n            await run_manager.on_chain_end(final_output, inputs=final_input)\n        finally:\n            if iterator_ is not None and hasattr(iterator_, \"aclose\"):\n                await iterator_.aclose()\n\n    @beta_decorator.beta(message=\"This API is in beta and may change in the future.\")\n    def as_tool(\n        self,\n        args_schema: Optional[type[BaseModel]] = None,\n        *,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n        arg_types: Optional[dict[str, type]] = None,\n    ) -> BaseTool:\n        \"\"\"Create a BaseTool from a Runnable.\n\n        ``as_tool`` will instantiate a BaseTool with a name, description, and\n        ``args_schema`` from a Runnable. Where possible, schemas are inferred\n        from ``runnable.get_input_schema``. Alternatively (e.g., if the\n        Runnable takes a dict as input and the specific dict keys are not typed),\n        the schema can be specified directly with ``args_schema``. You can also\n        pass ``arg_types`` to just specify the required arguments and their types.\n\n        Args:\n            args_schema: The schema for the tool. Defaults to None.\n            name: The name of the tool. Defaults to None.\n            description: The description of the tool. Defaults to None.\n            arg_types: A dictionary of argument names to types. Defaults to None.\n\n        Returns:\n            A BaseTool instance.\n\n        Typed dict input:\n\n        .. code-block:: python\n\n            from typing import List\n            from typing_extensions import TypedDict\n            from langchain_core.runnables import RunnableLambda\n\n            class Args(TypedDict):\n                a: int\n                b: List[int]\n\n            def f(x: Args) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool()\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        ``dict`` input, specifying schema via ``args_schema``:\n\n        .. code-block:: python\n\n            from typing import Any, Dict, List\n            from pydantic import BaseModel, Field\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: Dict[str, Any]) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            class FSchema(BaseModel):\n                \\\"\\\"\\\"Apply a function to an integer and list of integers.\\\"\\\"\\\"\n\n                a: int = Field(..., description=\"Integer\")\n                b: List[int] = Field(..., description=\"List of ints\")\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool(FSchema)\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        ``dict`` input, specifying schema via ``arg_types``:\n\n        .. code-block:: python\n\n            from typing import Any, Dict, List\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: Dict[str, Any]) -> str:\n                return str(x[\"a\"] * max(x[\"b\"]))\n\n            runnable = RunnableLambda(f)\n            as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n            as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n        String input:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def f(x: str) -> str:\n                return x + \"a\"\n\n            def g(x: str) -> str:\n                return x + \"z\"\n\n            runnable = RunnableLambda(f) | g\n            as_tool = runnable.as_tool()\n            as_tool.invoke(\"b\")\n\n        .. versionadded:: 0.2.14\n        \"\"\"\n        # Avoid circular import\n        from langchain_core.tools import convert_runnable_to_tool\n\n        return convert_runnable_to_tool(\n            self,\n            args_schema=args_schema,\n            name=name,\n            description=description,\n            arg_types=arg_types,\n        )\n\n\nclass RunnableSerializable(Serializable, Runnable[Input, Output]):\n    \"\"\"Runnable that can be serialized to JSON.\"\"\"\n\n    name: Optional[str] = None\n\n    model_config = ConfigDict(\n        # Suppress warnings from pydantic protected namespaces\n        # (e.g., `model_`)\n        protected_namespaces=(),\n    )\n\n    def to_json(self) -> Union[SerializedConstructor, SerializedNotImplemented]:\n        \"\"\"Serialize the Runnable to JSON.\n\n        Returns:\n            A JSON-serializable representation of the Runnable.\n        \"\"\"\n        dumped = super().to_json()\n        with contextlib.suppress(Exception):\n            dumped[\"name\"] = self.get_name()\n        return dumped\n\n    def configurable_fields(\n        self, **kwargs: AnyConfigurableField\n    ) -> RunnableSerializable[Input, Output]:\n        \"\"\"Configure particular Runnable fields at runtime.\n\n        Args:\n            **kwargs: A dictionary of ConfigurableField instances to configure.\n\n        Returns:\n            A new Runnable with the fields configured.\n\n        .. code-block:: python\n\n            from langchain_core.runnables import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            model = ChatOpenAI(max_tokens=20).configurable_fields(\n                max_tokens=ConfigurableField(\n                    id=\"output_token_number\",\n                    name=\"Max tokens in the output\",\n                    description=\"The maximum number of tokens in the output\",\n                )\n            )\n\n            # max_tokens = 20\n            print(\n                \"max_tokens_20: \",\n                model.invoke(\"tell me something about chess\").content\n            )\n\n            # max_tokens = 200\n            print(\"max_tokens_200: \", model.with_config(\n                configurable={\"output_token_number\": 200}\n                ).invoke(\"tell me something about chess\").content\n            )\n        \"\"\"\n        from langchain_core.runnables.configurable import RunnableConfigurableFields\n\n        for key in kwargs:\n            if key not in self.model_fields:\n                msg = (\n                    f\"Configuration key {key} not found in {self}: \"\n                    f\"available keys are {self.model_fields.keys()}\"\n                )\n                raise ValueError(msg)\n\n        return RunnableConfigurableFields(default=self, fields=kwargs)\n\n    def configurable_alternatives(\n        self,\n        which: ConfigurableField,\n        *,\n        default_key: str = \"default\",\n        prefix_keys: bool = False,\n        **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],\n    ) -> RunnableSerializable[Input, Output]:\n        \"\"\"Configure alternatives for Runnables that can be set at runtime.\n\n        Args:\n            which: The ConfigurableField instance that will be used to select the\n                alternative.\n            default_key: The default key to use if no alternative is selected.\n                Defaults to \"default\".\n            prefix_keys: Whether to prefix the keys with the ConfigurableField id.\n                Defaults to False.\n            **kwargs: A dictionary of keys to Runnable instances or callables that\n                return Runnable instances.\n\n        Returns:\n            A new Runnable with the alternatives configured.\n\n        .. code-block:: python\n\n            from langchain_anthropic import ChatAnthropic\n            from langchain_core.runnables.utils import ConfigurableField\n            from langchain_openai import ChatOpenAI\n\n            model = ChatAnthropic(\n                model_name=\"claude-3-sonnet-20240229\"\n            ).configurable_alternatives(\n                ConfigurableField(id=\"llm\"),\n                default_key=\"anthropic\",\n                openai=ChatOpenAI()\n            )\n\n            # uses the default model ChatAnthropic\n            print(model.invoke(\"which organization created you?\").content)\n\n            # uses ChatOpenAI\n            print(\n                model.with_config(\n                    configurable={\"llm\": \"openai\"}\n                ).invoke(\"which organization created you?\").content\n            )\n        \"\"\"\n        from langchain_core.runnables.configurable import (\n            RunnableConfigurableAlternatives,\n        )\n\n        return RunnableConfigurableAlternatives(\n            which=which,\n            default=self,\n            alternatives=kwargs,\n            default_key=default_key,\n            prefix_keys=prefix_keys,\n        )\n\n\ndef _seq_input_schema(\n    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]\n) -> type[BaseModel]:\n    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick\n\n    first = steps[0]\n    if len(steps) == 1:\n        return first.get_input_schema(config)\n    elif isinstance(first, RunnableAssign):\n        next_input_schema = _seq_input_schema(steps[1:], config)\n        if not issubclass(next_input_schema, RootModel):\n            # it's a dict as expected\n            return create_model_v2(  # type: ignore[call-overload]\n                \"RunnableSequenceInput\",\n                field_definitions={\n                    k: (v.annotation, v.default)\n                    for k, v in next_input_schema.model_fields.items()\n                    if k not in first.mapper.steps__\n                },\n            )\n    elif isinstance(first, RunnablePick):\n        return _seq_input_schema(steps[1:], config)\n\n    return first.get_input_schema(config)\n\n\ndef _seq_output_schema(\n    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]\n) -> type[BaseModel]:\n    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick\n\n    last = steps[-1]\n    if len(steps) == 1:\n        return last.get_input_schema(config)\n    elif isinstance(last, RunnableAssign):\n        mapper_output_schema = last.mapper.get_output_schema(config)\n        prev_output_schema = _seq_output_schema(steps[:-1], config)\n        if not issubclass(prev_output_schema, RootModel):\n            # it's a dict as expected\n            return create_model_v2(  # type: ignore[call-overload]\n                \"RunnableSequenceOutput\",\n                field_definitions={\n                    **{\n                        k: (v.annotation, v.default)\n                        for k, v in prev_output_schema.model_fields.items()\n                    },\n                    **{\n                        k: (v.annotation, v.default)\n                        for k, v in mapper_output_schema.model_fields.items()\n                    },\n                },\n            )\n    elif isinstance(last, RunnablePick):\n        prev_output_schema = _seq_output_schema(steps[:-1], config)\n        if not issubclass(prev_output_schema, RootModel):\n            # it's a dict as expected\n            if isinstance(last.keys, list):\n                return create_model_v2(  # type: ignore[call-overload]\n                    \"RunnableSequenceOutput\",\n                    field_definitions={\n                        k: (v.annotation, v.default)\n                        for k, v in prev_output_schema.model_fields.items()\n                        if k in last.keys\n                    },\n                )\n            else:\n                field = prev_output_schema.model_fields[last.keys]\n                return create_model_v2(  # type: ignore[call-overload]\n                    \"RunnableSequenceOutput\", root=(field.annotation, field.default)\n                )\n\n    return last.get_output_schema(config)\n\n\nclass RunnableSequence(RunnableSerializable[Input, Output]):\n    \"\"\"Sequence of Runnables, where the output of each is the input of the next.\n\n    **RunnableSequence** is the most important composition operator in LangChain\n    as it is used in virtually every chain.\n\n    A RunnableSequence can be instantiated directly or more commonly by using the `|`\n    operator where either the left or right operands (or both) must be a Runnable.\n\n    Any RunnableSequence automatically supports sync, async, batch.\n\n    The default implementations of `batch` and `abatch` utilize threadpools and\n    asyncio gather and will be faster than naive invocation of invoke or ainvoke\n    for IO bound Runnables.\n\n    Batching is implemented by invoking the batch method on each component of the\n    RunnableSequence in order.\n\n    A RunnableSequence preserves the streaming properties of its components, so if all\n    components of the sequence implement a `transform` method -- which\n    is the method that implements the logic to map a streaming input to a streaming\n    output -- then the sequence will be able to stream input to output!\n\n    If any component of the sequence does not implement transform then the\n    streaming will only begin after this component is run. If there are\n    multiple blocking components, streaming begins after the last one.\n\n    Please note: RunnableLambdas do not support `transform` by default! So if\n        you need to use a RunnableLambdas be careful about where you place them in a\n        RunnableSequence (if you need to use the .stream()/.astream() methods).\n\n        If you need arbitrary logic and need streaming, you can subclass\n        Runnable, and implement `transform` for whatever logic you need.\n\n    Here is a simple example that uses simple functions to illustrate the use of\n    RunnableSequence:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            def mul_two(x: int) -> int:\n                return x * 2\n\n            runnable_1 = RunnableLambda(add_one)\n            runnable_2 = RunnableLambda(mul_two)\n            sequence = runnable_1 | runnable_2\n            # Or equivalently:\n            # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n            sequence.invoke(1)\n            await sequence.ainvoke(1)\n\n            sequence.batch([1, 2, 3])\n            await sequence.abatch([1, 2, 3])\n\n    Here's an example that uses streams JSON output generated by an LLM:\n\n        .. code-block:: python\n\n            from langchain_core.output_parsers.json import SimpleJsonOutputParser\n            from langchain_openai import ChatOpenAI\n\n            prompt = PromptTemplate.from_template(\n                'In JSON format, give me a list of {topic} and their '\n                'corresponding names in French, Spanish and in a '\n                'Cat Language.'\n            )\n\n            model = ChatOpenAI()\n            chain = prompt | model | SimpleJsonOutputParser()\n\n            async for chunk in chain.astream({'topic': 'colors'}):\n                print('-')  # noqa: T201\n                print(chunk, sep='', flush=True)  # noqa: T201\n    \"\"\"\n\n    # The steps are broken into first, middle and last, solely for type checking\n    # purposes. It allows specifying the `Input` on the first type, the `Output` of\n    # the last type.\n    first: Runnable[Input, Any]\n    \"\"\"The first Runnable in the sequence.\"\"\"\n    middle: list[Runnable[Any, Any]] = Field(default_factory=list)\n    \"\"\"The middle Runnables in the sequence.\"\"\"\n    last: Runnable[Any, Output]\n    \"\"\"The last Runnable in the sequence.\"\"\"\n\n    def __init__(\n        self,\n        *steps: RunnableLike,\n        name: Optional[str] = None,\n        first: Optional[Runnable[Any, Any]] = None,\n        middle: Optional[list[Runnable[Any, Any]]] = None,\n        last: Optional[Runnable[Any, Any]] = None,\n    ) -> None:\n        \"\"\"Create a new RunnableSequence.\n\n        Args:\n            steps: The steps to include in the sequence.\n            name: The name of the Runnable. Defaults to None.\n            first: The first Runnable in the sequence. Defaults to None.\n            middle: The middle Runnables in the sequence. Defaults to None.\n            last: The last Runnable in the sequence. Defaults to None.\n\n        Raises:\n            ValueError: If the sequence has less than 2 steps.\n        \"\"\"\n        steps_flat: list[Runnable] = []\n        if not steps and first is not None and last is not None:\n            steps_flat = [first] + (middle or []) + [last]\n        for step in steps:\n            if isinstance(step, RunnableSequence):\n                steps_flat.extend(step.steps)\n            else:\n                steps_flat.append(coerce_to_runnable(step))\n        if len(steps_flat) < 2:\n            msg = f\"RunnableSequence must have at least 2 steps, got {len(steps_flat)}\"\n            raise ValueError(msg)\n        super().__init__(  # type: ignore[call-arg]\n            first=steps_flat[0],\n            middle=list(steps_flat[1:-1]),\n            last=steps_flat[-1],\n            name=name,\n        )\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    @property\n    def steps(self) -> list[Runnable[Any, Any]]:\n        \"\"\"All the Runnables that make up the sequence in order.\n\n        Returns:\n            A list of Runnables.\n        \"\"\"\n        return [self.first] + self.middle + [self.last]\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Check if the object is serializable.\n\n        Returns:\n            True if the object is serializable, False otherwise.\n                Defaults to True.\n        \"\"\"\n        return True\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    @override\n    def InputType(self) -> type[Input]:\n        \"\"\"The type of the input to the Runnable.\"\"\"\n        return self.first.InputType\n\n    @property\n    @override\n    def OutputType(self) -> type[Output]:\n        \"\"\"The type of the output of the Runnable.\"\"\"\n        return self.last.OutputType\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the input schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema of the Runnable.\n        \"\"\"\n        return _seq_input_schema(self.steps, config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the output schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The output schema of the Runnable.\n        \"\"\"\n        return _seq_output_schema(self.steps, config)\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"Get the config specs of the Runnable.\n\n        Returns:\n            The config specs of the Runnable.\n        \"\"\"\n        from langchain_core.beta.runnables.context import (\n            CONTEXT_CONFIG_PREFIX,\n            _key_from_id,\n        )\n\n        # get all specs\n        all_specs = [\n            (spec, idx)\n            for idx, step in enumerate(self.steps)\n            for spec in step.config_specs\n        ]\n        # calculate context dependencies\n        specs_by_pos = groupby(\n            [tup for tup in all_specs if tup[0].id.startswith(CONTEXT_CONFIG_PREFIX)],\n            itemgetter(1),\n        )\n        next_deps: set[str] = set()\n        deps_by_pos: dict[int, set[str]] = {}\n        for pos, specs in specs_by_pos:\n            deps_by_pos[pos] = next_deps\n            next_deps = next_deps | {spec[0].id for spec in specs}\n        # assign context dependencies\n        for pos, (spec, idx) in enumerate(all_specs):\n            if spec.id.startswith(CONTEXT_CONFIG_PREFIX):\n                all_specs[pos] = (\n                    ConfigurableFieldSpec(\n                        id=spec.id,\n                        annotation=spec.annotation,\n                        name=spec.name,\n                        default=spec.default,\n                        description=spec.description,\n                        is_shared=spec.is_shared,\n                        dependencies=[\n                            d\n                            for d in deps_by_pos[idx]\n                            if _key_from_id(d) != _key_from_id(spec.id)\n                        ]\n                        + (spec.dependencies or []),\n                    ),\n                    idx,\n                )\n\n        return get_unique_config_specs(spec for spec, _ in all_specs)\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Get the graph representation of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The graph representation of the Runnable.\n\n        Raises:\n            ValueError: If a Runnable has no first or last node.\n        \"\"\"\n        from langchain_core.runnables.graph import Graph\n\n        graph = Graph()\n        for step in self.steps:\n            current_last_node = graph.last_node()\n            step_graph = step.get_graph(config)\n            if step is not self.first:\n                step_graph.trim_first_node()\n            if step is not self.last:\n                step_graph.trim_last_node()\n            step_first_node, _ = graph.extend(step_graph)\n            if not step_first_node:\n                msg = f\"Runnable {step} has no first node\"\n                raise ValueError(msg)\n            if current_last_node:\n                graph.add_edge(current_last_node, step_first_node)\n\n        return graph\n\n    def __repr__(self) -> str:\n        return \"\\n| \".join(\n            repr(s) if i == 0 else indent_lines_after_first(repr(s), \"| \")\n            for i, s in enumerate(self.steps)\n        )\n\n    def __or__(\n        self,\n        other: Union[\n            Runnable[Any, Other],\n            Callable[[Any], Other],\n            Callable[[Iterator[Any]], Iterator[Other]],\n            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],\n        ],\n    ) -> RunnableSerializable[Input, Other]:\n        if isinstance(other, RunnableSequence):\n            return RunnableSequence(\n                self.first,\n                *self.middle,\n                self.last,\n                other.first,\n                *other.middle,\n                other.last,\n                name=self.name or other.name,\n            )\n        else:\n            return RunnableSequence(\n                self.first,\n                *self.middle,\n                self.last,\n                coerce_to_runnable(other),\n                name=self.name,\n            )\n\n    def __ror__(\n        self,\n        other: Union[\n            Runnable[Other, Any],\n            Callable[[Other], Any],\n            Callable[[Iterator[Other]], Iterator[Any]],\n            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],\n        ],\n    ) -> RunnableSerializable[Other, Output]:\n        if isinstance(other, RunnableSequence):\n            return RunnableSequence(\n                other.first,\n                *other.middle,\n                other.last,\n                self.first,\n                *self.middle,\n                self.last,\n                name=other.name or self.name,\n            )\n        else:\n            return RunnableSequence(\n                coerce_to_runnable(other),\n                self.first,\n                *self.middle,\n                self.last,\n                name=self.name,\n            )\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        from langchain_core.beta.runnables.context import config_with_context\n\n        # setup callbacks and context\n        config = config_with_context(ensure_config(config), self.steps)\n        callback_manager = get_callback_manager_for_config(config)\n        # start the root run\n        run_manager = callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        # invoke all steps in sequence\n        try:\n            for i, step in enumerate(self.steps):\n                # mark each step as a child run\n                config = patch_config(\n                    config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n                )\n                context = copy_context()\n                context.run(_set_config_context, config)\n                if i == 0:\n                    input = context.run(step.invoke, input, config, **kwargs)\n                else:\n                    input = context.run(step.invoke, input, config)\n        # finish the root run\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(input)\n            return cast(Output, input)\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n\n        # setup callbacks and context\n        config = aconfig_with_context(ensure_config(config), self.steps)\n        callback_manager = get_async_callback_manager_for_config(config)\n        # start the root run\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        # invoke all steps in sequence\n        try:\n            for i, step in enumerate(self.steps):\n                # mark each step as a child run\n                config = patch_config(\n                    config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n                )\n                context = copy_context()\n                context.run(_set_config_context, config)\n                if i == 0:\n                    part = functools.partial(step.ainvoke, input, config, **kwargs)\n                else:\n                    part = functools.partial(step.ainvoke, input, config)\n                if asyncio_accepts_context():\n                    input = await asyncio.create_task(part(), context=context)  # type: ignore\n                else:\n                    input = await asyncio.create_task(part())\n        # finish the root run\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(input)\n            return cast(Output, input)\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        from langchain_core.beta.runnables.context import config_with_context\n        from langchain_core.callbacks.manager import CallbackManager\n\n        if not inputs:\n            return []\n\n        # setup callbacks and context\n        configs = [\n            config_with_context(c, self.steps)\n            for c in get_config_list(config, len(inputs))\n        ]\n        callback_managers = [\n            CallbackManager.configure(\n                inheritable_callbacks=config.get(\"callbacks\"),\n                local_callbacks=None,\n                verbose=False,\n                inheritable_tags=config.get(\"tags\"),\n                local_tags=None,\n                inheritable_metadata=config.get(\"metadata\"),\n                local_metadata=None,\n            )\n            for config in configs\n        ]\n        # start the root runs, one per input\n        run_managers = [\n            cm.on_chain_start(\n                None,\n                input,\n                name=config.get(\"run_name\") or self.get_name(),\n                run_id=config.pop(\"run_id\", None),\n            )\n            for cm, input, config in zip(callback_managers, inputs, configs)\n        ]\n\n        # invoke\n        try:\n            if return_exceptions:\n                # Track which inputs (by index) failed so far\n                # If an input has failed it will be present in this map,\n                # and the value will be the exception that was raised.\n                failed_inputs_map: dict[int, Exception] = {}\n                for stepidx, step in enumerate(self.steps):\n                    # Assemble the original indexes of the remaining inputs\n                    # (i.e. the ones that haven't failed yet)\n                    remaining_idxs = [\n                        i for i in range(len(configs)) if i not in failed_inputs_map\n                    ]\n                    # Invoke the step on the remaining inputs\n                    inputs = step.batch(\n                        [\n                            inp\n                            for i, inp in zip(remaining_idxs, inputs)\n                            if i not in failed_inputs_map\n                        ],\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config,\n                                callbacks=rm.get_child(f\"seq:step:{stepidx + 1}\"),\n                            )\n                            for i, (rm, config) in enumerate(zip(run_managers, configs))\n                            if i not in failed_inputs_map\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if stepidx == 0 else {}),\n                    )\n                    # If an input failed, add it to the map\n                    for i, inp in zip(remaining_idxs, inputs):\n                        if isinstance(inp, Exception):\n                            failed_inputs_map[i] = inp\n                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]\n                    # If all inputs have failed, stop processing\n                    if len(failed_inputs_map) == len(configs):\n                        break\n\n                # Reassemble the outputs, inserting Exceptions for failed inputs\n                inputs_copy = inputs.copy()\n                inputs = []\n                for i in range(len(configs)):\n                    if i in failed_inputs_map:\n                        inputs.append(cast(Input, failed_inputs_map[i]))\n                    else:\n                        inputs.append(inputs_copy.pop(0))\n            else:\n                for i, step in enumerate(self.steps):\n                    inputs = step.batch(\n                        inputs,\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config, callbacks=rm.get_child(f\"seq:step:{i + 1}\")\n                            )\n                            for rm, config in zip(run_managers, configs)\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if i == 0 else {}),\n                    )\n\n        # finish the root runs\n        except BaseException as e:\n            for rm in run_managers:\n                rm.on_chain_error(e)\n            if return_exceptions:\n                return cast(list[Output], [e for _ in inputs])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            for run_manager, out in zip(run_managers, inputs):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    run_manager.on_chain_error(out)\n                else:\n                    run_manager.on_chain_end(out)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], inputs)\n            else:\n                raise first_exception\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n        from langchain_core.callbacks.manager import AsyncCallbackManager\n\n        if not inputs:\n            return []\n\n        # setup callbacks and context\n        configs = [\n            aconfig_with_context(c, self.steps)\n            for c in get_config_list(config, len(inputs))\n        ]\n        callback_managers = [\n            AsyncCallbackManager.configure(\n                inheritable_callbacks=config.get(\"callbacks\"),\n                local_callbacks=None,\n                verbose=False,\n                inheritable_tags=config.get(\"tags\"),\n                local_tags=None,\n                inheritable_metadata=config.get(\"metadata\"),\n                local_metadata=None,\n            )\n            for config in configs\n        ]\n        # start the root runs, one per input\n        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(\n            *(\n                cm.on_chain_start(\n                    None,\n                    input,\n                    name=config.get(\"run_name\") or self.get_name(),\n                    run_id=config.pop(\"run_id\", None),\n                )\n                for cm, input, config in zip(callback_managers, inputs, configs)\n            )\n        )\n\n        # invoke .batch() on each step\n        # this uses batching optimizations in Runnable subclasses, like LLM\n        try:\n            if return_exceptions:\n                # Track which inputs (by index) failed so far\n                # If an input has failed it will be present in this map,\n                # and the value will be the exception that was raised.\n                failed_inputs_map: dict[int, Exception] = {}\n                for stepidx, step in enumerate(self.steps):\n                    # Assemble the original indexes of the remaining inputs\n                    # (i.e. the ones that haven't failed yet)\n                    remaining_idxs = [\n                        i for i in range(len(configs)) if i not in failed_inputs_map\n                    ]\n                    # Invoke the step on the remaining inputs\n                    inputs = await step.abatch(\n                        [\n                            inp\n                            for i, inp in zip(remaining_idxs, inputs)\n                            if i not in failed_inputs_map\n                        ],\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config,\n                                callbacks=rm.get_child(f\"seq:step:{stepidx + 1}\"),\n                            )\n                            for i, (rm, config) in enumerate(zip(run_managers, configs))\n                            if i not in failed_inputs_map\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if stepidx == 0 else {}),\n                    )\n                    # If an input failed, add it to the map\n                    for i, inp in zip(remaining_idxs, inputs):\n                        if isinstance(inp, Exception):\n                            failed_inputs_map[i] = inp\n                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]\n                    # If all inputs have failed, stop processing\n                    if len(failed_inputs_map) == len(configs):\n                        break\n\n                # Reassemble the outputs, inserting Exceptions for failed inputs\n                inputs_copy = inputs.copy()\n                inputs = []\n                for i in range(len(configs)):\n                    if i in failed_inputs_map:\n                        inputs.append(cast(Input, failed_inputs_map[i]))\n                    else:\n                        inputs.append(inputs_copy.pop(0))\n            else:\n                for i, step in enumerate(self.steps):\n                    inputs = await step.abatch(\n                        inputs,\n                        [\n                            # each step a child run of the corresponding root run\n                            patch_config(\n                                config, callbacks=rm.get_child(f\"seq:step:{i + 1}\")\n                            )\n                            for rm, config in zip(run_managers, configs)\n                        ],\n                        return_exceptions=return_exceptions,\n                        **(kwargs if i == 0 else {}),\n                    )\n        # finish the root runs\n        except BaseException as e:\n            await asyncio.gather(*(rm.on_chain_error(e) for rm in run_managers))\n            if return_exceptions:\n                return cast(list[Output], [e for _ in inputs])\n            else:\n                raise\n        else:\n            first_exception: Optional[Exception] = None\n            coros: list[Awaitable[None]] = []\n            for run_manager, out in zip(run_managers, inputs):\n                if isinstance(out, Exception):\n                    first_exception = first_exception or out\n                    coros.append(run_manager.on_chain_error(out))\n                else:\n                    coros.append(run_manager.on_chain_end(out))\n            await asyncio.gather(*coros)\n            if return_exceptions or first_exception is None:\n                return cast(list[Output], inputs)\n            else:\n                raise first_exception\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        from langchain_core.beta.runnables.context import config_with_context\n\n        steps = [self.first] + self.middle + [self.last]\n        config = config_with_context(config, self.steps)\n\n        # transform the input stream of each step with the next\n        # steps that don't natively support transforming an input stream will\n        # buffer input in memory until all available, and then start emitting output\n        final_pipeline = cast(Iterator[Output], input)\n        for idx, step in enumerate(steps):\n            config = patch_config(\n                config, callbacks=run_manager.get_child(f\"seq:step:{idx + 1}\")\n            )\n            if idx == 0:\n                final_pipeline = step.transform(final_pipeline, config, **kwargs)\n            else:\n                final_pipeline = step.transform(final_pipeline, config)\n\n        yield from final_pipeline\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        from langchain_core.beta.runnables.context import aconfig_with_context\n\n        steps = [self.first] + self.middle + [self.last]\n        config = aconfig_with_context(config, self.steps)\n\n        # stream the last steps\n        # transform the input stream of each step with the next\n        # steps that don't natively support transforming an input stream will\n        # buffer input in memory until all available, and then start emitting output\n        final_pipeline = cast(AsyncIterator[Output], input)\n        for idx, step in enumerate(steps):\n            config = patch_config(\n                config,\n                callbacks=run_manager.get_child(f\"seq:step:{idx + 1}\"),\n            )\n            if idx == 0:\n                final_pipeline = step.atransform(final_pipeline, config, **kwargs)\n            else:\n                final_pipeline = step.atransform(final_pipeline, config)\n        async for output in final_pipeline:\n            yield output\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self._transform_stream_with_config(\n            input,\n            self._transform,\n            patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),\n            **kwargs,\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self.transform(iter([input]), config, **kwargs)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for chunk in self._atransform_stream_with_config(\n            input,\n            self._atransform,\n            patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),\n            **kwargs,\n        ):\n            yield chunk\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\nclass RunnableParallel(RunnableSerializable[Input, dict[str, Any]]):\n    \"\"\"Runnable that runs a mapping of Runnables in parallel, and returns a mapping\n    of their outputs.\n\n    RunnableParallel is one of the two main composition primitives for the LCEL,\n    alongside RunnableSequence. It invokes Runnables concurrently, providing the same\n    input to each.\n\n    A RunnableParallel can be instantiated directly or by using a dict literal within a\n    sequence.\n\n    Here is a simple example that uses functions to illustrate the use of\n    RunnableParallel:\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            def mul_two(x: int) -> int:\n                return x * 2\n\n            def mul_three(x: int) -> int:\n                return x * 3\n\n            runnable_1 = RunnableLambda(add_one)\n            runnable_2 = RunnableLambda(mul_two)\n            runnable_3 = RunnableLambda(mul_three)\n\n            sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel\n                \"mul_two\": runnable_2,\n                \"mul_three\": runnable_3,\n            }\n            # Or equivalently:\n            # sequence = runnable_1 | RunnableParallel(\n            #     {\"mul_two\": runnable_2, \"mul_three\": runnable_3}\n            # )\n            # Also equivalently:\n            # sequence = runnable_1 | RunnableParallel(\n            #     mul_two=runnable_2,\n            #     mul_three=runnable_3,\n            # )\n\n            sequence.invoke(1)\n            await sequence.ainvoke(1)\n\n            sequence.batch([1, 2, 3])\n            await sequence.abatch([1, 2, 3])\n\n    RunnableParallel makes it easy to run Runnables in parallel. In the below example,\n    we simultaneously stream output from two different Runnables:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.runnables import RunnableParallel\n            from langchain_openai import ChatOpenAI\n\n            model = ChatOpenAI()\n            joke_chain = (\n                ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n                | model\n            )\n            poem_chain = (\n                ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n                | model\n            )\n\n            runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)\n\n            # Display stream\n            output = {key: \"\" for key, _ in runnable.output_schema()}\n            for chunk in runnable.stream({\"topic\": \"bear\"}):\n                for key in chunk:\n                    output[key] = output[key] + chunk[key].content\n                print(output)  # noqa: T201\n    \"\"\"\n\n    steps__: Mapping[str, Runnable[Input, Any]]\n\n    def __init__(\n        self,\n        steps__: Optional[\n            Mapping[\n                str,\n                Union[\n                    Runnable[Input, Any],\n                    Callable[[Input], Any],\n                    Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],\n                ],\n            ]\n        ] = None,\n        **kwargs: Union[\n            Runnable[Input, Any],\n            Callable[[Input], Any],\n            Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],\n        ],\n    ) -> None:\n        merged = {**steps__} if steps__ is not None else {}\n        merged.update(kwargs)\n        super().__init__(  # type: ignore[call-arg]\n            steps__={key: coerce_to_runnable(r) for key, r in merged.items()}\n        )\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        \"\"\"Get the name of the Runnable.\n\n        Args:\n            suffix: The suffix to use. Defaults to None.\n            name: The name to use. Defaults to None.\n\n        Returns:\n            The name of the Runnable.\n        \"\"\"\n        name = name or self.name or f\"RunnableParallel<{','.join(self.steps__.keys())}>\"\n        return super().get_name(suffix, name=name)\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        \"\"\"The type of the input to the Runnable.\"\"\"\n        for step in self.steps__.values():\n            if step.InputType:\n                return step.InputType\n\n        return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the input schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema of the Runnable.\n        \"\"\"\n        if all(\n            s.get_input_schema(config).model_json_schema().get(\"type\", \"object\")\n            == \"object\"\n            for s in self.steps__.values()\n        ):\n            # This is correct, but pydantic typings/mypy don't think so.\n            return create_model_v2(  # type: ignore[call-overload]\n                self.get_name(\"Input\"),\n                field_definitions={\n                    k: (v.annotation, v.default)\n                    for step in self.steps__.values()\n                    for k, v in step.get_input_schema(config).model_fields.items()\n                    if k != \"__root__\"\n                },\n            )\n\n        return super().get_input_schema(config)\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"Get the output schema of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The output schema of the Runnable.\n        \"\"\"\n        fields = {k: (v.OutputType, ...) for k, v in self.steps__.items()}\n        return create_model_v2(self.get_name(\"Output\"), field_definitions=fields)\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        \"\"\"Get the config specs of the Runnable.\n\n        Returns:\n            The config specs of the Runnable.\n        \"\"\"\n        return get_unique_config_specs(\n            spec for step in self.steps__.values() for spec in step.config_specs\n        )\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        \"\"\"Get the graph representation of the Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The graph representation of the Runnable.\n\n        Raises:\n            ValueError: If a Runnable has no first or last node.\n        \"\"\"\n        from langchain_core.runnables.graph import Graph\n\n        graph = Graph()\n        input_node = graph.add_node(self.get_input_schema(config))\n        output_node = graph.add_node(self.get_output_schema(config))\n        for step in self.steps__.values():\n            step_graph = step.get_graph()\n            step_graph.trim_first_node()\n            step_graph.trim_last_node()\n            if not step_graph:\n                graph.add_edge(input_node, output_node)\n            else:\n                step_first_node, step_last_node = graph.extend(step_graph)\n                if not step_first_node:\n                    msg = f\"Runnable {step} has no first node\"\n                    raise ValueError(msg)\n                if not step_last_node:\n                    msg = f\"Runnable {step} has no last node\"\n                    raise ValueError(msg)\n                graph.add_edge(input_node, step_first_node)\n                graph.add_edge(step_last_node, output_node)\n\n        return graph\n\n    def __repr__(self) -> str:\n        map_for_repr = \",\\n  \".join(\n            f\"{k}: {indent_lines_after_first(repr(v), '  ' + k + ': ')}\"\n            for k, v in self.steps__.items()\n        )\n        return \"{\\n  \" + map_for_repr + \"\\n}\"\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> dict[str, Any]:\n        from langchain_core.callbacks.manager import CallbackManager\n\n        # setup callbacks\n        config = ensure_config(config)\n        callback_manager = CallbackManager.configure(\n            inheritable_callbacks=config.get(\"callbacks\"),\n            local_callbacks=None,\n            verbose=False,\n            inheritable_tags=config.get(\"tags\"),\n            local_tags=None,\n            inheritable_metadata=config.get(\"metadata\"),\n            local_metadata=None,\n        )\n        # start the root run\n        run_manager = callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        def _invoke_step(\n            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n                # mark each step as a child run\n                callbacks=run_manager.get_child(f\"map:key:{key}\"),\n            )\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            return context.run(\n                step.invoke,\n                input,\n                child_config,\n            )\n\n        # gather results from all steps\n        try:\n            # copy to avoid issues from the caller mutating the steps during invoke()\n            steps = dict(self.steps__)\n\n            with get_executor_for_config(config) as executor:\n                futures = [\n                    executor.submit(_invoke_step, step, input, config, key)\n                    for key, step in steps.items()\n                ]\n                output = {key: future.result() for key, future in zip(steps, futures)}\n        # finish the root run\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise\n        else:\n            run_manager.on_chain_end(output)\n            return output\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> dict[str, Any]:\n        # setup callbacks\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        # start the root run\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\") or self.get_name(),\n            run_id=config.pop(\"run_id\", None),\n        )\n\n        async def _ainvoke_step(\n            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str\n        ) -> Any:\n            child_config = patch_config(\n                config,\n                callbacks=run_manager.get_child(f\"map:key:{key}\"),\n            )\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            if asyncio_accepts_context():\n                return await asyncio.create_task(  # type: ignore\n                    step.ainvoke(input, child_config), context=context\n                )\n            else:\n                return await asyncio.create_task(step.ainvoke(input, child_config))\n\n        # gather results from all steps\n        try:\n            # copy to avoid issues from the caller mutating the steps during invoke()\n            steps = dict(self.steps__)\n            results = await asyncio.gather(\n                *(\n                    _ainvoke_step(\n                        step,\n                        input,\n                        # mark each step as a child run\n                        config,\n                        key,\n                    )\n                    for key, step in steps.items()\n                )\n            )\n            output = dict(zip(steps, results))\n        # finish the root run\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n        else:\n            await run_manager.on_chain_end(output)\n            return output\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n    ) -> Iterator[AddableDict]:\n        # Shallow copy steps to ignore mutations while in progress\n        steps = dict(self.steps__)\n        # Each step gets a copy of the input iterator,\n        # which is consumed in parallel in a separate thread.\n        input_copies = list(safetee(input, len(steps), lock=threading.Lock()))\n        with get_executor_for_config(config) as executor:\n            # Create the transform() generator for each step\n            named_generators = [\n                (\n                    name,\n                    step.transform(\n                        input_copies.pop(),\n                        patch_config(\n                            config, callbacks=run_manager.get_child(f\"map:key:{name}\")\n                        ),\n                    ),\n                )\n                for name, step in steps.items()\n            ]\n            # Start the first iteration of each generator\n            futures = {\n                executor.submit(next, generator): (step_name, generator)\n                for step_name, generator in named_generators\n            }\n            # Yield chunks from each as they become available,\n            # and start the next iteration of that generator that yielded it.\n            # When all generators are exhausted, stop.\n            while futures:\n                completed_futures, _ = wait(futures, return_when=FIRST_COMPLETED)\n                for future in completed_futures:\n                    (step_name, generator) = futures.pop(future)\n                    try:\n                        chunk = AddableDict({step_name: future.result()})\n                        yield chunk\n                        futures[executor.submit(next, generator)] = (\n                            step_name,\n                            generator,\n                        )\n                    except StopIteration:\n                        pass\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[dict[str, Any]]:\n        yield from self._transform_stream_with_config(\n            input, self._transform, config, **kwargs\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[dict[str, Any]]:\n        yield from self.transform(iter([input]), config)\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n    ) -> AsyncIterator[AddableDict]:\n        # Shallow copy steps to ignore mutations while in progress\n        steps = dict(self.steps__)\n        # Each step gets a copy of the input iterator,\n        # which is consumed in parallel in a separate thread.\n        input_copies = list(atee(input, len(steps), lock=asyncio.Lock()))\n        # Create the transform() generator for each step\n        named_generators = [\n            (\n                name,\n                step.atransform(\n                    input_copies.pop(),\n                    patch_config(\n                        config, callbacks=run_manager.get_child(f\"map:key:{name}\")\n                    ),\n                ),\n            )\n            for name, step in steps.items()\n        ]\n\n        # Wrap in a coroutine to satisfy linter\n        async def get_next_chunk(generator: AsyncIterator) -> Optional[Output]:\n            return await py_anext(generator)\n\n        # Start the first iteration of each generator\n        tasks = {\n            asyncio.create_task(get_next_chunk(generator)): (step_name, generator)\n            for step_name, generator in named_generators\n        }\n        # Yield chunks from each as they become available,\n        # and start the next iteration of the generator that yielded it.\n        # When all generators are exhausted, stop.\n        while tasks:\n            completed_tasks, _ = await asyncio.wait(\n                tasks, return_when=asyncio.FIRST_COMPLETED\n            )\n            for task in completed_tasks:\n                (step_name, generator) = tasks.pop(task)\n                try:\n                    chunk = AddableDict({step_name: task.result()})\n                    yield chunk\n                    new_task = asyncio.create_task(get_next_chunk(generator))\n                    tasks[new_task] = (step_name, generator)\n                except StopAsyncIteration:\n                    pass\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[dict[str, Any]]:\n        async for chunk in self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        ):\n            yield chunk\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[dict[str, Any]]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config):\n            yield chunk\n\n\n# We support both names\nRunnableMap = RunnableParallel\n\n\nclass RunnableGenerator(Runnable[Input, Output]):\n    \"\"\"Runnable that runs a generator function.\n\n    RunnableGenerators can be instantiated directly or by using a generator within\n    a sequence.\n\n    RunnableGenerators can be used to implement custom behavior, such as custom output\n    parsers, while preserving streaming capabilities. Given a generator function with\n    a signature Iterator[A] -> Iterator[B], wrapping it in a RunnableGenerator allows\n    it to emit output chunks as soon as they are streamed in from the previous step.\n\n    Note that if a generator function has a signature A -> Iterator[B], such that it\n    requires its input from the previous step to be completed before emitting chunks\n    (e.g., most LLMs need the entire prompt available to start generating), it can\n    instead be wrapped in a RunnableLambda.\n\n    Here is an example to show the basic mechanics of a RunnableGenerator:\n\n        .. code-block:: python\n\n            from typing import Any, AsyncIterator, Iterator\n\n            from langchain_core.runnables import RunnableGenerator\n\n\n            def gen(input: Iterator[Any]) -> Iterator[str]:\n                for token in [\"Have\", \" a\", \" nice\", \" day\"]:\n                    yield token\n\n\n            runnable = RunnableGenerator(gen)\n            runnable.invoke(None)  # \"Have a nice day\"\n            list(runnable.stream(None))  # [\"Have\", \" a\", \" nice\", \" day\"]\n            runnable.batch([None, None])  # [\"Have a nice day\", \"Have a nice day\"]\n\n\n            # Async version:\n            async def agen(input: AsyncIterator[Any]) -> AsyncIterator[str]:\n                for token in [\"Have\", \" a\", \" nice\", \" day\"]:\n                    yield token\n\n            runnable = RunnableGenerator(agen)\n            await runnable.ainvoke(None)  # \"Have a nice day\"\n            [p async for p in runnable.astream(None)] # [\"Have\", \" a\", \" nice\", \" day\"]\n\n    RunnableGenerator makes it easy to implement custom behavior within a streaming\n    context. Below we show an example:\n\n        .. code-block:: python\n\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.runnables import RunnableGenerator, RunnableLambda\n            from langchain_openai import ChatOpenAI\n            from langchain_core.output_parsers import StrOutputParser\n\n\n            model = ChatOpenAI()\n            chant_chain = (\n                ChatPromptTemplate.from_template(\"Give me a 3 word chant about {topic}\")\n                | model\n                | StrOutputParser()\n            )\n\n            def character_generator(input: Iterator[str]) -> Iterator[str]:\n                for token in input:\n                    if \",\" in token or \".\" in token:\n                        yield \"\ud83d\udc4f\" + token\n                    else:\n                        yield token\n\n\n            runnable = chant_chain | character_generator\n            assert type(runnable.last) is RunnableGenerator\n            \"\".join(runnable.stream({\"topic\": \"waste\"})) # Reduce\ud83d\udc4f, Reuse\ud83d\udc4f, Recycle\ud83d\udc4f.\n\n            # Note that RunnableLambda can be used to delay streaming of one step in a\n            # sequence until the previous step is finished:\n            def reverse_generator(input: str) -> Iterator[str]:\n                # Yield characters of input in reverse order.\n                for character in input[::-1]:\n                    yield character\n\n            runnable = chant_chain | RunnableLambda(reverse_generator)\n            \"\".join(runnable.stream({\"topic\": \"waste\"}))  # \".elcycer ,esuer ,ecudeR\"\n    \"\"\"\n\n    def __init__(\n        self,\n        transform: Union[\n            Callable[[Iterator[Input]], Iterator[Output]],\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n        ],\n        atransform: Optional[\n            Callable[[AsyncIterator[Input]], AsyncIterator[Output]]\n        ] = None,\n        *,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Initialize a RunnableGenerator.\n\n        Args:\n            transform: The transform function.\n            atransform: The async transform function. Defaults to None.\n\n        Raises:\n            TypeError: If the transform is not a generator function.\n        \"\"\"\n        if atransform is not None:\n            self._atransform = atransform\n            func_for_name: Callable = atransform\n\n        if is_async_generator(transform):\n            self._atransform = transform  # type: ignore[assignment]\n            func_for_name = transform\n        elif inspect.isgeneratorfunction(transform):\n            self._transform = transform\n            func_for_name = transform\n        else:\n            msg = (\n                \"Expected a generator function type for `transform`.\"\n                f\"Instead got an unsupported type: {type(transform)}\"\n            )\n            raise TypeError(msg)\n\n        try:\n            self.name = name or func_for_name.__name__\n        except AttributeError:\n            self.name = \"RunnableGenerator\"\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        func = getattr(self, \"_transform\", None) or self._atransform\n        try:\n            params = inspect.signature(func).parameters\n            first_param = next(iter(params.values()), None)\n            if first_param and first_param.annotation != inspect.Parameter.empty:\n                return getattr(first_param.annotation, \"__args__\", (Any,))[0]\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable generator, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.InputType\n\n        func = getattr(self, \"_transform\", None) or self._atransform\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        func = getattr(self, \"_transform\", None) or self._atransform\n        try:\n            sig = inspect.signature(func)\n            return (\n                getattr(sig.return_annotation, \"__args__\", (Any,))[0]\n                if sig.return_annotation != inspect.Signature.empty\n                else Any\n            )\n        except ValueError:\n            return Any\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable generator, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.OutputType\n        func = getattr(self, \"_transform\", None) or self._atransform\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, RunnableGenerator):\n            if hasattr(self, \"_transform\") and hasattr(other, \"_transform\"):\n                return self._transform == other._transform\n            elif hasattr(self, \"_atransform\") and hasattr(other, \"_atransform\"):\n                return self._atransform == other._atransform\n            else:\n                return False\n        else:\n            return False\n\n    def __repr__(self) -> str:\n        return f\"RunnableGenerator({self.name})\"\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        if not hasattr(self, \"_transform\"):\n            msg = f\"{repr(self)} only supports async methods.\"\n            raise NotImplementedError(msg)\n        return self._transform_stream_with_config(\n            input,\n            self._transform,  # type: ignore[arg-type]\n            config,\n            **kwargs,  # type: ignore[arg-type]\n        )\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        final: Optional[Output] = None\n        for output in self.stream(input, config, **kwargs):\n            final = output if final is None else final + output  # type: ignore[operator]\n        return cast(Output, final)\n\n    def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        if not hasattr(self, \"_atransform\"):\n            msg = f\"{repr(self)} only supports sync methods.\"\n            raise NotImplementedError(msg)\n\n        return self._atransform_stream_with_config(\n            input, self._atransform, config, **kwargs\n        )\n\n    def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        return self.atransform(input_aiter(), config, **kwargs)\n\n    async def ainvoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        final: Optional[Output] = None\n        async for output in self.astream(input, config, **kwargs):\n            final = output if final is None else final + output  # type: ignore[operator]\n        return cast(Output, final)\n\n\nclass RunnableLambda(Runnable[Input, Output]):\n    \"\"\"RunnableLambda converts a python callable into a Runnable.\n\n    Wrapping a callable in a RunnableLambda makes the callable usable\n    within either a sync or async context.\n\n    RunnableLambda can be composed as any other Runnable and provides\n    seamless integration with LangChain tracing.\n\n    ``RunnableLambda`` is best suited for code that does not need to support\n    streaming. If you need to support streaming (i.e., be able to operate\n    on chunks of inputs and yield chunks of outputs), use ``RunnableGenerator``\n    instead.\n\n    Note that if a ``RunnableLambda`` returns an instance of ``Runnable``, that\n    instance is invoked (or streamed) during execution.\n\n    Examples:\n\n        .. code-block:: python\n\n            # This is a RunnableLambda\n            from langchain_core.runnables import RunnableLambda\n\n            def add_one(x: int) -> int:\n                return x + 1\n\n            runnable = RunnableLambda(add_one)\n\n            runnable.invoke(1) # returns 2\n            runnable.batch([1, 2, 3]) # returns [2, 3, 4]\n\n            # Async is supported by default by delegating to the sync implementation\n            await runnable.ainvoke(1) # returns 2\n            await runnable.abatch([1, 2, 3]) # returns [2, 3, 4]\n\n\n            # Alternatively, can provide both synd and sync implementations\n            async def add_one_async(x: int) -> int:\n                return x + 1\n\n            runnable = RunnableLambda(add_one, afunc=add_one_async)\n            runnable.invoke(1) # Uses add_one\n            await runnable.ainvoke(1) # Uses add_one_async\n    \"\"\"\n\n    def __init__(\n        self,\n        func: Union[\n            Union[\n                Callable[[Input], Output],\n                Callable[[Input], Iterator[Output]],\n                Callable[[Input, RunnableConfig], Output],\n                Callable[[Input, CallbackManagerForChainRun], Output],\n                Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],\n            ],\n            Union[\n                Callable[[Input], Awaitable[Output]],\n                Callable[[Input], AsyncIterator[Output]],\n                Callable[[Input, RunnableConfig], Awaitable[Output]],\n                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n                Callable[\n                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                    Awaitable[Output],\n                ],\n            ],\n        ],\n        afunc: Optional[\n            Union[\n                Callable[[Input], Awaitable[Output]],\n                Callable[[Input], AsyncIterator[Output]],\n                Callable[[Input, RunnableConfig], Awaitable[Output]],\n                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],\n                Callable[\n                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],\n                    Awaitable[Output],\n                ],\n            ]\n        ] = None,\n        name: Optional[str] = None,\n    ) -> None:\n        \"\"\"Create a RunnableLambda from a callable, and async callable or both.\n\n        Accepts both sync and async variants to allow providing efficient\n        implementations for sync and async execution.\n\n        Args:\n            func: Either sync or async callable\n            afunc: An async callable that takes an input and returns an output.\n                Defaults to None.\n            name: The name of the Runnable. Defaults to None.\n\n        Raises:\n            TypeError: If the func is not a callable type.\n            TypeError: If both func and afunc are provided.\n        \"\"\"\n        if afunc is not None:\n            self.afunc = afunc\n            func_for_name: Callable = afunc\n\n        if is_async_callable(func) or is_async_generator(func):\n            if afunc is not None:\n                msg = (\n                    \"Func was provided as a coroutine function, but afunc was \"\n                    \"also provided. If providing both, func should be a regular \"\n                    \"function to avoid ambiguity.\"\n                )\n                raise TypeError(msg)\n            self.afunc = func\n            func_for_name = func\n        elif callable(func):\n            self.func = cast(Callable[[Input], Output], func)\n            func_for_name = func\n        else:\n            msg = (\n                \"Expected a callable type for `func`.\"\n                f\"Instead got an unsupported type: {type(func)}\"\n            )\n            raise TypeError(msg)\n\n        try:\n            if name is not None:\n                self.name = name\n            elif func_for_name.__name__ != \"<lambda>\":\n                self.name = func_for_name.__name__\n        except AttributeError:\n            pass\n\n        self._repr: Optional[str] = None\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        \"\"\"The type of the input to this Runnable.\"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n        try:\n            params = inspect.signature(func).parameters\n            first_param = next(iter(params.values()), None)\n            if first_param and first_param.annotation != inspect.Parameter.empty:\n                return first_param.annotation\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"The pydantic schema for the input to this Runnable.\n\n        Args:\n            config: The config to use. Defaults to None.\n\n        Returns:\n            The input schema for this Runnable.\n        \"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n\n        if isinstance(func, itemgetter):\n            # This is terrible, but afaict it's not possible to access _items\n            # on itemgetter objects, so we have to parse the repr\n            items = str(func).replace(\"operator.itemgetter(\", \"\")[:-1].split(\", \")\n            if all(\n                item[0] == \"'\" and item[-1] == \"'\" and len(item) > 2 for item in items\n            ):\n                fields = {item[1:-1]: (Any, ...) for item in items}\n                # It's a dict, lol\n                return create_model_v2(self.get_name(\"Input\"), field_definitions=fields)\n            else:\n                module = getattr(func, \"__module__\", None)\n                return create_model_v2(\n                    self.get_name(\"Input\"),\n                    root=list[Any],\n                    # To create the schema, we need to provide the module\n                    # where the underlying function is defined.\n                    # This allows pydantic to resolve type annotations appropriately.\n                    module_name=module,\n                )\n\n        if self.InputType != Any:\n            return super().get_input_schema(config)\n\n        if dict_keys := get_function_first_arg_dict_keys(func):\n            return create_model_v2(\n                self.get_name(\"Input\"),\n                field_definitions=dict.fromkeys(dict_keys, (Any, ...)),\n            )\n\n        return super().get_input_schema(config)\n\n    @property\n    @override\n    def OutputType(self) -> Any:\n        \"\"\"The type of the output of this Runnable as a type annotation.\n\n        Returns:\n            The type of the output of this Runnable.\n        \"\"\"\n        func = getattr(self, \"func\", None) or self.afunc\n        try:\n            sig = inspect.signature(func)\n            if sig.return_annotation != inspect.Signature.empty:\n                # unwrap iterator types\n                if getattr(sig.return_annotation, \"__origin__\", None) in (\n                    collections.abc.Iterator,\n                    collections.abc.AsyncIterator,\n                ):\n                    return getattr(sig.return_annotation, \"__args__\", (Any,))[0]\n                return sig.return_annotation\n            else:\n                return Any\n        except ValueError:\n            return Any\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        # Override the default implementation.\n        # For a runnable lambda, we need to bring to provide the\n        # module of the underlying function when creating the model.\n        root_type = self.OutputType\n        func = getattr(self, \"func\", None) or self.afunc\n        module = getattr(func, \"__module__\", None)\n\n        if (\n            inspect.isclass(root_type)\n            and not isinstance(root_type, GenericAlias)\n            and issubclass(root_type, BaseModel)\n        ):\n            return root_type\n\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=root_type,\n            # To create the schema, we need to provide the module\n            # where the underlying function is defined.\n            # This allows pydantic to resolve type annotations appropriately.\n            module_name=module,\n        )\n\n    @functools.cached_property\n    def deps(self) -> list[Runnable]:\n        \"\"\"The dependencies of this Runnable.\n\n        Returns:\n            The dependencies of this Runnable. If the function has nonlocal\n            variables that are Runnables, they are considered dependencies.\n        \"\"\"\n        if hasattr(self, \"func\"):\n            objects = get_function_nonlocals(self.func)\n        elif hasattr(self, \"afunc\"):\n            objects = get_function_nonlocals(self.afunc)\n        else:\n            objects = []\n\n        deps: list[Runnable] = []\n        for obj in objects:\n            if isinstance(obj, Runnable):\n                deps.append(obj)\n            elif isinstance(getattr(obj, \"__self__\", None), Runnable):\n                deps.append(obj.__self__)\n        return deps\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return get_unique_config_specs(\n            spec for dep in self.deps for spec in dep.config_specs\n        )\n\n    def get_graph(self, config: RunnableConfig | None = None) -> Graph:\n        if deps := self.deps:\n            graph = Graph()\n            input_node = graph.add_node(self.get_input_schema(config))\n            output_node = graph.add_node(self.get_output_schema(config))\n            for dep in deps:\n                dep_graph = dep.get_graph()\n                dep_graph.trim_first_node()\n                dep_graph.trim_last_node()\n                if not dep_graph:\n                    graph.add_edge(input_node, output_node)\n                else:\n                    dep_first_node, dep_last_node = graph.extend(dep_graph)\n                    if not dep_first_node:\n                        msg = f\"Runnable {dep} has no first node\"\n                        raise ValueError(msg)\n                    if not dep_last_node:\n                        msg = f\"Runnable {dep} has no last node\"\n                        raise ValueError(msg)\n                    graph.add_edge(input_node, dep_first_node)\n                    graph.add_edge(dep_last_node, output_node)\n        else:\n            graph = super().get_graph(config)\n\n        return graph\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, RunnableLambda):\n            if hasattr(self, \"func\") and hasattr(other, \"func\"):\n                return self.func == other.func\n            elif hasattr(self, \"afunc\") and hasattr(other, \"afunc\"):\n                return self.afunc == other.afunc\n            else:\n                return False\n        else:\n            return False\n\n    def __repr__(self) -> str:\n        \"\"\"A string representation of this Runnable.\"\"\"\n        if self._repr is None:\n            if hasattr(self, \"func\") and isinstance(self.func, itemgetter):\n                self._repr = f\"RunnableLambda({str(self.func)[len('operator.') :]})\"\n            elif hasattr(self, \"func\"):\n                self._repr = f\"RunnableLambda({get_lambda_source(self.func) or '...'})\"\n            elif hasattr(self, \"afunc\"):\n                self._repr = (\n                    f\"RunnableLambda(afunc={get_lambda_source(self.afunc) or '...'})\"\n                )\n            else:\n                self._repr = \"RunnableLambda(...)\"\n        return self._repr\n\n    def _invoke(\n        self,\n        input: Input,\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Output:\n        if inspect.isgeneratorfunction(self.func):\n            output: Optional[Output] = None\n            for chunk in call_func_with_variable_args(\n                cast(Callable[[Input], Iterator[Output]], self.func),\n                input,\n                config,\n                run_manager,\n                **kwargs,\n            ):\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk  # type: ignore[operator]\n                    except TypeError:\n                        output = chunk\n        else:\n            output = call_func_with_variable_args(\n                self.func, input, config, run_manager, **kwargs\n            )\n        # If the output is a Runnable, invoke it\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {input}.\"\n                )\n                raise RecursionError(msg)\n            output = output.invoke(\n                input,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            )\n        return cast(Output, output)\n\n    async def _ainvoke(\n        self,\n        input: Input,\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Output:\n        if hasattr(self, \"afunc\"):\n            afunc = self.afunc\n        else:\n            if inspect.isgeneratorfunction(self.func):\n\n                def func(\n                    input: Input,\n                    run_manager: AsyncCallbackManagerForChainRun,\n                    config: RunnableConfig,\n                    **kwargs: Any,\n                ) -> Output:\n                    output: Optional[Output] = None\n                    for chunk in call_func_with_variable_args(\n                        cast(Callable[[Input], Iterator[Output]], self.func),\n                        input,\n                        config,\n                        run_manager.get_sync(),\n                        **kwargs,\n                    ):\n                        if output is None:\n                            output = chunk\n                        else:\n                            try:\n                                output = output + chunk  # type: ignore[operator]\n                            except TypeError:\n                                output = chunk\n                    return cast(Output, output)\n\n            else:\n\n                def func(\n                    input: Input,\n                    run_manager: AsyncCallbackManagerForChainRun,\n                    config: RunnableConfig,\n                    **kwargs: Any,\n                ) -> Output:\n                    return call_func_with_variable_args(\n                        self.func, input, config, run_manager.get_sync(), **kwargs\n                    )\n\n            @wraps(func)\n            async def f(*args, **kwargs):  # type: ignore[no-untyped-def]\n                return await run_in_executor(config, func, *args, **kwargs)\n\n            afunc = f\n\n        if is_async_generator(afunc):\n            output: Optional[Output] = None\n            async with aclosing(\n                cast(\n                    AsyncGenerator[Any, Any],\n                    acall_func_with_variable_args(\n                        cast(Callable, afunc),\n                        input,\n                        config,\n                        run_manager,\n                        **kwargs,\n                    ),\n                )\n            ) as stream:\n                async for chunk in cast(\n                    AsyncIterator[Output],\n                    stream,\n                ):\n                    if output is None:\n                        output = chunk\n                    else:\n                        try:\n                            output = output + chunk  # type: ignore[operator]\n                        except TypeError:\n                            output = chunk\n        else:\n            output = await acall_func_with_variable_args(\n                cast(Callable, afunc), input, config, run_manager, **kwargs\n            )\n        # If the output is a Runnable, invoke it\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {input}.\"\n                )\n                raise RecursionError(msg)\n            output = await output.ainvoke(\n                input,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            )\n        return cast(Output, output)\n\n    def _config(\n        self, config: Optional[RunnableConfig], callable: Callable[..., Any]\n    ) -> RunnableConfig:\n        return ensure_config(config)\n\n    def invoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Invoke this Runnable synchronously.\n\n        Args:\n            input: The input to this Runnable.\n            config: The config to use. Defaults to None.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            The output of this Runnable.\n\n        Raises:\n            TypeError: If the Runnable is a coroutine function.\n        \"\"\"\n        if hasattr(self, \"func\"):\n            return self._call_with_config(\n                self._invoke,\n                input,\n                self._config(config, self.func),\n                **kwargs,\n            )\n        else:\n            msg = (\n                \"Cannot invoke a coroutine function synchronously.\"\n                \"Use `ainvoke` instead.\"\n            )\n            raise TypeError(msg)\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        \"\"\"Invoke this Runnable asynchronously.\n\n        Args:\n            input: The input to this Runnable.\n            config: The config to use. Defaults to None.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            The output of this Runnable.\n        \"\"\"\n        the_func = self.afunc if hasattr(self, \"afunc\") else self.func\n        return await self._acall_with_config(\n            self._ainvoke,\n            input,\n            self._config(config, the_func),\n            **kwargs,\n        )\n\n    def _transform(\n        self,\n        input: Iterator[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        final: Input\n        got_first_val = False\n        for ichunk in input:\n            # By definitions, RunnableLambdas consume all input before emitting output.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk.\n            # So we'll iterate until we get to the last chunk!\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if inspect.isgeneratorfunction(self.func):\n            output: Optional[Output] = None\n            for chunk in call_func_with_variable_args(\n                self.func, cast(Input, final), config, run_manager, **kwargs\n            ):\n                yield chunk\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk\n                    except TypeError:\n                        output = chunk\n        else:\n            output = call_func_with_variable_args(\n                self.func, cast(Input, final), config, run_manager, **kwargs\n            )\n\n        # If the output is a Runnable, use its stream output\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {final}.\"\n                )\n                raise RecursionError(msg)\n            for chunk in output.stream(\n                final,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            ):\n                yield chunk\n        elif not inspect.isgeneratorfunction(self.func):\n            # Otherwise, just yield it\n            yield cast(Output, output)\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        if hasattr(self, \"func\"):\n            yield from self._transform_stream_with_config(\n                input,\n                self._transform,\n                self._config(config, self.func),\n                **kwargs,\n            )\n        else:\n            msg = (\n                \"Cannot stream a coroutine function synchronously.\"\n                \"Use `astream` instead.\"\n            )\n            raise TypeError(msg)\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        return self.transform(iter([input]), config, **kwargs)\n\n    async def _atransform(\n        self,\n        input: AsyncIterator[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        final: Input\n        got_first_val = False\n        async for ichunk in input:\n            # By definitions, RunnableLambdas consume all input before emitting output.\n            # If the input is not addable, then we'll assume that we can\n            # only operate on the last chunk.\n            # So we'll iterate until we get to the last chunk!\n            if not got_first_val:\n                final = ichunk\n                got_first_val = True\n            else:\n                try:\n                    final = final + ichunk  # type: ignore[operator]\n                except TypeError:\n                    final = ichunk\n\n        if hasattr(self, \"afunc\"):\n            afunc = self.afunc\n        else:\n            if inspect.isgeneratorfunction(self.func):\n                msg = (\n                    \"Cannot stream from a generator function asynchronously.\"\n                    \"Use .stream() instead.\"\n                )\n                raise TypeError(msg)\n\n            def func(\n                input: Input,\n                run_manager: AsyncCallbackManagerForChainRun,\n                config: RunnableConfig,\n                **kwargs: Any,\n            ) -> Output:\n                return call_func_with_variable_args(\n                    self.func, input, config, run_manager.get_sync(), **kwargs\n                )\n\n            @wraps(func)\n            async def f(*args, **kwargs):  # type: ignore[no-untyped-def]\n                return await run_in_executor(config, func, *args, **kwargs)\n\n            afunc = f\n\n        if is_async_generator(afunc):\n            output: Optional[Output] = None\n            async for chunk in cast(\n                AsyncIterator[Output],\n                acall_func_with_variable_args(\n                    cast(Callable, afunc),\n                    cast(Input, final),\n                    config,\n                    run_manager,\n                    **kwargs,\n                ),\n            ):\n                yield chunk\n                if output is None:\n                    output = chunk\n                else:\n                    try:\n                        output = output + chunk  # type: ignore[operator]\n                    except TypeError:\n                        output = chunk\n        else:\n            output = await acall_func_with_variable_args(\n                cast(Callable, afunc), cast(Input, final), config, run_manager, **kwargs\n            )\n\n        # If the output is a Runnable, use its astream output\n        if isinstance(output, Runnable):\n            recursion_limit = config[\"recursion_limit\"]\n            if recursion_limit <= 0:\n                msg = (\n                    f\"Recursion limit reached when invoking {self} with input {final}.\"\n                )\n                raise RecursionError(msg)\n            async for chunk in output.astream(\n                final,\n                patch_config(\n                    config,\n                    callbacks=run_manager.get_child(),\n                    recursion_limit=recursion_limit - 1,\n                ),\n            ):\n                yield chunk\n        elif not is_async_generator(afunc):\n            # Otherwise, just yield it\n            yield cast(Output, output)\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for output in self._atransform_stream_with_config(\n            input,\n            self._atransform,\n            self._config(config, self.afunc if hasattr(self, \"afunc\") else self.func),\n            **kwargs,\n        ):\n            yield output\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async def input_aiter() -> AsyncIterator[Input]:\n            yield input\n\n        async for chunk in self.atransform(input_aiter(), config, **kwargs):\n            yield chunk\n\n\nclass RunnableEachBase(RunnableSerializable[list[Input], list[Output]]):\n    \"\"\"Runnable that delegates calls to another Runnable\n    with each element of the input sequence.\n\n    Use only if creating a new RunnableEach subclass with different __init__ args.\n\n    See documentation for RunnableEach for more details.\n    \"\"\"\n\n    bound: Runnable[Input, Output]\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    @override\n    def InputType(self) -> Any:\n        return list[self.bound.InputType]  # type: ignore[name-defined]\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        return create_model_v2(\n            self.get_name(\"Input\"),\n            root=(\n                list[self.bound.get_input_schema(config)],  # type: ignore\n                None,\n            ),\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    @property\n    @override\n    def OutputType(self) -> type[list[Output]]:\n        return list[self.bound.OutputType]  # type: ignore[name-defined]\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        schema = self.bound.get_output_schema(config)\n        return create_model_v2(\n            self.get_name(\"Output\"),\n            root=list[schema],  # type: ignore[valid-type]\n            # create model needs access to appropriate type annotations to be\n            # able to construct the pydantic model.\n            # When we create the model, we pass information about the namespace\n            # where the model is being created, so the type annotations can\n            # be resolved correctly as well.\n            # self.__class__.__module__ handles the case when the Runnable is\n            # being sub-classed in a different module.\n            module_name=self.__class__.__module__,\n        )\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return self.bound.config_specs\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        return self.bound.get_graph(config)\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def _invoke(\n        self,\n        inputs: list[Input],\n        run_manager: CallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> list[Output]:\n        configs = [\n            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs\n        ]\n        return self.bound.batch(inputs, configs, **kwargs)\n\n    def invoke(\n        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> list[Output]:\n        return self._call_with_config(self._invoke, input, config, **kwargs)\n\n    async def _ainvoke(\n        self,\n        inputs: list[Input],\n        run_manager: AsyncCallbackManagerForChainRun,\n        config: RunnableConfig,\n        **kwargs: Any,\n    ) -> list[Output]:\n        configs = [\n            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs\n        ]\n        return await self.bound.abatch(inputs, configs, **kwargs)\n\n    async def ainvoke(\n        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> list[Output]:\n        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)\n\n    async def astream_events(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[StreamEvent]:\n        for _ in range(1):\n            msg = \"RunnableEach does not support astream_events yet.\"\n            raise NotImplementedError(msg)\n            yield\n\n\nclass RunnableEach(RunnableEachBase[Input, Output]):\n    \"\"\"Runnable that delegates calls to another Runnable\n    with each element of the input sequence.\n\n    It allows you to call multiple inputs with the bounded Runnable.\n\n    RunnableEach makes it easy to run multiple inputs for the Runnable.\n    In the below example, we associate and run three inputs\n    with a Runnable:\n\n        .. code-block:: python\n\n            from langchain_core.runnables.base import RunnableEach\n            from langchain_openai import ChatOpenAI\n            from langchain_core.prompts import ChatPromptTemplate\n            from langchain_core.output_parsers import StrOutputParser\n            prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about\n            {topic}\")\n            model = ChatOpenAI()\n            output_parser = StrOutputParser()\n            runnable = prompt | model | output_parser\n            runnable_each = RunnableEach(bound=runnable)\n            output = runnable_each.invoke([{'topic':'Computer Science'},\n                                        {'topic':'Art'},\n                                        {'topic':'Biology'}])\n            print(output)  # noqa: T201\n    \"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        name = name or self.name or f\"RunnableEach<{self.bound.get_name()}>\"\n        return super().get_name(suffix, name=name)\n\n    def bind(self, **kwargs: Any) -> RunnableEach[Input, Output]:\n        return RunnableEach(bound=self.bound.bind(**kwargs))\n\n    def with_config(\n        self, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> RunnableEach[Input, Output]:\n        return RunnableEach(bound=self.bound.with_config(config, **kwargs))\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> RunnableEach[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called before the Runnable starts running, with the Run object.\n                Defaults to None.\n            on_end: Called after the Runnable finishes running, with the Run object.\n                Defaults to None.\n            on_error: Called if the Runnable throws an error, with the Run object.\n                Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n        \"\"\"\n        return RunnableEach(\n            bound=self.bound.with_listeners(\n                on_start=on_start, on_end=on_end, on_error=on_error\n            )\n        )\n\n    def with_alisteners(\n        self,\n        *,\n        on_start: Optional[AsyncListener] = None,\n        on_end: Optional[AsyncListener] = None,\n        on_error: Optional[AsyncListener] = None,\n    ) -> RunnableEach[Input, Output]:\n        \"\"\"Bind async lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called asynchronously before the Runnable starts running,\n                      with the Run object. Defaults to None.\n            on_end: Called asynchronously after the Runnable finishes running,\n                    with the Run object. Defaults to None.\n            on_error: Called asynchronously if the Runnable throws an error,\n                    with the Run object. Defaults to None.\n\n        Returns:\n            A new Runnable with the listeners bound.\n\n        The Run object contains information about the run, including its id,\n        type, input, output, error, start_time, end_time, and any tags or metadata\n        added to the run.\n        \"\"\"\n        return RunnableEach(\n            bound=self.bound.with_alisteners(\n                on_start=on_start, on_end=on_end, on_error=on_error\n            )\n        )\n\n\nclass RunnableBindingBase(RunnableSerializable[Input, Output]):\n    \"\"\"Runnable that delegates calls to another Runnable with a set of kwargs.\n\n    Use only if creating a new RunnableBinding subclass with different __init__ args.\n\n    See documentation for RunnableBinding for more details.\n    \"\"\"\n\n    bound: Runnable[Input, Output]\n    \"\"\"The underlying Runnable that this Runnable delegates to.\"\"\"\n\n    kwargs: Mapping[str, Any] = Field(default_factory=dict)\n    \"\"\"kwargs to pass to the underlying Runnable when running.\n\n    For example, when the Runnable binding is invoked the underlying\n    Runnable will be invoked with the same input but with these additional\n    kwargs.\n    \"\"\"\n\n    config: RunnableConfig = Field(default_factory=RunnableConfig)  # type: ignore\n    \"\"\"The config to bind to the underlying Runnable.\"\"\"\n\n    config_factories: list[Callable[[RunnableConfig], RunnableConfig]] = Field(\n        default_factory=list\n    )\n    \"\"\"The config factories to bind to the underlying Runnable.\"\"\"\n\n    # Union[Type[Input], BaseModel] + things like List[str]\n    custom_input_type: Optional[Any] = None\n    \"\"\"Override the input type of the underlying Runnable with a custom type.\n\n    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n    \"\"\"\n    # Union[Type[Output], BaseModel] + things like List[str]\n    custom_output_type: Optional[Any] = None\n    \"\"\"Override the output type of the underlying Runnable with a custom type.\n\n    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).\n    \"\"\"\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    def __init__(\n        self,\n        *,\n        bound: Runnable[Input, Output],\n        kwargs: Optional[Mapping[str, Any]] = None,\n        config: Optional[RunnableConfig] = None,\n        config_factories: Optional[\n            list[Callable[[RunnableConfig], RunnableConfig]]\n        ] = None,\n        custom_input_type: Optional[Union[type[Input], BaseModel]] = None,\n        custom_output_type: Optional[Union[type[Output], BaseModel]] = None,\n        **other_kwargs: Any,\n    ) -> None:\n        \"\"\"Create a RunnableBinding from a Runnable and kwargs.\n\n        Args:\n            bound: The underlying Runnable that this Runnable delegates calls to.\n            kwargs: optional kwargs to pass to the underlying Runnable, when running\n                    the underlying Runnable (e.g., via `invoke`, `batch`,\n                    `transform`, or `stream` or async variants)\n                    Defaults to None.\n            config: optional config to bind to the underlying Runnable.\n                    Defaults to None.\n            config_factories: optional list of config factories to apply to the\n                    config before binding to the underlying Runnable.\n                    Defaults to None.\n            custom_input_type: Specify to override the input type of the underlying\n                               Runnable with a custom type. Defaults to None.\n            custom_output_type: Specify to override the output type of the underlying\n                Runnable with a custom type. Defaults to None.\n            **other_kwargs: Unpacked into the base class.\n        \"\"\"\n        super().__init__(  # type: ignore[call-arg]\n            bound=bound,\n            kwargs=kwargs or {},\n            config=config or {},\n            config_factories=config_factories or [],\n            custom_input_type=custom_input_type,\n            custom_output_type=custom_output_type,\n            **other_kwargs,\n        )\n        # if we don't explicitly set config to the TypedDict here,\n        # the pydantic init above will strip out any of the \"extra\"\n        # fields even though total=False on the typed dict.\n        self.config = config or {}\n\n    def get_name(\n        self, suffix: Optional[str] = None, *, name: Optional[str] = None\n    ) -> str:\n        return self.bound.get_name(suffix, name=name)\n\n    @property\n    @override\n    def InputType(self) -> type[Input]:\n        return (\n            cast(type[Input], self.custom_input_type)\n            if self.custom_input_type is not None\n            else self.bound.InputType\n        )\n\n    @property\n    @override\n    def OutputType(self) -> type[Output]:\n        return (\n            cast(type[Output], self.custom_output_type)\n            if self.custom_output_type is not None\n            else self.bound.OutputType\n        )\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        if self.custom_input_type is not None:\n            return super().get_input_schema(config)\n        return self.bound.get_input_schema(merge_configs(self.config, config))\n\n    def get_output_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        if self.custom_output_type is not None:\n            return super().get_output_schema(config)\n        return self.bound.get_output_schema(merge_configs(self.config, config))\n\n    @property\n    def config_specs(self) -> list[ConfigurableFieldSpec]:\n        return self.bound.config_specs\n\n    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:\n        return self.bound.get_graph(self._merge_configs(config))\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:\n        config = merge_configs(self.config, *configs)\n        return merge_configs(config, *(f(config) for f in self.config_factories))\n\n    def invoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        return self.bound.invoke(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def ainvoke(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Output:\n        return await self.bound.ainvoke(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    def batch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if isinstance(config, list):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        return self.bound.batch(\n            inputs,\n            configs,\n            return_exceptions=return_exceptions,\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def abatch(\n        self,\n        inputs: list[Input],\n        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> list[Output]:\n        if isinstance(config, list):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        return await self.bound.abatch(\n            inputs,\n            configs,\n            return_exceptions=return_exceptions,\n            **{**self.kwargs, **kwargs},\n        )\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Output]]: ...\n\n    @overload\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Any,\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...\n\n    def batch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> Iterator[tuple[int, Union[Output, Exception]]]:\n        if isinstance(config, Sequence):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        # lol mypy\n        if return_exceptions:\n            yield from self.bound.batch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            )\n        else:\n            yield from self.bound.batch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            )\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[False] = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Output]]: ...\n\n    @overload\n    def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: Literal[True],\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...\n\n    async def abatch_as_completed(\n        self,\n        inputs: Sequence[Input],\n        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,\n        *,\n        return_exceptions: bool = False,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:\n        if isinstance(config, Sequence):\n            configs = cast(\n                list[RunnableConfig],\n                [self._merge_configs(conf) for conf in config],\n            )\n        else:\n            configs = [self._merge_configs(config) for _ in range(len(inputs))]\n        if return_exceptions:\n            async for item in self.bound.abatch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            ):\n                yield item\n        else:\n            async for item in self.bound.abatch_as_completed(\n                inputs,\n                configs,\n                return_exceptions=return_exceptions,\n                **{**self.kwargs, **kwargs},\n            ):\n                yield item\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        yield from self.bound.stream(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        async for item in self.bound.astream(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        ):\n            yield item\n\n    async def astream_events(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[StreamEvent]:\n        async for item in self.bound.astream_events(\n            input, self._merge_configs(config), **{**self.kwargs, **kwargs}\n        ):\n            yield item\n\n    def transform(\n        self,\n        input: Iterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Iterator[Output]:\n        yield from self.bound.transform(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        )\n\n    async def atransform(\n        self,\n        input: AsyncIterator[Input],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[Output]:\n        async for item in self.bound.atransform(\n            input,\n            self._merge_configs(config),\n            **{**self.kwargs, **kwargs},\n        ):\n            yield item\n\n\nRunnableBindingBase.model_rebuild()\n\n\nclass RunnableBinding(RunnableBindingBase[Input, Output]):\n    \"\"\"Wrap a Runnable with additional functionality.\n\n    A RunnableBinding can be thought of as a \"runnable decorator\" that\n    preserves the essential features of Runnable; i.e., batching, streaming,\n    and async support, while adding additional functionality.\n\n    Any class that inherits from Runnable can be bound to a `RunnableBinding`.\n    Runnables expose a standard set of methods for creating `RunnableBindings`\n    or sub-classes of `RunnableBindings` (e.g., `RunnableRetry`,\n    `RunnableWithFallbacks`) that add additional functionality.\n\n    These methods include:\n\n    - ``bind``: Bind kwargs to pass to the underlying Runnable when running it.\n    - ``with_config``: Bind config to pass to the underlying Runnable when running it.\n    - ``with_listeners``:  Bind lifecycle listeners to the underlying Runnable.\n    - ``with_types``: Override the input and output types of the underlying Runnable.\n    - ``with_retry``: Bind a retry policy to the underlying Runnable.\n    - ``with_fallbacks``: Bind a fallback policy to the underlying Runnable.\n\n    Example:\n    `bind`: Bind kwargs to pass to the underlying Runnable when running it.\n\n        .. code-block:: python\n\n            # Create a Runnable binding that invokes the ChatModel with the\n            # additional kwarg `stop=['-']` when running it.\n            from langchain_community.chat_models import ChatOpenAI\n            model = ChatOpenAI()\n            model.invoke('Say \"Parrot-MAGIC\"', stop=['-']) # Should return `Parrot`\n            # Using it the easy way via `bind` method which returns a new\n            # RunnableBinding\n            runnable_binding = model.bind(stop=['-'])\n            runnable_binding.invoke('Say \"Parrot-MAGIC\"') # Should return `Parrot`\n\n        Can also be done by instantiating a RunnableBinding directly (not recommended):\n\n        .. code-block:: python\n\n            from langchain_core.runnables import RunnableBinding\n            runnable_binding = RunnableBinding(\n                bound=model,\n                kwargs={'stop': ['-']} # <-- Note the additional kwargs\n            )\n            runnable_binding.invoke('Say \"Parrot-MAGIC\"') # Should return `Parrot`\n    \"\"\"\n\n    @classmethod\n    def get_lc_namespace(cls) -> list[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"schema\", \"runnable\"]\n\n    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:\n        \"\"\"Bind additional kwargs to a Runnable, returning a new Runnable.\n\n        Args:\n            **kwargs: The kwargs to bind to the Runnable.\n\n        Returns:\n            A new Runnable with the same type and config as the original,\n            but with the additional kwargs bound.\n        \"\"\"\n        return self.__class__(\n            bound=self.bound,\n            config=self.config,\n            kwargs={**self.kwargs, **kwargs},\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_config(\n        self,\n        config: Optional[RunnableConfig] = None,\n        # Sadly Unpack is not well supported by mypy so this will have to be untyped\n        **kwargs: Any,\n    ) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=cast(RunnableConfig, {**self.config, **(config or {}), **kwargs}),\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_listeners(\n        self,\n        *,\n        on_start: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_end: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n        on_error: Optional[\n            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]\n        ] = None,\n    ) -> Runnable[Input, Output]:\n        \"\"\"Bind lifecycle listeners to a Runnable, returning a new Runnable.\n\n        Args:\n            on_start: Called before the Runnable starts running, with the Run object.\n                Defaults to None.\n            on_end: Called after the Runnable finishes running, with the Run object.\n                Defaults to None.\n            on_error: Called if the Runnable throws an error, with the Run object.\n                Defaults to None.\n\n        Returns:\n            The Runnable object contains information about the run, including its id,\n            type, input, output, error, start_time, end_time, and any tags or metadata\n            added to the run.\n        \"\"\"\n        from langchain_core.tracers.root_listeners import RootListenersTracer\n\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=self.config,\n            config_factories=[\n                lambda config: {\n                    \"callbacks\": [\n                        RootListenersTracer(\n                            config=config,\n                            on_start=on_start,\n                            on_end=on_end,\n                            on_error=on_error,\n                        )\n                    ],\n                }\n            ],\n            custom_input_type=self.custom_input_type,\n            custom_output_type=self.custom_output_type,\n        )\n\n    def with_types(\n        self,\n        input_type: Optional[Union[type[Input], BaseModel]] = None,\n        output_type: Optional[Union[type[Output], BaseModel]] = None,\n    ) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound,\n            kwargs=self.kwargs,\n            config=self.config,\n            custom_input_type=(\n                input_type if input_type is not None else self.custom_input_type\n            ),\n            custom_output_type=(\n                output_type if output_type is not None else self.custom_output_type\n            ),\n        )\n\n    def with_retry(self, **kwargs: Any) -> Runnable[Input, Output]:\n        return self.__class__(\n            bound=self.bound.with_retry(**kwargs),\n            kwargs=self.kwargs,\n            config=self.config,\n        )\n\n    def __getattr__(self, name: str) -> Any:\n        attr = getattr(self.bound, name)\n\n        if callable(attr) and (\n            config_param := inspect.signature(attr).parameters.get(\"config\")\n        ):\n            if config_param.kind == inspect.Parameter.KEYWORD_ONLY:\n\n                @wraps(attr)\n                def wrapper(*args: Any, **kwargs: Any) -> Any:\n                    return attr(\n                        *args,\n                        config=merge_configs(self.config, kwargs.pop(\"config\", None)),\n                        **kwargs,\n                    )\n\n                return wrapper\n            elif config_param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                idx = list(inspect.signature(attr).parameters).index(\"config\")\n\n                @wraps(attr)\n                def wrapper(*args: Any, **kwargs: Any) -> Any:\n                    if len(args) >= idx + 1:\n                        argsl = list(args)\n                        argsl[idx] = merge_configs(self.config, argsl[idx])\n                        return attr(*argsl, **kwargs)\n                    else:\n                        return attr(\n                            *args,\n                            config=merge_configs(\n                                self.config, kwargs.pop(\"config\", None)\n                            ),\n                            **kwargs,\n                        )\n\n                return wrapper\n\n        return attr\n\n\nclass _RunnableCallableSync(Protocol[Input, Output]):\n    def __call__(self, __in: Input, *, config: RunnableConfig) -> Output: ...\n\n\nclass _RunnableCallableAsync(Protocol[Input, Output]):\n    def __call__(self, __in: Input, *, config: RunnableConfig) -> Awaitable[Output]: ...\n\n\nclass _RunnableCallableIterator(Protocol[Input, Output]):\n    def __call__(\n        self, __in: Iterator[Input], *, config: RunnableConfig\n    ) -> Iterator[Output]: ...\n\n\nclass _RunnableCallableAsyncIterator(Protocol[Input, Output]):\n    def __call__(\n        self, __in: AsyncIterator[Input], *, config: RunnableConfig\n    ) -> AsyncIterator[Output]: ...\n\n\nRunnableLike = Union[\n    Runnable[Input, Output],\n    Callable[[Input], Output],\n    Callable[[Input], Awaitable[Output]],\n    Callable[[Iterator[Input]], Iterator[Output]],\n    Callable[[AsyncIterator[Input]], AsyncIterator[Output]],\n    _RunnableCallableSync[Input, Output],\n    _RunnableCallableAsync[Input, Output],\n    _RunnableCallableIterator[Input, Output],\n    _RunnableCallableAsyncIterator[Input, Output],\n    Mapping[str, Any],\n]\n\n\ndef coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:\n    \"\"\"Coerce a Runnable-like object into a Runnable.\n\n    Args:\n        thing: A Runnable-like object.\n\n    Returns:\n        A Runnable.\n\n    Raises:\n        TypeError: If the object is not Runnable-like.\n    \"\"\"\n    if isinstance(thing, Runnable):\n        return thing\n    elif is_async_generator(thing) or inspect.isgeneratorfunction(thing):\n        return RunnableGenerator(thing)\n    elif callable(thing):\n        return RunnableLambda(cast(Callable[[Input], Output], thing))\n    elif isinstance(thing, dict):\n        return cast(Runnable[Input, Output], RunnableParallel(thing))\n    else:\n        msg = (\n            f\"Expected a Runnable, callable or dict.\"\n            f\"Instead got an unsupported type: {type(thing)}\"\n        )\n        raise TypeError(msg)\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Coroutine[Any, Any, Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Iterator[Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], AsyncIterator[Output]],\n) -> Runnable[Input, Output]: ...\n\n\n@overload\ndef chain(\n    func: Callable[[Input], Output],\n) -> Runnable[Input, Output]: ...\n\n\ndef chain(\n    func: Union[\n        Callable[[Input], Output],\n        Callable[[Input], Iterator[Output]],\n        Callable[[Input], Coroutine[Any, Any, Output]],\n        Callable[[Input], AsyncIterator[Output]],\n    ],\n) -> Runnable[Input, Output]:\n    \"\"\"Decorate a function to make it a Runnable.\n    Sets the name of the Runnable to the name of the function.\n    Any runnables called by the function will be traced as dependencies.\n\n    Args:\n        func: A callable.\n\n    Returns:\n        A Runnable.\n\n    Example:\n\n    .. code-block:: python\n\n        from langchain_core.runnables import chain\n        from langchain_core.prompts import PromptTemplate\n        from langchain_openai import OpenAI\n\n        @chain\n        def my_func(fields):\n            prompt = PromptTemplate(\"Hello, {name}!\")\n            llm = OpenAI()\n            formatted = prompt.invoke(**fields)\n\n            for chunk in llm.stream(formatted):\n                yield chunk\n    \"\"\"\n    return RunnableLambda(func)\n",
        "patch": "@@ -4641,7 +4641,7 @@ def func(\n                     )\n \n             @wraps(func)\n-            async def f(*args, **kwargs):  # type: ignore[no-untyped-def]\n+            async def f(*args: Any, **kwargs: Any) -> Any:\n                 return await run_in_executor(config, func, *args, **kwargs)\n \n             afunc = f\n@@ -4889,7 +4889,7 @@ def func(\n                 )\n \n             @wraps(func)\n-            async def f(*args, **kwargs):  # type: ignore[no-untyped-def]\n+            async def f(*args: Any, **kwargs: Any) -> Any:\n                 return await run_in_executor(config, func, *args, **kwargs)\n \n             afunc = f"
      },
      {
        "filename": "libs/core/langchain_core/utils/utils.py",
        "content_before": "\"\"\"Generic utility functions.\"\"\"\n\nimport contextlib\nimport datetime\nimport functools\nimport importlib\nimport os\nimport warnings\nfrom collections.abc import Sequence\nfrom importlib.metadata import version\nfrom typing import Any, Callable, Optional, Union, overload\n\nfrom packaging.version import parse\nfrom pydantic import SecretStr\nfrom requests import HTTPError, Response\n\nfrom langchain_core.utils.pydantic import (\n    is_pydantic_v1_subclass,\n)\n\n\ndef xor_args(*arg_groups: tuple[str, ...]) -> Callable:\n    \"\"\"Validate specified keyword args are mutually exclusive.\".\n\n    Args:\n        *arg_groups (Tuple[str, ...]): Groups of mutually exclusive keyword args.\n\n    Returns:\n        Callable: Decorator that validates the specified keyword args\n            are mutually exclusive\n\n    Raises:\n        ValueError: If more than one arg in a group is defined.\n    \"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            \"\"\"Validate exactly one arg in each group is not None.\"\"\"\n            counts = [\n                sum(1 for arg in arg_group if kwargs.get(arg) is not None)\n                for arg_group in arg_groups\n            ]\n            invalid_groups = [i for i, count in enumerate(counts) if count != 1]\n            if invalid_groups:\n                invalid_group_names = [\", \".join(arg_groups[i]) for i in invalid_groups]\n                msg = (\n                    \"Exactly one argument in each of the following\"\n                    \" groups must be defined:\"\n                    f\" {', '.join(invalid_group_names)}\"\n                )\n                raise ValueError(msg)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\ndef raise_for_status_with_text(response: Response) -> None:\n    \"\"\"Raise an error with the response text.\n\n    Args:\n        response (Response): The response to check for errors.\n\n    Raises:\n        ValueError: If the response has an error status code.\n    \"\"\"\n    try:\n        response.raise_for_status()\n    except HTTPError as e:\n        raise ValueError(response.text) from e\n\n\n@contextlib.contextmanager\ndef mock_now(dt_value):  # type: ignore\n    \"\"\"Context manager for mocking out datetime.now() in unit tests.\n\n    Args:\n        dt_value: The datetime value to use for datetime.now().\n\n    Yields:\n        datetime.datetime: The mocked datetime class.\n\n    Example:\n    with mock_now(datetime.datetime(2011, 2, 3, 10, 11)):\n        assert datetime.datetime.now() == datetime.datetime(2011, 2, 3, 10, 11)\n    \"\"\"\n\n    class MockDateTime(datetime.datetime):\n        \"\"\"Mock datetime.datetime.now() with a fixed datetime.\"\"\"\n\n        @classmethod\n        def now(cls):  # type: ignore\n            # Create a copy of dt_value.\n            return datetime.datetime(\n                dt_value.year,\n                dt_value.month,\n                dt_value.day,\n                dt_value.hour,\n                dt_value.minute,\n                dt_value.second,\n                dt_value.microsecond,\n                dt_value.tzinfo,\n            )\n\n    real_datetime = datetime.datetime\n    datetime.datetime = MockDateTime\n    try:\n        yield datetime.datetime\n    finally:\n        datetime.datetime = real_datetime\n\n\ndef guard_import(\n    module_name: str, *, pip_name: Optional[str] = None, package: Optional[str] = None\n) -> Any:\n    \"\"\"Dynamically import a module and raise an exception if the module is not\n    installed.\n\n    Args:\n        module_name (str): The name of the module to import.\n        pip_name (str, optional): The name of the module to install with pip.\n            Defaults to None.\n        package (str, optional): The package to import the module from.\n            Defaults to None.\n\n    Returns:\n        Any: The imported module.\n\n    Raises:\n        ImportError: If the module is not installed.\n    \"\"\"\n    try:\n        module = importlib.import_module(module_name, package)\n    except (ImportError, ModuleNotFoundError) as e:\n        pip_name = pip_name or module_name.split(\".\")[0].replace(\"_\", \"-\")\n        msg = (\n            f\"Could not import {module_name} python package. \"\n            f\"Please install it with `pip install {pip_name}`.\"\n        )\n        raise ImportError(msg) from e\n    return module\n\n\ndef check_package_version(\n    package: str,\n    lt_version: Optional[str] = None,\n    lte_version: Optional[str] = None,\n    gt_version: Optional[str] = None,\n    gte_version: Optional[str] = None,\n) -> None:\n    \"\"\"Check the version of a package.\n\n    Args:\n        package (str): The name of the package.\n        lt_version (str, optional): The version must be less than this.\n            Defaults to None.\n        lte_version (str, optional): The version must be less than or equal to this.\n            Defaults to None.\n        gt_version (str, optional): The version must be greater than this.\n            Defaults to None.\n        gte_version (str, optional): The version must be greater than or equal to this.\n            Defaults to None.\n\n    Raises:\n        ValueError: If the package version does not meet the requirements.\n    \"\"\"\n    imported_version = parse(version(package))\n    if lt_version is not None and imported_version >= parse(lt_version):\n        msg = (\n            f\"Expected {package} version to be < {lt_version}. Received \"\n            f\"{imported_version}.\"\n        )\n        raise ValueError(msg)\n    if lte_version is not None and imported_version > parse(lte_version):\n        msg = (\n            f\"Expected {package} version to be <= {lte_version}. Received \"\n            f\"{imported_version}.\"\n        )\n        raise ValueError(msg)\n    if gt_version is not None and imported_version <= parse(gt_version):\n        msg = (\n            f\"Expected {package} version to be > {gt_version}. Received \"\n            f\"{imported_version}.\"\n        )\n        raise ValueError(msg)\n    if gte_version is not None and imported_version < parse(gte_version):\n        msg = (\n            f\"Expected {package} version to be >= {gte_version}. Received \"\n            f\"{imported_version}.\"\n        )\n        raise ValueError(msg)\n\n\ndef get_pydantic_field_names(pydantic_cls: Any) -> set[str]:\n    \"\"\"Get field names, including aliases, for a pydantic class.\n\n    Args:\n        pydantic_cls: Pydantic class.\n\n    Returns:\n        Set[str]: Field names.\n    \"\"\"\n    all_required_field_names = set()\n    if is_pydantic_v1_subclass(pydantic_cls):\n        for field in pydantic_cls.__fields__.values():\n            all_required_field_names.add(field.name)\n            if field.has_alias:\n                all_required_field_names.add(field.alias)\n    else:  # Assuming pydantic 2 for now\n        for name, field in pydantic_cls.model_fields.items():\n            all_required_field_names.add(name)\n            if field.alias:\n                all_required_field_names.add(field.alias)\n    return all_required_field_names\n\n\ndef _build_model_kwargs(\n    values: dict[str, Any],\n    all_required_field_names: set[str],\n) -> dict[str, Any]:\n    \"\"\"Build \"model_kwargs\" param from Pydanitc constructor values.\n\n    Args:\n        values: All init args passed in by user.\n        all_required_field_names: All required field names for the pydantic class.\n\n    Returns:\n        Dict[str, Any]: Extra kwargs.\n\n    Raises:\n        ValueError: If a field is specified in both values and extra_kwargs.\n        ValueError: If a field is specified in model_kwargs.\n    \"\"\"\n    extra_kwargs = values.get(\"model_kwargs\", {})\n    for field_name in list(values):\n        if field_name in extra_kwargs:\n            msg = f\"Found {field_name} supplied twice.\"\n            raise ValueError(msg)\n        if field_name not in all_required_field_names:\n            warnings.warn(\n                f\"\"\"WARNING! {field_name} is not default parameter.\n                {field_name} was transferred to model_kwargs.\n                Please confirm that {field_name} is what you intended.\"\"\",\n                stacklevel=7,\n            )\n            extra_kwargs[field_name] = values.pop(field_name)\n\n    invalid_model_kwargs = all_required_field_names.intersection(extra_kwargs.keys())\n    if invalid_model_kwargs:\n        warnings.warn(\n            f\"Parameters {invalid_model_kwargs} should be specified explicitly. \"\n            f\"Instead they were passed in as part of `model_kwargs` parameter.\",\n            stacklevel=7,\n        )\n        for k in invalid_model_kwargs:\n            values[k] = extra_kwargs.pop(k)\n\n    values[\"model_kwargs\"] = extra_kwargs\n    return values\n\n\n# DON'T USE! Kept for backwards-compatibility but should never have been public.\ndef build_extra_kwargs(\n    extra_kwargs: dict[str, Any],\n    values: dict[str, Any],\n    all_required_field_names: set[str],\n) -> dict[str, Any]:\n    \"\"\"Build extra kwargs from values and extra_kwargs.\n\n    Args:\n        extra_kwargs: Extra kwargs passed in by user.\n        values: Values passed in by user.\n        all_required_field_names: All required field names for the pydantic class.\n\n    Returns:\n        Dict[str, Any]: Extra kwargs.\n\n    Raises:\n        ValueError: If a field is specified in both values and extra_kwargs.\n        ValueError: If a field is specified in model_kwargs.\n    \"\"\"\n    for field_name in list(values):\n        if field_name in extra_kwargs:\n            msg = f\"Found {field_name} supplied twice.\"\n            raise ValueError(msg)\n        if field_name not in all_required_field_names:\n            warnings.warn(\n                f\"\"\"WARNING! {field_name} is not default parameter.\n                {field_name} was transferred to model_kwargs.\n                Please confirm that {field_name} is what you intended.\"\"\",\n                stacklevel=7,\n            )\n            extra_kwargs[field_name] = values.pop(field_name)\n\n    invalid_model_kwargs = all_required_field_names.intersection(extra_kwargs.keys())\n    if invalid_model_kwargs:\n        msg = (\n            f\"Parameters {invalid_model_kwargs} should be specified explicitly. \"\n            f\"Instead they were passed in as part of `model_kwargs` parameter.\"\n        )\n        raise ValueError(msg)\n\n    return extra_kwargs\n\n\ndef convert_to_secret_str(value: Union[SecretStr, str]) -> SecretStr:\n    \"\"\"Convert a string to a SecretStr if needed.\n\n    Args:\n        value (Union[SecretStr, str]): The value to convert.\n\n    Returns:\n        SecretStr: The SecretStr value.\n    \"\"\"\n    if isinstance(value, SecretStr):\n        return value\n    return SecretStr(value)\n\n\nclass _NoDefaultType:\n    \"\"\"Type to indicate no default value is provided.\"\"\"\n\n\n_NoDefault = _NoDefaultType()\n\n\n@overload\ndef from_env(key: str, /) -> Callable[[], str]: ...\n\n\n@overload\ndef from_env(key: str, /, *, default: str) -> Callable[[], str]: ...\n\n\n@overload\ndef from_env(key: Sequence[str], /, *, default: str) -> Callable[[], str]: ...\n\n\n@overload\ndef from_env(key: str, /, *, error_message: str) -> Callable[[], str]: ...\n\n\n@overload\ndef from_env(\n    key: Union[str, Sequence[str]], /, *, default: str, error_message: Optional[str]\n) -> Callable[[], str]: ...\n\n\n@overload\ndef from_env(\n    key: str, /, *, default: None, error_message: Optional[str]\n) -> Callable[[], Optional[str]]: ...\n\n\n@overload\ndef from_env(\n    key: Union[str, Sequence[str]], /, *, default: None\n) -> Callable[[], Optional[str]]: ...\n\n\ndef from_env(\n    key: Union[str, Sequence[str]],\n    /,\n    *,\n    default: Union[str, _NoDefaultType, None] = _NoDefault,\n    error_message: Optional[str] = None,\n) -> Union[Callable[[], str], Callable[[], Optional[str]]]:\n    \"\"\"Create a factory method that gets a value from an environment variable.\n\n    Args:\n        key: The environment variable to look up. If a list of keys is provided,\n            the first key found in the environment will be used.\n            If no key is found, the default value will be used if set,\n            otherwise an error will be raised.\n        default: The default value to return if the environment variable is not set.\n        error_message: the error message which will be raised if the key is not found\n            and no default value is provided.\n            This will be raised as a ValueError.\n    \"\"\"\n\n    def get_from_env_fn() -> Optional[str]:\n        \"\"\"Get a value from an environment variable.\"\"\"\n        if isinstance(key, (list, tuple)):\n            for k in key:\n                if k in os.environ:\n                    return os.environ[k]\n        if isinstance(key, str) and key in os.environ:\n            return os.environ[key]\n\n        if isinstance(default, (str, type(None))):\n            return default\n        else:\n            if error_message:\n                raise ValueError(error_message)\n            else:\n                msg = (\n                    f\"Did not find {key}, please add an environment variable\"\n                    f\" `{key}` which contains it, or pass\"\n                    f\" `{key}` as a named parameter.\"\n                )\n                raise ValueError(msg)\n\n    return get_from_env_fn\n\n\n@overload\ndef secret_from_env(key: Union[str, Sequence[str]], /) -> Callable[[], SecretStr]: ...\n\n\n@overload\ndef secret_from_env(key: str, /, *, default: str) -> Callable[[], SecretStr]: ...\n\n\n@overload\ndef secret_from_env(\n    key: Union[str, Sequence[str]], /, *, default: None\n) -> Callable[[], Optional[SecretStr]]: ...\n\n\n@overload\ndef secret_from_env(key: str, /, *, error_message: str) -> Callable[[], SecretStr]: ...\n\n\ndef secret_from_env(\n    key: Union[str, Sequence[str]],\n    /,\n    *,\n    default: Union[str, _NoDefaultType, None] = _NoDefault,\n    error_message: Optional[str] = None,\n) -> Union[Callable[[], Optional[SecretStr]], Callable[[], SecretStr]]:\n    \"\"\"Secret from env.\n\n    Args:\n        key: The environment variable to look up.\n        default: The default value to return if the environment variable is not set.\n        error_message: the error message which will be raised if the key is not found\n            and no default value is provided.\n            This will be raised as a ValueError.\n\n    Returns:\n        factory method that will look up the secret from the environment.\n    \"\"\"\n\n    def get_secret_from_env() -> Optional[SecretStr]:\n        \"\"\"Get a value from an environment variable.\"\"\"\n        if isinstance(key, (list, tuple)):\n            for k in key:\n                if k in os.environ:\n                    return SecretStr(os.environ[k])\n        if isinstance(key, str) and key in os.environ:\n            return SecretStr(os.environ[key])\n        if isinstance(default, str):\n            return SecretStr(default)\n        elif default is None:\n            return None\n        else:\n            if error_message:\n                raise ValueError(error_message)\n            else:\n                msg = (\n                    f\"Did not find {key}, please add an environment variable\"\n                    f\" `{key}` which contains it, or pass\"\n                    f\" `{key}` as a named parameter.\"\n                )\n                raise ValueError(msg)\n\n    return get_secret_from_env\n",
        "patch": "@@ -6,7 +6,7 @@\n import importlib\n import os\n import warnings\n-from collections.abc import Sequence\n+from collections.abc import Iterator, Sequence\n from importlib.metadata import version\n from typing import Any, Callable, Optional, Union, overload\n \n@@ -73,7 +73,7 @@ def raise_for_status_with_text(response: Response) -> None:\n \n \n @contextlib.contextmanager\n-def mock_now(dt_value):  # type: ignore\n+def mock_now(dt_value: datetime.datetime) -> Iterator[type]:\n     \"\"\"Context manager for mocking out datetime.now() in unit tests.\n \n     Args:\n@@ -91,9 +91,9 @@ class MockDateTime(datetime.datetime):\n         \"\"\"Mock datetime.datetime.now() with a fixed datetime.\"\"\"\n \n         @classmethod\n-        def now(cls):  # type: ignore\n+        def now(cls, tz: Union[datetime.tzinfo, None] = None) -> \"MockDateTime\":\n             # Create a copy of dt_value.\n-            return datetime.datetime(\n+            return MockDateTime(\n                 dt_value.year,\n                 dt_value.month,\n                 dt_value.day,\n@@ -105,11 +105,11 @@ def now(cls):  # type: ignore\n             )\n \n     real_datetime = datetime.datetime\n-    datetime.datetime = MockDateTime\n+    datetime.datetime = MockDateTime  # type: ignore[misc]\n     try:\n         yield datetime.datetime\n     finally:\n-        datetime.datetime = real_datetime\n+        datetime.datetime = real_datetime  # type: ignore[misc]\n \n \n def guard_import("
      },
      {
        "filename": "libs/core/tests/unit_tests/runnables/test_runnable.py",
        "content_before": "import asyncio\nimport sys\nimport uuid\nimport warnings\nfrom collections.abc import AsyncIterator, Awaitable, Iterator, Sequence\nfrom functools import partial\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    Callable,\n    Optional,\n    Union,\n    cast,\n)\nfrom uuid import UUID\n\nimport pydantic\nimport pytest\nfrom freezegun import freeze_time\nfrom pydantic import BaseModel, Field\nfrom pytest_mock import MockerFixture\nfrom syrupy import SnapshotAssertion\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.callbacks.manager import (\n    Callbacks,\n    atrace_as_chain_group,\n    trace_as_chain_group,\n)\nfrom langchain_core.documents import Document\nfrom langchain_core.language_models import (\n    FakeListChatModel,\n    FakeListLLM,\n    FakeStreamingListLLM,\n)\nfrom langchain_core.load import dumpd, dumps\nfrom langchain_core.load.load import loads\nfrom langchain_core.messages import (\n    AIMessageChunk,\n    HumanMessage,\n    SystemMessage,\n)\nfrom langchain_core.messages.base import BaseMessage\nfrom langchain_core.output_parsers import (\n    BaseOutputParser,\n    CommaSeparatedListOutputParser,\n    StrOutputParser,\n)\nfrom langchain_core.outputs.chat_generation import ChatGeneration\nfrom langchain_core.outputs.llm_result import LLMResult\nfrom langchain_core.prompt_values import ChatPromptValue, StringPromptValue\nfrom langchain_core.prompts import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    MessagesPlaceholder,\n    PromptTemplate,\n    SystemMessagePromptTemplate,\n)\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.runnables import (\n    AddableDict,\n    ConfigurableField,\n    ConfigurableFieldMultiOption,\n    ConfigurableFieldSingleOption,\n    RouterRunnable,\n    Runnable,\n    RunnableAssign,\n    RunnableBinding,\n    RunnableBranch,\n    RunnableConfig,\n    RunnableGenerator,\n    RunnableLambda,\n    RunnableParallel,\n    RunnablePassthrough,\n    RunnablePick,\n    RunnableSequence,\n    add,\n    chain,\n)\nfrom langchain_core.runnables.base import RunnableMap, RunnableSerializable\nfrom langchain_core.runnables.utils import Input, Output\nfrom langchain_core.tools import BaseTool, tool\nfrom langchain_core.tracers import (\n    BaseTracer,\n    ConsoleCallbackHandler,\n    Run,\n    RunLog,\n    RunLogPatch,\n)\nfrom langchain_core.tracers.context import collect_runs\nfrom langchain_core.utils.pydantic import PYDANTIC_MAJOR_VERSION, PYDANTIC_MINOR_VERSION\nfrom tests.unit_tests.pydantic_utils import _normalize_schema, _schema\nfrom tests.unit_tests.stubs import AnyStr, _any_id_ai_message, _any_id_ai_message_chunk\n\nPYDANTIC_VERSION = tuple(map(int, pydantic.__version__.split(\".\")))\n\n\nclass FakeTracer(BaseTracer):\n    \"\"\"Fake tracer that records LangChain execution.\n    It replaces run ids with deterministic UUIDs for snapshotting.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the tracer.\"\"\"\n        super().__init__()\n        self.runs: list[Run] = []\n        self.uuids_map: dict[UUID, UUID] = {}\n        self.uuids_generator = (\n            UUID(f\"00000000-0000-4000-8000-{i:012}\", version=4) for i in range(10000)\n        )\n\n    def _replace_uuid(self, uuid: UUID) -> UUID:\n        if uuid not in self.uuids_map:\n            self.uuids_map[uuid] = next(self.uuids_generator)\n        return self.uuids_map[uuid]\n\n    def _replace_message_id(self, maybe_message: Any) -> Any:\n        if isinstance(maybe_message, BaseMessage):\n            maybe_message.id = str(next(self.uuids_generator))\n        if isinstance(maybe_message, ChatGeneration):\n            maybe_message.message.id = str(next(self.uuids_generator))\n        if isinstance(maybe_message, LLMResult):\n            for i, gen_list in enumerate(maybe_message.generations):\n                for j, gen in enumerate(gen_list):\n                    maybe_message.generations[i][j] = self._replace_message_id(gen)\n        if isinstance(maybe_message, dict):\n            for k, v in maybe_message.items():\n                maybe_message[k] = self._replace_message_id(v)\n        if isinstance(maybe_message, list):\n            for i, v in enumerate(maybe_message):\n                maybe_message[i] = self._replace_message_id(v)\n\n        return maybe_message\n\n    def _copy_run(self, run: Run) -> Run:\n        if run.dotted_order:\n            levels = run.dotted_order.split(\".\")\n            processed_levels = []\n            for level in levels:\n                timestamp, run_id = level.split(\"Z\")\n                new_run_id = self._replace_uuid(UUID(run_id))\n                processed_level = f\"{timestamp}Z{new_run_id}\"\n                processed_levels.append(processed_level)\n            new_dotted_order = \".\".join(processed_levels)\n        else:\n            new_dotted_order = None\n        return run.copy(\n            update={\n                \"id\": self._replace_uuid(run.id),\n                \"parent_run_id\": (\n                    self.uuids_map[run.parent_run_id] if run.parent_run_id else None\n                ),\n                \"child_runs\": [self._copy_run(child) for child in run.child_runs],\n                \"trace_id\": self._replace_uuid(run.trace_id) if run.trace_id else None,\n                \"dotted_order\": new_dotted_order,\n                \"inputs\": self._replace_message_id(run.inputs),\n                \"outputs\": self._replace_message_id(run.outputs),\n            }\n        )\n\n    def _persist_run(self, run: Run) -> None:\n        \"\"\"Persist a run.\"\"\"\n        self.runs.append(self._copy_run(run))\n\n    def flattened_runs(self) -> list[Run]:\n        q = [] + self.runs\n        result = []\n        while q:\n            parent = q.pop()\n            result.append(parent)\n            if parent.child_runs:\n                q.extend(parent.child_runs)\n        return result\n\n    @property\n    def run_ids(self) -> list[Optional[uuid.UUID]]:\n        runs = self.flattened_runs()\n        uuids_map = {v: k for k, v in self.uuids_map.items()}\n        return [uuids_map.get(r.id) for r in runs]\n\n\nclass FakeRunnable(Runnable[str, int]):\n    def invoke(\n        self,\n        input: str,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> int:\n        return len(input)\n\n\nclass FakeRunnableSerializable(RunnableSerializable[str, int]):\n    hello: str = \"\"\n\n    def invoke(\n        self,\n        input: str,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> int:\n        return len(input)\n\n\nclass FakeRetriever(BaseRetriever):\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        callbacks: Callbacks = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> list[Document]:\n        return [Document(page_content=\"foo\"), Document(page_content=\"bar\")]\n\n    async def _aget_relevant_documents(\n        self,\n        query: str,\n        *,\n        callbacks: Callbacks = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> list[Document]:\n        return [Document(page_content=\"foo\"), Document(page_content=\"bar\")]\n\n\n@pytest.mark.skipif(\n    (PYDANTIC_MAJOR_VERSION, PYDANTIC_MINOR_VERSION) >= (2, 10),\n    reason=(\n        \"Only test with most recent version of pydantic. \"\n        \"Pydantic introduced small fixes to generated JSONSchema on minor versions.\"\n    ),\n)\ndef test_schemas(snapshot: SnapshotAssertion) -> None:\n    fake = FakeRunnable()  # str -> int\n\n    assert fake.get_input_jsonschema() == {\n        \"title\": \"FakeRunnableInput\",\n        \"type\": \"string\",\n    }\n    assert fake.get_output_jsonschema() == {\n        \"title\": \"FakeRunnableOutput\",\n        \"type\": \"integer\",\n    }\n    assert fake.get_config_jsonschema(include=[\"tags\", \"metadata\", \"run_name\"]) == {\n        \"properties\": {\n            \"metadata\": {\"default\": None, \"title\": \"Metadata\", \"type\": \"object\"},\n            \"run_name\": {\"default\": None, \"title\": \"Run Name\", \"type\": \"string\"},\n            \"tags\": {\n                \"default\": None,\n                \"items\": {\"type\": \"string\"},\n                \"title\": \"Tags\",\n                \"type\": \"array\",\n            },\n        },\n        \"title\": \"FakeRunnableConfig\",\n        \"type\": \"object\",\n    }\n\n    fake_bound = FakeRunnable().bind(a=\"b\")  # str -> int\n\n    assert fake_bound.get_input_jsonschema() == {\n        \"title\": \"FakeRunnableInput\",\n        \"type\": \"string\",\n    }\n    assert fake_bound.get_output_jsonschema() == {\n        \"title\": \"FakeRunnableOutput\",\n        \"type\": \"integer\",\n    }\n\n    fake_w_fallbacks = FakeRunnable().with_fallbacks((fake,))  # str -> int\n\n    assert fake_w_fallbacks.get_input_jsonschema() == {\n        \"title\": \"FakeRunnableInput\",\n        \"type\": \"string\",\n    }\n    assert fake_w_fallbacks.get_output_jsonschema() == {\n        \"title\": \"FakeRunnableOutput\",\n        \"type\": \"integer\",\n    }\n\n    def typed_lambda_impl(x: str) -> int:\n        return len(x)\n\n    typed_lambda = RunnableLambda(typed_lambda_impl)  # str -> int\n\n    assert typed_lambda.get_input_jsonschema() == {\n        \"title\": \"typed_lambda_impl_input\",\n        \"type\": \"string\",\n    }\n    assert typed_lambda.get_output_jsonschema() == {\n        \"title\": \"typed_lambda_impl_output\",\n        \"type\": \"integer\",\n    }\n\n    async def typed_async_lambda_impl(x: str) -> int:\n        return len(x)\n\n    typed_async_lambda: Runnable = RunnableLambda(typed_async_lambda_impl)  # str -> int\n\n    assert typed_async_lambda.get_input_jsonschema() == {\n        \"title\": \"typed_async_lambda_impl_input\",\n        \"type\": \"string\",\n    }\n    assert typed_async_lambda.get_output_jsonschema() == {\n        \"title\": \"typed_async_lambda_impl_output\",\n        \"type\": \"integer\",\n    }\n\n    fake_ret = FakeRetriever()  # str -> List[Document]\n\n    assert fake_ret.get_input_jsonschema() == {\n        \"title\": \"FakeRetrieverInput\",\n        \"type\": \"string\",\n    }\n    assert _normalize_schema(fake_ret.get_output_jsonschema()) == {\n        \"$defs\": {\n            \"Document\": {\n                \"description\": \"Class for storing a piece of text and \"\n                \"associated metadata.\\n\"\n                \"\\n\"\n                \"Example:\\n\"\n                \"\\n\"\n                \"    .. code-block:: python\\n\"\n                \"\\n\"\n                \"        from langchain_core.documents \"\n                \"import Document\\n\"\n                \"\\n\"\n                \"        document = Document(\\n\"\n                '            page_content=\"Hello, '\n                'world!\",\\n'\n                '            metadata={\"source\": '\n                '\"https://example.com\"}\\n'\n                \"        )\",\n                \"properties\": {\n                    \"id\": {\n                        \"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}],\n                        \"default\": None,\n                        \"title\": \"Id\",\n                    },\n                    \"metadata\": {\"title\": \"Metadata\", \"type\": \"object\"},\n                    \"page_content\": {\"title\": \"Page Content\", \"type\": \"string\"},\n                    \"type\": {\n                        \"const\": \"Document\",\n                        \"default\": \"Document\",\n                        \"title\": \"Type\",\n                    },\n                },\n                \"required\": [\"page_content\"],\n                \"title\": \"Document\",\n                \"type\": \"object\",\n            }\n        },\n        \"items\": {\"$ref\": \"#/$defs/Document\"},\n        \"title\": \"FakeRetrieverOutput\",\n        \"type\": \"array\",\n    }\n\n    fake_llm = FakeListLLM(responses=[\"a\"])  # str -> List[List[str]]\n\n    assert _schema(fake_llm.input_schema) == snapshot(name=\"fake_llm_input_schema\")\n    assert _schema(fake_llm.output_schema) == {\n        \"title\": \"FakeListLLMOutput\",\n        \"type\": \"string\",\n    }\n\n    fake_chat = FakeListChatModel(responses=[\"a\"])  # str -> List[List[str]]\n\n    assert _schema(fake_chat.input_schema) == snapshot(name=\"fake_chat_input_schema\")\n    assert _schema(fake_chat.output_schema) == snapshot(name=\"fake_chat_output_schema\")\n\n    chat_prompt = ChatPromptTemplate.from_messages(\n        [\n            MessagesPlaceholder(variable_name=\"history\"),\n            (\"human\", \"Hello, how are you?\"),\n        ]\n    )\n\n    assert _normalize_schema(chat_prompt.get_input_jsonschema()) == snapshot(\n        name=\"chat_prompt_input_schema\"\n    )\n    assert _normalize_schema(chat_prompt.get_output_jsonschema()) == snapshot(\n        name=\"chat_prompt_output_schema\"\n    )\n\n    prompt = PromptTemplate.from_template(\"Hello, {name}!\")\n\n    assert prompt.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}},\n        \"required\": [\"name\"],\n    }\n    assert _schema(prompt.output_schema) == snapshot(name=\"prompt_output_schema\")\n\n    prompt_mapper = PromptTemplate.from_template(\"Hello, {name}!\").map()\n\n    assert _normalize_schema(prompt_mapper.get_input_jsonschema()) == {\n        \"$defs\": {\n            \"PromptInput\": {\n                \"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}},\n                \"required\": [\"name\"],\n                \"title\": \"PromptInput\",\n                \"type\": \"object\",\n            }\n        },\n        \"default\": None,\n        \"items\": {\"$ref\": \"#/$defs/PromptInput\"},\n        \"title\": \"RunnableEach<PromptTemplate>Input\",\n        \"type\": \"array\",\n    }\n    assert _schema(prompt_mapper.output_schema) == snapshot(\n        name=\"prompt_mapper_output_schema\"\n    )\n\n    list_parser = CommaSeparatedListOutputParser()\n\n    assert _schema(list_parser.input_schema) == snapshot(\n        name=\"list_parser_input_schema\"\n    )\n    assert _schema(list_parser.output_schema) == {\n        \"title\": \"CommaSeparatedListOutputParserOutput\",\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\"},\n    }\n\n    seq = prompt | fake_llm | list_parser\n\n    assert seq.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}},\n        \"required\": [\"name\"],\n    }\n    assert seq.get_output_jsonschema() == {\n        \"type\": \"array\",\n        \"items\": {\"type\": \"string\"},\n        \"title\": \"CommaSeparatedListOutputParserOutput\",\n    }\n\n    router: Runnable = RouterRunnable({})\n\n    assert _schema(router.input_schema) == {\n        \"$ref\": \"#/definitions/RouterInput\",\n        \"definitions\": {\n            \"RouterInput\": {\n                \"description\": \"Router input.\\n\"\n                \"\\n\"\n                \"Attributes:\\n\"\n                \"    key: The key to route \"\n                \"on.\\n\"\n                \"    input: The input to pass \"\n                \"to the selected Runnable.\",\n                \"properties\": {\n                    \"input\": {\"title\": \"Input\"},\n                    \"key\": {\"title\": \"Key\", \"type\": \"string\"},\n                },\n                \"required\": [\"key\", \"input\"],\n                \"title\": \"RouterInput\",\n                \"type\": \"object\",\n            }\n        },\n        \"title\": \"RouterRunnableInput\",\n    }\n    assert router.get_output_jsonschema() == {\"title\": \"RouterRunnableOutput\"}\n\n    seq_w_map: Runnable = (\n        prompt\n        | fake_llm\n        | {\n            \"original\": RunnablePassthrough(input_type=str),\n            \"as_list\": list_parser,\n            \"length\": typed_lambda_impl,\n        }\n    )\n\n    assert seq_w_map.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}},\n        \"required\": [\"name\"],\n    }\n    assert seq_w_map.get_output_jsonschema() == {\n        \"title\": \"RunnableParallel<original,as_list,length>Output\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"original\": {\"title\": \"Original\", \"type\": \"string\"},\n            \"length\": {\"title\": \"Length\", \"type\": \"integer\"},\n            \"as_list\": {\n                \"title\": \"As List\",\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n            },\n        },\n        \"required\": [\"original\", \"as_list\", \"length\"],\n    }\n\n    # Add a test for schema of runnable assign\n    def foo(x: int) -> int:\n        return x\n\n    foo_ = RunnableLambda(foo)\n\n    assert foo_.assign(bar=lambda x: \"foo\").get_output_schema().model_json_schema() == {\n        \"properties\": {\"bar\": {\"title\": \"Bar\"}, \"root\": {\"title\": \"Root\"}},\n        \"required\": [\"root\", \"bar\"],\n        \"title\": \"RunnableAssignOutput\",\n        \"type\": \"object\",\n    }\n\n\ndef test_passthrough_assign_schema() -> None:\n    retriever = FakeRetriever()  # str -> List[Document]\n    prompt = PromptTemplate.from_template(\"{context} {question}\")\n    fake_llm = FakeListLLM(responses=[\"a\"])  # str -> List[List[str]]\n\n    seq_w_assign: Runnable = (\n        RunnablePassthrough.assign(context=itemgetter(\"question\") | retriever)\n        | prompt\n        | fake_llm\n    )\n\n    assert seq_w_assign.get_input_jsonschema() == {\n        \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}},\n        \"title\": \"RunnableSequenceInput\",\n        \"type\": \"object\",\n        \"required\": [\"question\"],\n    }\n    assert seq_w_assign.get_output_jsonschema() == {\n        \"title\": \"FakeListLLMOutput\",\n        \"type\": \"string\",\n    }\n\n    invalid_seq_w_assign: Runnable = (\n        RunnablePassthrough.assign(context=itemgetter(\"question\") | retriever)\n        | fake_llm\n    )\n\n    # fallback to RunnableAssign.input_schema if next runnable doesn't have\n    # expected dict input_schema\n    assert invalid_seq_w_assign.get_input_jsonschema() == {\n        \"properties\": {\"question\": {\"title\": \"Question\"}},\n        \"title\": \"RunnableParallel<context>Input\",\n        \"type\": \"object\",\n        \"required\": [\"question\"],\n    }\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 9), reason=\"Requires python version >= 3.9 to run.\"\n)\ndef test_lambda_schemas(snapshot: SnapshotAssertion) -> None:\n    first_lambda = lambda x: x[\"hello\"]  # noqa: E731\n    assert RunnableLambda(first_lambda).get_input_jsonschema() == {\n        \"title\": \"RunnableLambdaInput\",\n        \"type\": \"object\",\n        \"properties\": {\"hello\": {\"title\": \"Hello\"}},\n        \"required\": [\"hello\"],\n    }\n\n    second_lambda = lambda x, y: (x[\"hello\"], x[\"bye\"], y[\"bah\"])  # noqa: E731\n    assert RunnableLambda(second_lambda).get_input_jsonschema() == {  # type: ignore[arg-type]\n        \"title\": \"RunnableLambdaInput\",\n        \"type\": \"object\",\n        \"properties\": {\"hello\": {\"title\": \"Hello\"}, \"bye\": {\"title\": \"Bye\"}},\n        \"required\": [\"bye\", \"hello\"],\n    }\n\n    def get_value(input):  # type: ignore[no-untyped-def]\n        return input[\"variable_name\"]\n\n    assert RunnableLambda(get_value).get_input_jsonschema() == {\n        \"title\": \"get_value_input\",\n        \"type\": \"object\",\n        \"properties\": {\"variable_name\": {\"title\": \"Variable Name\"}},\n        \"required\": [\"variable_name\"],\n    }\n\n    async def aget_value(input):  # type: ignore[no-untyped-def]\n        return (input[\"variable_name\"], input.get(\"another\"))\n\n    assert RunnableLambda(aget_value).get_input_jsonschema() == {\n        \"title\": \"aget_value_input\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"another\": {\"title\": \"Another\"},\n            \"variable_name\": {\"title\": \"Variable Name\"},\n        },\n        \"required\": [\"another\", \"variable_name\"],\n    }\n\n    async def aget_values(input):  # type: ignore[no-untyped-def]\n        return {\n            \"hello\": input[\"variable_name\"],\n            \"bye\": input[\"variable_name\"],\n            \"byebye\": input[\"yo\"],\n        }\n\n    assert RunnableLambda(aget_values).get_input_jsonschema() == {\n        \"title\": \"aget_values_input\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"variable_name\": {\"title\": \"Variable Name\"},\n            \"yo\": {\"title\": \"Yo\"},\n        },\n        \"required\": [\"variable_name\", \"yo\"],\n    }\n\n    class InputType(TypedDict):\n        variable_name: str\n        yo: int\n\n    class OutputType(TypedDict):\n        hello: str\n        bye: str\n        byebye: int\n\n    async def aget_values_typed(input: InputType) -> OutputType:\n        return {\n            \"hello\": input[\"variable_name\"],\n            \"bye\": input[\"variable_name\"],\n            \"byebye\": input[\"yo\"],\n        }\n\n    assert _normalize_schema(\n        RunnableLambda(\n            aget_values_typed  # type: ignore[arg-type]\n        ).get_input_jsonschema()\n    ) == _normalize_schema(\n        {\n            \"$defs\": {\n                \"InputType\": {\n                    \"properties\": {\n                        \"variable_name\": {\n                            \"title\": \"Variable Name\",\n                            \"type\": \"string\",\n                        },\n                        \"yo\": {\"title\": \"Yo\", \"type\": \"integer\"},\n                    },\n                    \"required\": [\"variable_name\", \"yo\"],\n                    \"title\": \"InputType\",\n                    \"type\": \"object\",\n                }\n            },\n            \"allOf\": [{\"$ref\": \"#/$defs/InputType\"}],\n            \"title\": \"aget_values_typed_input\",\n        }\n    )\n\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(\n            RunnableLambda(aget_values_typed).get_output_jsonschema()  # type: ignore\n        ) == snapshot(name=\"schema8\")\n\n\ndef test_with_types_with_type_generics() -> None:\n    \"\"\"Verify that with_types works if we use things like List[int].\"\"\"\n\n    def foo(x: int) -> None:\n        \"\"\"Add one to the input.\"\"\"\n        raise NotImplementedError\n\n    # Try specifying some\n    RunnableLambda(foo).with_types(\n        output_type=list[int],  # type: ignore[arg-type]\n        input_type=list[int],  # type: ignore[arg-type]\n    )\n    RunnableLambda(foo).with_types(\n        output_type=Sequence[int],  # type: ignore[arg-type]\n        input_type=Sequence[int],  # type: ignore[arg-type]\n    )\n\n\ndef test_schema_with_itemgetter() -> None:\n    \"\"\"Test runnable with itemgetter.\"\"\"\n    foo = RunnableLambda(itemgetter(\"hello\"))\n    assert _schema(foo.input_schema) == {\n        \"properties\": {\"hello\": {\"title\": \"Hello\"}},\n        \"required\": [\"hello\"],\n        \"title\": \"RunnableLambdaInput\",\n        \"type\": \"object\",\n    }\n    prompt = ChatPromptTemplate.from_template(\"what is {language}?\")\n    chain: Runnable = {\"language\": itemgetter(\"language\")} | prompt\n    assert _schema(chain.input_schema) == {\n        \"properties\": {\"language\": {\"title\": \"Language\"}},\n        \"required\": [\"language\"],\n        \"title\": \"RunnableParallel<language>Input\",\n        \"type\": \"object\",\n    }\n\n\ndef test_schema_complex_seq() -> None:\n    prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n    prompt2 = ChatPromptTemplate.from_template(\n        \"what country is the city {city} in? respond in {language}\"\n    )\n\n    model = FakeListChatModel(responses=[\"\"])\n\n    chain1: Runnable = RunnableSequence(\n        prompt1, model, StrOutputParser(), name=\"city_chain\"\n    )\n\n    assert chain1.name == \"city_chain\"\n\n    chain2: Runnable = (\n        {\"city\": chain1, \"language\": itemgetter(\"language\")}\n        | prompt2\n        | model\n        | StrOutputParser()\n    )\n\n    assert chain2.get_input_jsonschema() == {\n        \"title\": \"RunnableParallel<city,language>Input\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"person\": {\"title\": \"Person\", \"type\": \"string\"},\n            \"language\": {\"title\": \"Language\"},\n        },\n        \"required\": [\"person\", \"language\"],\n    }\n\n    assert chain2.get_output_jsonschema() == {\n        \"title\": \"StrOutputParserOutput\",\n        \"type\": \"string\",\n    }\n\n    assert chain2.with_types(input_type=str).get_input_jsonschema() == {\n        \"title\": \"RunnableSequenceInput\",\n        \"type\": \"string\",\n    }\n\n    assert chain2.with_types(input_type=int).get_output_jsonschema() == {\n        \"title\": \"StrOutputParserOutput\",\n        \"type\": \"string\",\n    }\n\n    class InputType(BaseModel):\n        person: str\n\n    assert chain2.with_types(input_type=InputType).get_input_jsonschema() == {\n        \"title\": \"InputType\",\n        \"type\": \"object\",\n        \"properties\": {\"person\": {\"title\": \"Person\", \"type\": \"string\"}},\n        \"required\": [\"person\"],\n    }\n\n\ndef test_configurable_fields(snapshot: SnapshotAssertion) -> None:\n    fake_llm = FakeListLLM(responses=[\"a\"])  # str -> List[List[str]]\n\n    assert fake_llm.invoke(\"...\") == \"a\"\n\n    fake_llm_configurable = fake_llm.configurable_fields(\n        responses=ConfigurableField(\n            id=\"llm_responses\",\n            name=\"LLM Responses\",\n            description=\"A list of fake responses for this LLM\",\n        )\n    )\n\n    assert fake_llm_configurable.invoke(\"...\") == \"a\"\n\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(\n            fake_llm_configurable.get_config_jsonschema()\n        ) == snapshot(name=\"schema2\")\n\n    fake_llm_configured = fake_llm_configurable.with_config(\n        configurable={\"llm_responses\": [\"b\"]}\n    )\n\n    assert fake_llm_configured.invoke(\"...\") == \"b\"\n\n    prompt = PromptTemplate.from_template(\"Hello, {name}!\")\n\n    assert prompt.invoke({\"name\": \"John\"}) == StringPromptValue(text=\"Hello, John!\")\n\n    prompt_configurable = prompt.configurable_fields(\n        template=ConfigurableField(\n            id=\"prompt_template\",\n            name=\"Prompt Template\",\n            description=\"The prompt template for this chain\",\n        )\n    )\n\n    assert prompt_configurable.invoke({\"name\": \"John\"}) == StringPromptValue(\n        text=\"Hello, John!\"\n    )\n\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(\n            prompt_configurable.get_config_jsonschema()\n        ) == snapshot(name=\"schema3\")\n\n    prompt_configured = prompt_configurable.with_config(\n        configurable={\"prompt_template\": \"Hello, {name}! {name}!\"}\n    )\n\n    assert prompt_configured.invoke({\"name\": \"John\"}) == StringPromptValue(\n        text=\"Hello, John! John!\"\n    )\n\n    assert prompt_configurable.with_config(\n        configurable={\"prompt_template\": \"Hello {name} in {lang}\"}\n    ).get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"lang\": {\"title\": \"Lang\", \"type\": \"string\"},\n            \"name\": {\"title\": \"Name\", \"type\": \"string\"},\n        },\n        \"required\": [\"lang\", \"name\"],\n    }\n\n    chain_configurable = prompt_configurable | fake_llm_configurable | StrOutputParser()\n\n    assert chain_configurable.invoke({\"name\": \"John\"}) == \"a\"\n\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(\n            chain_configurable.get_config_jsonschema()\n        ) == snapshot(name=\"schema4\")\n\n    assert (\n        chain_configurable.with_config(\n            configurable={\n                \"prompt_template\": \"A very good morning to you, {name} {lang}!\",\n                \"llm_responses\": [\"c\"],\n            }\n        ).invoke({\"name\": \"John\", \"lang\": \"en\"})\n        == \"c\"\n    )\n\n    assert chain_configurable.with_config(\n        configurable={\n            \"prompt_template\": \"A very good morning to you, {name} {lang}!\",\n            \"llm_responses\": [\"c\"],\n        }\n    ).get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"lang\": {\"title\": \"Lang\", \"type\": \"string\"},\n            \"name\": {\"title\": \"Name\", \"type\": \"string\"},\n        },\n        \"required\": [\"lang\", \"name\"],\n    }\n\n    chain_with_map_configurable: Runnable = prompt_configurable | {\n        \"llm1\": fake_llm_configurable | StrOutputParser(),\n        \"llm2\": fake_llm_configurable | StrOutputParser(),\n        \"llm3\": fake_llm.configurable_fields(\n            responses=ConfigurableField(\"other_responses\")\n        )\n        | StrOutputParser(),\n    }\n\n    assert chain_with_map_configurable.invoke({\"name\": \"John\"}) == {\n        \"llm1\": \"a\",\n        \"llm2\": \"a\",\n        \"llm3\": \"a\",\n    }\n\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(\n            chain_with_map_configurable.get_config_jsonschema()\n        ) == snapshot(name=\"schema5\")\n\n    assert chain_with_map_configurable.with_config(\n        configurable={\n            \"prompt_template\": \"A very good morning to you, {name}!\",\n            \"llm_responses\": [\"c\"],\n            \"other_responses\": [\"d\"],\n        }\n    ).invoke({\"name\": \"John\"}) == {\"llm1\": \"c\", \"llm2\": \"c\", \"llm3\": \"d\"}\n\n\ndef test_configurable_alts_factory() -> None:\n    fake_llm = FakeListLLM(responses=[\"a\"]).configurable_alternatives(\n        ConfigurableField(id=\"llm\", name=\"LLM\"),\n        chat=partial(FakeListLLM, responses=[\"b\"]),\n    )\n\n    assert fake_llm.invoke(\"...\") == \"a\"\n\n    assert fake_llm.with_config(configurable={\"llm\": \"chat\"}).invoke(\"...\") == \"b\"\n\n\ndef test_configurable_fields_prefix_keys(snapshot: SnapshotAssertion) -> None:\n    fake_chat = FakeListChatModel(responses=[\"b\"]).configurable_fields(\n        responses=ConfigurableFieldMultiOption(\n            id=\"responses\",\n            name=\"Chat Responses\",\n            options={\n                \"hello\": \"A good morning to you!\",\n                \"bye\": \"See you later!\",\n                \"helpful\": \"How can I help you?\",\n            },\n            default=[\"hello\", \"bye\"],\n        ),\n        # (sleep is a configurable field in FakeListChatModel)\n        sleep=ConfigurableField(\n            id=\"chat_sleep\",\n            is_shared=True,\n        ),\n    )\n    fake_llm = (\n        FakeListLLM(responses=[\"a\"])\n        .configurable_fields(\n            responses=ConfigurableField(\n                id=\"responses\",\n                name=\"LLM Responses\",\n                description=\"A list of fake responses for this LLM\",\n            )\n        )\n        .configurable_alternatives(\n            ConfigurableField(id=\"llm\", name=\"LLM\"),\n            chat=fake_chat | StrOutputParser(),\n            prefix_keys=True,\n        )\n    )\n    prompt = PromptTemplate.from_template(\"Hello, {name}!\").configurable_fields(\n        template=ConfigurableFieldSingleOption(\n            id=\"prompt_template\",\n            name=\"Prompt Template\",\n            description=\"The prompt template for this chain\",\n            options={\n                \"hello\": \"Hello, {name}!\",\n                \"good_morning\": \"A very good morning to you, {name}!\",\n            },\n            default=\"hello\",\n        )\n    )\n\n    chain = prompt | fake_llm\n\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(_schema(chain.config_schema())) == snapshot(\n            name=\"schema6\"\n        )\n\n\ndef test_configurable_fields_example(snapshot: SnapshotAssertion) -> None:\n    fake_chat = FakeListChatModel(responses=[\"b\"]).configurable_fields(\n        responses=ConfigurableFieldMultiOption(\n            id=\"chat_responses\",\n            name=\"Chat Responses\",\n            options={\n                \"hello\": \"A good morning to you!\",\n                \"bye\": \"See you later!\",\n                \"helpful\": \"How can I help you?\",\n            },\n            default=[\"hello\", \"bye\"],\n        )\n    )\n    fake_llm = (\n        FakeListLLM(responses=[\"a\"])\n        .configurable_fields(\n            responses=ConfigurableField(\n                id=\"llm_responses\",\n                name=\"LLM Responses\",\n                description=\"A list of fake responses for this LLM\",\n            )\n        )\n        .configurable_alternatives(\n            ConfigurableField(id=\"llm\", name=\"LLM\"),\n            chat=fake_chat | StrOutputParser(),\n        )\n    )\n\n    prompt = PromptTemplate.from_template(\"Hello, {name}!\").configurable_fields(\n        template=ConfigurableFieldSingleOption(\n            id=\"prompt_template\",\n            name=\"Prompt Template\",\n            description=\"The prompt template for this chain\",\n            options={\n                \"hello\": \"Hello, {name}!\",\n                \"good_morning\": \"A very good morning to you, {name}!\",\n            },\n            default=\"hello\",\n        )\n    )\n\n    # deduplication of configurable fields\n    chain_configurable = prompt | fake_llm | (lambda x: {\"name\": x}) | prompt | fake_llm\n\n    assert chain_configurable.invoke({\"name\": \"John\"}) == \"a\"\n\n    if PYDANTIC_VERSION >= (2, 9):\n        assert _normalize_schema(\n            chain_configurable.get_config_jsonschema()\n        ) == snapshot(name=\"schema7\")\n\n    assert (\n        chain_configurable.with_config(configurable={\"llm\": \"chat\"}).invoke(\n            {\"name\": \"John\"}\n        )\n        == \"A good morning to you!\"\n    )\n\n    assert (\n        chain_configurable.with_config(\n            configurable={\"llm\": \"chat\", \"chat_responses\": [\"helpful\"]}\n        ).invoke({\"name\": \"John\"})\n        == \"How can I help you?\"\n    )\n\n\ndef test_passthrough_tap(mocker: MockerFixture) -> None:\n    fake = FakeRunnable()\n    mock = mocker.Mock()\n\n    seq: Runnable = RunnablePassthrough(mock) | fake | RunnablePassthrough(mock)\n\n    assert seq.invoke(\"hello\", my_kwarg=\"value\") == 5  # type: ignore[call-arg]\n    assert mock.call_args_list == [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(5),\n    ]\n    mock.reset_mock()\n\n    assert seq.batch([\"hello\", \"byebye\"], my_kwarg=\"value\") == [5, 6]\n    assert len(mock.call_args_list) == 4\n    for call in [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(\"byebye\", my_kwarg=\"value\"),\n        mocker.call(5),\n        mocker.call(6),\n    ]:\n        assert call in mock.call_args_list\n    mock.reset_mock()\n\n    assert seq.batch([\"hello\", \"byebye\"], my_kwarg=\"value\", return_exceptions=True) == [\n        5,\n        6,\n    ]\n    assert len(mock.call_args_list) == 4\n    for call in [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(\"byebye\", my_kwarg=\"value\"),\n        mocker.call(5),\n        mocker.call(6),\n    ]:\n        assert call in mock.call_args_list\n    mock.reset_mock()\n\n    assert sorted(\n        a\n        for a in seq.batch_as_completed(\n            [\"hello\", \"byebye\"], my_kwarg=\"value\", return_exceptions=True\n        )\n    ) == [\n        (0, 5),\n        (1, 6),\n    ]\n    assert len(mock.call_args_list) == 4\n    for call in [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(\"byebye\", my_kwarg=\"value\"),\n        mocker.call(5),\n        mocker.call(6),\n    ]:\n        assert call in mock.call_args_list\n    mock.reset_mock()\n\n    assert list(\n        seq.stream(\"hello\", {\"metadata\": {\"key\": \"value\"}}, my_kwarg=\"value\")\n    ) == [5]\n    assert mock.call_args_list == [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(5),\n    ]\n    mock.reset_mock()\n\n\nasync def test_passthrough_tap_async(mocker: MockerFixture) -> None:\n    fake = FakeRunnable()\n    mock = mocker.Mock()\n\n    seq: Runnable = RunnablePassthrough(mock) | fake | RunnablePassthrough(mock)\n\n    assert await seq.ainvoke(\"hello\", my_kwarg=\"value\") == 5\n    assert mock.call_args_list == [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(5),\n    ]\n    mock.reset_mock()\n\n    assert await seq.abatch([\"hello\", \"byebye\"], my_kwarg=\"value\") == [5, 6]\n    assert len(mock.call_args_list) == 4\n    for call in [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(\"byebye\", my_kwarg=\"value\"),\n        mocker.call(5),\n        mocker.call(6),\n    ]:\n        assert call in mock.call_args_list\n    mock.reset_mock()\n\n    assert await seq.abatch(\n        [\"hello\", \"byebye\"], my_kwarg=\"value\", return_exceptions=True\n    ) == [\n        5,\n        6,\n    ]\n    assert len(mock.call_args_list) == 4\n    for call in [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(\"byebye\", my_kwarg=\"value\"),\n        mocker.call(5),\n        mocker.call(6),\n    ]:\n        assert call in mock.call_args_list\n    mock.reset_mock()\n\n    assert sorted(\n        [\n            a\n            async for a in seq.abatch_as_completed(\n                [\"hello\", \"byebye\"], my_kwarg=\"value\", return_exceptions=True\n            )\n        ]\n    ) == [\n        (0, 5),\n        (1, 6),\n    ]\n    assert len(mock.call_args_list) == 4\n    for call in [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(\"byebye\", my_kwarg=\"value\"),\n        mocker.call(5),\n        mocker.call(6),\n    ]:\n        assert call in mock.call_args_list\n    mock.reset_mock()\n\n    assert [\n        part\n        async for part in seq.astream(\n            \"hello\", {\"metadata\": {\"key\": \"value\"}}, my_kwarg=\"value\"\n        )\n    ] == [5]\n    assert mock.call_args_list == [\n        mocker.call(\"hello\", my_kwarg=\"value\"),\n        mocker.call(5),\n    ]\n\n\nasync def test_with_config_metadata_passthrough(mocker: MockerFixture) -> None:\n    fake = FakeRunnableSerializable()\n    spy = mocker.spy(fake.__class__, \"invoke\")\n    fakew = fake.configurable_fields(hello=ConfigurableField(id=\"hello\", name=\"Hello\"))\n\n    assert (\n        fakew.with_config(tags=[\"a-tag\"]).invoke(\n            \"hello\",\n            {\n                \"configurable\": {\"hello\": \"there\", \"__secret_key\": \"nahnah\"},\n                \"metadata\": {\"bye\": \"now\"},\n            },\n        )\n        == 5\n    )\n    assert spy.call_args_list[0].args[1:] == (\n        \"hello\",\n        {\n            \"tags\": [\"a-tag\"],\n            \"callbacks\": None,\n            \"recursion_limit\": 25,\n            \"configurable\": {\"hello\": \"there\", \"__secret_key\": \"nahnah\"},\n            \"metadata\": {\"hello\": \"there\", \"bye\": \"now\"},\n        },\n    )\n    spy.reset_mock()\n\n\ndef test_with_config(mocker: MockerFixture) -> None:\n    fake = FakeRunnable()\n    spy = mocker.spy(fake, \"invoke\")\n\n    assert fake.with_config(tags=[\"a-tag\"]).invoke(\"hello\") == 5\n    assert spy.call_args_list == [\n        mocker.call(\n            \"hello\",\n            {\"tags\": [\"a-tag\"], \"metadata\": {}, \"configurable\": {}},\n        ),\n    ]\n    spy.reset_mock()\n\n    fake_1: Runnable = RunnablePassthrough()\n    fake_2: Runnable = RunnablePassthrough()\n    spy_seq_step = mocker.spy(fake_1.__class__, \"invoke\")\n\n    sequence = fake_1.with_config(tags=[\"a-tag\"]) | fake_2.with_config(\n        tags=[\"b-tag\"], max_concurrency=5\n    )\n    assert sequence.invoke(\"hello\") == \"hello\"\n    assert len(spy_seq_step.call_args_list) == 2\n    for i, call in enumerate(spy_seq_step.call_args_list):\n        assert call.args[1] == \"hello\"\n        if i == 0:\n            assert call.args[2].get(\"tags\") == [\"a-tag\"]\n            assert call.args[2].get(\"max_concurrency\") is None\n        else:\n            assert call.args[2].get(\"tags\") == [\"b-tag\"]\n            assert call.args[2].get(\"max_concurrency\") == 5\n    mocker.stop(spy_seq_step)\n\n    assert [\n        *fake.with_config(tags=[\"a-tag\"]).stream(\n            \"hello\", {\"metadata\": {\"key\": \"value\"}}\n        )\n    ] == [5]\n    assert spy.call_args_list == [\n        mocker.call(\n            \"hello\",\n            {\"tags\": [\"a-tag\"], \"metadata\": {\"key\": \"value\"}, \"configurable\": {}},\n        ),\n    ]\n    spy.reset_mock()\n\n    assert fake.with_config(recursion_limit=5).batch(\n        [\"hello\", \"wooorld\"], [{\"tags\": [\"a-tag\"]}, {\"metadata\": {\"key\": \"value\"}}]\n    ) == [5, 7]\n\n    assert len(spy.call_args_list) == 2\n    for i, call in enumerate(\n        sorted(spy.call_args_list, key=lambda x: 0 if x.args[0] == \"hello\" else 1)\n    ):\n        assert call.args[0] == (\"hello\" if i == 0 else \"wooorld\")\n        if i == 0:\n            assert call.args[1].get(\"recursion_limit\") == 5\n            assert call.args[1].get(\"tags\") == [\"a-tag\"]\n            assert call.args[1].get(\"metadata\") == {}\n        else:\n            assert call.args[1].get(\"recursion_limit\") == 5\n            assert call.args[1].get(\"tags\") == []\n            assert call.args[1].get(\"metadata\") == {\"key\": \"value\"}\n\n    spy.reset_mock()\n\n    assert sorted(\n        c\n        for c in fake.with_config(recursion_limit=5).batch_as_completed(\n            [\"hello\", \"wooorld\"],\n            [{\"tags\": [\"a-tag\"]}, {\"metadata\": {\"key\": \"value\"}}],\n        )\n    ) == [(0, 5), (1, 7)]\n\n    assert len(spy.call_args_list) == 2\n    for i, call in enumerate(\n        sorted(spy.call_args_list, key=lambda x: 0 if x.args[0] == \"hello\" else 1)\n    ):\n        assert call.args[0] == (\"hello\" if i == 0 else \"wooorld\")\n        if i == 0:\n            assert call.args[1].get(\"recursion_limit\") == 5\n            assert call.args[1].get(\"tags\") == [\"a-tag\"]\n            assert call.args[1].get(\"metadata\") == {}\n        else:\n            assert call.args[1].get(\"recursion_limit\") == 5\n            assert call.args[1].get(\"tags\") == []\n            assert call.args[1].get(\"metadata\") == {\"key\": \"value\"}\n\n    spy.reset_mock()\n\n    assert fake.with_config(metadata={\"a\": \"b\"}).batch(\n        [\"hello\", \"wooorld\"], {\"tags\": [\"a-tag\"]}\n    ) == [5, 7]\n    assert len(spy.call_args_list) == 2\n    for i, call in enumerate(spy.call_args_list):\n        assert call.args[0] == (\"hello\" if i == 0 else \"wooorld\")\n        assert call.args[1].get(\"tags\") == [\"a-tag\"]\n        assert call.args[1].get(\"metadata\") == {\"a\": \"b\"}\n    spy.reset_mock()\n\n    assert sorted(\n        c for c in fake.batch_as_completed([\"hello\", \"wooorld\"], {\"tags\": [\"a-tag\"]})\n    ) == [(0, 5), (1, 7)]\n    assert len(spy.call_args_list) == 2\n    for i, call in enumerate(spy.call_args_list):\n        assert call.args[0] == (\"hello\" if i == 0 else \"wooorld\")\n        assert call.args[1].get(\"tags\") == [\"a-tag\"]\n\n\nasync def test_with_config_async(mocker: MockerFixture) -> None:\n    fake = FakeRunnable()\n    spy = mocker.spy(fake, \"invoke\")\n\n    handler = ConsoleCallbackHandler()\n    assert (\n        await fake.with_config(metadata={\"a\": \"b\"}).ainvoke(\n            \"hello\", config={\"callbacks\": [handler]}\n        )\n        == 5\n    )\n    assert spy.call_args_list == [\n        mocker.call(\n            \"hello\",\n            {\n                \"callbacks\": [handler],\n                \"metadata\": {\"a\": \"b\"},\n                \"configurable\": {},\n                \"tags\": [],\n            },\n        ),\n    ]\n    spy.reset_mock()\n\n    assert [\n        part async for part in fake.with_config(metadata={\"a\": \"b\"}).astream(\"hello\")\n    ] == [5]\n    assert spy.call_args_list == [\n        mocker.call(\"hello\", {\"metadata\": {\"a\": \"b\"}, \"tags\": [], \"configurable\": {}}),\n    ]\n    spy.reset_mock()\n\n    assert await fake.with_config(recursion_limit=5, tags=[\"c\"]).abatch(\n        [\"hello\", \"wooorld\"], {\"metadata\": {\"key\": \"value\"}}\n    ) == [\n        5,\n        7,\n    ]\n    assert sorted(spy.call_args_list) == [\n        mocker.call(\n            \"hello\",\n            {\n                \"metadata\": {\"key\": \"value\"},\n                \"tags\": [\"c\"],\n                \"callbacks\": None,\n                \"recursion_limit\": 5,\n                \"configurable\": {},\n            },\n        ),\n        mocker.call(\n            \"wooorld\",\n            {\n                \"metadata\": {\"key\": \"value\"},\n                \"tags\": [\"c\"],\n                \"callbacks\": None,\n                \"recursion_limit\": 5,\n                \"configurable\": {},\n            },\n        ),\n    ]\n    spy.reset_mock()\n\n    assert sorted(\n        [\n            c\n            async for c in fake.with_config(\n                recursion_limit=5, tags=[\"c\"]\n            ).abatch_as_completed([\"hello\", \"wooorld\"], {\"metadata\": {\"key\": \"value\"}})\n        ]\n    ) == [\n        (0, 5),\n        (1, 7),\n    ]\n    assert len(spy.call_args_list) == 2\n    first_call = next(call for call in spy.call_args_list if call.args[0] == \"hello\")\n    assert first_call == mocker.call(\n        \"hello\",\n        {\n            \"metadata\": {\"key\": \"value\"},\n            \"tags\": [\"c\"],\n            \"callbacks\": None,\n            \"recursion_limit\": 5,\n            \"configurable\": {},\n        },\n    )\n    second_call = next(call for call in spy.call_args_list if call.args[0] == \"wooorld\")\n    assert second_call == mocker.call(\n        \"wooorld\",\n        {\n            \"metadata\": {\"key\": \"value\"},\n            \"tags\": [\"c\"],\n            \"callbacks\": None,\n            \"recursion_limit\": 5,\n            \"configurable\": {},\n        },\n    )\n\n\ndef test_default_method_implementations(mocker: MockerFixture) -> None:\n    fake = FakeRunnable()\n    spy = mocker.spy(fake, \"invoke\")\n\n    assert fake.invoke(\"hello\", {\"tags\": [\"a-tag\"]}) == 5\n    assert spy.call_args_list == [\n        mocker.call(\"hello\", {\"tags\": [\"a-tag\"]}),\n    ]\n    spy.reset_mock()\n\n    assert [*fake.stream(\"hello\", {\"metadata\": {\"key\": \"value\"}})] == [5]\n    assert spy.call_args_list == [\n        mocker.call(\"hello\", {\"metadata\": {\"key\": \"value\"}}),\n    ]\n    spy.reset_mock()\n\n    assert fake.batch(\n        [\"hello\", \"wooorld\"], [{\"tags\": [\"a-tag\"]}, {\"metadata\": {\"key\": \"value\"}}]\n    ) == [5, 7]\n\n    assert len(spy.call_args_list) == 2\n    for call in spy.call_args_list:\n        call_arg = call.args[0]\n\n        if call_arg == \"hello\":\n            assert call_arg == \"hello\"\n            assert call.args[1].get(\"tags\") == [\"a-tag\"]\n            assert call.args[1].get(\"metadata\") == {}\n        else:\n            assert call_arg == \"wooorld\"\n            assert call.args[1].get(\"tags\") == []\n            assert call.args[1].get(\"metadata\") == {\"key\": \"value\"}\n\n    spy.reset_mock()\n\n    assert fake.batch([\"hello\", \"wooorld\"], {\"tags\": [\"a-tag\"]}) == [5, 7]\n    assert len(spy.call_args_list) == 2\n    assert {call.args[0] for call in spy.call_args_list} == {\"hello\", \"wooorld\"}\n    for call in spy.call_args_list:\n        assert call.args[1].get(\"tags\") == [\"a-tag\"]\n        assert call.args[1].get(\"metadata\") == {}\n\n\nasync def test_default_method_implementations_async(mocker: MockerFixture) -> None:\n    fake = FakeRunnable()\n    spy = mocker.spy(fake, \"invoke\")\n\n    assert await fake.ainvoke(\"hello\", config={\"callbacks\": []}) == 5\n    assert spy.call_args_list == [\n        mocker.call(\"hello\", {\"callbacks\": []}),\n    ]\n    spy.reset_mock()\n\n    assert [part async for part in fake.astream(\"hello\")] == [5]\n    assert spy.call_args_list == [\n        mocker.call(\"hello\", None),\n    ]\n    spy.reset_mock()\n\n    assert await fake.abatch([\"hello\", \"wooorld\"], {\"metadata\": {\"key\": \"value\"}}) == [\n        5,\n        7,\n    ]\n    assert {call.args[0] for call in spy.call_args_list} == {\"hello\", \"wooorld\"}\n    for call in spy.call_args_list:\n        assert call.args[1] == {\n            \"metadata\": {\"key\": \"value\"},\n            \"tags\": [],\n            \"callbacks\": None,\n            \"recursion_limit\": 25,\n            \"configurable\": {},\n        }\n\n\ndef test_prompt() -> None:\n    prompt = ChatPromptTemplate.from_messages(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessagePromptTemplate.from_template(\"{question}\"),\n        ]\n    )\n    expected = ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n    assert prompt.invoke({\"question\": \"What is your name?\"}) == expected\n\n    assert prompt.batch(\n        [\n            {\"question\": \"What is your name?\"},\n            {\"question\": \"What is your favorite color?\"},\n        ]\n    ) == [\n        expected,\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your favorite color?\"),\n            ]\n        ),\n    ]\n\n    assert [*prompt.stream({\"question\": \"What is your name?\"})] == [expected]\n\n\nasync def test_prompt_async() -> None:\n    prompt = ChatPromptTemplate.from_messages(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessagePromptTemplate.from_template(\"{question}\"),\n        ]\n    )\n    expected = ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n    assert await prompt.ainvoke({\"question\": \"What is your name?\"}) == expected\n\n    assert await prompt.abatch(\n        [\n            {\"question\": \"What is your name?\"},\n            {\"question\": \"What is your favorite color?\"},\n        ]\n    ) == [\n        expected,\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your favorite color?\"),\n            ]\n        ),\n    ]\n\n    assert [\n        part async for part in prompt.astream({\"question\": \"What is your name?\"})\n    ] == [expected]\n\n    stream_log = [\n        part async for part in prompt.astream_log({\"question\": \"What is your name?\"})\n    ]\n\n    assert len(stream_log[0].ops) == 1\n    assert stream_log[0].ops[0][\"op\"] == \"replace\"\n    assert stream_log[0].ops[0][\"path\"] == \"\"\n    assert stream_log[0].ops[0][\"value\"][\"logs\"] == {}\n    assert stream_log[0].ops[0][\"value\"][\"final_output\"] is None\n    assert stream_log[0].ops[0][\"value\"][\"streamed_output\"] == []\n    assert isinstance(stream_log[0].ops[0][\"value\"][\"id\"], str)\n\n    assert stream_log[1:] == [\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": expected},\n            {\n                \"op\": \"replace\",\n                \"path\": \"/final_output\",\n                \"value\": ChatPromptValue(\n                    messages=[\n                        SystemMessage(content=\"You are a nice assistant.\"),\n                        HumanMessage(content=\"What is your name?\"),\n                    ]\n                ),\n            },\n        ),\n    ]\n\n    stream_log_state = [\n        part\n        async for part in prompt.astream_log(\n            {\"question\": \"What is your name?\"}, diff=False\n        )\n    ]\n\n    # remove random id\n    stream_log[0].ops[0][\"value\"][\"id\"] = \"00000000-0000-0000-0000-000000000000\"\n    stream_log_state[-1].ops[0][\"value\"][\"id\"] = \"00000000-0000-0000-0000-000000000000\"\n    stream_log_state[-1].state[\"id\"] = \"00000000-0000-0000-0000-000000000000\"\n\n    # assert output with diff=False matches output with diff=True\n    assert stream_log_state[-1].ops == [op for chunk in stream_log for op in chunk.ops]\n    assert stream_log_state[-1] == RunLog(\n        *[op for chunk in stream_log for op in chunk.ops],\n        state={\n            \"final_output\": ChatPromptValue(\n                messages=[\n                    SystemMessage(content=\"You are a nice assistant.\"),\n                    HumanMessage(content=\"What is your name?\"),\n                ]\n            ),\n            \"id\": \"00000000-0000-0000-0000-000000000000\",\n            \"logs\": {},\n            \"streamed_output\": [\n                ChatPromptValue(\n                    messages=[\n                        SystemMessage(content=\"You are a nice assistant.\"),\n                        HumanMessage(content=\"What is your name?\"),\n                    ]\n                )\n            ],\n            \"type\": \"prompt\",\n            \"name\": \"ChatPromptTemplate\",\n        },\n    )\n\n    # nested inside trace_with_chain_group\n\n    async with atrace_as_chain_group(\"a_group\") as manager:\n        stream_log_nested = [\n            part\n            async for part in prompt.astream_log(\n                {\"question\": \"What is your name?\"}, config={\"callbacks\": manager}\n            )\n        ]\n\n    assert len(stream_log_nested[0].ops) == 1\n    assert stream_log_nested[0].ops[0][\"op\"] == \"replace\"\n    assert stream_log_nested[0].ops[0][\"path\"] == \"\"\n    assert stream_log_nested[0].ops[0][\"value\"][\"logs\"] == {}\n    assert stream_log_nested[0].ops[0][\"value\"][\"final_output\"] is None\n    assert stream_log_nested[0].ops[0][\"value\"][\"streamed_output\"] == []\n    assert isinstance(stream_log_nested[0].ops[0][\"value\"][\"id\"], str)\n\n    assert stream_log_nested[1:] == [\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": expected},\n            {\n                \"op\": \"replace\",\n                \"path\": \"/final_output\",\n                \"value\": ChatPromptValue(\n                    messages=[\n                        SystemMessage(content=\"You are a nice assistant.\"),\n                        HumanMessage(content=\"What is your name?\"),\n                    ]\n                ),\n            },\n        ),\n    ]\n\n\ndef test_prompt_template_params() -> None:\n    prompt = ChatPromptTemplate.from_template(\n        \"Respond to the following question: {question}\"\n    )\n    result = prompt.invoke(\n        {\n            \"question\": \"test\",\n            \"topic\": \"test\",\n        }\n    )\n    assert result == ChatPromptValue(\n        messages=[HumanMessage(content=\"Respond to the following question: test\")]\n    )\n\n    with pytest.raises(KeyError):\n        prompt.invoke({})\n\n\ndef test_with_listeners(mocker: MockerFixture) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    chat = FakeListChatModel(responses=[\"foo\"])\n\n    chain: Runnable = prompt | chat\n\n    mock_start = mocker.Mock()\n    mock_end = mocker.Mock()\n\n    chain.with_listeners(on_start=mock_start, on_end=mock_end).invoke(\n        {\"question\": \"Who are you?\"}\n    )\n\n    assert mock_start.call_count == 1\n    assert mock_start.call_args[0][0].name == \"RunnableSequence\"\n    assert mock_end.call_count == 1\n\n    mock_start.reset_mock()\n    mock_end.reset_mock()\n\n    with trace_as_chain_group(\"hello\") as manager:\n        chain.with_listeners(on_start=mock_start, on_end=mock_end).invoke(\n            {\"question\": \"Who are you?\"}, {\"callbacks\": manager}\n        )\n\n    assert mock_start.call_count == 1\n    assert mock_start.call_args[0][0].name == \"RunnableSequence\"\n    assert mock_end.call_count == 1\n\n\nasync def test_with_listeners_async(mocker: MockerFixture) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    chat = FakeListChatModel(responses=[\"foo\"])\n\n    chain: Runnable = prompt | chat\n\n    mock_start = mocker.Mock()\n    mock_end = mocker.Mock()\n\n    await chain.with_listeners(on_start=mock_start, on_end=mock_end).ainvoke(\n        {\"question\": \"Who are you?\"}\n    )\n\n    assert mock_start.call_count == 1\n    assert mock_start.call_args[0][0].name == \"RunnableSequence\"\n    assert mock_end.call_count == 1\n\n    mock_start.reset_mock()\n    mock_end.reset_mock()\n\n    async with atrace_as_chain_group(\"hello\") as manager:\n        await chain.with_listeners(on_start=mock_start, on_end=mock_end).ainvoke(\n            {\"question\": \"Who are you?\"}, {\"callbacks\": manager}\n        )\n\n    assert mock_start.call_count == 1\n    assert mock_start.call_args[0][0].name == \"RunnableSequence\"\n    assert mock_end.call_count == 1\n\n\n@freeze_time(\"2023-01-01\")\ndef test_prompt_with_chat_model(\n    mocker: MockerFixture,\n    snapshot: SnapshotAssertion,\n    deterministic_uuids: MockerFixture,\n) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    chat = FakeListChatModel(responses=[\"foo\"])\n\n    chain: Runnable = prompt | chat\n\n    assert repr(chain) == snapshot\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == []\n    assert chain.last == chat\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"invoke\")\n    chat_spy = mocker.spy(chat.__class__, \"invoke\")\n    tracer = FakeTracer()\n    assert chain.invoke(\n        {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n    ) == _any_id_ai_message(content=\"foo\")\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n    assert tracer.runs == snapshot\n\n    mocker.stop(prompt_spy)\n    mocker.stop(chat_spy)\n\n    # Test batch\n    prompt_spy = mocker.spy(prompt.__class__, \"batch\")\n    chat_spy = mocker.spy(chat.__class__, \"batch\")\n    tracer = FakeTracer()\n    assert chain.batch(\n        [\n            {\"question\": \"What is your name?\"},\n            {\"question\": \"What is your favorite color?\"},\n        ],\n        {\"callbacks\": [tracer]},\n    ) == [\n        _any_id_ai_message(content=\"foo\"),\n        _any_id_ai_message(content=\"foo\"),\n    ]\n    assert prompt_spy.call_args.args[1] == [\n        {\"question\": \"What is your name?\"},\n        {\"question\": \"What is your favorite color?\"},\n    ]\n    assert chat_spy.call_args.args[1] == [\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your name?\"),\n            ]\n        ),\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your favorite color?\"),\n            ]\n        ),\n    ]\n    assert (\n        len(\n            [\n                r\n                for r in tracer.runs\n                if r.parent_run_id is None and len(r.child_runs) == 2\n            ]\n        )\n        == 2\n    ), \"Each of 2 outer runs contains exactly two inner runs (1 prompt, 1 chat)\"\n    mocker.stop(prompt_spy)\n    mocker.stop(chat_spy)\n\n    # Test stream\n    prompt_spy = mocker.spy(prompt.__class__, \"invoke\")\n    chat_spy = mocker.spy(chat.__class__, \"stream\")\n    tracer = FakeTracer()\n    assert [\n        *chain.stream({\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]})\n    ] == [\n        _any_id_ai_message_chunk(content=\"f\"),\n        _any_id_ai_message_chunk(content=\"o\"),\n        _any_id_ai_message_chunk(content=\"o\"),\n    ]\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n\n@freeze_time(\"2023-01-01\")\nasync def test_prompt_with_chat_model_async(\n    mocker: MockerFixture,\n    snapshot: SnapshotAssertion,\n    deterministic_uuids: MockerFixture,\n) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    chat = FakeListChatModel(responses=[\"foo\"])\n\n    chain: Runnable = prompt | chat\n\n    assert repr(chain) == snapshot\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == []\n    assert chain.last == chat\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"ainvoke\")\n    chat_spy = mocker.spy(chat.__class__, \"ainvoke\")\n    tracer = FakeTracer()\n    assert await chain.ainvoke(\n        {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n    ) == _any_id_ai_message(content=\"foo\")\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n    assert tracer.runs == snapshot\n\n    mocker.stop(prompt_spy)\n    mocker.stop(chat_spy)\n\n    # Test batch\n    prompt_spy = mocker.spy(prompt.__class__, \"abatch\")\n    chat_spy = mocker.spy(chat.__class__, \"abatch\")\n    tracer = FakeTracer()\n    assert await chain.abatch(\n        [\n            {\"question\": \"What is your name?\"},\n            {\"question\": \"What is your favorite color?\"},\n        ],\n        {\"callbacks\": [tracer]},\n    ) == [\n        _any_id_ai_message(content=\"foo\"),\n        _any_id_ai_message(content=\"foo\"),\n    ]\n    assert prompt_spy.call_args.args[1] == [\n        {\"question\": \"What is your name?\"},\n        {\"question\": \"What is your favorite color?\"},\n    ]\n    assert chat_spy.call_args.args[1] == [\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your name?\"),\n            ]\n        ),\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your favorite color?\"),\n            ]\n        ),\n    ]\n    assert (\n        len(\n            [\n                r\n                for r in tracer.runs\n                if r.parent_run_id is None and len(r.child_runs) == 2\n            ]\n        )\n        == 2\n    ), \"Each of 2 outer runs contains exactly two inner runs (1 prompt, 1 chat)\"\n    mocker.stop(prompt_spy)\n    mocker.stop(chat_spy)\n\n    # Test stream\n    prompt_spy = mocker.spy(prompt.__class__, \"ainvoke\")\n    chat_spy = mocker.spy(chat.__class__, \"astream\")\n    tracer = FakeTracer()\n    assert [\n        a\n        async for a in chain.astream(\n            {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n        )\n    ] == [\n        _any_id_ai_message_chunk(content=\"f\"),\n        _any_id_ai_message_chunk(content=\"o\"),\n        _any_id_ai_message_chunk(content=\"o\"),\n    ]\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n\n@pytest.mark.skipif(\n    condition=sys.version_info[1] == 13,\n    reason=(\n        \"temporary, py3.13 exposes some invalid assumptions about order of batch async \"\n        \"executions.\"\n    ),\n)\n@freeze_time(\"2023-01-01\")\nasync def test_prompt_with_llm(\n    mocker: MockerFixture, snapshot: SnapshotAssertion\n) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeListLLM(responses=[\"foo\", \"bar\"])\n\n    chain: Runnable = prompt | llm\n\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == []\n    assert chain.last == llm\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"ainvoke\")\n    llm_spy = mocker.spy(llm.__class__, \"ainvoke\")\n    tracer = FakeTracer()\n    assert (\n        await chain.ainvoke({\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]})\n        == \"foo\"\n    )\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert llm_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert tracer.runs == snapshot\n    mocker.stop(prompt_spy)\n    mocker.stop(llm_spy)\n\n    # Test batch\n    prompt_spy = mocker.spy(prompt.__class__, \"abatch\")\n    llm_spy = mocker.spy(llm.__class__, \"abatch\")\n    tracer = FakeTracer()\n    assert await chain.abatch(\n        [\n            {\"question\": \"What is your name?\"},\n            {\"question\": \"What is your favorite color?\"},\n        ],\n        {\"callbacks\": [tracer]},\n    ) == [\"bar\", \"foo\"]\n    assert prompt_spy.call_args.args[1] == [\n        {\"question\": \"What is your name?\"},\n        {\"question\": \"What is your favorite color?\"},\n    ]\n    assert llm_spy.call_args.args[1] == [\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your name?\"),\n            ]\n        ),\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your favorite color?\"),\n            ]\n        ),\n    ]\n    assert tracer.runs == snapshot\n    mocker.stop(prompt_spy)\n    mocker.stop(llm_spy)\n\n    # Test stream\n    prompt_spy = mocker.spy(prompt.__class__, \"ainvoke\")\n    llm_spy = mocker.spy(llm.__class__, \"astream\")\n    tracer = FakeTracer()\n    assert [\n        token\n        async for token in chain.astream(\n            {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n        )\n    ] == [\"bar\"]\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert llm_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n    prompt_spy.reset_mock()\n    llm_spy.reset_mock()\n    stream_log = [\n        part async for part in chain.astream_log({\"question\": \"What is your name?\"})\n    ]\n\n    # remove ids from logs\n    for part in stream_log:\n        for op in part.ops:\n            if (\n                isinstance(op[\"value\"], dict)\n                and \"id\" in op[\"value\"]\n                and not isinstance(op[\"value\"][\"id\"], list)  # serialized lc id\n            ):\n                del op[\"value\"][\"id\"]\n\n    expected = [\n        RunLogPatch(\n            {\n                \"op\": \"replace\",\n                \"path\": \"\",\n                \"value\": {\n                    \"logs\": {},\n                    \"final_output\": None,\n                    \"streamed_output\": [],\n                    \"name\": \"RunnableSequence\",\n                    \"type\": \"chain\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/ChatPromptTemplate\",\n                \"value\": {\n                    \"end_time\": None,\n                    \"final_output\": None,\n                    \"metadata\": {},\n                    \"name\": \"ChatPromptTemplate\",\n                    \"start_time\": \"2023-01-01T00:00:00.000+00:00\",\n                    \"streamed_output\": [],\n                    \"streamed_output_str\": [],\n                    \"tags\": [\"seq:step:1\"],\n                    \"type\": \"prompt\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/ChatPromptTemplate/final_output\",\n                \"value\": ChatPromptValue(\n                    messages=[\n                        SystemMessage(content=\"You are a nice assistant.\"),\n                        HumanMessage(content=\"What is your name?\"),\n                    ]\n                ),\n            },\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/ChatPromptTemplate/end_time\",\n                \"value\": \"2023-01-01T00:00:00.000+00:00\",\n            },\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/FakeListLLM\",\n                \"value\": {\n                    \"end_time\": None,\n                    \"final_output\": None,\n                    \"metadata\": {\"ls_model_type\": \"llm\", \"ls_provider\": \"fakelist\"},\n                    \"name\": \"FakeListLLM\",\n                    \"start_time\": \"2023-01-01T00:00:00.000+00:00\",\n                    \"streamed_output\": [],\n                    \"streamed_output_str\": [],\n                    \"tags\": [\"seq:step:2\"],\n                    \"type\": \"llm\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/FakeListLLM/final_output\",\n                \"value\": {\n                    \"generations\": [\n                        [{\"generation_info\": None, \"text\": \"foo\", \"type\": \"Generation\"}]\n                    ],\n                    \"llm_output\": None,\n                    \"run\": None,\n                    \"type\": \"LLMResult\",\n                },\n            },\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/FakeListLLM/end_time\",\n                \"value\": \"2023-01-01T00:00:00.000+00:00\",\n            },\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": \"foo\"},\n            {\"op\": \"replace\", \"path\": \"/final_output\", \"value\": \"foo\"},\n        ),\n    ]\n    assert stream_log == expected\n\n\n@freeze_time(\"2023-01-01\")\nasync def test_prompt_with_llm_parser(\n    mocker: MockerFixture, snapshot: SnapshotAssertion\n) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeStreamingListLLM(responses=[\"bear, dog, cat\", \"tomato, lettuce, onion\"])\n    parser = CommaSeparatedListOutputParser()\n\n    chain: Runnable = prompt | llm | parser\n\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == [llm]\n    assert chain.last == parser\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"ainvoke\")\n    llm_spy = mocker.spy(llm.__class__, \"ainvoke\")\n    parser_spy = mocker.spy(parser.__class__, \"ainvoke\")\n    tracer = FakeTracer()\n    assert await chain.ainvoke(\n        {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n    ) == [\"bear\", \"dog\", \"cat\"]\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert llm_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert parser_spy.call_args.args[1] == \"bear, dog, cat\"\n    assert tracer.runs == snapshot\n    mocker.stop(prompt_spy)\n    mocker.stop(llm_spy)\n    mocker.stop(parser_spy)\n\n    # Test batch\n    prompt_spy = mocker.spy(prompt.__class__, \"abatch\")\n    llm_spy = mocker.spy(llm.__class__, \"abatch\")\n    parser_spy = mocker.spy(parser.__class__, \"abatch\")\n    tracer = FakeTracer()\n    assert await chain.abatch(\n        [\n            {\"question\": \"What is your name?\"},\n            {\"question\": \"What is your favorite color?\"},\n        ],\n        {\"callbacks\": [tracer]},\n    ) == [[\"tomato\", \"lettuce\", \"onion\"], [\"bear\", \"dog\", \"cat\"]]\n    assert prompt_spy.call_args.args[1] == [\n        {\"question\": \"What is your name?\"},\n        {\"question\": \"What is your favorite color?\"},\n    ]\n    assert llm_spy.call_args.args[1] == [\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your name?\"),\n            ]\n        ),\n        ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your favorite color?\"),\n            ]\n        ),\n    ]\n    assert parser_spy.call_args.args[1] == [\n        \"tomato, lettuce, onion\",\n        \"bear, dog, cat\",\n    ]\n    assert len(tracer.runs) == 2\n    assert all(\n        run.name == \"RunnableSequence\"\n        and run.run_type == \"chain\"\n        and len(run.child_runs) == 3\n        for run in tracer.runs\n    )\n    mocker.stop(prompt_spy)\n    mocker.stop(llm_spy)\n    mocker.stop(parser_spy)\n\n    # Test stream\n    prompt_spy = mocker.spy(prompt.__class__, \"ainvoke\")\n    llm_spy = mocker.spy(llm.__class__, \"astream\")\n    tracer = FakeTracer()\n    assert [\n        token\n        async for token in chain.astream(\n            {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n        )\n    ] == [[\"tomato\"], [\"lettuce\"], [\"onion\"]]\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert llm_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n\n    prompt_spy.reset_mock()\n    llm_spy.reset_mock()\n    stream_log = [\n        part async for part in chain.astream_log({\"question\": \"What is your name?\"})\n    ]\n\n    # remove ids from logs\n    for part in stream_log:\n        for op in part.ops:\n            if (\n                isinstance(op[\"value\"], dict)\n                and \"id\" in op[\"value\"]\n                and not isinstance(op[\"value\"][\"id\"], list)  # serialized lc id\n            ):\n                del op[\"value\"][\"id\"]\n\n    expected = [\n        RunLogPatch(\n            {\n                \"op\": \"replace\",\n                \"path\": \"\",\n                \"value\": {\n                    \"logs\": {},\n                    \"final_output\": None,\n                    \"streamed_output\": [],\n                    \"name\": \"RunnableSequence\",\n                    \"type\": \"chain\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/ChatPromptTemplate\",\n                \"value\": {\n                    \"end_time\": None,\n                    \"final_output\": None,\n                    \"metadata\": {},\n                    \"name\": \"ChatPromptTemplate\",\n                    \"start_time\": \"2023-01-01T00:00:00.000+00:00\",\n                    \"streamed_output\": [],\n                    \"streamed_output_str\": [],\n                    \"tags\": [\"seq:step:1\"],\n                    \"type\": \"prompt\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/ChatPromptTemplate/final_output\",\n                \"value\": ChatPromptValue(\n                    messages=[\n                        SystemMessage(content=\"You are a nice assistant.\"),\n                        HumanMessage(content=\"What is your name?\"),\n                    ]\n                ),\n            },\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/ChatPromptTemplate/end_time\",\n                \"value\": \"2023-01-01T00:00:00.000+00:00\",\n            },\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/FakeStreamingListLLM\",\n                \"value\": {\n                    \"end_time\": None,\n                    \"final_output\": None,\n                    \"metadata\": {\n                        \"ls_model_type\": \"llm\",\n                        \"ls_provider\": \"fakestreaminglist\",\n                    },\n                    \"name\": \"FakeStreamingListLLM\",\n                    \"start_time\": \"2023-01-01T00:00:00.000+00:00\",\n                    \"streamed_output\": [],\n                    \"streamed_output_str\": [],\n                    \"tags\": [\"seq:step:2\"],\n                    \"type\": \"llm\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/FakeStreamingListLLM/final_output\",\n                \"value\": {\n                    \"generations\": [\n                        [\n                            {\n                                \"generation_info\": None,\n                                \"text\": \"bear, dog, cat\",\n                                \"type\": \"Generation\",\n                            }\n                        ]\n                    ],\n                    \"llm_output\": None,\n                    \"run\": None,\n                    \"type\": \"LLMResult\",\n                },\n            },\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/FakeStreamingListLLM/end_time\",\n                \"value\": \"2023-01-01T00:00:00.000+00:00\",\n            },\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/CommaSeparatedListOutputParser\",\n                \"value\": {\n                    \"end_time\": None,\n                    \"final_output\": None,\n                    \"metadata\": {},\n                    \"name\": \"CommaSeparatedListOutputParser\",\n                    \"start_time\": \"2023-01-01T00:00:00.000+00:00\",\n                    \"streamed_output\": [],\n                    \"streamed_output_str\": [],\n                    \"tags\": [\"seq:step:3\"],\n                    \"type\": \"parser\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/CommaSeparatedListOutputParser/streamed_output/-\",\n                \"value\": [\"bear\"],\n            }\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": [\"bear\"]},\n            {\"op\": \"replace\", \"path\": \"/final_output\", \"value\": [\"bear\"]},\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/CommaSeparatedListOutputParser/streamed_output/-\",\n                \"value\": [\"dog\"],\n            }\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": [\"dog\"]},\n            {\"op\": \"add\", \"path\": \"/final_output/1\", \"value\": \"dog\"},\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/CommaSeparatedListOutputParser/streamed_output/-\",\n                \"value\": [\"cat\"],\n            }\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": [\"cat\"]},\n            {\"op\": \"add\", \"path\": \"/final_output/2\", \"value\": \"cat\"},\n        ),\n        RunLogPatch(\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/CommaSeparatedListOutputParser/final_output\",\n                \"value\": {\"output\": [\"bear\", \"dog\", \"cat\"]},\n            },\n            {\n                \"op\": \"add\",\n                \"path\": \"/logs/CommaSeparatedListOutputParser/end_time\",\n                \"value\": \"2023-01-01T00:00:00.000+00:00\",\n            },\n        ),\n    ]\n    assert stream_log == expected\n\n\n@freeze_time(\"2023-01-01\")\nasync def test_stream_log_retriever() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{documents}\"\n        + \"{question}\"\n    )\n    llm = FakeListLLM(responses=[\"foo\", \"bar\"])\n\n    chain: Runnable = (\n        {\"documents\": FakeRetriever(), \"question\": itemgetter(\"question\")}\n        | prompt\n        | {\"one\": llm, \"two\": llm}\n    )\n\n    stream_log = [\n        part async for part in chain.astream_log({\"question\": \"What is your name?\"})\n    ]\n\n    # remove ids from logs\n    for part in stream_log:\n        for op in part.ops:\n            if (\n                isinstance(op[\"value\"], dict)\n                and \"id\" in op[\"value\"]\n                and not isinstance(op[\"value\"][\"id\"], list)  # serialized lc id\n            ):\n                del op[\"value\"][\"id\"]\n\n    assert sorted(cast(RunLog, add(stream_log)).state[\"logs\"]) == [\n        \"ChatPromptTemplate\",\n        \"FakeListLLM\",\n        \"FakeListLLM:2\",\n        \"FakeRetriever\",\n        \"RunnableLambda\",\n        \"RunnableParallel<documents,question>\",\n        \"RunnableParallel<one,two>\",\n    ]\n\n\n@freeze_time(\"2023-01-01\")\nasync def test_stream_log_lists() -> None:\n    async def list_producer(input: AsyncIterator[Any]) -> AsyncIterator[AddableDict]:\n        for i in range(4):\n            yield AddableDict(alist=[str(i)])\n\n    chain: Runnable = RunnableGenerator(list_producer)\n\n    stream_log = [\n        part async for part in chain.astream_log({\"question\": \"What is your name?\"})\n    ]\n\n    # remove ids from logs\n    for part in stream_log:\n        for op in part.ops:\n            if (\n                isinstance(op[\"value\"], dict)\n                and \"id\" in op[\"value\"]\n                and not isinstance(op[\"value\"][\"id\"], list)  # serialized lc id\n            ):\n                del op[\"value\"][\"id\"]\n\n    assert stream_log == [\n        RunLogPatch(\n            {\n                \"op\": \"replace\",\n                \"path\": \"\",\n                \"value\": {\n                    \"final_output\": None,\n                    \"logs\": {},\n                    \"streamed_output\": [],\n                    \"name\": \"list_producer\",\n                    \"type\": \"chain\",\n                },\n            }\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": {\"alist\": [\"0\"]}},\n            {\"op\": \"replace\", \"path\": \"/final_output\", \"value\": {\"alist\": [\"0\"]}},\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": {\"alist\": [\"1\"]}},\n            {\"op\": \"add\", \"path\": \"/final_output/alist/1\", \"value\": \"1\"},\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": {\"alist\": [\"2\"]}},\n            {\"op\": \"add\", \"path\": \"/final_output/alist/2\", \"value\": \"2\"},\n        ),\n        RunLogPatch(\n            {\"op\": \"add\", \"path\": \"/streamed_output/-\", \"value\": {\"alist\": [\"3\"]}},\n            {\"op\": \"add\", \"path\": \"/final_output/alist/3\", \"value\": \"3\"},\n        ),\n    ]\n\n    state = add(stream_log)\n\n    assert isinstance(state, RunLog)\n\n    assert state.state == {\n        \"final_output\": {\"alist\": [\"0\", \"1\", \"2\", \"3\"]},\n        \"logs\": {},\n        \"name\": \"list_producer\",\n        \"streamed_output\": [\n            {\"alist\": [\"0\"]},\n            {\"alist\": [\"1\"]},\n            {\"alist\": [\"2\"]},\n            {\"alist\": [\"3\"]},\n        ],\n        \"type\": \"chain\",\n    }\n\n\n@freeze_time(\"2023-01-01\")\nasync def test_prompt_with_llm_and_async_lambda(\n    mocker: MockerFixture, snapshot: SnapshotAssertion\n) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeListLLM(responses=[\"foo\", \"bar\"])\n\n    async def passthrough(input: Any) -> Any:\n        return input\n\n    chain = prompt | llm | passthrough\n\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == [llm]\n    assert chain.last == RunnableLambda(func=passthrough)\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"ainvoke\")\n    llm_spy = mocker.spy(llm.__class__, \"ainvoke\")\n    tracer = FakeTracer()\n    assert (\n        await chain.ainvoke({\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]})\n        == \"foo\"\n    )\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert llm_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert tracer.runs == snapshot\n    mocker.stop(prompt_spy)\n    mocker.stop(llm_spy)\n\n\n@freeze_time(\"2023-01-01\")\ndef test_prompt_with_chat_model_and_parser(\n    mocker: MockerFixture,\n    snapshot: SnapshotAssertion,\n    deterministic_uuids: MockerFixture,\n) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    chat = FakeListChatModel(responses=[\"foo, bar\"])\n    parser = CommaSeparatedListOutputParser()\n\n    chain = prompt | chat | parser\n\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == [chat]\n    assert chain.last == parser\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"invoke\")\n    chat_spy = mocker.spy(chat.__class__, \"invoke\")\n    parser_spy = mocker.spy(parser.__class__, \"invoke\")\n    tracer = FakeTracer()\n    assert chain.invoke(\n        {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n    ) == [\"foo\", \"bar\"]\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert parser_spy.call_args.args[1] == _any_id_ai_message(content=\"foo, bar\")\n\n    assert tracer.runs == snapshot\n\n\n@freeze_time(\"2023-01-01\")\ndef test_combining_sequences(\n    mocker: MockerFixture,\n    snapshot: SnapshotAssertion,\n    deterministic_uuids: MockerFixture,\n) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    chat = FakeListChatModel(responses=[\"foo, bar\"])\n    parser = CommaSeparatedListOutputParser()\n\n    chain = prompt | chat | parser\n\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == [chat]\n    assert chain.last == parser\n    assert dumps(chain, pretty=True) == snapshot\n\n    prompt2 = (\n        SystemMessagePromptTemplate.from_template(\"You are a nicer assistant.\")\n        + \"{question}\"\n    )\n    chat2 = FakeListChatModel(responses=[\"baz, qux\"])\n    parser2 = CommaSeparatedListOutputParser()\n    input_formatter: RunnableLambda[list[str], dict[str, Any]] = RunnableLambda(\n        lambda x: {\"question\": x[0] + x[1]}\n    )\n\n    chain2 = cast(RunnableSequence, input_formatter | prompt2 | chat2 | parser2)\n\n    assert isinstance(chain, RunnableSequence)\n    assert chain2.first == input_formatter\n    assert chain2.middle == [prompt2, chat2]\n    assert chain2.last == parser2\n    assert dumps(chain2, pretty=True) == snapshot\n\n    combined_chain = cast(RunnableSequence, chain | chain2)\n\n    assert combined_chain.first == prompt\n    assert combined_chain.middle == [\n        chat,\n        parser,\n        input_formatter,\n        prompt2,\n        chat2,\n    ]\n    assert combined_chain.last == parser2\n    assert dumps(combined_chain, pretty=True) == snapshot\n\n    # Test invoke\n    tracer = FakeTracer()\n    assert combined_chain.invoke(\n        {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n    ) == [\"baz\", \"qux\"]\n\n    assert tracer.runs == snapshot\n\n\n@freeze_time(\"2023-01-01\")\ndef test_seq_dict_prompt_llm(\n    mocker: MockerFixture, snapshot: SnapshotAssertion\n) -> None:\n    passthrough = mocker.Mock(side_effect=lambda x: x)\n\n    retriever = FakeRetriever()\n\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"\"\"Context:\n{documents}\n\nQuestion:\n{question}\"\"\"\n    )\n\n    chat = FakeListChatModel(responses=[\"foo, bar\"])\n\n    parser = CommaSeparatedListOutputParser()\n\n    chain: Runnable = (\n        {\n            \"question\": RunnablePassthrough[str]() | passthrough,\n            \"documents\": passthrough | retriever,\n            \"just_to_test_lambda\": passthrough,\n        }\n        | prompt\n        | chat\n        | parser\n    )\n\n    assert repr(chain) == snapshot\n    assert isinstance(chain, RunnableSequence)\n    assert isinstance(chain.first, RunnableParallel)\n    assert chain.middle == [prompt, chat]\n    assert chain.last == parser\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"invoke\")\n    chat_spy = mocker.spy(chat.__class__, \"invoke\")\n    parser_spy = mocker.spy(parser.__class__, \"invoke\")\n    tracer = FakeTracer()\n    assert chain.invoke(\"What is your name?\", {\"callbacks\": [tracer]}) == [\n        \"foo\",\n        \"bar\",\n    ]\n    assert prompt_spy.call_args.args[1] == {\n        \"documents\": [Document(page_content=\"foo\"), Document(page_content=\"bar\")],\n        \"question\": \"What is your name?\",\n        \"just_to_test_lambda\": \"What is your name?\",\n    }\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(\n                content=\"You are a nice assistant.\",\n                additional_kwargs={},\n                response_metadata={},\n            ),\n            HumanMessage(\n                content=\"Context:\\n[Document(metadata={}, page_content='foo'), Document(metadata={}, page_content='bar')]\\n\\nQuestion:\\nWhat is your name?\",\n                additional_kwargs={},\n                response_metadata={},\n            ),\n        ]\n    )\n    assert parser_spy.call_args.args[1] == _any_id_ai_message(content=\"foo, bar\")\n    assert len([r for r in tracer.runs if r.parent_run_id is None]) == 1\n    parent_run = next(r for r in tracer.runs if r.parent_run_id is None)\n    assert len(parent_run.child_runs) == 4\n    map_run = parent_run.child_runs[0]\n    assert map_run.name == \"RunnableParallel<question,documents,just_to_test_lambda>\"\n    assert len(map_run.child_runs) == 3\n\n\n@freeze_time(\"2023-01-01\")\ndef test_seq_prompt_dict(mocker: MockerFixture, snapshot: SnapshotAssertion) -> None:\n    passthrough = mocker.Mock(side_effect=lambda x: x)\n\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n\n    chat = FakeListChatModel(responses=[\"i'm a chatbot\"])\n\n    llm = FakeListLLM(responses=[\"i'm a textbot\"])\n\n    chain = (\n        prompt\n        | passthrough\n        | {\n            \"chat\": chat,\n            \"llm\": llm,\n        }\n    )\n\n    assert repr(chain) == snapshot\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == [RunnableLambda(passthrough)]\n    assert isinstance(chain.last, RunnableParallel)\n    assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"invoke\")\n    chat_spy = mocker.spy(chat.__class__, \"invoke\")\n    llm_spy = mocker.spy(llm.__class__, \"invoke\")\n    tracer = FakeTracer()\n    assert chain.invoke(\n        {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n    ) == {\n        \"chat\": _any_id_ai_message(content=\"i'm a chatbot\"),\n        \"llm\": \"i'm a textbot\",\n    }\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert llm_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert len([r for r in tracer.runs if r.parent_run_id is None]) == 1\n    parent_run = next(r for r in tracer.runs if r.parent_run_id is None)\n    assert len(parent_run.child_runs) == 3\n    map_run = parent_run.child_runs[2]\n    assert map_run.name == \"RunnableParallel<chat,llm>\"\n    assert len(map_run.child_runs) == 2\n\n\n@freeze_time(\"2023-01-01\")\ndef test_router_runnable(mocker: MockerFixture, snapshot: SnapshotAssertion) -> None:\n    chain1: Runnable = ChatPromptTemplate.from_template(\n        \"You are a math genius. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"4\"])\n    chain2: Runnable = ChatPromptTemplate.from_template(\n        \"You are an english major. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"2\"])\n    router: Runnable = RouterRunnable({\"math\": chain1, \"english\": chain2})\n    chain: Runnable = {\n        \"key\": lambda x: x[\"key\"],\n        \"input\": {\"question\": lambda x: x[\"question\"]},\n    } | router\n    assert dumps(chain, pretty=True) == snapshot\n\n    result = chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n    assert result == \"4\"\n\n    result2 = chain.batch(\n        [\n            {\"key\": \"math\", \"question\": \"2 + 2\"},\n            {\"key\": \"english\", \"question\": \"2 + 2\"},\n        ]\n    )\n    assert result2 == [\"4\", \"2\"]\n\n    # Test invoke\n    router_spy = mocker.spy(router.__class__, \"invoke\")\n    tracer = FakeTracer()\n    assert (\n        chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"}, {\"callbacks\": [tracer]})\n        == \"4\"\n    )\n    assert router_spy.call_args.args[1] == {\n        \"key\": \"math\",\n        \"input\": {\"question\": \"2 + 2\"},\n    }\n    assert len([r for r in tracer.runs if r.parent_run_id is None]) == 1\n    parent_run = next(r for r in tracer.runs if r.parent_run_id is None)\n    assert len(parent_run.child_runs) == 2\n    router_run = parent_run.child_runs[1]\n    assert router_run.name == \"RunnableSequence\"  # TODO: should be RunnableRouter\n    assert len(router_run.child_runs) == 2\n\n\nasync def test_router_runnable_async() -> None:\n    chain1: Runnable = ChatPromptTemplate.from_template(\n        \"You are a math genius. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"4\"])\n    chain2: Runnable = ChatPromptTemplate.from_template(\n        \"You are an english major. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"2\"])\n    router: Runnable = RouterRunnable({\"math\": chain1, \"english\": chain2})\n    chain: Runnable = {\n        \"key\": lambda x: x[\"key\"],\n        \"input\": {\"question\": lambda x: x[\"question\"]},\n    } | router\n\n    result = await chain.ainvoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n    assert result == \"4\"\n\n    result2 = await chain.abatch(\n        [\n            {\"key\": \"math\", \"question\": \"2 + 2\"},\n            {\"key\": \"english\", \"question\": \"2 + 2\"},\n        ]\n    )\n    assert result2 == [\"4\", \"2\"]\n\n\n@freeze_time(\"2023-01-01\")\ndef test_higher_order_lambda_runnable(\n    mocker: MockerFixture, snapshot: SnapshotAssertion\n) -> None:\n    math_chain: Runnable = ChatPromptTemplate.from_template(\n        \"You are a math genius. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"4\"])\n    english_chain: Runnable = ChatPromptTemplate.from_template(\n        \"You are an english major. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"2\"])\n    input_map: Runnable = RunnableParallel(\n        key=lambda x: x[\"key\"],\n        input={\"question\": lambda x: x[\"question\"]},\n    )\n\n    def router(input: dict[str, Any]) -> Runnable:\n        if input[\"key\"] == \"math\":\n            return itemgetter(\"input\") | math_chain\n        elif input[\"key\"] == \"english\":\n            return itemgetter(\"input\") | english_chain\n        else:\n            msg = f\"Unknown key: {input['key']}\"\n            raise ValueError(msg)\n\n    chain: Runnable = input_map | router\n    assert dumps(chain, pretty=True) == snapshot\n\n    result = chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n    assert result == \"4\"\n\n    result2 = chain.batch(\n        [\n            {\"key\": \"math\", \"question\": \"2 + 2\"},\n            {\"key\": \"english\", \"question\": \"2 + 2\"},\n        ]\n    )\n    assert result2 == [\"4\", \"2\"]\n\n    # Test invoke\n    math_spy = mocker.spy(math_chain.__class__, \"invoke\")\n    tracer = FakeTracer()\n    assert (\n        chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"}, {\"callbacks\": [tracer]})\n        == \"4\"\n    )\n    assert math_spy.call_args.args[1] == {\n        \"key\": \"math\",\n        \"input\": {\"question\": \"2 + 2\"},\n    }\n    assert len([r for r in tracer.runs if r.parent_run_id is None]) == 1\n    parent_run = next(r for r in tracer.runs if r.parent_run_id is None)\n    assert len(parent_run.child_runs) == 2\n    router_run = parent_run.child_runs[1]\n    assert router_run.name == \"router\"\n    assert len(router_run.child_runs) == 1\n    math_run = router_run.child_runs[0]\n    assert math_run.name == \"RunnableSequence\"\n    assert len(math_run.child_runs) == 3\n\n\nasync def test_higher_order_lambda_runnable_async(mocker: MockerFixture) -> None:\n    math_chain: Runnable = ChatPromptTemplate.from_template(\n        \"You are a math genius. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"4\"])\n    english_chain: Runnable = ChatPromptTemplate.from_template(\n        \"You are an english major. Answer the question: {question}\"\n    ) | FakeListLLM(responses=[\"2\"])\n    input_map: Runnable = RunnableParallel(\n        key=lambda x: x[\"key\"],\n        input={\"question\": lambda x: x[\"question\"]},\n    )\n\n    def router(input: dict[str, Any]) -> Runnable:\n        if input[\"key\"] == \"math\":\n            return itemgetter(\"input\") | math_chain\n        elif input[\"key\"] == \"english\":\n            return itemgetter(\"input\") | english_chain\n        else:\n            msg = f\"Unknown key: {input['key']}\"\n            raise ValueError(msg)\n\n    chain: Runnable = input_map | router\n\n    result = await chain.ainvoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n    assert result == \"4\"\n\n    result2 = await chain.abatch(\n        [\n            {\"key\": \"math\", \"question\": \"2 + 2\"},\n            {\"key\": \"english\", \"question\": \"2 + 2\"},\n        ]\n    )\n    assert result2 == [\"4\", \"2\"]\n\n    # Test ainvoke\n    async def arouter(input: dict[str, Any]) -> Runnable:\n        if input[\"key\"] == \"math\":\n            return itemgetter(\"input\") | math_chain\n        elif input[\"key\"] == \"english\":\n            return itemgetter(\"input\") | english_chain\n        else:\n            msg = f\"Unknown key: {input['key']}\"\n            raise ValueError(msg)\n\n    achain: Runnable = input_map | arouter\n    math_spy = mocker.spy(math_chain.__class__, \"ainvoke\")\n    tracer = FakeTracer()\n    assert (\n        await achain.ainvoke(\n            {\"key\": \"math\", \"question\": \"2 + 2\"}, {\"callbacks\": [tracer]}\n        )\n        == \"4\"\n    )\n    assert math_spy.call_args.args[1] == {\n        \"key\": \"math\",\n        \"input\": {\"question\": \"2 + 2\"},\n    }\n    assert len([r for r in tracer.runs if r.parent_run_id is None]) == 1\n    parent_run = next(r for r in tracer.runs if r.parent_run_id is None)\n    assert len(parent_run.child_runs) == 2\n    router_run = parent_run.child_runs[1]\n    assert router_run.name == \"arouter\"\n    assert len(router_run.child_runs) == 1\n    math_run = router_run.child_runs[0]\n    assert math_run.name == \"RunnableSequence\"\n    assert len(math_run.child_runs) == 3\n\n\n@freeze_time(\"2023-01-01\")\ndef test_seq_prompt_map(mocker: MockerFixture, snapshot: SnapshotAssertion) -> None:\n    passthrough = mocker.Mock(side_effect=lambda x: x)\n\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n\n    chat = FakeListChatModel(responses=[\"i'm a chatbot\"])\n\n    llm = FakeListLLM(responses=[\"i'm a textbot\"])\n\n    chain = (\n        prompt\n        | passthrough\n        | {\n            \"chat\": chat.bind(stop=[\"Thought:\"]),\n            \"llm\": llm,\n            \"passthrough\": passthrough,\n        }\n    )\n\n    assert isinstance(chain, RunnableSequence)\n    assert chain.first == prompt\n    assert chain.middle == [RunnableLambda(passthrough)]\n    assert isinstance(chain.last, RunnableParallel)\n\n    if (PYDANTIC_MAJOR_VERSION, PYDANTIC_MINOR_VERSION) >= (2, 10):\n        assert dumps(chain, pretty=True) == snapshot\n\n    # Test invoke\n    prompt_spy = mocker.spy(prompt.__class__, \"invoke\")\n    chat_spy = mocker.spy(chat.__class__, \"invoke\")\n    llm_spy = mocker.spy(llm.__class__, \"invoke\")\n    tracer = FakeTracer()\n    assert chain.invoke(\n        {\"question\": \"What is your name?\"}, {\"callbacks\": [tracer]}\n    ) == {\n        \"chat\": _any_id_ai_message(content=\"i'm a chatbot\"),\n        \"llm\": \"i'm a textbot\",\n        \"passthrough\": ChatPromptValue(\n            messages=[\n                SystemMessage(content=\"You are a nice assistant.\"),\n                HumanMessage(content=\"What is your name?\"),\n            ]\n        ),\n    }\n    assert prompt_spy.call_args.args[1] == {\"question\": \"What is your name?\"}\n    assert chat_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert llm_spy.call_args.args[1] == ChatPromptValue(\n        messages=[\n            SystemMessage(content=\"You are a nice assistant.\"),\n            HumanMessage(content=\"What is your name?\"),\n        ]\n    )\n    assert len([r for r in tracer.runs if r.parent_run_id is None]) == 1\n    parent_run = next(r for r in tracer.runs if r.parent_run_id is None)\n    assert len(parent_run.child_runs) == 3\n    map_run = parent_run.child_runs[2]\n    assert map_run.name == \"RunnableParallel<chat,llm,passthrough>\"\n    assert len(map_run.child_runs) == 3\n\n\ndef test_map_stream() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n\n    chat_res = \"i'm a chatbot\"\n    # sleep to better simulate a real stream\n    chat = FakeListChatModel(responses=[chat_res], sleep=0.01)\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    chain: Runnable = prompt | {\n        \"chat\": chat.bind(stop=[\"Thought:\"]),\n        \"llm\": llm,\n        \"passthrough\": RunnablePassthrough(),\n    }\n\n    stream = chain.stream({\"question\": \"What is your name?\"})\n\n    final_value = None\n    streamed_chunks = []\n    for chunk in stream:\n        streamed_chunks.append(chunk)\n        if final_value is None:\n            final_value = chunk\n        else:\n            final_value += chunk\n\n    assert streamed_chunks[0] in [\n        {\"passthrough\": prompt.invoke({\"question\": \"What is your name?\"})},\n        {\"llm\": \"i\"},\n        {\"chat\": _any_id_ai_message_chunk(content=\"i\")},\n    ]\n    assert len(streamed_chunks) == len(chat_res) + len(llm_res) + 1\n    assert all(len(c.keys()) == 1 for c in streamed_chunks)\n    assert final_value is not None\n    assert final_value.get(\"chat\").content == \"i'm a chatbot\"\n    assert final_value.get(\"llm\") == \"i'm a textbot\"\n    assert final_value.get(\"passthrough\") == prompt.invoke(\n        {\"question\": \"What is your name?\"}\n    )\n\n    chain_pick_one = chain.pick(\"llm\")\n\n    assert chain_pick_one.get_output_jsonschema() == {\n        \"title\": \"RunnableSequenceOutput\",\n        \"type\": \"string\",\n    }\n\n    stream = chain_pick_one.stream({\"question\": \"What is your name?\"})\n\n    final_value = None\n    streamed_chunks = []\n    for chunk in stream:\n        streamed_chunks.append(chunk)\n        if final_value is None:\n            final_value = chunk\n        else:\n            final_value += chunk\n\n    assert streamed_chunks[0] == \"i\"\n    assert len(streamed_chunks) == len(llm_res)\n\n    chain_pick_two = chain.assign(hello=RunnablePick(\"llm\").pipe(llm)).pick(\n        [\n            \"llm\",\n            \"hello\",\n        ]\n    )\n\n    assert chain_pick_two.get_output_jsonschema() == {\n        \"title\": \"RunnableSequenceOutput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"hello\": {\"title\": \"Hello\", \"type\": \"string\"},\n            \"llm\": {\"title\": \"Llm\", \"type\": \"string\"},\n        },\n        \"required\": [\"llm\", \"hello\"],\n    }\n\n    stream = chain_pick_two.stream({\"question\": \"What is your name?\"})\n\n    final_value = None\n    streamed_chunks = []\n    for chunk in stream:\n        streamed_chunks.append(chunk)\n        if final_value is None:\n            final_value = chunk\n        else:\n            final_value += chunk\n\n    assert streamed_chunks[0] in [\n        {\"llm\": \"i\"},\n        {\"chat\": _any_id_ai_message_chunk(content=\"i\")},\n    ]\n    if not (  # TODO(Rewrite properly) statement above\n        streamed_chunks[0] == {\"llm\": \"i\"}\n        or {\"chat\": _any_id_ai_message_chunk(content=\"i\")}\n    ):\n        msg = f\"Got an unexpected chunk: {streamed_chunks[0]}\"\n        raise AssertionError(msg)\n\n    assert len(streamed_chunks) == len(llm_res) + len(chat_res)\n\n\ndef test_map_stream_iterator_input() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n\n    chat_res = \"i'm a chatbot\"\n    # sleep to better simulate a real stream\n    chat = FakeListChatModel(responses=[chat_res], sleep=0.01)\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    chain: Runnable = (\n        prompt\n        | llm\n        | {\n            \"chat\": chat.bind(stop=[\"Thought:\"]),\n            \"llm\": llm,\n            \"passthrough\": RunnablePassthrough(),\n        }\n    )\n\n    stream = chain.stream({\"question\": \"What is your name?\"})\n\n    final_value = None\n    streamed_chunks = []\n    for chunk in stream:\n        streamed_chunks.append(chunk)\n        if final_value is None:\n            final_value = chunk\n        else:\n            final_value += chunk\n\n    assert streamed_chunks[0] in [\n        {\"passthrough\": \"i\"},\n        {\"llm\": \"i\"},\n        {\"chat\": _any_id_ai_message_chunk(content=\"i\")},\n    ]\n    assert len(streamed_chunks) == len(chat_res) + len(llm_res) + len(llm_res)\n    assert all(len(c.keys()) == 1 for c in streamed_chunks)\n    assert final_value is not None\n    assert final_value.get(\"chat\").content == \"i'm a chatbot\"\n    assert final_value.get(\"llm\") == \"i'm a textbot\"\n    assert final_value.get(\"passthrough\") == \"i'm a textbot\"\n\n\nasync def test_map_astream() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n\n    chat_res = \"i'm a chatbot\"\n    # sleep to better simulate a real stream\n    chat = FakeListChatModel(responses=[chat_res], sleep=0.01)\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    chain: Runnable = prompt | {\n        \"chat\": chat.bind(stop=[\"Thought:\"]),\n        \"llm\": llm,\n        \"passthrough\": RunnablePassthrough(),\n    }\n\n    stream = chain.astream({\"question\": \"What is your name?\"})\n\n    final_value = None\n    streamed_chunks = []\n    async for chunk in stream:\n        streamed_chunks.append(chunk)\n        if final_value is None:\n            final_value = chunk\n        else:\n            final_value += chunk\n\n    assert streamed_chunks[0] in [\n        {\"passthrough\": prompt.invoke({\"question\": \"What is your name?\"})},\n        {\"llm\": \"i\"},\n        {\"chat\": _any_id_ai_message_chunk(content=\"i\")},\n    ]\n    assert len(streamed_chunks) == len(chat_res) + len(llm_res) + 1\n    assert all(len(c.keys()) == 1 for c in streamed_chunks)\n    assert final_value is not None\n    assert final_value.get(\"chat\").content == \"i'm a chatbot\"\n    final_value[\"chat\"].id = AnyStr()\n    assert final_value.get(\"llm\") == \"i'm a textbot\"\n    assert final_value.get(\"passthrough\") == prompt.invoke(\n        {\"question\": \"What is your name?\"}\n    )\n\n    # Test astream_log state accumulation\n\n    final_state = None\n    streamed_ops = []\n    async for chunk in chain.astream_log({\"question\": \"What is your name?\"}):\n        streamed_ops.extend(chunk.ops)\n        if final_state is None:\n            final_state = chunk\n        else:\n            final_state += chunk\n    final_state = cast(RunLog, final_state)\n\n    assert final_state.state[\"final_output\"] == final_value\n    assert len(final_state.state[\"streamed_output\"]) == len(streamed_chunks)\n    assert isinstance(final_state.state[\"id\"], str)\n    assert len(final_state.ops) == len(streamed_ops)\n    assert len(final_state.state[\"logs\"]) == 5\n    assert (\n        final_state.state[\"logs\"][\"ChatPromptTemplate\"][\"name\"] == \"ChatPromptTemplate\"\n    )\n    assert final_state.state[\"logs\"][\"ChatPromptTemplate\"][\n        \"final_output\"\n    ] == prompt.invoke({\"question\": \"What is your name?\"})\n    assert (\n        final_state.state[\"logs\"][\"RunnableParallel<chat,llm,passthrough>\"][\"name\"]\n        == \"RunnableParallel<chat,llm,passthrough>\"\n    )\n    assert sorted(final_state.state[\"logs\"]) == [\n        \"ChatPromptTemplate\",\n        \"FakeListChatModel\",\n        \"FakeStreamingListLLM\",\n        \"RunnableParallel<chat,llm,passthrough>\",\n        \"RunnablePassthrough\",\n    ]\n\n    # Test astream_log with include filters\n    final_state = None\n    async for chunk in chain.astream_log(\n        {\"question\": \"What is your name?\"}, include_names=[\"FakeListChatModel\"]\n    ):\n        if final_state is None:\n            final_state = chunk\n        else:\n            final_state += chunk\n    final_state = cast(RunLog, final_state)\n\n    assert final_state.state[\"final_output\"] == final_value\n    assert len(final_state.state[\"streamed_output\"]) == len(streamed_chunks)\n    assert len(final_state.state[\"logs\"]) == 1\n    assert final_state.state[\"logs\"][\"FakeListChatModel\"][\"name\"] == \"FakeListChatModel\"\n\n    # Test astream_log with exclude filters\n    final_state = None\n    async for chunk in chain.astream_log(\n        {\"question\": \"What is your name?\"}, exclude_names=[\"FakeListChatModel\"]\n    ):\n        if final_state is None:\n            final_state = chunk\n        else:\n            final_state += chunk\n    final_state = cast(RunLog, final_state)\n\n    assert final_state.state[\"final_output\"] == final_value\n    assert len(final_state.state[\"streamed_output\"]) == len(streamed_chunks)\n    assert len(final_state.state[\"logs\"]) == 4\n    assert (\n        final_state.state[\"logs\"][\"ChatPromptTemplate\"][\"name\"] == \"ChatPromptTemplate\"\n    )\n    assert final_state.state[\"logs\"][\"ChatPromptTemplate\"][\"final_output\"] == (\n        prompt.invoke({\"question\": \"What is your name?\"})\n    )\n    assert (\n        final_state.state[\"logs\"][\"RunnableParallel<chat,llm,passthrough>\"][\"name\"]\n        == \"RunnableParallel<chat,llm,passthrough>\"\n    )\n    assert sorted(final_state.state[\"logs\"]) == [\n        \"ChatPromptTemplate\",\n        \"FakeStreamingListLLM\",\n        \"RunnableParallel<chat,llm,passthrough>\",\n        \"RunnablePassthrough\",\n    ]\n\n\nasync def test_map_astream_iterator_input() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n\n    chat_res = \"i'm a chatbot\"\n    # sleep to better simulate a real stream\n    chat = FakeListChatModel(responses=[chat_res], sleep=0.01)\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    chain: Runnable = (\n        prompt\n        | llm\n        | {\n            \"chat\": chat.bind(stop=[\"Thought:\"]),\n            \"llm\": llm,\n            \"passthrough\": RunnablePassthrough(),\n        }\n    )\n\n    stream = chain.astream({\"question\": \"What is your name?\"})\n\n    final_value = None\n    streamed_chunks = []\n    async for chunk in stream:\n        streamed_chunks.append(chunk)\n        if final_value is None:\n            final_value = chunk\n        else:\n            final_value += chunk\n\n    assert streamed_chunks[0] in [\n        {\"passthrough\": \"i\"},\n        {\"llm\": \"i\"},\n        {\"chat\": AIMessageChunk(content=\"i\")},\n    ]\n    assert len(streamed_chunks) == len(chat_res) + len(llm_res) + len(llm_res)\n    assert all(len(c.keys()) == 1 for c in streamed_chunks)\n    assert final_value is not None\n    assert final_value.get(\"chat\").content == \"i'm a chatbot\"\n    assert final_value.get(\"llm\") == \"i'm a textbot\"\n    assert final_value.get(\"passthrough\") == llm_res\n\n    simple_map = RunnableMap(passthrough=RunnablePassthrough())\n    assert loads(dumps(simple_map)) == simple_map\n\n\ndef test_with_config_with_config() -> None:\n    llm = FakeListLLM(responses=[\"i'm a textbot\"])\n\n    assert dumpd(\n        llm.with_config({\"metadata\": {\"a\": \"b\"}}).with_config(tags=[\"a-tag\"])\n    ) == dumpd(llm.with_config({\"metadata\": {\"a\": \"b\"}, \"tags\": [\"a-tag\"]}))\n\n\ndef test_metadata_is_merged() -> None:\n    \"\"\"Test metadata and tags defined in with_config and at are merged/concatend.\"\"\"\n    foo = RunnableLambda(lambda x: x).with_config({\"metadata\": {\"my_key\": \"my_value\"}})\n    expected_metadata = {\n        \"my_key\": \"my_value\",\n        \"my_other_key\": \"my_other_value\",\n    }\n    with collect_runs() as cb:\n        foo.invoke(\"hi\", {\"metadata\": {\"my_other_key\": \"my_other_value\"}})\n        run = cb.traced_runs[0]\n    assert run.extra is not None\n    assert run.extra[\"metadata\"] == expected_metadata\n\n\ndef test_tags_are_appended() -> None:\n    \"\"\"Test tags from with_config are concatenated with those in invocation.\"\"\"\n    foo = RunnableLambda(lambda x: x).with_config({\"tags\": [\"my_key\"]})\n    with collect_runs() as cb:\n        foo.invoke(\"hi\", {\"tags\": [\"invoked_key\"]})\n        run = cb.traced_runs[0]\n    assert isinstance(run.tags, list)\n    assert sorted(run.tags) == sorted([\"my_key\", \"invoked_key\"])\n\n\ndef test_bind_bind() -> None:\n    llm = FakeListLLM(responses=[\"i'm a textbot\"])\n\n    assert dumpd(\n        llm.bind(stop=[\"Thought:\"], one=\"two\").bind(\n            stop=[\"Observation:\"], hello=\"world\"\n        )\n    ) == dumpd(llm.bind(stop=[\"Observation:\"], one=\"two\", hello=\"world\"))\n\n\ndef test_bind_with_lambda() -> None:\n    def my_function(*args: Any, **kwargs: Any) -> int:\n        return 3 + kwargs.get(\"n\", 0)\n\n    runnable = RunnableLambda(my_function).bind(n=1)\n    assert runnable.invoke({}) == 4\n    chunks = list(runnable.stream({}))\n    assert chunks == [4]\n\n\nasync def test_bind_with_lambda_async() -> None:\n    def my_function(*args: Any, **kwargs: Any) -> int:\n        return 3 + kwargs.get(\"n\", 0)\n\n    runnable = RunnableLambda(my_function).bind(n=1)\n    assert await runnable.ainvoke({}) == 4\n    chunks = [item async for item in runnable.astream({})]\n    assert chunks == [4]\n\n\ndef test_deep_stream() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n    chain = prompt | llm | StrOutputParser()\n\n    stream = chain.stream({\"question\": \"What up\"})\n\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert \"\".join(chunks) == \"foo-lish\"\n\n    chunks = []\n    for chunk in (chain | RunnablePassthrough()).stream({\"question\": \"What up\"}):\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert \"\".join(chunks) == \"foo-lish\"\n\n\ndef test_deep_stream_assign() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n    chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n\n    stream = chain.stream({\"question\": \"What up\"})\n\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert add(chunks) == {\"str\": \"foo-lish\"}\n\n    chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n\n    assert chain_with_assign.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}},\n        \"required\": [\"question\"],\n    }\n    assert chain_with_assign.get_output_jsonschema() == {\n        \"title\": \"RunnableSequenceOutput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"str\": {\"title\": \"Str\", \"type\": \"string\"},\n            \"hello\": {\"title\": \"Hello\", \"type\": \"string\"},\n        },\n        \"required\": [\"str\", \"hello\"],\n    }\n\n    chunks = []\n    for chunk in chain_with_assign.stream({\"question\": \"What up\"}):\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\") * 2\n    assert chunks == [\n        # first stream passthrough input chunks\n        {\"str\": \"f\"},\n        {\"str\": \"o\"},\n        {\"str\": \"o\"},\n        {\"str\": \"-\"},\n        {\"str\": \"l\"},\n        {\"str\": \"i\"},\n        {\"str\": \"s\"},\n        {\"str\": \"h\"},\n        # then stream assign output chunks\n        {\"hello\": \"f\"},\n        {\"hello\": \"o\"},\n        {\"hello\": \"o\"},\n        {\"hello\": \"-\"},\n        {\"hello\": \"l\"},\n        {\"hello\": \"i\"},\n        {\"hello\": \"s\"},\n        {\"hello\": \"h\"},\n    ]\n    assert add(chunks) == {\"str\": \"foo-lish\", \"hello\": \"foo-lish\"}\n    assert chain_with_assign.invoke({\"question\": \"What up\"}) == {\n        \"str\": \"foo-lish\",\n        \"hello\": \"foo-lish\",\n    }\n\n    chain_with_assign_shadow = chain.assign(\n        str=lambda _: \"shadow\",\n        hello=itemgetter(\"str\") | llm,\n    )\n\n    assert chain_with_assign_shadow.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}},\n        \"required\": [\"question\"],\n    }\n    assert chain_with_assign_shadow.get_output_jsonschema() == {\n        \"title\": \"RunnableSequenceOutput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"str\": {\"title\": \"Str\"},\n            \"hello\": {\"title\": \"Hello\", \"type\": \"string\"},\n        },\n        \"required\": [\"str\", \"hello\"],\n    }\n\n    chunks = []\n    for chunk in chain_with_assign_shadow.stream({\"question\": \"What up\"}):\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\") + 1\n    assert add(chunks) == {\"str\": \"shadow\", \"hello\": \"foo-lish\"}\n    assert chain_with_assign_shadow.invoke({\"question\": \"What up\"}) == {\n        \"str\": \"shadow\",\n        \"hello\": \"foo-lish\",\n    }\n\n\nasync def test_deep_astream() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n    chain = prompt | llm | StrOutputParser()\n\n    stream = chain.astream({\"question\": \"What up\"})\n\n    chunks = []\n    async for chunk in stream:\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert \"\".join(chunks) == \"foo-lish\"\n\n    chunks = []\n    async for chunk in (chain | RunnablePassthrough()).astream({\"question\": \"What up\"}):\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert \"\".join(chunks) == \"foo-lish\"\n\n\nasync def test_deep_astream_assign() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n    chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n\n    stream = chain.astream({\"question\": \"What up\"})\n\n    chunks = []\n    async for chunk in stream:\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert add(chunks) == {\"str\": \"foo-lish\"}\n\n    chain_with_assign = chain.assign(\n        hello=itemgetter(\"str\") | llm,\n    )\n\n    assert chain_with_assign.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}},\n        \"required\": [\"question\"],\n    }\n    assert chain_with_assign.get_output_jsonschema() == {\n        \"title\": \"RunnableSequenceOutput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"str\": {\"title\": \"Str\", \"type\": \"string\"},\n            \"hello\": {\"title\": \"Hello\", \"type\": \"string\"},\n        },\n        \"required\": [\"str\", \"hello\"],\n    }\n\n    chunks = []\n    async for chunk in chain_with_assign.astream({\"question\": \"What up\"}):\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\") * 2\n    assert chunks == [\n        # first stream passthrough input chunks\n        {\"str\": \"f\"},\n        {\"str\": \"o\"},\n        {\"str\": \"o\"},\n        {\"str\": \"-\"},\n        {\"str\": \"l\"},\n        {\"str\": \"i\"},\n        {\"str\": \"s\"},\n        {\"str\": \"h\"},\n        # then stream assign output chunks\n        {\"hello\": \"f\"},\n        {\"hello\": \"o\"},\n        {\"hello\": \"o\"},\n        {\"hello\": \"-\"},\n        {\"hello\": \"l\"},\n        {\"hello\": \"i\"},\n        {\"hello\": \"s\"},\n        {\"hello\": \"h\"},\n    ]\n    assert add(chunks) == {\"str\": \"foo-lish\", \"hello\": \"foo-lish\"}\n    assert await chain_with_assign.ainvoke({\"question\": \"What up\"}) == {\n        \"str\": \"foo-lish\",\n        \"hello\": \"foo-lish\",\n    }\n\n    chain_with_assign_shadow = chain | RunnablePassthrough.assign(\n        str=lambda _: \"shadow\",\n        hello=itemgetter(\"str\") | llm,\n    )\n\n    assert chain_with_assign_shadow.get_input_jsonschema() == {\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}},\n        \"required\": [\"question\"],\n    }\n    assert chain_with_assign_shadow.get_output_jsonschema() == {\n        \"title\": \"RunnableSequenceOutput\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"str\": {\"title\": \"Str\"},\n            \"hello\": {\"title\": \"Hello\", \"type\": \"string\"},\n        },\n        \"required\": [\"str\", \"hello\"],\n    }\n\n    chunks = []\n    async for chunk in chain_with_assign_shadow.astream({\"question\": \"What up\"}):\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\") + 1\n    assert add(chunks) == {\"str\": \"shadow\", \"hello\": \"foo-lish\"}\n    assert await chain_with_assign_shadow.ainvoke({\"question\": \"What up\"}) == {\n        \"str\": \"shadow\",\n        \"hello\": \"foo-lish\",\n    }\n\n\ndef test_runnable_sequence_transform() -> None:\n    llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n    chain: Runnable = llm | StrOutputParser()\n\n    stream = chain.transform(llm.stream(\"Hi there!\"))\n\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert \"\".join(chunks) == \"foo-lish\"\n\n\nasync def test_runnable_sequence_atransform() -> None:\n    llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n    chain: Runnable = llm | StrOutputParser()\n\n    stream = chain.atransform(llm.astream(\"Hi there!\"))\n\n    chunks = []\n    async for chunk in stream:\n        chunks.append(chunk)\n\n    assert len(chunks) == len(\"foo-lish\")\n    assert \"\".join(chunks) == \"foo-lish\"\n\n\nclass FakeSplitIntoListParser(BaseOutputParser[list[str]]):\n    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether or not the class is serializable.\"\"\"\n        return True\n\n    def get_format_instructions(self) -> str:\n        return (\n            \"Your response should be a list of comma separated values, \"\n            \"eg: `foo, bar, baz`\"\n        )\n\n    def parse(self, text: str) -> list[str]:\n        \"\"\"Parse the output of an LLM call.\"\"\"\n        return text.strip().split(\", \")\n\n\ndef test_each_simple() -> None:\n    \"\"\"Test that each() works with a simple runnable.\"\"\"\n    parser = FakeSplitIntoListParser()\n    assert parser.invoke(\"first item, second item\") == [\"first item\", \"second item\"]\n    assert parser.map().invoke([\"a, b\", \"c\"]) == [[\"a\", \"b\"], [\"c\"]]\n    assert parser.map().map().invoke([[\"a, b\", \"c\"], [\"c, e\"]]) == [\n        [[\"a\", \"b\"], [\"c\"]],\n        [[\"c\", \"e\"]],\n    ]\n\n\ndef test_each(snapshot: SnapshotAssertion) -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    first_llm = FakeStreamingListLLM(responses=[\"first item, second item, third item\"])\n    parser = FakeSplitIntoListParser()\n    second_llm = FakeStreamingListLLM(responses=[\"this\", \"is\", \"a\", \"test\"])\n\n    chain = prompt | first_llm | parser | second_llm.map()\n\n    assert dumps(chain, pretty=True) == snapshot\n    output = chain.invoke({\"question\": \"What up\"})\n    assert output == [\"this\", \"is\", \"a\"]\n\n    assert (parser | second_llm.map()).invoke(\"first item, second item\") == [\n        \"test\",\n        \"this\",\n    ]\n\n\ndef test_recursive_lambda() -> None:\n    def _simple_recursion(x: int) -> Union[int, Runnable]:\n        if x < 10:\n            return RunnableLambda(lambda *args: _simple_recursion(x + 1))\n        else:\n            return x\n\n    runnable = RunnableLambda(_simple_recursion)\n    assert runnable.invoke(5) == 10\n\n    with pytest.raises(RecursionError):\n        runnable.invoke(0, {\"recursion_limit\": 9})\n\n\ndef test_retrying(mocker: MockerFixture) -> None:\n    def _lambda(x: int) -> Union[int, Runnable]:\n        if x == 1:\n            msg = \"x is 1\"\n            raise ValueError(msg)\n        elif x == 2:\n            msg = \"x is 2\"\n            raise RuntimeError(msg)\n        else:\n            return x\n\n    _lambda_mock = mocker.Mock(side_effect=_lambda)\n    runnable = RunnableLambda(_lambda_mock)\n\n    with pytest.raises(ValueError):\n        runnable.invoke(1)\n\n    assert _lambda_mock.call_count == 1\n    _lambda_mock.reset_mock()\n\n    with pytest.raises(ValueError):\n        runnable.with_retry(\n            stop_after_attempt=2,\n            retry_if_exception_type=(ValueError,),\n        ).invoke(1)\n\n    assert _lambda_mock.call_count == 2  # retried\n    _lambda_mock.reset_mock()\n\n    with pytest.raises(RuntimeError):\n        runnable.with_retry(\n            stop_after_attempt=2,\n            wait_exponential_jitter=False,\n            retry_if_exception_type=(ValueError,),\n        ).invoke(2)\n\n    assert _lambda_mock.call_count == 1  # did not retry\n    _lambda_mock.reset_mock()\n\n    with pytest.raises(ValueError):\n        runnable.with_retry(\n            stop_after_attempt=2,\n            wait_exponential_jitter=False,\n            retry_if_exception_type=(ValueError,),\n        ).batch([1, 2, 0])\n\n    # 3rd input isn't retried because it succeeded\n    assert _lambda_mock.call_count == 3 + 2\n    _lambda_mock.reset_mock()\n\n    output = runnable.with_retry(\n        stop_after_attempt=2,\n        wait_exponential_jitter=False,\n        retry_if_exception_type=(ValueError,),\n    ).batch([1, 2, 0], return_exceptions=True)\n\n    # 3rd input isn't retried because it succeeded\n    assert _lambda_mock.call_count == 3 + 2\n    assert len(output) == 3\n    assert isinstance(output[0], ValueError)\n    assert isinstance(output[1], RuntimeError)\n    assert output[2] == 0\n    _lambda_mock.reset_mock()\n\n\nasync def test_async_retrying(mocker: MockerFixture) -> None:\n    def _lambda(x: int) -> Union[int, Runnable]:\n        if x == 1:\n            msg = \"x is 1\"\n            raise ValueError(msg)\n        elif x == 2:\n            msg = \"x is 2\"\n            raise RuntimeError(msg)\n        else:\n            return x\n\n    _lambda_mock = mocker.Mock(side_effect=_lambda)\n    runnable = RunnableLambda(_lambda_mock)\n\n    with pytest.raises(ValueError):\n        await runnable.ainvoke(1)\n\n    assert _lambda_mock.call_count == 1\n    _lambda_mock.reset_mock()\n\n    with pytest.raises(ValueError):\n        await runnable.with_retry(\n            stop_after_attempt=2,\n            wait_exponential_jitter=False,\n            retry_if_exception_type=(ValueError, KeyError),\n        ).ainvoke(1)\n\n    assert _lambda_mock.call_count == 2  # retried\n    _lambda_mock.reset_mock()\n\n    with pytest.raises(RuntimeError):\n        await runnable.with_retry(\n            stop_after_attempt=2,\n            wait_exponential_jitter=False,\n            retry_if_exception_type=(ValueError,),\n        ).ainvoke(2)\n\n    assert _lambda_mock.call_count == 1  # did not retry\n    _lambda_mock.reset_mock()\n\n    with pytest.raises(ValueError):\n        await runnable.with_retry(\n            stop_after_attempt=2,\n            wait_exponential_jitter=False,\n            retry_if_exception_type=(ValueError,),\n        ).abatch([1, 2, 0])\n\n    # 3rd input isn't retried because it succeeded\n    assert _lambda_mock.call_count == 3 + 2\n    _lambda_mock.reset_mock()\n\n    output = await runnable.with_retry(\n        stop_after_attempt=2,\n        wait_exponential_jitter=False,\n        retry_if_exception_type=(ValueError,),\n    ).abatch([1, 2, 0], return_exceptions=True)\n\n    # 3rd input isn't retried because it succeeded\n    assert _lambda_mock.call_count == 3 + 2\n    assert len(output) == 3\n    assert isinstance(output[0], ValueError)\n    assert isinstance(output[1], RuntimeError)\n    assert output[2] == 0\n    _lambda_mock.reset_mock()\n\n\ndef test_runnable_lambda_stream() -> None:\n    \"\"\"Test that stream works for both normal functions & those returning Runnable.\"\"\"\n    # Normal output should work\n    output: list[Any] = list(RunnableLambda(range).stream(5))\n    assert output == [range(5)]\n\n    # Runnable output should also work\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    output = list(RunnableLambda(lambda x: llm).stream(\"\"))\n    assert output == list(llm_res)\n\n\ndef test_runnable_lambda_stream_with_callbacks() -> None:\n    \"\"\"Test that stream works for RunnableLambda when using callbacks.\"\"\"\n    tracer = FakeTracer()\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n    config: RunnableConfig = {\"callbacks\": [tracer]}\n\n    assert list(RunnableLambda(lambda x: llm).stream(\"\", config=config)) == list(\n        llm_res\n    )\n\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].error is None\n    assert tracer.runs[0].outputs == {\"output\": llm_res}\n\n    def raise_value_error(x: int) -> int:\n        \"\"\"Raise a value error.\"\"\"\n        msg = \"x is too large\"\n        raise ValueError(msg)\n\n    # Check that the chain on error is invoked\n    with pytest.raises(ValueError):\n        for _ in RunnableLambda(raise_value_error).stream(1000, config=config):\n            pass\n\n    assert len(tracer.runs) == 2\n    assert \"ValueError('x is too large')\" in str(tracer.runs[1].error)\n    assert not tracer.runs[1].outputs\n\n\nasync def test_runnable_lambda_astream() -> None:\n    \"\"\"Test that astream works for both normal functions & those returning Runnable.\"\"\"\n\n    # Wrapper to make a normal function async\n    def awrapper(func: Callable) -> Callable[..., Awaitable[Any]]:\n        async def afunc(*args: Any, **kwargs: Any) -> Any:\n            return func(*args, **kwargs)\n\n        return afunc\n\n    # Normal output should work\n    output: list[Any] = [\n        chunk\n        async for chunk in RunnableLambda(\n            func=id,\n            afunc=awrapper(range),  # id func is just dummy\n        ).astream(5)\n    ]\n    assert output == [range(5)]\n\n    # Normal output using func should also work\n    output = [_ async for _ in RunnableLambda(range).astream(5)]\n    assert output == [range(5)]\n\n    # Runnable output should also work\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    output = [\n        _\n        async for _ in RunnableLambda(\n            func=id,\n            afunc=awrapper(lambda x: llm),\n        ).astream(\"\")\n    ]\n    assert output == list(llm_res)\n\n    output = [\n        chunk\n        async for chunk in cast(\n            AsyncIterator[str], RunnableLambda(lambda x: llm).astream(\"\")\n        )\n    ]\n    assert output == list(llm_res)\n\n\nasync def test_runnable_lambda_astream_with_callbacks() -> None:\n    \"\"\"Test that astream works for RunnableLambda when using callbacks.\"\"\"\n    tracer = FakeTracer()\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n    config: RunnableConfig = {\"callbacks\": [tracer]}\n\n    assert [\n        _ async for _ in RunnableLambda(lambda x: llm).astream(\"\", config=config)\n    ] == list(llm_res)\n\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].error is None\n    assert tracer.runs[0].outputs == {\"output\": llm_res}\n\n    def raise_value_error(x: int) -> int:\n        \"\"\"Raise a value error.\"\"\"\n        msg = \"x is too large\"\n        raise ValueError(msg)\n\n    # Check that the chain on error is invoked\n    with pytest.raises(ValueError):\n        async for _ in RunnableLambda(raise_value_error).astream(1000, config=config):\n            pass\n\n    assert len(tracer.runs) == 2\n    assert \"ValueError('x is too large')\" in str(tracer.runs[1].error)\n    assert not tracer.runs[1].outputs\n\n\n@freeze_time(\"2023-01-01\")\ndef test_seq_batch_return_exceptions(mocker: MockerFixture) -> None:\n    class ControlledExceptionRunnable(Runnable[str, str]):\n        def __init__(self, fail_starts_with: str) -> None:\n            self.fail_starts_with = fail_starts_with\n\n        def invoke(\n            self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any\n        ) -> Any:\n            raise NotImplementedError\n\n        def _batch(\n            self,\n            inputs: list[str],\n        ) -> list:\n            outputs: list[Any] = []\n            for input in inputs:\n                if input.startswith(self.fail_starts_with):\n                    outputs.append(ValueError())\n                else:\n                    outputs.append(input + \"a\")\n            return outputs\n\n        def batch(\n            self,\n            inputs: list[str],\n            config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n            *,\n            return_exceptions: bool = False,\n            **kwargs: Any,\n        ) -> list[str]:\n            return self._batch_with_config(\n                self._batch,\n                inputs,\n                config,\n                return_exceptions=return_exceptions,\n                **kwargs,\n            )\n\n    chain = (\n        ControlledExceptionRunnable(\"bux\")\n        | ControlledExceptionRunnable(\"bar\")\n        | ControlledExceptionRunnable(\"baz\")\n        | ControlledExceptionRunnable(\"foo\")\n    )\n\n    assert isinstance(chain, RunnableSequence)\n\n    # Test batch\n    with pytest.raises(ValueError):\n        chain.batch([\"foo\", \"bar\", \"baz\", \"qux\"])\n\n    spy = mocker.spy(ControlledExceptionRunnable, \"batch\")\n    tracer = FakeTracer()\n    inputs = [\"foo\", \"bar\", \"baz\", \"qux\"]\n    outputs = chain.batch(inputs, {\"callbacks\": [tracer]}, return_exceptions=True)\n    assert len(outputs) == 4\n    assert isinstance(outputs[0], ValueError)\n    assert isinstance(outputs[1], ValueError)\n    assert isinstance(outputs[2], ValueError)\n    assert outputs[3] == \"quxaaaa\"\n    assert spy.call_count == 4\n    inputs_to_batch = [c[0][1] for c in spy.call_args_list]\n    assert inputs_to_batch == [\n        # inputs to sequence step 0\n        # same as inputs to sequence.batch()\n        [\"foo\", \"bar\", \"baz\", \"qux\"],\n        # inputs to sequence step 1\n        # == outputs of sequence step 0 as no exceptions were raised\n        [\"fooa\", \"bara\", \"baza\", \"quxa\"],\n        # inputs to sequence step 2\n        # 'bar' was dropped as it raised an exception in step 1\n        [\"fooaa\", \"bazaa\", \"quxaa\"],\n        # inputs to sequence step 3\n        # 'baz' was dropped as it raised an exception in step 2\n        [\"fooaaa\", \"quxaaa\"],\n    ]\n    parent_runs = sorted(\n        (r for r in tracer.runs if r.parent_run_id is None),\n        key=lambda run: inputs.index(run.inputs[\"input\"]),\n    )\n    assert len(parent_runs) == 4\n\n    parent_run_foo = parent_runs[0]\n    assert parent_run_foo.inputs[\"input\"] == \"foo\"\n    assert repr(ValueError()) in str(parent_run_foo.error)\n    assert len(parent_run_foo.child_runs) == 4\n    assert [r.error for r in parent_run_foo.child_runs[:-1]] == [\n        None,\n        None,\n        None,\n    ]\n    assert repr(ValueError()) in str(parent_run_foo.child_runs[-1].error)\n\n    parent_run_bar = parent_runs[1]\n    assert parent_run_bar.inputs[\"input\"] == \"bar\"\n    assert repr(ValueError()) in str(parent_run_bar.error)\n    assert len(parent_run_bar.child_runs) == 2\n    assert parent_run_bar.child_runs[0].error is None\n    assert repr(ValueError()) in str(parent_run_bar.child_runs[1].error)\n\n    parent_run_baz = parent_runs[2]\n    assert parent_run_baz.inputs[\"input\"] == \"baz\"\n    assert repr(ValueError()) in str(parent_run_baz.error)\n    assert len(parent_run_baz.child_runs) == 3\n\n    assert [r.error for r in parent_run_baz.child_runs[:-1]] == [\n        None,\n        None,\n    ]\n    assert repr(ValueError()) in str(parent_run_baz.child_runs[-1].error)\n\n    parent_run_qux = parent_runs[3]\n    assert parent_run_qux.inputs[\"input\"] == \"qux\"\n    assert parent_run_qux.error is None\n    assert parent_run_qux.outputs is not None\n    assert parent_run_qux.outputs[\"output\"] == \"quxaaaa\"\n    assert len(parent_run_qux.child_runs) == 4\n    assert [r.error for r in parent_run_qux.child_runs] == [None, None, None, None]\n\n\n@freeze_time(\"2023-01-01\")\nasync def test_seq_abatch_return_exceptions(mocker: MockerFixture) -> None:\n    class ControlledExceptionRunnable(Runnable[str, str]):\n        def __init__(self, fail_starts_with: str) -> None:\n            self.fail_starts_with = fail_starts_with\n\n        def invoke(\n            self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any\n        ) -> Any:\n            raise NotImplementedError\n\n        async def _abatch(\n            self,\n            inputs: list[str],\n        ) -> list:\n            outputs: list[Any] = []\n            for input in inputs:\n                if input.startswith(self.fail_starts_with):\n                    outputs.append(ValueError())\n                else:\n                    outputs.append(input + \"a\")\n            return outputs\n\n        async def abatch(\n            self,\n            inputs: list[str],\n            config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,\n            *,\n            return_exceptions: bool = False,\n            **kwargs: Any,\n        ) -> list[str]:\n            return await self._abatch_with_config(\n                self._abatch,\n                inputs,\n                config,\n                return_exceptions=return_exceptions,\n                **kwargs,\n            )\n\n    chain = (\n        ControlledExceptionRunnable(\"bux\")\n        | ControlledExceptionRunnable(\"bar\")\n        | ControlledExceptionRunnable(\"baz\")\n        | ControlledExceptionRunnable(\"foo\")\n    )\n\n    assert isinstance(chain, RunnableSequence)\n\n    # Test abatch\n    with pytest.raises(ValueError):\n        await chain.abatch([\"foo\", \"bar\", \"baz\", \"qux\"])\n\n    spy = mocker.spy(ControlledExceptionRunnable, \"abatch\")\n    tracer = FakeTracer()\n    inputs = [\"foo\", \"bar\", \"baz\", \"qux\"]\n    outputs = await chain.abatch(\n        inputs, {\"callbacks\": [tracer]}, return_exceptions=True\n    )\n    assert len(outputs) == 4\n    assert isinstance(outputs[0], ValueError)\n    assert isinstance(outputs[1], ValueError)\n    assert isinstance(outputs[2], ValueError)\n    assert outputs[3] == \"quxaaaa\"\n    assert spy.call_count == 4\n    inputs_to_batch = [c[0][1] for c in spy.call_args_list]\n    assert inputs_to_batch == [\n        # inputs to sequence step 0\n        # same as inputs to sequence.batch()\n        [\"foo\", \"bar\", \"baz\", \"qux\"],\n        # inputs to sequence step 1\n        # == outputs of sequence step 0 as no exceptions were raised\n        [\"fooa\", \"bara\", \"baza\", \"quxa\"],\n        # inputs to sequence step 2\n        # 'bar' was dropped as it raised an exception in step 1\n        [\"fooaa\", \"bazaa\", \"quxaa\"],\n        # inputs to sequence step 3\n        # 'baz' was dropped as it raised an exception in step 2\n        [\"fooaaa\", \"quxaaa\"],\n    ]\n    parent_runs = sorted(\n        (r for r in tracer.runs if r.parent_run_id is None),\n        key=lambda run: inputs.index(run.inputs[\"input\"]),\n    )\n    assert len(parent_runs) == 4\n\n    parent_run_foo = parent_runs[0]\n    assert parent_run_foo.inputs[\"input\"] == \"foo\"\n    assert repr(ValueError()) in str(parent_run_foo.error)\n    assert len(parent_run_foo.child_runs) == 4\n    assert [r.error for r in parent_run_foo.child_runs[:-1]] == [\n        None,\n        None,\n        None,\n    ]\n    assert repr(ValueError()) in str(parent_run_foo.child_runs[-1].error)\n\n    parent_run_bar = parent_runs[1]\n    assert parent_run_bar.inputs[\"input\"] == \"bar\"\n    assert repr(ValueError()) in str(parent_run_bar.error)\n    assert len(parent_run_bar.child_runs) == 2\n    assert parent_run_bar.child_runs[0].error is None\n    assert repr(ValueError()) in str(parent_run_bar.child_runs[1].error)\n\n    parent_run_baz = parent_runs[2]\n    assert parent_run_baz.inputs[\"input\"] == \"baz\"\n    assert repr(ValueError()) in str(parent_run_baz.error)\n    assert len(parent_run_baz.child_runs) == 3\n    assert [r.error for r in parent_run_baz.child_runs[:-1]] == [\n        None,\n        None,\n    ]\n    assert repr(ValueError()) in str(parent_run_baz.child_runs[-1].error)\n\n    parent_run_qux = parent_runs[3]\n    assert parent_run_qux.inputs[\"input\"] == \"qux\"\n    assert parent_run_qux.error is None\n    assert parent_run_qux.outputs is not None\n    assert parent_run_qux.outputs[\"output\"] == \"quxaaaa\"\n    assert len(parent_run_qux.child_runs) == 4\n    assert [r.error for r in parent_run_qux.child_runs] == [None, None, None, None]\n\n\ndef test_runnable_branch_init() -> None:\n    \"\"\"Verify that runnable branch gets initialized properly.\"\"\"\n    add = RunnableLambda(lambda x: x + 1)\n    condition = RunnableLambda(lambda x: x > 0)\n\n    # Test failure with less than 2 branches\n    with pytest.raises(ValueError):\n        RunnableBranch((condition, add))\n\n    # Test failure with less than 2 branches\n    with pytest.raises(ValueError):\n        RunnableBranch(condition)\n\n\n@pytest.mark.parametrize(\n    \"branches\",\n    [\n        [\n            (RunnableLambda(lambda x: x > 0), RunnableLambda(lambda x: x + 1)),\n            RunnableLambda(lambda x: x - 1),\n        ],\n        [\n            (RunnableLambda(lambda x: x > 0), RunnableLambda(lambda x: x + 1)),\n            (RunnableLambda(lambda x: x > 5), RunnableLambda(lambda x: x + 1)),\n            RunnableLambda(lambda x: x - 1),\n        ],\n        [\n            (lambda x: x > 0, lambda x: x + 1),\n            (lambda x: x > 5, lambda x: x + 1),\n            lambda x: x - 1,\n        ],\n    ],\n)\ndef test_runnable_branch_init_coercion(branches: Sequence[Any]) -> None:\n    \"\"\"Verify that runnable branch gets initialized properly.\"\"\"\n    runnable = RunnableBranch[int, int](*branches)\n    for branch in runnable.branches:\n        condition, body = branch\n        assert isinstance(condition, Runnable)\n        assert isinstance(body, Runnable)\n\n    assert isinstance(runnable.default, Runnable)\n    assert _schema(runnable.input_schema) == {\n        \"title\": \"RunnableBranchInput\",\n        \"type\": \"integer\",\n    }\n\n\ndef test_runnable_branch_invoke_call_counts(mocker: MockerFixture) -> None:\n    \"\"\"Verify that runnables are invoked only when necessary.\"\"\"\n    # Test with single branch\n    add = RunnableLambda(lambda x: x + 1)\n    sub = RunnableLambda(lambda x: x - 1)\n    condition = RunnableLambda(lambda x: x > 0)\n    spy = mocker.spy(condition, \"invoke\")\n    add_spy = mocker.spy(add, \"invoke\")\n\n    branch = RunnableBranch[int, int]((condition, add), (condition, add), sub)\n    assert spy.call_count == 0\n    assert add_spy.call_count == 0\n\n    assert branch.invoke(1) == 2\n    assert add_spy.call_count == 1\n    assert spy.call_count == 1\n\n    assert branch.invoke(2) == 3\n    assert spy.call_count == 2\n    assert add_spy.call_count == 2\n\n    assert branch.invoke(-3) == -4\n    # Should fall through to default branch with condition being evaluated twice!\n    assert spy.call_count == 4\n    # Add should not be invoked\n    assert add_spy.call_count == 2\n\n\ndef test_runnable_branch_invoke() -> None:\n    # Test with single branch\n    def raise_value_error(x: int) -> int:\n        \"\"\"Raise a value error.\"\"\"\n        msg = \"x is too large\"\n        raise ValueError(msg)\n\n    branch = RunnableBranch[int, int](\n        (lambda x: x > 100, raise_value_error),\n        # mypy cannot infer types from the lambda\n        (lambda x: x > 0 and x < 5, lambda x: x + 1),  # type: ignore[misc]\n        (lambda x: x > 5, lambda x: x * 10),\n        lambda x: x - 1,\n    )\n\n    assert branch.invoke(1) == 2\n    assert branch.invoke(10) == 100\n    assert branch.invoke(0) == -1\n    # Should raise an exception\n    with pytest.raises(ValueError):\n        branch.invoke(1000)\n\n\ndef test_runnable_branch_batch() -> None:\n    \"\"\"Test batch variant.\"\"\"\n    # Test with single branch\n    branch = RunnableBranch[int, int](\n        (lambda x: x > 0 and x < 5, lambda x: x + 1),\n        (lambda x: x > 5, lambda x: x * 10),\n        lambda x: x - 1,\n    )\n\n    assert branch.batch([1, 10, 0]) == [2, 100, -1]\n\n\nasync def test_runnable_branch_ainvoke() -> None:\n    \"\"\"Test async variant of invoke.\"\"\"\n    branch = RunnableBranch[int, int](\n        (lambda x: x > 0 and x < 5, lambda x: x + 1),\n        (lambda x: x > 5, lambda x: x * 10),\n        lambda x: x - 1,\n    )\n\n    assert await branch.ainvoke(1) == 2\n    assert await branch.ainvoke(10) == 100\n    assert await branch.ainvoke(0) == -1\n\n    # Verify that the async variant is used if available\n    async def condition(x: int) -> bool:\n        return x > 0\n\n    async def add(x: int) -> int:\n        return x + 1\n\n    async def sub(x: int) -> int:\n        return x - 1\n\n    branch = RunnableBranch[int, int]((condition, add), sub)\n\n    assert await branch.ainvoke(1) == 2\n    assert await branch.ainvoke(-10) == -11\n\n\ndef test_runnable_branch_invoke_callbacks() -> None:\n    \"\"\"Verify that callbacks are correctly used in invoke.\"\"\"\n    tracer = FakeTracer()\n\n    def raise_value_error(x: int) -> int:\n        \"\"\"Raise a value error.\"\"\"\n        msg = \"x is too large\"\n        raise ValueError(msg)\n\n    branch = RunnableBranch[int, int](\n        (lambda x: x > 100, raise_value_error),\n        lambda x: x - 1,\n    )\n\n    assert branch.invoke(1, config={\"callbacks\": [tracer]}) == 0\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].error is None\n    assert tracer.runs[0].outputs == {\"output\": 0}\n\n    # Check that the chain on end is invoked\n    with pytest.raises(ValueError):\n        branch.invoke(1000, config={\"callbacks\": [tracer]})\n\n    assert len(tracer.runs) == 2\n    assert \"ValueError('x is too large')\" in str(tracer.runs[1].error)\n    assert not tracer.runs[1].outputs\n\n\nasync def test_runnable_branch_ainvoke_callbacks() -> None:\n    \"\"\"Verify that callbacks are invoked correctly in ainvoke.\"\"\"\n    tracer = FakeTracer()\n\n    async def raise_value_error(x: int) -> int:\n        \"\"\"Raise a value error.\"\"\"\n        msg = \"x is too large\"\n        raise ValueError(msg)\n\n    branch = RunnableBranch[int, int](\n        (lambda x: x > 100, raise_value_error),\n        lambda x: x - 1,\n    )\n\n    assert await branch.ainvoke(1, config={\"callbacks\": [tracer]}) == 0\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].error is None\n    assert tracer.runs[0].outputs == {\"output\": 0}\n\n    # Check that the chain on end is invoked\n    with pytest.raises(ValueError):\n        await branch.ainvoke(1000, config={\"callbacks\": [tracer]})\n\n    assert len(tracer.runs) == 2\n    assert \"ValueError('x is too large')\" in str(tracer.runs[1].error)\n    assert not tracer.runs[1].outputs\n\n\nasync def test_runnable_branch_abatch() -> None:\n    \"\"\"Test async variant of invoke.\"\"\"\n    branch = RunnableBranch[int, int](\n        (lambda x: x > 0 and x < 5, lambda x: x + 1),\n        (lambda x: x > 5, lambda x: x * 10),\n        lambda x: x - 1,\n    )\n\n    assert await branch.abatch([1, 10, 0]) == [2, 100, -1]\n\n\ndef test_runnable_branch_stream() -> None:\n    \"\"\"Verify that stream works for RunnableBranch.\"\"\"\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    branch = RunnableBranch[str, Any](\n        (lambda x: x == \"hello\", llm),\n        lambda x: x,\n    )\n\n    assert list(branch.stream(\"hello\")) == list(llm_res)\n    assert list(branch.stream(\"bye\")) == [\"bye\"]\n\n\ndef test_runnable_branch_stream_with_callbacks() -> None:\n    \"\"\"Verify that stream works for RunnableBranch when using callbacks.\"\"\"\n    tracer = FakeTracer()\n\n    def raise_value_error(x: str) -> Any:\n        \"\"\"Raise a value error.\"\"\"\n        msg = f\"x is {x}\"\n        raise ValueError(msg)\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    branch = RunnableBranch[str, Any](\n        (lambda x: x == \"error\", raise_value_error),\n        (lambda x: x == \"hello\", llm),\n        lambda x: x,\n    )\n    config: RunnableConfig = {\"callbacks\": [tracer]}\n\n    assert list(branch.stream(\"hello\", config=config)) == list(llm_res)\n\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].error is None\n    assert tracer.runs[0].outputs == {\"output\": llm_res}\n\n    # Verify that the chain on error is invoked\n    with pytest.raises(ValueError):\n        for _ in branch.stream(\"error\", config=config):\n            pass\n\n    assert len(tracer.runs) == 2\n    assert \"ValueError('x is error')\" in str(tracer.runs[1].error)\n    assert not tracer.runs[1].outputs\n\n    assert list(branch.stream(\"bye\", config=config)) == [\"bye\"]\n\n    assert len(tracer.runs) == 3\n    assert tracer.runs[2].error is None\n    assert tracer.runs[2].outputs == {\"output\": \"bye\"}\n\n\nasync def test_runnable_branch_astream() -> None:\n    \"\"\"Verify that astream works for RunnableBranch.\"\"\"\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    branch = RunnableBranch[str, Any](\n        (lambda x: x == \"hello\", llm),\n        lambda x: x,\n    )\n\n    assert [_ async for _ in branch.astream(\"hello\")] == list(llm_res)\n    assert [_ async for _ in branch.astream(\"bye\")] == [\"bye\"]\n\n    # Verify that the async variant is used if available\n    async def condition(x: str) -> bool:\n        return x == \"hello\"\n\n    async def repeat(x: str) -> str:\n        return x + x\n\n    async def reverse(x: str) -> str:\n        return x[::-1]\n\n    branch = RunnableBranch[str, Any]((condition, repeat), llm)\n\n    assert [_ async for _ in branch.astream(\"hello\")] == [\"hello\" * 2]\n    assert [_ async for _ in branch.astream(\"bye\")] == list(llm_res)\n\n    branch = RunnableBranch[str, Any]((condition, llm), reverse)\n\n    assert [_ async for _ in branch.astream(\"hello\")] == list(llm_res)\n    assert [_ async for _ in branch.astream(\"bye\")] == [\"eyb\"]\n\n\nasync def test_runnable_branch_astream_with_callbacks() -> None:\n    \"\"\"Verify that astream works for RunnableBranch when using callbacks.\"\"\"\n    tracer = FakeTracer()\n\n    def raise_value_error(x: str) -> Any:\n        \"\"\"Raise a value error.\"\"\"\n        msg = f\"x is {x}\"\n        raise ValueError(msg)\n\n    llm_res = \"i'm a textbot\"\n    # sleep to better simulate a real stream\n    llm = FakeStreamingListLLM(responses=[llm_res], sleep=0.01)\n\n    branch = RunnableBranch[str, Any](\n        (lambda x: x == \"error\", raise_value_error),\n        (lambda x: x == \"hello\", llm),\n        lambda x: x,\n    )\n    config: RunnableConfig = {\"callbacks\": [tracer]}\n\n    assert [_ async for _ in branch.astream(\"hello\", config=config)] == list(llm_res)\n\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].error is None\n    assert tracer.runs[0].outputs == {\"output\": llm_res}\n\n    # Verify that the chain on error is invoked\n    with pytest.raises(ValueError):\n        async for _ in branch.astream(\"error\", config=config):\n            pass\n\n    assert len(tracer.runs) == 2\n    assert \"ValueError('x is error')\" in str(tracer.runs[1].error)\n    assert not tracer.runs[1].outputs\n\n    assert [_ async for _ in branch.astream(\"bye\", config=config)] == [\"bye\"]\n\n    assert len(tracer.runs) == 3\n    assert tracer.runs[2].error is None\n    assert tracer.runs[2].outputs == {\"output\": \"bye\"}\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 9), reason=\"Requires python version >= 3.9 to run.\"\n)\ndef test_representation_of_runnables() -> None:\n    \"\"\"Test representation of runnables.\"\"\"\n    runnable = RunnableLambda(lambda x: x * 2)\n    assert repr(runnable) == \"RunnableLambda(lambda x: x * 2)\"\n\n    def f(x: int) -> int:\n        \"\"\"Return 2.\"\"\"\n        return 2\n\n    assert repr(RunnableLambda(func=f)) == \"RunnableLambda(f)\"\n\n    async def af(x: int) -> int:\n        \"\"\"Return 2.\"\"\"\n        return 2\n\n    assert repr(RunnableLambda(func=f, afunc=af)) == \"RunnableLambda(f)\"\n\n    assert repr(\n        RunnableLambda(lambda x: x * 2)\n        | {\n            \"a\": RunnableLambda(lambda x: x * 2),\n            \"b\": RunnableLambda(lambda x: x * 3),\n        }\n    ) == (\n        \"RunnableLambda(lambda x: x * 2)\\n\"\n        \"| {\\n\"\n        \"    a: RunnableLambda(...),\\n\"\n        \"    b: RunnableLambda(...)\\n\"\n        \"  }\"\n    )\n\n\nasync def test_tool_from_runnable() -> None:\n    prompt = (\n        SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n        + \"{question}\"\n    )\n    llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n\n    chain = prompt | llm | StrOutputParser()\n\n    chain_tool = tool(\"chain_tool\", chain)\n\n    assert isinstance(chain_tool, BaseTool)\n    assert chain_tool.name == \"chain_tool\"\n    assert chain_tool.run({\"question\": \"What up\"}) == chain.invoke(\n        {\"question\": \"What up\"}\n    )\n    assert await chain_tool.arun({\"question\": \"What up\"}) == await chain.ainvoke(\n        {\"question\": \"What up\"}\n    )\n    assert chain_tool.description.endswith(repr(chain))\n    assert _schema(chain_tool.args_schema) == chain.get_input_jsonschema()\n    assert _schema(chain_tool.args_schema) == {\n        \"properties\": {\"question\": {\"title\": \"Question\", \"type\": \"string\"}},\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n        \"required\": [\"question\"],\n    }\n\n\ndef test_runnable_gen() -> None:\n    \"\"\"Test that a generator can be used as a runnable.\"\"\"\n\n    def gen(input: Iterator[Any]) -> Iterator[int]:\n        yield 1\n        yield 2\n        yield 3\n\n    runnable = RunnableGenerator(gen)\n\n    assert runnable.get_input_jsonschema() == {\"title\": \"gen_input\"}\n    assert runnable.get_output_jsonschema() == {\n        \"title\": \"gen_output\",\n        \"type\": \"integer\",\n    }\n\n    assert runnable.invoke(None) == 6\n    assert list(runnable.stream(None)) == [1, 2, 3]\n    assert runnable.batch([None, None]) == [6, 6]\n\n\nasync def test_runnable_gen_async() -> None:\n    \"\"\"Test that a generator can be used as a runnable.\"\"\"\n\n    async def agen(input: AsyncIterator[Any]) -> AsyncIterator[int]:\n        yield 1\n        yield 2\n        yield 3\n\n    arunnable = RunnableGenerator(agen)\n\n    assert await arunnable.ainvoke(None) == 6\n    assert [p async for p in arunnable.astream(None)] == [1, 2, 3]\n    assert await arunnable.abatch([None, None]) == [6, 6]\n\n    class AsyncGen:\n        async def __call__(self, input: AsyncIterator[Any]) -> AsyncIterator[int]:\n            yield 1\n            yield 2\n            yield 3\n\n    arunnablecallable = RunnableGenerator(AsyncGen())\n    assert await arunnablecallable.ainvoke(None) == 6\n    assert [p async for p in arunnablecallable.astream(None)] == [1, 2, 3]\n    assert await arunnablecallable.abatch([None, None]) == [6, 6]\n    with pytest.raises(NotImplementedError):\n        await asyncio.to_thread(arunnablecallable.invoke, None)\n    with pytest.raises(NotImplementedError):\n        await asyncio.to_thread(arunnablecallable.stream, None)\n    with pytest.raises(NotImplementedError):\n        await asyncio.to_thread(arunnablecallable.batch, [None, None])\n\n\ndef test_runnable_gen_context_config() -> None:\n    \"\"\"Test that a generator can call other runnables with config\n    propagated from the context.\n    \"\"\"\n    fake = RunnableLambda(len)\n\n    def gen(input: Iterator[Any]) -> Iterator[int]:\n        yield fake.invoke(\"a\")\n        yield fake.invoke(\"aa\")\n        yield fake.invoke(\"aaa\")\n\n    runnable = RunnableGenerator(gen)\n\n    assert runnable.get_input_jsonschema() == {\"title\": \"gen_input\"}\n    assert runnable.get_output_jsonschema() == {\n        \"title\": \"gen_output\",\n        \"type\": \"integer\",\n    }\n\n    tracer = FakeTracer()\n    run_id = uuid.uuid4()\n    assert runnable.invoke(None, {\"callbacks\": [tracer], \"run_id\": run_id}) == 6\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    run_ids = tracer.run_ids\n    assert run_id in run_ids\n    assert len(run_ids) == len(set(run_ids))\n    tracer.runs.clear()\n\n    assert list(runnable.stream(None)) == [1, 2, 3]\n    assert len(tracer.runs) == 0, \"callbacks doesn't persist from previous call\"\n\n    tracer = FakeTracer()\n    run_id = uuid.uuid4()\n    assert list(runnable.stream(None, {\"callbacks\": [tracer], \"run_id\": run_id})) == [\n        1,\n        2,\n        3,\n    ]\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    run_ids = tracer.run_ids\n    assert run_id in run_ids\n    assert len(run_ids) == len(set(run_ids))\n    tracer.runs.clear()\n\n    tracer = FakeTracer()\n    run_id = uuid.uuid4()\n\n    with pytest.warns(RuntimeWarning):\n        assert runnable.batch(\n            [None, None], {\"callbacks\": [tracer], \"run_id\": run_id}\n        ) == [6, 6]\n    assert len(tracer.runs) == 2\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert tracer.runs[1].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    assert len(tracer.runs[1].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[1].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[1].child_runs] == [1, 2, 3]\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=\"Python 3.10 and below don't support running async tasks in a specific context\",\n)\nasync def test_runnable_gen_context_config_async() -> None:\n    \"\"\"Test that a generator can call other runnables with config\n    propagated from the context.\"\"\"\n\n    fake = RunnableLambda(len)\n\n    async def agen(input: AsyncIterator[Any]) -> AsyncIterator[int]:\n        yield await fake.ainvoke(\"a\")\n        yield await fake.ainvoke(\"aa\")\n        yield await fake.ainvoke(\"aaa\")\n\n    arunnable = RunnableGenerator(agen)\n\n    tracer = FakeTracer()\n\n    run_id = uuid.uuid4()\n    assert await arunnable.ainvoke(None, {\"callbacks\": [tracer], \"run_id\": run_id}) == 6\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    run_ids = tracer.run_ids\n    assert run_id in run_ids\n    assert len(run_ids) == len(set(run_ids))\n    tracer.runs.clear()\n\n    assert [p async for p in arunnable.astream(None)] == [1, 2, 3]\n    assert len(tracer.runs) == 0, \"callbacks doesn't persist from previous call\"\n\n    tracer = FakeTracer()\n    run_id = uuid.uuid4()\n    assert [\n        p\n        async for p in arunnable.astream(\n            None, {\"callbacks\": [tracer], \"run_id\": run_id}\n        )\n    ] == [\n        1,\n        2,\n        3,\n    ]\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    run_ids = tracer.run_ids\n    assert run_id in run_ids\n    assert len(run_ids) == len(set(run_ids))\n\n    tracer = FakeTracer()\n    run_id = uuid.uuid4()\n    with pytest.warns(RuntimeWarning):\n        assert await arunnable.abatch(\n            [None, None], {\"callbacks\": [tracer], \"run_id\": run_id}\n        ) == [6, 6]\n    assert len(tracer.runs) == 2\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert tracer.runs[1].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    assert len(tracer.runs[1].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[1].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[1].child_runs] == [1, 2, 3]\n\n\ndef test_runnable_iter_context_config() -> None:\n    \"\"\"Test that a generator can call other runnables with config\n    propagated from the context.\n    \"\"\"\n    fake = RunnableLambda(len)\n\n    @chain\n    def gen(input: str) -> Iterator[int]:\n        yield fake.invoke(input)\n        yield fake.invoke(input * 2)\n        yield fake.invoke(input * 3)\n\n    assert gen.get_input_jsonschema() == {\n        \"title\": \"gen_input\",\n        \"type\": \"string\",\n    }\n    assert gen.get_output_jsonschema() == {\n        \"title\": \"gen_output\",\n        \"type\": \"integer\",\n    }\n\n    tracer = FakeTracer()\n    assert gen.invoke(\"a\", {\"callbacks\": [tracer]}) == 6\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    tracer.runs.clear()\n\n    assert list(gen.stream(\"a\")) == [1, 2, 3]\n    assert len(tracer.runs) == 0, \"callbacks doesn't persist from previous call\"\n\n    tracer = FakeTracer()\n    assert list(gen.stream(\"a\", {\"callbacks\": [tracer]})) == [1, 2, 3]\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n\n    tracer = FakeTracer()\n    assert gen.batch([\"a\", \"a\"], {\"callbacks\": [tracer]}) == [6, 6]\n    assert len(tracer.runs) == 2\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert tracer.runs[1].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    assert len(tracer.runs[1].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[1].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[1].child_runs] == [1, 2, 3]\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=\"Python 3.10 and below don't support running async tasks in a specific context\",\n)\nasync def test_runnable_iter_context_config_async() -> None:\n    \"\"\"Test that a generator can call other runnables with config\n    propagated from the context.\"\"\"\n\n    fake = RunnableLambda(len)\n\n    @chain\n    async def agen(input: str) -> AsyncIterator[int]:\n        yield await fake.ainvoke(input)\n        yield await fake.ainvoke(input * 2)\n        yield await fake.ainvoke(input * 3)\n\n    assert agen.get_input_jsonschema() == {\n        \"title\": \"agen_input\",\n        \"type\": \"string\",\n    }\n    assert agen.get_output_jsonschema() == {\n        \"title\": \"agen_output\",\n        \"type\": \"integer\",\n    }\n\n    tracer = FakeTracer()\n    assert await agen.ainvoke(\"a\", {\"callbacks\": [tracer]}) == 6\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    tracer.runs.clear()\n\n    assert [p async for p in agen.astream(\"a\")] == [1, 2, 3]\n    assert len(tracer.runs) == 0, \"callbacks doesn't persist from previous call\"\n\n    tracer = FakeTracer()\n    assert [p async for p in agen.astream(\"a\", {\"callbacks\": [tracer]})] == [\n        1,\n        2,\n        3,\n    ]\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n\n    tracer = FakeTracer()\n    assert [p async for p in agen.astream_log(\"a\", {\"callbacks\": [tracer]})]\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n\n    tracer = FakeTracer()\n    assert await agen.abatch([\"a\", \"a\"], {\"callbacks\": [tracer]}) == [6, 6]\n    assert len(tracer.runs) == 2\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert tracer.runs[1].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    assert len(tracer.runs[1].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[1].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[1].child_runs] == [1, 2, 3]\n\n\ndef test_runnable_lambda_context_config() -> None:\n    \"\"\"Test that a function can call other runnables with config\n    propagated from the context.\n    \"\"\"\n    fake = RunnableLambda(len)\n\n    @chain\n    def fun(input: str) -> int:\n        output = fake.invoke(input)\n        output += fake.invoke(input * 2)\n        output += fake.invoke(input * 3)\n        return output\n\n    assert fun.get_input_jsonschema() == {\"title\": \"fun_input\", \"type\": \"string\"}\n    assert fun.get_output_jsonschema() == {\n        \"title\": \"fun_output\",\n        \"type\": \"integer\",\n    }\n\n    tracer = FakeTracer()\n    assert fun.invoke(\"a\", {\"callbacks\": [tracer]}) == 6\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    tracer.runs.clear()\n\n    assert list(fun.stream(\"a\")) == [6]\n    assert len(tracer.runs) == 0, \"callbacks doesn't persist from previous call\"\n\n    tracer = FakeTracer()\n    assert list(fun.stream(\"a\", {\"callbacks\": [tracer]})) == [6]\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n\n    tracer = FakeTracer()\n    assert fun.batch([\"a\", \"a\"], {\"callbacks\": [tracer]}) == [6, 6]\n    assert len(tracer.runs) == 2\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert tracer.runs[1].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    assert len(tracer.runs[1].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[1].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[1].child_runs] == [1, 2, 3]\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=\"Python 3.10 and below don't support running async tasks in a specific context\",\n)\nasync def test_runnable_lambda_context_config_async() -> None:\n    \"\"\"Test that a function can call other runnables with config\n    propagated from the context.\"\"\"\n\n    fake = RunnableLambda(len)\n\n    @chain\n    async def afun(input: str) -> int:\n        output = await fake.ainvoke(input)\n        output += await fake.ainvoke(input * 2)\n        output += await fake.ainvoke(input * 3)\n        return output\n\n    assert afun.get_input_jsonschema() == {\"title\": \"afun_input\", \"type\": \"string\"}\n    assert afun.get_output_jsonschema() == {\n        \"title\": \"afun_output\",\n        \"type\": \"integer\",\n    }\n\n    tracer = FakeTracer()\n    assert await afun.ainvoke(\"a\", {\"callbacks\": [tracer]}) == 6\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    tracer.runs.clear()\n\n    assert [p async for p in afun.astream(\"a\")] == [6]\n    assert len(tracer.runs) == 0, \"callbacks doesn't persist from previous call\"\n\n    tracer = FakeTracer()\n    assert [p async for p in afun.astream(\"a\", {\"callbacks\": [tracer]})] == [6]\n    assert len(tracer.runs) == 1\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n\n    tracer = FakeTracer()\n    assert await afun.abatch([\"a\", \"a\"], {\"callbacks\": [tracer]}) == [6, 6]\n    assert len(tracer.runs) == 2\n    assert tracer.runs[0].outputs == {\"output\": 6}\n    assert tracer.runs[1].outputs == {\"output\": 6}\n    assert len(tracer.runs[0].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[0].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[0].child_runs] == [1, 2, 3]\n    assert len(tracer.runs[1].child_runs) == 3\n    assert [r.inputs[\"input\"] for r in tracer.runs[1].child_runs] == [\"a\", \"aa\", \"aaa\"]\n    assert [(r.outputs or {})[\"output\"] for r in tracer.runs[1].child_runs] == [1, 2, 3]\n\n\nasync def test_runnable_gen_transform() -> None:\n    \"\"\"Test that a generator can be used as a runnable.\"\"\"\n\n    def gen_indexes(length_iter: Iterator[int]) -> Iterator[int]:\n        yield from range(next(length_iter))\n\n    async def agen_indexes(length_iter: AsyncIterator[int]) -> AsyncIterator[int]:\n        async for length in length_iter:\n            for i in range(length):\n                yield i\n\n    def plus_one(input: Iterator[int]) -> Iterator[int]:\n        for i in input:\n            yield i + 1\n\n    async def aplus_one(input: AsyncIterator[int]) -> AsyncIterator[int]:\n        async for i in input:\n            yield i + 1\n\n    chain: Runnable = RunnableGenerator(gen_indexes, agen_indexes) | plus_one\n    achain = RunnableGenerator(gen_indexes, agen_indexes) | aplus_one\n\n    assert chain.get_input_jsonschema() == {\n        \"title\": \"gen_indexes_input\",\n        \"type\": \"integer\",\n    }\n    assert chain.get_output_jsonschema() == {\n        \"title\": \"plus_one_output\",\n        \"type\": \"integer\",\n    }\n    assert achain.get_input_jsonschema() == {\n        \"title\": \"gen_indexes_input\",\n        \"type\": \"integer\",\n    }\n    assert achain.get_output_jsonschema() == {\n        \"title\": \"aplus_one_output\",\n        \"type\": \"integer\",\n    }\n\n    assert list(chain.stream(3)) == [1, 2, 3]\n    assert [p async for p in achain.astream(4)] == [1, 2, 3, 4]\n\n\ndef test_with_config_callbacks() -> None:\n    result = RunnableLambda(lambda x: x).with_config({\"callbacks\": []})\n    # Bugfix from version 0.0.325\n    # ConfigError: field \"callbacks\" not yet prepared so type is still a ForwardRef,\n    # you might need to call RunnableConfig.update_forward_refs().\n    assert isinstance(result, RunnableBinding)\n\n\nasync def test_ainvoke_on_returned_runnable() -> None:\n    \"\"\"Verify that a runnable returned by a sync runnable in the async path will\n    be runthroughaasync path (issue #13407).\n    \"\"\"\n\n    def idchain_sync(__input: dict) -> bool:\n        return False\n\n    async def idchain_async(__input: dict) -> bool:\n        return True\n\n    idchain = RunnableLambda(func=idchain_sync, afunc=idchain_async)\n\n    def func(__input: dict) -> Runnable:\n        return idchain\n\n    assert await RunnableLambda(func).ainvoke({})\n\n\ndef test_invoke_stream_passthrough_assign_trace() -> None:\n    def idchain_sync(__input: dict) -> bool:\n        return False\n\n    chain = RunnablePassthrough.assign(urls=idchain_sync)\n\n    tracer = FakeTracer()\n    chain.invoke({\"example\": [1, 2, 3]}, {\"callbacks\": [tracer]})\n\n    assert tracer.runs[0].name == \"RunnableAssign<urls>\"\n    assert tracer.runs[0].child_runs[0].name == \"RunnableParallel<urls>\"\n\n    tracer = FakeTracer()\n    for _ in chain.stream({\"example\": [1, 2, 3]}, {\"callbacks\": [tracer]}):\n        pass\n\n    assert tracer.runs[0].name == \"RunnableAssign<urls>\"\n    assert tracer.runs[0].child_runs[0].name == \"RunnableParallel<urls>\"\n\n\nasync def test_ainvoke_astream_passthrough_assign_trace() -> None:\n    def idchain_sync(__input: dict) -> bool:\n        return False\n\n    chain = RunnablePassthrough.assign(urls=idchain_sync)\n\n    tracer = FakeTracer()\n    await chain.ainvoke({\"example\": [1, 2, 3]}, {\"callbacks\": [tracer]})\n\n    assert tracer.runs[0].name == \"RunnableAssign<urls>\"\n    assert tracer.runs[0].child_runs[0].name == \"RunnableParallel<urls>\"\n\n    tracer = FakeTracer()\n    async for _ in chain.astream({\"example\": [1, 2, 3]}, {\"callbacks\": [tracer]}):\n        pass\n\n    assert tracer.runs[0].name == \"RunnableAssign<urls>\"\n    assert tracer.runs[0].child_runs[0].name == \"RunnableParallel<urls>\"\n\n\nasync def test_astream_log_deep_copies() -> None:\n    \"\"\"Verify that deep copies are used when using jsonpatch in astream log.\n\n    jsonpatch re-uses objects in its API; e.g.,\n\n    import jsonpatch\n    obj1 = { \"a\": 1 }\n    value = { \"b\": 2 }\n    obj2 = { \"a\": 1, \"value\": value }\n\n    ops = list(jsonpatch.JsonPatch.from_diff(obj1, obj2))\n    assert id(ops[0]['value']) == id(value)\n\n    This can create unexpected consequences for downstream code.\n    \"\"\"\n\n    def _get_run_log(run_log_patches: Sequence[RunLogPatch]) -> RunLog:\n        \"\"\"Get run log.\"\"\"\n        run_log = RunLog(state=None)  # type: ignore\n        for log_patch in run_log_patches:\n            run_log = run_log + log_patch\n        return run_log\n\n    def add_one(x: int) -> int:\n        \"\"\"Add one.\"\"\"\n        return x + 1\n\n    chain = RunnableLambda(add_one)\n    chunks = []\n    final_output: Optional[RunLogPatch] = None\n    async for chunk in chain.astream_log(1):\n        chunks.append(chunk)\n        final_output = chunk if final_output is None else final_output + chunk\n\n    run_log = _get_run_log(chunks)\n    state = run_log.state.copy()\n    # Ignoring type here since we know that the state is a dict\n    # so we can delete `id` for testing purposes\n    state.pop(\"id\")  # type: ignore\n    assert state == {\n        \"final_output\": 2,\n        \"logs\": {},\n        \"streamed_output\": [2],\n        \"name\": \"add_one\",\n        \"type\": \"chain\",\n    }\n\n\ndef test_transform_of_runnable_lambda_with_dicts() -> None:\n    \"\"\"Test transform of runnable lamdbda.\"\"\"\n    runnable = RunnableLambda(lambda x: x)\n    chunks = iter(\n        [\n            {\"foo\": \"n\"},\n        ]\n    )\n    assert list(runnable.transform(chunks)) == [{\"foo\": \"n\"}]\n\n    # Test as part of a sequence\n    seq = runnable | runnable\n    chunks = iter(\n        [\n            {\"foo\": \"n\"},\n        ]\n    )\n    assert list(seq.transform(chunks)) == [{\"foo\": \"n\"}]\n    # Test some other edge cases\n    assert list(seq.stream({\"foo\": \"n\"})) == [{\"foo\": \"n\"}]\n\n\nasync def test_atransform_of_runnable_lambda_with_dicts() -> None:\n    async def identity(x: dict[str, str]) -> dict[str, str]:\n        \"\"\"Return x.\"\"\"\n        return x\n\n    runnable = RunnableLambda[dict[str, str], dict[str, str]](identity)\n\n    async def chunk_iterator() -> AsyncIterator[dict[str, str]]:\n        yield {\"foo\": \"a\"}\n        yield {\"foo\": \"n\"}\n\n    chunks = [chunk async for chunk in runnable.atransform(chunk_iterator())]\n    assert chunks == [{\"foo\": \"n\"}]\n\n    seq = runnable | runnable\n    chunks = [chunk async for chunk in seq.atransform(chunk_iterator())]\n    assert chunks == [{\"foo\": \"n\"}]\n\n\ndef test_default_transform_with_dicts() -> None:\n    \"\"\"Test that default transform works with dicts.\"\"\"\n\n    class CustomRunnable(RunnableSerializable[Input, Output]):\n        def invoke(\n            self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n        ) -> Output:\n            return cast(Output, input)  # type: ignore\n\n    runnable = CustomRunnable[dict[str, str], dict[str, str]]()\n    chunks = iter(\n        [\n            {\"foo\": \"a\"},\n            {\"foo\": \"n\"},\n        ]\n    )\n\n    assert list(runnable.transform(chunks)) == [{\"foo\": \"n\"}]\n    assert list(runnable.stream({\"foo\": \"n\"})) == [{\"foo\": \"n\"}]\n\n\nasync def test_default_atransform_with_dicts() -> None:\n    \"\"\"Test that default transform works with dicts.\"\"\"\n\n    class CustomRunnable(RunnableSerializable[Input, Output]):\n        def invoke(\n            self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n        ) -> Output:\n            return cast(Output, input)\n\n    runnable = CustomRunnable[dict[str, str], dict[str, str]]()\n\n    async def chunk_iterator() -> AsyncIterator[dict[str, str]]:\n        yield {\"foo\": \"a\"}\n        yield {\"foo\": \"n\"}\n\n    chunks = [chunk async for chunk in runnable.atransform(chunk_iterator())]\n\n    assert chunks == [{\"foo\": \"n\"}]\n\n    # Test with addable dict\n    async def chunk_iterator_with_addable() -> AsyncIterator[dict[str, str]]:\n        yield AddableDict({\"foo\": \"a\"})\n        yield AddableDict({\"foo\": \"n\"})\n\n    chunks = [\n        chunk async for chunk in runnable.atransform(chunk_iterator_with_addable())\n    ]\n\n    assert chunks == [{\"foo\": \"an\"}]\n\n\ndef test_passthrough_transform_with_dicts() -> None:\n    \"\"\"Test that default transform works with dicts.\"\"\"\n    runnable = RunnablePassthrough(lambda x: x)\n    chunks = list(runnable.transform(iter([{\"foo\": \"a\"}, {\"foo\": \"n\"}])))\n    assert chunks == [{\"foo\": \"a\"}, {\"foo\": \"n\"}]\n\n\nasync def test_passthrough_atransform_with_dicts() -> None:\n    \"\"\"Test that default transform works with dicts.\"\"\"\n    runnable = RunnablePassthrough(lambda x: x)\n\n    async def chunk_iterator() -> AsyncIterator[dict[str, str]]:\n        yield {\"foo\": \"a\"}\n        yield {\"foo\": \"n\"}\n\n    chunks = [chunk async for chunk in runnable.atransform(chunk_iterator())]\n    assert chunks == [{\"foo\": \"a\"}, {\"foo\": \"n\"}]\n\n\ndef test_listeners() -> None:\n    from langchain_core.runnables import RunnableLambda\n    from langchain_core.tracers.schemas import Run\n\n    def fake_chain(inputs: dict) -> dict:\n        return {**inputs, \"key\": \"extra\"}\n\n    shared_state = {}\n    value1 = {\"inputs\": {\"name\": \"one\"}, \"outputs\": {\"name\": \"one\"}}\n    value2 = {\"inputs\": {\"name\": \"two\"}, \"outputs\": {\"name\": \"two\"}}\n\n    def on_start(run: Run) -> None:\n        shared_state[run.id] = {\"inputs\": run.inputs}\n\n    def on_end(run: Run) -> None:\n        shared_state[run.id][\"outputs\"] = run.inputs\n\n    chain = (\n        RunnableLambda(fake_chain)\n        .with_listeners(on_end=on_end, on_start=on_start)\n        .map()\n    )\n\n    data = [{\"name\": \"one\"}, {\"name\": \"two\"}]\n    chain.invoke(data, config={\"max_concurrency\": 1})\n    assert len(shared_state) == 2\n    assert value1 in shared_state.values(), \"Value not found in the dictionary.\"\n    assert value2 in shared_state.values(), \"Value not found in the dictionary.\"\n\n\nasync def test_listeners_async() -> None:\n    from langchain_core.runnables import RunnableLambda\n    from langchain_core.tracers.schemas import Run\n\n    def fake_chain(inputs: dict) -> dict:\n        return {**inputs, \"key\": \"extra\"}\n\n    shared_state = {}\n    value1 = {\"inputs\": {\"name\": \"one\"}, \"outputs\": {\"name\": \"one\"}}\n    value2 = {\"inputs\": {\"name\": \"two\"}, \"outputs\": {\"name\": \"two\"}}\n\n    def on_start(run: Run) -> None:\n        shared_state[run.id] = {\"inputs\": run.inputs}\n\n    def on_end(run: Run) -> None:\n        shared_state[run.id][\"outputs\"] = run.inputs\n\n    chain: Runnable = (\n        RunnableLambda(fake_chain)\n        .with_listeners(on_end=on_end, on_start=on_start)\n        .map()\n    )\n\n    data = [{\"name\": \"one\"}, {\"name\": \"two\"}]\n    await chain.ainvoke(data, config={\"max_concurrency\": 1})\n\n    assert len(shared_state) == 2\n    assert value1 in shared_state.values(), \"Value not found in the dictionary.\"\n    assert value2 in shared_state.values(), \"Value not found in the dictionary.\"\n\n\ndef test_closing_iterator_doesnt_raise_error() -> None:\n    \"\"\"Test that closing an iterator calls on_chain_end rather than on_chain_error.\"\"\"\n    import time\n\n    from langchain_core.callbacks import BaseCallbackHandler\n    from langchain_core.language_models.fake_chat_models import GenericFakeChatModel\n    from langchain_core.output_parsers import StrOutputParser\n\n    on_chain_error_triggered = False\n    on_chain_end_triggered = False\n\n    class MyHandler(BaseCallbackHandler):\n        def on_chain_error(\n            self,\n            error: BaseException,\n            *,\n            run_id: UUID,\n            parent_run_id: Optional[UUID] = None,\n            tags: Optional[list[str]] = None,\n            **kwargs: Any,\n        ) -> None:\n            \"\"\"Run when chain errors.\"\"\"\n            nonlocal on_chain_error_triggered\n            on_chain_error_triggered = True\n\n        def on_chain_end(\n            self,\n            outputs: dict[str, Any],\n            *,\n            run_id: UUID,\n            parent_run_id: Optional[UUID] = None,\n            **kwargs: Any,\n        ) -> None:\n            nonlocal on_chain_end_triggered\n            on_chain_end_triggered = True\n\n    llm = GenericFakeChatModel(messages=iter([\"hi there\"]))\n    chain = llm | StrOutputParser()\n    chain_ = chain.with_config({\"callbacks\": [MyHandler()]})\n    st = chain_.stream(\"hello\")\n    next(st)\n    # This is a generator so close is defined on it.\n    st.close()  # type: ignore\n    # Wait for a bit to make sure that the callback is called.\n    time.sleep(0.05)\n    assert on_chain_error_triggered is False\n    assert on_chain_end_triggered is True\n\n\ndef test_pydantic_protected_namespaces() -> None:\n    # Check that protected namespaces (e.g., `model_kwargs`) do not raise warnings\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n\n        class CustomChatModel(RunnableSerializable):\n            model_kwargs: dict[str, Any] = Field(default_factory=dict)\n\n\ndef test_schema_for_prompt_and_chat_model() -> None:\n    \"\"\"Testing that schema is generated properly when using variable names\n    that collide with pydantic attributes.\n    \"\"\"\n    prompt = ChatPromptTemplate([(\"system\", \"{model_json_schema}, {_private}, {json}\")])\n    chat_res = \"i'm a chatbot\"\n    # sleep to better simulate a real stream\n    chat = FakeListChatModel(responses=[chat_res], sleep=0.01)\n    chain = prompt | chat\n    assert (\n        chain.invoke(\n            {\n                \"model_json_schema\": \"hello\",\n                \"_private\": \"goodbye\",\n                \"json\": \"json\",\n            }\n        ).content\n        == chat_res\n    )\n\n    assert chain.get_input_jsonschema() == {\n        \"properties\": {\n            \"model_json_schema\": {\"title\": \"Model Json Schema\", \"type\": \"string\"},\n            \"_private\": {\"title\": \"Private\", \"type\": \"string\"},\n            \"json\": {\"title\": \"Json\", \"type\": \"string\"},\n        },\n        \"required\": [\n            \"_private\",\n            \"json\",\n            \"model_json_schema\",\n        ],\n        \"title\": \"PromptInput\",\n        \"type\": \"object\",\n    }\n\n\ndef test_runnable_assign() -> None:\n    def add_ten(x: dict[str, int]) -> dict[str, int]:\n        return {\"added\": x[\"input\"] + 10}\n\n    mapper = RunnableParallel({\"add_step\": RunnableLambda(add_ten)})\n    runnable_assign = RunnableAssign(mapper)\n\n    result = runnable_assign.invoke({\"input\": 5})\n    assert result == {\"input\": 5, \"add_step\": {\"added\": 15}}\n",
        "patch": "@@ -567,7 +567,7 @@ def test_lambda_schemas(snapshot: SnapshotAssertion) -> None:\n         \"required\": [\"bye\", \"hello\"],\n     }\n \n-    def get_value(input):  # type: ignore[no-untyped-def]\n+    def get_value(input):  # type: ignore[no-untyped-def] # noqa: ANN001,ANN202\n         return input[\"variable_name\"]\n \n     assert RunnableLambda(get_value).get_input_jsonschema() == {\n@@ -577,7 +577,7 @@ def get_value(input):  # type: ignore[no-untyped-def]\n         \"required\": [\"variable_name\"],\n     }\n \n-    async def aget_value(input):  # type: ignore[no-untyped-def]\n+    async def aget_value(input):  # type: ignore[no-untyped-def] # noqa: ANN001,ANN202\n         return (input[\"variable_name\"], input.get(\"another\"))\n \n     assert RunnableLambda(aget_value).get_input_jsonschema() == {\n@@ -590,7 +590,7 @@ async def aget_value(input):  # type: ignore[no-untyped-def]\n         \"required\": [\"another\", \"variable_name\"],\n     }\n \n-    async def aget_values(input):  # type: ignore[no-untyped-def]\n+    async def aget_values(input):  # type: ignore[no-untyped-def] # noqa: ANN001,ANN202\n         return {\n             \"hello\": input[\"variable_name\"],\n             \"bye\": input[\"variable_name\"],"
      },
      {
        "filename": "libs/core/tests/unit_tests/runnables/test_runnable_events_v2.py",
        "content_before": "\"\"\"Module that contains tests for runnable.astream_events API.\"\"\"\n\nimport asyncio\nimport sys\nimport uuid\nfrom collections.abc import AsyncIterator, Iterable, Iterator, Sequence\nfrom functools import partial\nfrom itertools import cycle\nfrom typing import (\n    Any,\n    Optional,\n    cast,\n)\n\nimport pytest\nfrom blockbuster import BlockBuster\nfrom pydantic import BaseModel\n\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun, Callbacks\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.documents import Document\nfrom langchain_core.language_models import FakeStreamingListLLM, GenericFakeChatModel\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n)\nfrom langchain_core.prompt_values import ChatPromptValue\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.runnables import (\n    ConfigurableField,\n    Runnable,\n    RunnableConfig,\n    RunnableGenerator,\n    RunnableLambda,\n    chain,\n    ensure_config,\n)\nfrom langchain_core.runnables.config import (\n    get_async_callback_manager_for_config,\n)\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.runnables.schema import StreamEvent\nfrom langchain_core.runnables.utils import Input, Output\nfrom langchain_core.tools import tool\nfrom langchain_core.utils.aiter import aclosing\nfrom tests.unit_tests.runnables.test_runnable_events_v1 import (\n    _assert_events_equal_allow_superset_metadata,\n)\nfrom tests.unit_tests.stubs import _any_id_ai_message, _any_id_ai_message_chunk\n\n\ndef _with_nulled_run_id(events: Sequence[StreamEvent]) -> list[StreamEvent]:\n    \"\"\"Removes the run ids from events.\"\"\"\n    for event in events:\n        assert \"run_id\" in event, f\"Event {event} does not have a run_id.\"\n        assert \"parent_ids\" in event, f\"Event {event} does not have parent_ids.\"\n        assert isinstance(event[\"run_id\"], str), (\n            f\"Event {event} run_id is not a string.\"\n        )\n        assert isinstance(event[\"parent_ids\"], list), (\n            f\"Event {event} parent_ids is not a list.\"\n        )\n\n    return cast(\n        list[StreamEvent],\n        [{**event, \"run_id\": \"\", \"parent_ids\": []} for event in events],\n    )\n\n\nasync def _as_async_iterator(iterable: list) -> AsyncIterator:\n    \"\"\"Converts an iterable into an async iterator.\"\"\"\n    for item in iterable:\n        yield item\n\n\nasync def _collect_events(\n    events: AsyncIterator[StreamEvent], with_nulled_ids: bool = True\n) -> list[StreamEvent]:\n    \"\"\"Collect the events and remove the run ids.\"\"\"\n    materialized_events = [event async for event in events]\n\n    if with_nulled_ids:\n        events_ = _with_nulled_run_id(materialized_events)\n    else:\n        events_ = materialized_events\n    for event in events_:\n        event[\"tags\"] = sorted(event[\"tags\"])\n    return events_\n\n\nasync def test_event_stream_with_simple_function_tool() -> None:\n    \"\"\"Test the event stream with a function and tool.\"\"\"\n\n    def foo(x: int) -> dict:\n        \"\"\"Foo.\"\"\"\n        return {\"x\": 5}\n\n    @tool\n    def get_docs(x: int) -> list[Document]:\n        \"\"\"Hello Doc.\"\"\"\n        return [Document(page_content=\"hello\")]\n\n    chain = RunnableLambda(foo) | get_docs\n    events = await _collect_events(chain.astream_events({}, version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"event\": \"on_chain_start\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"name\": \"RunnableSequence\",\n                \"tags\": [],\n                \"metadata\": {},\n                \"data\": {\"input\": {}},\n            },\n            {\n                \"event\": \"on_chain_start\",\n                \"name\": \"foo\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n                \"metadata\": {},\n                \"data\": {},\n            },\n            {\n                \"event\": \"on_chain_stream\",\n                \"name\": \"foo\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n                \"metadata\": {},\n                \"data\": {\"chunk\": {\"x\": 5}},\n            },\n            {\n                \"event\": \"on_chain_end\",\n                \"name\": \"foo\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n                \"metadata\": {},\n                \"data\": {\"input\": {}, \"output\": {\"x\": 5}},\n            },\n            {\n                \"event\": \"on_tool_start\",\n                \"name\": \"get_docs\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n                \"metadata\": {},\n                \"data\": {\"input\": {\"x\": 5}},\n            },\n            {\n                \"event\": \"on_tool_end\",\n                \"name\": \"get_docs\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n                \"metadata\": {},\n                \"data\": {\"input\": {\"x\": 5}, \"output\": [Document(page_content=\"hello\")]},\n            },\n            {\n                \"event\": \"on_chain_stream\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"data\": {\"chunk\": [Document(page_content=\"hello\")]},\n            },\n            {\n                \"event\": \"on_chain_end\",\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n                \"metadata\": {},\n                \"data\": {\"output\": [Document(page_content=\"hello\")]},\n            },\n        ],\n    )\n\n\nasync def test_event_stream_with_single_lambda() -> None:\n    \"\"\"Test the event stream with a tool.\"\"\"\n\n    def reverse(s: str) -> str:\n        \"\"\"Reverse a string.\"\"\"\n        return s[::-1]\n\n    chain = RunnableLambda(func=reverse)\n\n    events = await _collect_events(chain.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"reverse\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"olleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"reverse\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": \"olleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"reverse\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_event_stream_with_triple_lambda() -> None:\n    def reverse(s: str) -> str:\n        \"\"\"Reverse a string.\"\"\"\n        return s[::-1]\n\n    r = RunnableLambda(func=reverse)\n\n    chain = (\n        r.with_config({\"run_name\": \"1\"})\n        | r.with_config({\"run_name\": \"2\"})\n        | r.with_config({\"run_name\": \"3\"})\n    )\n    events = await _collect_events(chain.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"1\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"chunk\": \"olleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"1\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"2\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"input\": \"hello\", \"output\": \"olleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"1\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"chunk\": \"hello\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"2\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"3\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:3\"],\n            },\n            {\n                \"data\": {\"input\": \"olleh\", \"output\": \"hello\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"2\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"olleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"3\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:3\"],\n            },\n            {\n                \"data\": {\"chunk\": \"olleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": \"hello\", \"output\": \"olleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"3\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:3\"],\n            },\n            {\n                \"data\": {\"output\": \"olleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_event_stream_exception() -> None:\n    def step(name: str, err: Optional[str], val: str) -> str:\n        if err:\n            raise ValueError(err)\n        return val + name[-1]\n\n    chain = (\n        RunnableLambda(partial(step, \"step1\", None))\n        | RunnableLambda(partial(step, \"step2\", \"ERR\"))\n        | RunnableLambda(partial(step, \"step3\", None))\n    )\n\n    with pytest.raises(ValueError, match=\"ERR\"):\n        await _collect_events(chain.astream_events(\"X\", version=\"v2\"))\n\n\nasync def test_event_stream_with_triple_lambda_test_filtering() -> None:\n    \"\"\"Test filtering based on tags / names.\"\"\"\n\n    def reverse(s: str) -> str:\n        \"\"\"Reverse a string.\"\"\"\n        return s[::-1]\n\n    r = RunnableLambda(func=reverse)\n\n    chain = (\n        r.with_config({\"run_name\": \"1\"})\n        | r.with_config({\"run_name\": \"2\", \"tags\": [\"my_tag\"]})\n        | r.with_config({\"run_name\": \"3\", \"tags\": [\"my_tag\"]})\n    )\n    events = await _collect_events(\n        chain.astream_events(\"hello\", include_names=[\"1\"], version=\"v2\")\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"1\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"chunk\": \"olleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"1\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"output\": \"olleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"1\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n        ],\n    )\n\n    events = await _collect_events(\n        chain.astream_events(\n            \"hello\", include_tags=[\"my_tag\"], exclude_names=[\"2\"], version=\"v2\"\n        )\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"3\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_tag\", \"seq:step:3\"],\n            },\n            {\n                \"data\": {\"chunk\": \"olleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"3\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_tag\", \"seq:step:3\"],\n            },\n            {\n                \"data\": {\"output\": \"olleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"3\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_tag\", \"seq:step:3\"],\n            },\n        ],\n    )\n\n\nasync def test_event_stream_with_lambdas_from_lambda() -> None:\n    as_lambdas = RunnableLambda(lambda x: {\"answer\": \"goodbye\"}).with_config(\n        {\"run_name\": \"my_lambda\"}\n    )\n    events = await _collect_events(\n        as_lambdas.astream_events({\"question\": \"hello\"}, version=\"v2\")\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {\"question\": \"hello\"}},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"my_lambda\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": {\"answer\": \"goodbye\"}},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"my_lambda\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": {\"answer\": \"goodbye\"}},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"my_lambda\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_astream_events_from_model() -> None:\n    \"\"\"Test the output of a model.\"\"\"\n    infinite_cycle = cycle([AIMessage(content=\"hello world!\")])\n    # When streaming GenericFakeChatModel breaks AIMessage into chunks based on spaces\n    model = (\n        GenericFakeChatModel(messages=infinite_cycle)\n        .with_config(\n            {\n                \"metadata\": {\"a\": \"b\"},\n                \"tags\": [\"my_model\"],\n                \"run_name\": \"my_model\",\n            }\n        )\n        .bind(stop=\"<stop_token>\")\n    )\n    events = await _collect_events(model.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chat_model_start\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\"hello\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\" \")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\"world!\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\n                    \"output\": _any_id_ai_message_chunk(content=\"hello world!\"),\n                },\n                \"event\": \"on_chat_model_end\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n        ],\n    )\n\n\nasync def test_astream_with_model_in_chain() -> None:\n    \"\"\"Scenarios with model when it is not the only runnable in the chain.\"\"\"\n    infinite_cycle = cycle([AIMessage(content=\"hello world!\")])\n    # When streaming GenericFakeChatModel breaks AIMessage into chunks based on spaces\n    model = (\n        GenericFakeChatModel(messages=infinite_cycle)\n        .with_config(\n            {\n                \"metadata\": {\"a\": \"b\"},\n                \"tags\": [\"my_model\"],\n                \"run_name\": \"my_model\",\n            }\n        )\n        .bind(stop=\"<stop_token>\")\n    )\n\n    @RunnableLambda\n    def i_dont_stream(input: Any, config: RunnableConfig) -> Any:\n        if sys.version_info >= (3, 11):\n            return model.invoke(input)\n        else:\n            return model.invoke(input, config)\n\n    events = await _collect_events(i_dont_stream.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"i_dont_stream\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": {\"messages\": [[HumanMessage(content=\"hello\")]]}},\n                \"event\": \"on_chat_model_start\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\"hello\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\" \")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\"world!\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\"messages\": [[HumanMessage(content=\"hello\")]]},\n                    \"output\": _any_id_ai_message(content=\"hello world!\"),\n                },\n                \"event\": \"on_chat_model_end\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message(content=\"hello world!\")},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"i_dont_stream\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": _any_id_ai_message(content=\"hello world!\")},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"i_dont_stream\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n    @RunnableLambda\n    async def ai_dont_stream(input: Any, config: RunnableConfig) -> Any:\n        if sys.version_info >= (3, 11):\n            return await model.ainvoke(input)\n        else:\n            return await model.ainvoke(input, config)\n\n    events = await _collect_events(ai_dont_stream.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"ai_dont_stream\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": {\"messages\": [[HumanMessage(content=\"hello\")]]}},\n                \"event\": \"on_chat_model_start\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\"hello\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\" \")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message_chunk(content=\"world!\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\"messages\": [[HumanMessage(content=\"hello\")]]},\n                    \"output\": _any_id_ai_message(content=\"hello world!\"),\n                },\n                \"event\": \"on_chat_model_end\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_model\"],\n            },\n            {\n                \"data\": {\"chunk\": _any_id_ai_message(content=\"hello world!\")},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"ai_dont_stream\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": _any_id_ai_message(content=\"hello world!\")},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"ai_dont_stream\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_event_stream_with_simple_chain() -> None:\n    \"\"\"Test as event stream.\"\"\"\n    template = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", \"You are Cat Agent 007\"),\n            (\"human\", \"{question}\"),\n        ]\n    ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n\n    infinite_cycle = cycle(\n        [\n            AIMessage(content=\"hello world!\", id=\"ai1\"),\n            AIMessage(content=\"goodbye world!\", id=\"ai2\"),\n        ]\n    )\n    # When streaming GenericFakeChatModel breaks AIMessage into chunks based on spaces\n    model = (\n        GenericFakeChatModel(messages=infinite_cycle)\n        .with_config(\n            {\n                \"metadata\": {\"a\": \"b\"},\n                \"tags\": [\"my_model\"],\n                \"run_name\": \"my_model\",\n            }\n        )\n        .bind(stop=\"<stop_token>\")\n    )\n\n    chain = (template | model).with_config(\n        {\n            \"metadata\": {\"foo\": \"bar\"},\n            \"tags\": [\"my_chain\"],\n            \"run_name\": \"my_chain\",\n        }\n    )\n\n    events = await _collect_events(\n        chain.astream_events({\"question\": \"hello\"}, version=\"v2\")\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {\"question\": \"hello\"}},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {\"foo\": \"bar\"},\n                \"name\": \"my_chain\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\"],\n            },\n            {\n                \"data\": {\"input\": {\"question\": \"hello\"}},\n                \"event\": \"on_prompt_start\",\n                \"metadata\": {\"foo\": \"bar\"},\n                \"name\": \"my_template\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\", \"my_template\", \"seq:step:1\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\"question\": \"hello\"},\n                    \"output\": ChatPromptValue(\n                        messages=[\n                            SystemMessage(content=\"You are Cat Agent 007\"),\n                            HumanMessage(content=\"hello\"),\n                        ]\n                    ),\n                },\n                \"event\": \"on_prompt_end\",\n                \"metadata\": {\"foo\": \"bar\"},\n                \"name\": \"my_template\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\", \"my_template\", \"seq:step:1\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\n                        \"messages\": [\n                            [\n                                SystemMessage(content=\"You are Cat Agent 007\"),\n                                HumanMessage(content=\"hello\"),\n                            ]\n                        ]\n                    }\n                },\n                \"event\": \"on_chat_model_start\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"foo\": \"bar\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\", \"my_model\", \"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\"hello\", id=\"ai1\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"foo\": \"bar\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\", \"my_model\", \"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\"hello\", id=\"ai1\")},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {\"foo\": \"bar\"},\n                \"name\": \"my_chain\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\"],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\" \", id=\"ai1\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"foo\": \"bar\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\", \"my_model\", \"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\" \", id=\"ai1\")},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {\"foo\": \"bar\"},\n                \"name\": \"my_chain\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\"],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\"world!\", id=\"ai1\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"foo\": \"bar\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\", \"my_model\", \"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\"world!\", id=\"ai1\")},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {\"foo\": \"bar\"},\n                \"name\": \"my_chain\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\n                        \"messages\": [\n                            [\n                                SystemMessage(content=\"You are Cat Agent 007\"),\n                                HumanMessage(content=\"hello\"),\n                            ]\n                        ]\n                    },\n                    \"output\": AIMessageChunk(content=\"hello world!\", id=\"ai1\"),\n                },\n                \"event\": \"on_chat_model_end\",\n                \"metadata\": {\n                    \"a\": \"b\",\n                    \"foo\": \"bar\",\n                    \"ls_model_type\": \"chat\",\n                    \"ls_stop\": \"<stop_token>\",\n                },\n                \"name\": \"my_model\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\", \"my_model\", \"seq:step:2\"],\n            },\n            {\n                \"data\": {\"output\": AIMessageChunk(content=\"hello world!\", id=\"ai1\")},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {\"foo\": \"bar\"},\n                \"name\": \"my_chain\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_chain\"],\n            },\n        ],\n    )\n\n\nasync def test_event_streaming_with_tools() -> None:\n    \"\"\"Test streaming events with different tool definitions.\"\"\"\n\n    @tool\n    def parameterless() -> str:\n        \"\"\"A tool that does nothing.\"\"\"\n        return \"hello\"\n\n    @tool\n    def with_callbacks(callbacks: Callbacks) -> str:\n        \"\"\"A tool that does nothing.\"\"\"\n        return \"world\"\n\n    @tool\n    def with_parameters(x: int, y: str) -> dict:\n        \"\"\"A tool that does nothing.\"\"\"\n        return {\"x\": x, \"y\": y}\n\n    @tool\n    def with_parameters_and_callbacks(x: int, y: str, callbacks: Callbacks) -> dict:\n        \"\"\"A tool that does nothing.\"\"\"\n        return {\"x\": x, \"y\": y}\n\n    # type ignores below because the tools don't appear to be runnables to type checkers\n    # we can remove as soon as that's fixed\n    events = await _collect_events(parameterless.astream_events({}, version=\"v2\"))  # type: ignore\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {}},\n                \"event\": \"on_tool_start\",\n                \"metadata\": {},\n                \"name\": \"parameterless\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": \"hello\"},\n                \"event\": \"on_tool_end\",\n                \"metadata\": {},\n                \"name\": \"parameterless\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n    events = await _collect_events(with_callbacks.astream_events({}, version=\"v2\"))  # type: ignore\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {}},\n                \"event\": \"on_tool_start\",\n                \"metadata\": {},\n                \"name\": \"with_callbacks\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": \"world\"},\n                \"event\": \"on_tool_end\",\n                \"metadata\": {},\n                \"name\": \"with_callbacks\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n    events = await _collect_events(\n        with_parameters.astream_events({\"x\": 1, \"y\": \"2\"}, version=\"v2\")  # type: ignore\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {\"x\": 1, \"y\": \"2\"}},\n                \"event\": \"on_tool_start\",\n                \"metadata\": {},\n                \"name\": \"with_parameters\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": {\"x\": 1, \"y\": \"2\"}},\n                \"event\": \"on_tool_end\",\n                \"metadata\": {},\n                \"name\": \"with_parameters\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n    events = await _collect_events(\n        with_parameters_and_callbacks.astream_events({\"x\": 1, \"y\": \"2\"}, version=\"v2\")  # type: ignore\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {\"x\": 1, \"y\": \"2\"}},\n                \"event\": \"on_tool_start\",\n                \"metadata\": {},\n                \"name\": \"with_parameters_and_callbacks\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": {\"x\": 1, \"y\": \"2\"}},\n                \"event\": \"on_tool_end\",\n                \"metadata\": {},\n                \"name\": \"with_parameters_and_callbacks\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nclass HardCodedRetriever(BaseRetriever):\n    documents: list[Document]\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> list[Document]:\n        return self.documents\n\n\nasync def test_event_stream_with_retriever() -> None:\n    \"\"\"Test the event stream with a retriever.\"\"\"\n    retriever = HardCodedRetriever(\n        documents=[\n            Document(\n                page_content=\"hello world!\",\n                metadata={\"foo\": \"bar\"},\n            ),\n            Document(\n                page_content=\"goodbye world!\",\n                metadata={\"food\": \"spare\"},\n            ),\n        ]\n    )\n    events = await _collect_events(\n        retriever.astream_events({\"query\": \"hello\"}, version=\"v2\")\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\n                    \"input\": {\"query\": \"hello\"},\n                },\n                \"event\": \"on_retriever_start\",\n                \"metadata\": {},\n                \"name\": \"HardCodedRetriever\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\n                    \"output\": [\n                        Document(page_content=\"hello world!\", metadata={\"foo\": \"bar\"}),\n                        Document(\n                            page_content=\"goodbye world!\", metadata={\"food\": \"spare\"}\n                        ),\n                    ]\n                },\n                \"event\": \"on_retriever_end\",\n                \"metadata\": {},\n                \"name\": \"HardCodedRetriever\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_event_stream_with_retriever_and_formatter() -> None:\n    \"\"\"Test the event stream with a retriever.\"\"\"\n    retriever = HardCodedRetriever(\n        documents=[\n            Document(\n                page_content=\"hello world!\",\n                metadata={\"foo\": \"bar\"},\n            ),\n            Document(\n                page_content=\"goodbye world!\",\n                metadata={\"food\": \"spare\"},\n            ),\n        ]\n    )\n\n    def format_docs(docs: list[Document]) -> str:\n        \"\"\"Format the docs.\"\"\"\n        return \", \".join([doc.page_content for doc in docs])\n\n    chain = retriever | format_docs\n    events = await _collect_events(chain.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": {\"query\": \"hello\"}},\n                \"event\": \"on_retriever_start\",\n                \"metadata\": {},\n                \"name\": \"HardCodedRetriever\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\"query\": \"hello\"},\n                    \"output\": [\n                        Document(page_content=\"hello world!\", metadata={\"foo\": \"bar\"}),\n                        Document(\n                            page_content=\"goodbye world!\", metadata={\"food\": \"spare\"}\n                        ),\n                    ],\n                },\n                \"event\": \"on_retriever_end\",\n                \"metadata\": {},\n                \"name\": \"HardCodedRetriever\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"format_docs\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"hello world!, goodbye world!\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"format_docs\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"hello world!, goodbye world!\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\n                    \"input\": [\n                        Document(page_content=\"hello world!\", metadata={\"foo\": \"bar\"}),\n                        Document(\n                            page_content=\"goodbye world!\", metadata={\"food\": \"spare\"}\n                        ),\n                    ],\n                    \"output\": \"hello world!, goodbye world!\",\n                },\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"format_docs\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"output\": \"hello world!, goodbye world!\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_event_stream_on_chain_with_tool() -> None:\n    \"\"\"Test the event stream with a tool.\"\"\"\n\n    @tool\n    def concat(a: str, b: str) -> str:\n        \"\"\"A tool that does nothing.\"\"\"\n        return a + b\n\n    def reverse(s: str) -> str:\n        \"\"\"Reverse a string.\"\"\"\n        return s[::-1]\n\n    # For whatever reason type annotations fail here because reverse\n    # does not appear to be a runnable\n    chain = concat | reverse  # type: ignore\n\n    events = await _collect_events(\n        chain.astream_events({\"a\": \"hello\", \"b\": \"world\"}, version=\"v2\")\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {\"a\": \"hello\", \"b\": \"world\"}},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": {\"a\": \"hello\", \"b\": \"world\"}},\n                \"event\": \"on_tool_start\",\n                \"metadata\": {},\n                \"name\": \"concat\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"input\": {\"a\": \"hello\", \"b\": \"world\"}, \"output\": \"helloworld\"},\n                \"event\": \"on_tool_end\",\n                \"metadata\": {},\n                \"name\": \"concat\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"reverse\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"dlrowolleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"reverse\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"dlrowolleh\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": \"helloworld\", \"output\": \"dlrowolleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"reverse\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"output\": \"dlrowolleh\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\n@pytest.mark.xfail(reason=\"Fix order of callback invocations in RunnableSequence\")\nasync def test_chain_ordering() -> None:\n    \"\"\"Test the event stream with a tool.\"\"\"\n\n    def foo(a: str) -> str:\n        return a\n\n    def bar(a: str) -> str:\n        return a\n\n    chain = RunnableLambda(foo) | RunnableLambda(bar)\n    iterable = chain.astream_events(\"q\", version=\"v2\")\n\n    events = []\n\n    for _ in range(10):\n        try:\n            next_chunk = await iterable.__anext__()\n            events.append(next_chunk)\n        except Exception:\n            break\n\n    events = _with_nulled_run_id(events)\n    for event in events:\n        event[\"tags\"] = sorted(event[\"tags\"])\n\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"q\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"chunk\": \"q\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"input\": \"q\", \"output\": \"q\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"bar\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"q\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"bar\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"q\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": \"q\", \"output\": \"q\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"bar\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"output\": \"q\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_event_stream_with_retry() -> None:\n    \"\"\"Test the event stream with a tool.\"\"\"\n\n    def success(inputs: str) -> str:\n        return \"success\"\n\n    def fail(inputs: str) -> None:\n        \"\"\"Simple func.\"\"\"\n        msg = \"fail\"\n        raise ValueError(msg)\n\n    chain = RunnableLambda(success) | RunnableLambda(fail).with_retry(\n        stop_after_attempt=1,\n    )\n    iterable = chain.astream_events(\"q\", version=\"v2\")\n\n    events = []\n\n    for _ in range(10):\n        try:\n            next_chunk = await iterable.__anext__()\n            events.append(next_chunk)\n        except Exception:\n            break\n\n    events = _with_nulled_run_id(events)\n    for event in events:\n        event[\"tags\"] = sorted(event[\"tags\"])\n\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"q\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"success\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {\"chunk\": \"success\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"success\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n            {\n                \"data\": {},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"fail\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"input\": \"q\", \"output\": \"success\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"success\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:1\"],\n            },\n        ],\n    )\n\n\nasync def test_with_llm() -> None:\n    \"\"\"Test with regular llm.\"\"\"\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", \"You are Cat Agent 007\"),\n            (\"human\", \"{question}\"),\n        ]\n    ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n    llm = FakeStreamingListLLM(responses=[\"abc\"])\n\n    chain = prompt | llm\n    events = await _collect_events(\n        chain.astream_events({\"question\": \"hello\"}, version=\"v2\")\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {\"question\": \"hello\"}},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": {\"question\": \"hello\"}},\n                \"event\": \"on_prompt_start\",\n                \"metadata\": {},\n                \"name\": \"my_template\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_template\", \"seq:step:1\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\"question\": \"hello\"},\n                    \"output\": ChatPromptValue(\n                        messages=[\n                            SystemMessage(content=\"You are Cat Agent 007\"),\n                            HumanMessage(content=\"hello\"),\n                        ]\n                    ),\n                },\n                \"event\": \"on_prompt_end\",\n                \"metadata\": {},\n                \"name\": \"my_template\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"my_template\", \"seq:step:1\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\n                        \"prompts\": [\"System: You are Cat Agent 007\\nHuman: hello\"]\n                    }\n                },\n                \"event\": \"on_llm_start\",\n                \"metadata\": {},\n                \"name\": \"FakeStreamingListLLM\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\n                    \"input\": {\n                        \"prompts\": [\"System: You are Cat Agent 007\\nHuman: hello\"]\n                    },\n                    \"output\": {\n                        \"generations\": [\n                            [\n                                {\n                                    \"generation_info\": None,\n                                    \"text\": \"abc\",\n                                    \"type\": \"Generation\",\n                                }\n                            ]\n                        ],\n                        \"llm_output\": None,\n                    },\n                },\n                \"event\": \"on_llm_end\",\n                \"metadata\": {},\n                \"name\": \"FakeStreamingListLLM\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [\"seq:step:2\"],\n            },\n            {\n                \"data\": {\"chunk\": \"a\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"b\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"c\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": \"abc\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"RunnableSequence\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_runnable_each() -> None:\n    \"\"\"Test runnable each astream_events.\"\"\"\n\n    async def add_one(x: int) -> int:\n        return x + 1\n\n    add_one_map = RunnableLambda(add_one).map()  # type: ignore\n    assert await add_one_map.ainvoke([1, 2, 3]) == [2, 3, 4]\n\n    with pytest.raises(NotImplementedError):\n        async for _ in add_one_map.astream_events([1, 2, 3], version=\"v2\"):\n            pass\n\n\nasync def test_events_astream_config() -> None:\n    \"\"\"Test that astream events support accepting config.\"\"\"\n    infinite_cycle = cycle([AIMessage(content=\"hello world!\", id=\"ai1\")])\n    good_world_on_repeat = cycle([AIMessage(content=\"Goodbye world\", id=\"ai2\")])\n    model = GenericFakeChatModel(messages=infinite_cycle).configurable_fields(\n        messages=ConfigurableField(\n            id=\"messages\",\n            name=\"Messages\",\n            description=\"Messages return by the LLM\",\n        )\n    )\n\n    model_02 = model.with_config({\"configurable\": {\"messages\": good_world_on_repeat}})\n    assert model_02.invoke(\"hello\") == AIMessage(content=\"Goodbye world\", id=\"ai2\")\n\n    events = await _collect_events(model_02.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chat_model_start\",\n                \"metadata\": {\"ls_model_type\": \"chat\"},\n                \"name\": \"GenericFakeChatModel\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\"Goodbye\", id=\"ai2\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\"ls_model_type\": \"chat\"},\n                \"name\": \"GenericFakeChatModel\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\" \", id=\"ai2\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\"ls_model_type\": \"chat\"},\n                \"name\": \"GenericFakeChatModel\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": AIMessageChunk(content=\"world\", id=\"ai2\")},\n                \"event\": \"on_chat_model_stream\",\n                \"metadata\": {\"ls_model_type\": \"chat\"},\n                \"name\": \"GenericFakeChatModel\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\n                    \"output\": AIMessageChunk(content=\"Goodbye world\", id=\"ai2\"),\n                },\n                \"event\": \"on_chat_model_end\",\n                \"metadata\": {\"ls_model_type\": \"chat\"},\n                \"name\": \"GenericFakeChatModel\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_runnable_with_message_history() -> None:\n    class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n        \"\"\"In memory implementation of chat message history.\"\"\"\n\n        # Attention: for the tests use an Any type to work-around a pydantic issue\n        # where it re-instantiates a list, so mutating the list doesn't end up mutating\n        # the content in the store!\n\n        # Using Any type here rather than List[BaseMessage] due to pydantic issue!\n        messages: Any\n\n        def add_message(self, message: BaseMessage) -> None:\n            \"\"\"Add a self-created message to the store.\"\"\"\n            self.messages.append(message)\n\n        def clear(self) -> None:\n            self.messages = []\n\n    # Here we use a global variable to store the chat message history.\n    # This will make it easier to inspect it to see the underlying results.\n    store: dict = {}\n\n    def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n        \"\"\"Get a chat message history.\"\"\"\n        if session_id not in store:\n            store[session_id] = []\n        return InMemoryHistory(messages=store[session_id])\n\n    infinite_cycle = cycle(\n        [\n            AIMessage(content=\"hello\", id=\"ai3\"),\n            AIMessage(content=\"world\", id=\"ai4\"),\n        ]\n    )\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", \"You are a cat\"),\n            MessagesPlaceholder(variable_name=\"history\"),\n            (\"human\", \"{question}\"),\n        ]\n    )\n    model = GenericFakeChatModel(messages=infinite_cycle)\n\n    chain: Runnable = prompt | model\n    with_message_history = RunnableWithMessageHistory(\n        chain,\n        get_session_history=get_by_session_id,\n        input_messages_key=\"question\",\n        history_messages_key=\"history\",\n    )\n\n    # patch with_message_history._get_output_messages to listen for errors\n    # so we can raise them in this main thread\n    raised_errors = []\n\n    def collect_errors(fn):  # type: ignore\n        nonlocal raised_errors\n\n        def _get_output_messages(*args, **kwargs):  # type: ignore\n            try:\n                return fn(*args, **kwargs)\n            except Exception as e:\n                raised_errors.append(e)\n                raise\n\n        return _get_output_messages\n\n    old_ref = with_message_history._get_output_messages\n    with_message_history.__dict__[\"_get_output_messages\"] = collect_errors(old_ref)\n    await with_message_history.with_config(\n        {\"configurable\": {\"session_id\": \"session-123\"}}\n    ).ainvoke({\"question\": \"hello\"})\n\n    assert store == {\n        \"session-123\": [\n            HumanMessage(content=\"hello\"),\n            AIMessage(content=\"hello\", id=\"ai3\"),\n        ]\n    }\n\n    await asyncio.to_thread(\n        with_message_history.with_config(\n            {\"configurable\": {\"session_id\": \"session-123\"}}\n        ).invoke,\n        {\"question\": \"meow\"},\n    )\n    assert store == {\n        \"session-123\": [\n            HumanMessage(content=\"hello\"),\n            AIMessage(content=\"hello\", id=\"ai3\"),\n            HumanMessage(content=\"meow\"),\n            AIMessage(content=\"world\", id=\"ai4\"),\n        ]\n    }\n    assert not raised_errors\n\n\nEXPECTED_EVENTS = [\n    {\n        \"data\": {\"input\": 1},\n        \"event\": \"on_chain_start\",\n        \"metadata\": {},\n        \"name\": \"add_one_proxy\",\n        \"run_id\": \"\",\n        \"parent_ids\": [],\n        \"tags\": [],\n    },\n    {\n        \"data\": {},\n        \"event\": \"on_chain_start\",\n        \"metadata\": {},\n        \"name\": \"add_one\",\n        \"run_id\": \"\",\n        \"parent_ids\": [],\n        \"tags\": [],\n    },\n    {\n        \"data\": {\"chunk\": 2},\n        \"event\": \"on_chain_stream\",\n        \"metadata\": {},\n        \"name\": \"add_one\",\n        \"run_id\": \"\",\n        \"parent_ids\": [],\n        \"tags\": [],\n    },\n    {\n        \"data\": {\"input\": 1, \"output\": 2},\n        \"event\": \"on_chain_end\",\n        \"metadata\": {},\n        \"name\": \"add_one\",\n        \"run_id\": \"\",\n        \"parent_ids\": [],\n        \"tags\": [],\n    },\n    {\n        \"data\": {\"chunk\": 2},\n        \"event\": \"on_chain_stream\",\n        \"metadata\": {},\n        \"name\": \"add_one_proxy\",\n        \"run_id\": \"\",\n        \"parent_ids\": [],\n        \"tags\": [],\n    },\n    {\n        \"data\": {\"output\": 2},\n        \"event\": \"on_chain_end\",\n        \"metadata\": {},\n        \"name\": \"add_one_proxy\",\n        \"run_id\": \"\",\n        \"parent_ids\": [],\n        \"tags\": [],\n    },\n]\n\n\nasync def test_sync_in_async_stream_lambdas(blockbuster: BlockBuster) -> None:\n    \"\"\"Test invoking nested runnable lambda.\"\"\"\n    blockbuster.deactivate()\n\n    def add_one(x: int) -> int:\n        return x + 1\n\n    add_one_ = RunnableLambda(add_one)\n\n    async def add_one_proxy(x: int, config: RunnableConfig) -> int:\n        streaming = add_one_.stream(x, config)\n        results = list(streaming)\n        return results[0]\n\n    add_one_proxy_ = RunnableLambda(add_one_proxy)  # type: ignore\n\n    events = await _collect_events(add_one_proxy_.astream_events(1, version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(events, EXPECTED_EVENTS)\n\n\nasync def test_async_in_async_stream_lambdas() -> None:\n    \"\"\"Test invoking nested runnable lambda.\"\"\"\n\n    async def add_one(x: int) -> int:\n        return x + 1\n\n    add_one_ = RunnableLambda(add_one)  # type: ignore\n\n    async def add_one_proxy(x: int, config: RunnableConfig) -> int:\n        # Use sync streaming\n        streaming = add_one_.astream(x, config)\n        results = [result async for result in streaming]\n        return results[0]\n\n    add_one_proxy_ = RunnableLambda(add_one_proxy)  # type: ignore\n\n    events = await _collect_events(add_one_proxy_.astream_events(1, version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(events, EXPECTED_EVENTS)\n\n\nasync def test_sync_in_sync_lambdas() -> None:\n    \"\"\"Test invoking nested runnable lambda.\"\"\"\n\n    def add_one(x: int) -> int:\n        return x + 1\n\n    add_one_ = RunnableLambda(add_one)\n\n    def add_one_proxy(x: int, config: RunnableConfig) -> int:\n        # Use sync streaming\n        streaming = add_one_.stream(x, config)\n        results = list(streaming)\n        return results[0]\n\n    add_one_proxy_ = RunnableLambda(add_one_proxy)\n\n    events = await _collect_events(add_one_proxy_.astream_events(1, version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(events, EXPECTED_EVENTS)\n\n\nclass StreamingRunnable(Runnable[Input, Output]):\n    \"\"\"A custom runnable used for testing purposes.\"\"\"\n\n    iterable: Iterable[Any]\n\n    def __init__(self, iterable: Iterable[Any]) -> None:\n        \"\"\"Initialize the runnable.\"\"\"\n        self.iterable = iterable\n\n    def invoke(\n        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> Output:\n        \"\"\"Invoke the runnable.\"\"\"\n        msg = \"Server side error\"\n        raise ValueError(msg)\n\n    def stream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> Iterator[Output]:\n        raise NotImplementedError\n\n    async def astream(\n        self,\n        input: Input,\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Optional[Any],\n    ) -> AsyncIterator[Output]:\n        config = ensure_config(config)\n        callback_manager = get_async_callback_manager_for_config(config)\n        run_manager = await callback_manager.on_chain_start(\n            None,\n            input,\n            name=config.get(\"run_name\", self.get_name()),\n            run_id=config.get(\"run_id\"),\n        )\n\n        try:\n            final_output = None\n            for element in self.iterable:\n                if isinstance(element, BaseException):\n                    raise element  # noqa: TRY301\n                yield element\n\n                if final_output is None:\n                    final_output = element\n                else:\n                    try:\n                        final_output = final_output + element\n                    except TypeError:\n                        final_output = element\n\n            # set final channel values as run output\n            await run_manager.on_chain_end(final_output)\n        except BaseException as e:\n            await run_manager.on_chain_error(e)\n            raise\n\n\nasync def test_astream_events_from_custom_runnable() -> None:\n    \"\"\"Test astream events from a custom runnable.\"\"\"\n    iterator = [\"1\", \"2\", \"3\"]\n    runnable: Runnable[int, str] = StreamingRunnable(iterator)\n    chunks = [chunk async for chunk in runnable.astream(1, version=\"v2\")]\n    assert chunks == [\"1\", \"2\", \"3\"]\n    events = await _collect_events(runnable.astream_events(1, version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": 1},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"StreamingRunnable\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"1\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"StreamingRunnable\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"2\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"StreamingRunnable\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"3\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"StreamingRunnable\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": \"123\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"StreamingRunnable\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_parent_run_id_assignment() -> None:\n    \"\"\"Test assignment of parent run id.\"\"\"\n\n    # Type ignores in the code below need to be investigated.\n    # Looks like a typing issue when using RunnableLambda as a decorator\n    # with async functions.\n    @RunnableLambda  # type: ignore\n    async def grandchild(x: str) -> str:\n        return x\n\n    @RunnableLambda  # type: ignore\n    async def child(x: str, config: RunnableConfig) -> str:\n        config[\"run_id\"] = uuid.UUID(int=9)\n        return await grandchild.ainvoke(x, config)  # type: ignore\n\n    @RunnableLambda  # type: ignore\n    async def parent(x: str, config: RunnableConfig) -> str:\n        config[\"run_id\"] = uuid.UUID(int=8)\n        return await child.ainvoke(x, config)  # type: ignore\n\n    bond = uuid.UUID(int=7)\n    events = await _collect_events(\n        parent.astream_events(\"hello\", {\"run_id\": bond}, version=\"v2\"),\n        with_nulled_ids=False,\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"parent\",\n                \"parent_ids\": [],\n                \"run_id\": \"00000000-0000-0000-0000-000000000007\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"child\",\n                \"parent_ids\": [\"00000000-0000-0000-0000-000000000007\"],\n                \"run_id\": \"00000000-0000-0000-0000-000000000008\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"grandchild\",\n                \"parent_ids\": [\n                    \"00000000-0000-0000-0000-000000000007\",\n                    \"00000000-0000-0000-0000-000000000008\",\n                ],\n                \"run_id\": \"00000000-0000-0000-0000-000000000009\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": \"hello\", \"output\": \"hello\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"grandchild\",\n                \"parent_ids\": [\n                    \"00000000-0000-0000-0000-000000000007\",\n                    \"00000000-0000-0000-0000-000000000008\",\n                ],\n                \"run_id\": \"00000000-0000-0000-0000-000000000009\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": \"hello\", \"output\": \"hello\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"child\",\n                \"parent_ids\": [\"00000000-0000-0000-0000-000000000007\"],\n                \"run_id\": \"00000000-0000-0000-0000-000000000008\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"hello\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"parent\",\n                \"parent_ids\": [],\n                \"run_id\": \"00000000-0000-0000-0000-000000000007\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": \"hello\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"parent\",\n                \"parent_ids\": [],\n                \"run_id\": \"00000000-0000-0000-0000-000000000007\",\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_bad_parent_ids() -> None:\n    \"\"\"Test handling of situation where a run id is duplicated in the run tree.\"\"\"\n\n    # Type ignores in the code below need to be investigated.\n    # Looks like a typing issue when using RunnableLambda as a decorator\n    # with async functions.\n    @RunnableLambda  # type: ignore\n    async def child(x: str) -> str:\n        return x\n\n    @RunnableLambda  # type: ignore\n    async def parent(x: str, config: RunnableConfig) -> str:\n        config[\"run_id\"] = uuid.UUID(int=7)\n        return await child.ainvoke(x, config)  # type: ignore\n\n    bond = uuid.UUID(int=7)\n    events = await _collect_events(\n        parent.astream_events(\"hello\", {\"run_id\": bond}, version=\"v2\"),\n        with_nulled_ids=False,\n    )\n    # Includes only a partial list of events since the run ID gets duplicated\n    # between parent and child run ID and the callback handler throws an exception.\n    # The exception does not get bubbled up to the user.\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"parent\",\n                \"parent_ids\": [],\n                \"run_id\": \"00000000-0000-0000-0000-000000000007\",\n                \"tags\": [],\n            }\n        ],\n    )\n\n\nasync def test_runnable_generator() -> None:\n    \"\"\"Test async events from sync lambda.\"\"\"\n\n    async def generator(inputs: AsyncIterator[str]) -> AsyncIterator[str]:\n        yield \"1\"\n        yield \"2\"\n\n    runnable: Runnable[str, str] = RunnableGenerator(transform=generator)\n    events = await _collect_events(runnable.astream_events(\"hello\", version=\"v2\"))\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": \"hello\"},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"generator\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"1\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"generator\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": \"2\"},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"generator\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": \"12\"},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"generator\",\n                \"run_id\": \"\",\n                \"parent_ids\": [],\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_with_explicit_config() -> None:\n    \"\"\"Test astream events with explicit callbacks being passed.\"\"\"\n    infinite_cycle = cycle([AIMessage(content=\"hello world\", id=\"ai3\")])\n    model = GenericFakeChatModel(messages=infinite_cycle)\n\n    @tool\n    async def say_hello(query: str, callbacks: Callbacks) -> BaseMessage:\n        \"\"\"Use this tool to look up which items are in the given place.\"\"\"\n\n        @RunnableLambda\n        def passthrough_to_trigger_issue(x: str) -> str:\n            \"\"\"Add passthrough to trigger issue.\"\"\"\n            return x\n\n        chain = passthrough_to_trigger_issue | model.with_config(\n            {\n                \"tags\": [\"hello\"],\n                \"callbacks\": callbacks,\n            }\n        )\n\n        return await chain.ainvoke(query)\n\n    events = await _collect_events(\n        say_hello.astream_events(\"meow\", version=\"v2\")  # type: ignore\n    )\n\n    assert [\n        event[\"data\"][\"chunk\"].content\n        for event in events\n        if event[\"event\"] == \"on_chat_model_stream\"\n    ] == [\"hello\", \" \", \"world\"]\n\n\nasync def test_break_astream_events() -> None:\n    class AwhileMaker:\n        def __init__(self) -> None:\n            self.reset()\n\n        async def __call__(self, input: Any) -> Any:\n            self.started = True\n            try:\n                await asyncio.sleep(0.5)\n            except asyncio.CancelledError:\n                self.cancelled = True\n                raise\n            return input\n\n        def reset(self) -> None:\n            self.started = False\n            self.cancelled = False\n\n    alittlewhile = AwhileMaker()\n    awhile = AwhileMaker()\n    anotherwhile = AwhileMaker()\n\n    outer_cancelled = False\n\n    @chain\n    async def sequence(input: Any) -> Any:\n        try:\n            yield await alittlewhile(input)\n            yield await awhile(input)\n            yield await anotherwhile(input)\n        except asyncio.CancelledError:\n            nonlocal outer_cancelled\n            outer_cancelled = True\n            raise\n\n    # test interrupting astream_events v2\n\n    got_event = False\n    thread2: RunnableConfig = {\"configurable\": {\"thread_id\": 2}}\n    async with aclosing(\n        sequence.astream_events({\"value\": 1}, thread2, version=\"v2\")\n    ) as stream:\n        async for chunk in stream:\n            if chunk[\"event\"] == \"on_chain_stream\":\n                got_event = True\n                assert chunk[\"data\"][\"chunk\"] == {\"value\": 1}\n                break\n\n    # did break\n    assert got_event\n    # did cancel outer chain\n    assert outer_cancelled\n\n    # node \"alittlewhile\" starts, not cancelled\n    assert alittlewhile.started is True\n    assert alittlewhile.cancelled is False\n\n    # node \"awhile\" starts but is cancelled\n    assert awhile.started is True\n    assert awhile.cancelled is True\n\n    # node \"anotherwhile\" should never start\n    assert anotherwhile.started is False\n\n\nasync def test_cancel_astream_events() -> None:\n    class AwhileMaker:\n        def __init__(self) -> None:\n            self.reset()\n\n        async def __call__(self, input: Any) -> Any:\n            self.started = True\n            try:\n                await asyncio.sleep(0.5)\n            except asyncio.CancelledError:\n                self.cancelled = True\n                raise\n            return input\n\n        def reset(self) -> None:\n            self.started = False\n            self.cancelled = False\n\n    alittlewhile = AwhileMaker()\n    awhile = AwhileMaker()\n    anotherwhile = AwhileMaker()\n\n    outer_cancelled = False\n\n    @chain\n    async def sequence(input: Any) -> Any:\n        try:\n            yield await alittlewhile(input)\n            yield await awhile(input)\n            yield await anotherwhile(input)\n        except asyncio.CancelledError:\n            nonlocal outer_cancelled\n            outer_cancelled = True\n            raise\n\n    got_event = False\n\n    async def aconsume(stream: AsyncIterator[Any]) -> None:\n        nonlocal got_event\n        # here we don't need aclosing as cancelling the task is propagated\n        # to the async generator being consumed\n        async for chunk in stream:\n            if chunk[\"event\"] == \"on_chain_stream\":\n                got_event = True\n                assert chunk[\"data\"][\"chunk\"] == {\"value\": 1}\n                task.cancel()\n\n    thread2: RunnableConfig = {\"configurable\": {\"thread_id\": 2}}\n    task = asyncio.create_task(\n        aconsume(sequence.astream_events({\"value\": 1}, thread2, version=\"v2\"))\n    )\n\n    with pytest.raises(asyncio.CancelledError):\n        await task\n\n    # did break\n    assert got_event\n    # did cancel outer chain\n    assert outer_cancelled\n\n    # node \"alittlewhile\" starts, not cancelled\n    assert alittlewhile.started is True\n    assert alittlewhile.cancelled is False\n\n    # node \"awhile\" starts but is cancelled\n    assert awhile.started is True\n    assert awhile.cancelled is True\n\n    # node \"anotherwhile\" should never start\n    assert anotherwhile.started is False\n\n\nasync def test_custom_event() -> None:\n    \"\"\"Test adhoc event.\"\"\"\n    from langchain_core.callbacks.manager import adispatch_custom_event\n\n    # Ignoring type due to RunnableLamdba being dynamic when it comes to being\n    # applied as a decorator to async functions.\n    @RunnableLambda  # type: ignore[arg-type]\n    async def foo(x: int, config: RunnableConfig) -> int:\n        \"\"\"Simple function that emits some adhoc events.\"\"\"\n        await adispatch_custom_event(\"event1\", {\"x\": x}, config=config)\n        await adispatch_custom_event(\"event2\", \"foo\", config=config)\n        return x + 1\n\n    uuid1 = uuid.UUID(int=7)\n\n    events = await _collect_events(\n        foo.astream_events(\n            1,\n            version=\"v2\",\n            config={\"run_id\": uuid1},\n        ),\n        with_nulled_ids=False,\n    )\n\n    run_id = str(uuid1)\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": 1},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"parent_ids\": [],\n                \"run_id\": run_id,\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"x\": 1},\n                \"event\": \"on_custom_event\",\n                \"metadata\": {},\n                \"name\": \"event1\",\n                \"parent_ids\": [],\n                \"run_id\": run_id,\n                \"tags\": [],\n            },\n            {\n                \"data\": \"foo\",\n                \"event\": \"on_custom_event\",\n                \"metadata\": {},\n                \"name\": \"event2\",\n                \"parent_ids\": [],\n                \"run_id\": run_id,\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": 2},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"parent_ids\": [],\n                \"run_id\": run_id,\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": 2},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"parent_ids\": [],\n                \"run_id\": run_id,\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_custom_event_nested() -> None:\n    \"\"\"Test adhoc event in a nested chain.\"\"\"\n    from langchain_core.callbacks.manager import adispatch_custom_event\n\n    # Ignoring type due to RunnableLamdba being dynamic when it comes to being\n    # applied as a decorator to async functions.\n    @RunnableLambda  # type: ignore[arg-type]\n    async def foo(x: int, config: RunnableConfig) -> int:\n        \"\"\"Simple function that emits some adhoc events.\"\"\"\n        await adispatch_custom_event(\"event1\", {\"x\": x}, config=config)\n        await adispatch_custom_event(\"event2\", \"foo\", config=config)\n        return x + 1\n\n    run_id = uuid.UUID(int=7)\n    child_run_id = uuid.UUID(int=8)\n\n    # Ignoring type due to RunnableLamdba being dynamic when it comes to being\n    # applied as a decorator to async functions.\n    @RunnableLambda  # type: ignore[arg-type]\n    async def bar(x: int, config: RunnableConfig) -> int:\n        \"\"\"Simple function that emits some adhoc events.\"\"\"\n        return await foo.ainvoke(\n            x,  # type: ignore[arg-type]\n            {\"run_id\": child_run_id, **config},\n        )\n\n    events = await _collect_events(\n        bar.astream_events(\n            1,\n            version=\"v2\",\n            config={\"run_id\": run_id},\n        ),\n        with_nulled_ids=False,\n    )\n\n    run_id = str(run_id)  # type: ignore[assignment]\n    child_run_id = str(child_run_id)  # type: ignore[assignment]\n\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": 1},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"bar\",\n                \"parent_ids\": [],\n                \"run_id\": \"00000000-0000-0000-0000-000000000007\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": 1},\n                \"event\": \"on_chain_start\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"parent_ids\": [\"00000000-0000-0000-0000-000000000007\"],\n                \"run_id\": \"00000000-0000-0000-0000-000000000008\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"x\": 1},\n                \"event\": \"on_custom_event\",\n                \"metadata\": {},\n                \"name\": \"event1\",\n                \"parent_ids\": [\"00000000-0000-0000-0000-000000000007\"],\n                \"run_id\": \"00000000-0000-0000-0000-000000000008\",\n                \"tags\": [],\n            },\n            {\n                \"data\": \"foo\",\n                \"event\": \"on_custom_event\",\n                \"metadata\": {},\n                \"name\": \"event2\",\n                \"parent_ids\": [\"00000000-0000-0000-0000-000000000007\"],\n                \"run_id\": \"00000000-0000-0000-0000-000000000008\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"input\": 1, \"output\": 2},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"parent_ids\": [\"00000000-0000-0000-0000-000000000007\"],\n                \"run_id\": \"00000000-0000-0000-0000-000000000008\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"chunk\": 2},\n                \"event\": \"on_chain_stream\",\n                \"metadata\": {},\n                \"name\": \"bar\",\n                \"parent_ids\": [],\n                \"run_id\": \"00000000-0000-0000-0000-000000000007\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": 2},\n                \"event\": \"on_chain_end\",\n                \"metadata\": {},\n                \"name\": \"bar\",\n                \"parent_ids\": [],\n                \"run_id\": \"00000000-0000-0000-0000-000000000007\",\n                \"tags\": [],\n            },\n        ],\n    )\n\n\nasync def test_custom_event_root_dispatch() -> None:\n    \"\"\"Test adhoc event in a nested chain.\"\"\"\n    # This just tests that nothing breaks on the path.\n    # It shouldn't do anything at the moment, since the tracer isn't configured\n    # to handle adhoc events.\n    from langchain_core.callbacks.manager import adispatch_custom_event\n\n    # Expected behavior is that the event cannot be dispatched\n    with pytest.raises(RuntimeError):\n        await adispatch_custom_event(\"event1\", {\"x\": 1})\n\n\nIS_GTE_3_11 = sys.version_info >= (3, 11)\n\n\n# Test relies on automatically picking up RunnableConfig from contextvars\n@pytest.mark.skipif(not IS_GTE_3_11, reason=\"Requires Python >=3.11\")\nasync def test_custom_event_root_dispatch_with_in_tool() -> None:\n    \"\"\"Test adhoc event in a nested chain.\"\"\"\n    from langchain_core.callbacks.manager import adispatch_custom_event\n    from langchain_core.tools import tool\n\n    @tool\n    async def foo(x: int) -> int:\n        \"\"\"Foo.\"\"\"\n        await adispatch_custom_event(\"event1\", {\"x\": x})\n        return x + 1\n\n    # Ignoring type due to @tool not returning correct type annotations\n    events = await _collect_events(\n        foo.astream_events({\"x\": 2}, version=\"v2\")  # type: ignore[attr-defined]\n    )\n    _assert_events_equal_allow_superset_metadata(\n        events,\n        [\n            {\n                \"data\": {\"input\": {\"x\": 2}},\n                \"event\": \"on_tool_start\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"parent_ids\": [],\n                \"run_id\": \"\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"x\": 2},\n                \"event\": \"on_custom_event\",\n                \"metadata\": {},\n                \"name\": \"event1\",\n                \"parent_ids\": [],\n                \"run_id\": \"\",\n                \"tags\": [],\n            },\n            {\n                \"data\": {\"output\": 3},\n                \"event\": \"on_tool_end\",\n                \"metadata\": {},\n                \"name\": \"foo\",\n                \"parent_ids\": [],\n                \"run_id\": \"\",\n                \"tags\": [],\n            },\n        ],\n    )\n",
        "patch": "@@ -8,6 +8,7 @@\n from itertools import cycle\n from typing import (\n     Any,\n+    Callable,\n     Optional,\n     cast,\n )\n@@ -1901,10 +1902,10 @@ def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n     # so we can raise them in this main thread\n     raised_errors = []\n \n-    def collect_errors(fn):  # type: ignore\n+    def collect_errors(fn: Callable[..., Any]) -> Callable[..., Any]:\n         nonlocal raised_errors\n \n-        def _get_output_messages(*args, **kwargs):  # type: ignore\n+        def _get_output_messages(*args: Any, **kwargs: Any) -> Any:\n             try:\n                 return fn(*args, **kwargs)\n             except Exception as e:"
      },
      {
        "filename": "libs/core/tests/unit_tests/runnables/test_tracing_interops.py",
        "content_before": "from __future__ import annotations\n\nimport json\nimport sys\nimport uuid\nfrom collections.abc import AsyncGenerator, Coroutine, Generator\nfrom inspect import isasyncgenfunction\nfrom typing import Any, Callable, Optional\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\nfrom langsmith import Client, get_current_run_tree, traceable\nfrom langsmith.run_helpers import tracing_context\nfrom langsmith.run_trees import RunTree\nfrom langsmith.utils import get_env_var\nfrom typing_extensions import Literal\n\nfrom langchain_core.callbacks import BaseCallbackHandler\nfrom langchain_core.runnables.base import RunnableLambda, RunnableParallel\nfrom langchain_core.tracers.langchain import LangChainTracer\n\n\ndef _get_posts(client: Client) -> list:\n    mock_calls = client.session.request.mock_calls  # type: ignore\n    posts = []\n    for call in mock_calls:\n        if call.args:\n            if call.args[0] != \"POST\":\n                continue\n            assert call.args[0] == \"POST\"\n            assert call.args[1].startswith(\"https://api.smith.langchain.com\")\n            body = json.loads(call.kwargs[\"data\"])\n            if \"post\" in body:\n                # Batch request\n                assert body[\"post\"]\n                posts.extend(body[\"post\"])\n            else:\n                posts.append(body)\n    return posts\n\n\ndef _create_tracer_with_mocked_client(\n    project_name: Optional[str] = None,\n    tags: Optional[list[str]] = None,\n) -> LangChainTracer:\n    mock_session = MagicMock()\n    mock_client_ = Client(\n        session=mock_session, api_key=\"test\", auto_batch_tracing=False\n    )\n    return LangChainTracer(client=mock_client_, project_name=project_name, tags=tags)\n\n\ndef test_tracing_context() -> None:\n    mock_session = MagicMock()\n    mock_client_ = Client(\n        session=mock_session, api_key=\"test\", auto_batch_tracing=False\n    )\n\n    @RunnableLambda\n    def my_function(a: int) -> int:\n        return a + 1\n\n    name = uuid.uuid4().hex\n    project_name = f\"Some project {name}\"\n    with tracing_context(project_name=project_name, client=mock_client_, enabled=True):\n        assert my_function.invoke(1) == 2\n    posts = _get_posts(mock_client_)\n    assert posts\n    assert all(post[\"session_name\"] == project_name for post in posts)\n\n\ndef test_config_traceable_handoff() -> None:\n    get_env_var.cache_clear()\n    tracer = _create_tracer_with_mocked_client(\n        project_name=\"another-flippin-project\", tags=[\"such-a-tag\"]\n    )\n\n    @traceable\n    def my_great_great_grandchild_function(a: int) -> int:\n        rt = get_current_run_tree()\n        assert rt\n        assert rt.session_name == \"another-flippin-project\"\n        return a + 1\n\n    @RunnableLambda\n    def my_great_grandchild_function(a: int) -> int:\n        return my_great_great_grandchild_function(a)\n\n    @RunnableLambda\n    def my_grandchild_function(a: int) -> int:\n        return my_great_grandchild_function.invoke(a)\n\n    @traceable\n    def my_child_function(a: int) -> int:\n        return my_grandchild_function.invoke(a) * 3\n\n    @traceable()\n    def my_function(a: int) -> int:\n        rt = get_current_run_tree()\n        assert rt\n        assert rt.session_name == \"another-flippin-project\"\n        assert rt.parent_run and rt.parent_run.name == \"my_parent_function\"\n        return my_child_function(a)\n\n    def my_parent_function(a: int) -> int:\n        rt = get_current_run_tree()\n        assert rt\n        assert rt.session_name == \"another-flippin-project\"\n        return my_function(a)\n\n    my_parent_runnable = RunnableLambda(my_parent_function)\n\n    assert my_parent_runnable.invoke(1, {\"callbacks\": [tracer]}) == 6\n    posts = _get_posts(tracer.client)\n    assert all(post[\"session_name\"] == \"another-flippin-project\" for post in posts)\n    # There should have been 6 runs created,\n    # one for each function invocation\n    assert len(posts) == 6\n    name_to_body = {post[\"name\"]: post for post in posts}\n\n    ordered_names = [\n        \"my_parent_function\",\n        \"my_function\",\n        \"my_child_function\",\n        \"my_grandchild_function\",\n        \"my_great_grandchild_function\",\n        \"my_great_great_grandchild_function\",\n    ]\n    trace_id = posts[0][\"trace_id\"]\n    last_dotted_order = None\n    parent_run_id = None\n    for name in ordered_names:\n        id_ = name_to_body[name][\"id\"]\n        parent_run_id_ = name_to_body[name][\"parent_run_id\"]\n        if parent_run_id_ is not None:\n            assert parent_run_id == parent_run_id_\n        assert name in name_to_body\n        # All within the same trace\n        assert name_to_body[name][\"trace_id\"] == trace_id\n        dotted_order: str = name_to_body[name][\"dotted_order\"]\n        assert dotted_order is not None\n        if last_dotted_order is not None:\n            assert dotted_order > last_dotted_order\n            assert dotted_order.startswith(last_dotted_order), (\n                \"Unexpected dotted order for run\"\n                f\" {name}\\n{dotted_order}\\n{last_dotted_order}\"\n            )\n        last_dotted_order = dotted_order\n        parent_run_id = id_\n    assert \"such-a-tag\" in name_to_body[\"my_parent_function\"][\"tags\"]\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11), reason=\"Asyncio context vars require Python 3.11+\"\n)\nasync def test_config_traceable_async_handoff() -> None:\n    tracer = _create_tracer_with_mocked_client()\n\n    @traceable\n    def my_great_great_grandchild_function(a: int) -> int:\n        return a + 1\n\n    @RunnableLambda\n    def my_great_grandchild_function(a: int) -> int:\n        return my_great_great_grandchild_function(a)\n\n    @RunnableLambda  # type: ignore\n    async def my_grandchild_function(a: int) -> int:\n        return my_great_grandchild_function.invoke(a)\n\n    @traceable\n    async def my_child_function(a: int) -> int:\n        return await my_grandchild_function.ainvoke(a) * 3  # type: ignore\n\n    @traceable()\n    async def my_function(a: int) -> int:\n        return await my_child_function(a)\n\n    async def my_parent_function(a: int) -> int:\n        return await my_function(a)\n\n    my_parent_runnable = RunnableLambda(my_parent_function)  # type: ignore\n    result = await my_parent_runnable.ainvoke(1, {\"callbacks\": [tracer]})\n    assert result == 6\n    posts = _get_posts(tracer.client)\n    # There should have been 6 runs created,\n    # one for each function invocation\n    assert len(posts) == 6\n    name_to_body = {post[\"name\"]: post for post in posts}\n    ordered_names = [\n        \"my_parent_function\",\n        \"my_function\",\n        \"my_child_function\",\n        \"my_grandchild_function\",\n        \"my_great_grandchild_function\",\n        \"my_great_great_grandchild_function\",\n    ]\n    trace_id = posts[0][\"trace_id\"]\n    last_dotted_order = None\n    parent_run_id = None\n    for name in ordered_names:\n        id_ = name_to_body[name][\"id\"]\n        parent_run_id_ = name_to_body[name][\"parent_run_id\"]\n        if parent_run_id_ is not None:\n            assert parent_run_id == parent_run_id_\n        assert name in name_to_body\n        # All within the same trace\n        assert name_to_body[name][\"trace_id\"] == trace_id\n        dotted_order: str = name_to_body[name][\"dotted_order\"]\n        assert dotted_order is not None\n        if last_dotted_order is not None:\n            assert dotted_order > last_dotted_order\n            assert dotted_order.startswith(last_dotted_order), (\n                \"Unexpected dotted order for run\"\n                f\" {name}\\n{dotted_order}\\n{last_dotted_order}\"\n            )\n        last_dotted_order = dotted_order\n        parent_run_id = id_\n\n\n@patch(\"langchain_core.tracers.langchain.get_client\")\n@pytest.mark.parametrize(\"enabled\", [None, True, False])\n@pytest.mark.parametrize(\"env\", [\"\", \"true\"])\ndef test_tracing_enable_disable(\n    mock_get_client: MagicMock, enabled: bool, env: str\n) -> None:\n    mock_session = MagicMock()\n    mock_client_ = Client(\n        session=mock_session, api_key=\"test\", auto_batch_tracing=False\n    )\n    mock_get_client.return_value = mock_client_\n\n    def my_func(a: int) -> int:\n        return a + 1\n\n    get_env_var.cache_clear()\n    env_on = env == \"true\"\n    with (\n        patch.dict(\"os.environ\", {\"LANGSMITH_TRACING\": env}),\n        tracing_context(enabled=enabled),\n    ):\n        RunnableLambda(my_func).invoke(1)\n\n    mock_posts = _get_posts(mock_client_)\n    if enabled is True:\n        assert len(mock_posts) == 1\n    elif enabled is False:\n        assert not mock_posts\n    elif env_on:\n        assert len(mock_posts) == 1\n    else:\n        assert not mock_posts\n\n\nclass TestRunnableSequenceParallelTraceNesting:\n    @pytest.fixture(autouse=True)\n    def _setup(self) -> None:\n        self.tracer = _create_tracer_with_mocked_client()\n\n    @staticmethod\n    def _create_parent(\n        other_thing: Callable[\n            [int], Generator[int, None, None] | AsyncGenerator[int, None]\n        ],\n    ) -> RunnableLambda:\n        @RunnableLambda\n        def my_child_function(a: int) -> int:\n            return a + 2\n\n        parallel = RunnableParallel(\n            chain_result=my_child_function.with_config(tags=[\"atag\"]),\n            other_thing=other_thing,\n        )\n\n        def before(x: int) -> int:\n            return x\n\n        def after(x: dict) -> int:\n            return x[\"chain_result\"]\n\n        sequence = before | parallel | after\n        if isasyncgenfunction(other_thing):\n\n            @RunnableLambda  # type: ignore\n            async def parent(a: int) -> int:\n                return await sequence.ainvoke(a)\n\n        else:\n\n            @RunnableLambda\n            def parent(a: int) -> int:\n                return sequence.invoke(a)\n\n        return parent\n\n    def _check_posts(self) -> None:\n        posts = _get_posts(self.tracer.client)\n        name_order = [\n            \"parent\",\n            \"RunnableSequence\",\n            \"before\",\n            \"RunnableParallel<chain_result,other_thing>\",\n            [\"my_child_function\", \"other_thing\"],\n            \"after\",\n        ]\n        expected_parents = {\n            \"parent\": None,\n            \"RunnableSequence\": \"parent\",\n            \"before\": \"RunnableSequence\",\n            \"RunnableParallel<chain_result,other_thing>\": \"RunnableSequence\",\n            \"my_child_function\": \"RunnableParallel<chain_result,other_thing>\",\n            \"other_thing\": \"RunnableParallel<chain_result,other_thing>\",\n            \"after\": \"RunnableSequence\",\n        }\n        assert len(posts) == sum(\n            1 if isinstance(n, str) else len(n) for n in name_order\n        )\n        prev_dotted_order = None\n        dotted_order_map = {}\n        id_map = {}\n        parent_id_map = {}\n        i = 0\n        for name in name_order:\n            if isinstance(name, list):\n                for n in name:\n                    matching_post = next(\n                        p for p in posts[i : i + len(name)] if p[\"name\"] == n\n                    )\n                    assert matching_post\n                    dotted_order = matching_post[\"dotted_order\"]\n                    if prev_dotted_order is not None:\n                        assert dotted_order > prev_dotted_order\n                    dotted_order_map[n] = dotted_order\n                    id_map[n] = matching_post[\"id\"]\n                    parent_id_map[n] = matching_post.get(\"parent_run_id\")\n                i += len(name)\n                continue\n            else:\n                assert posts[i][\"name\"] == name\n                dotted_order = posts[i][\"dotted_order\"]\n                if prev_dotted_order is not None and not str(\n                    expected_parents[name]\n                ).startswith(\"RunnableParallel\"):\n                    assert dotted_order > prev_dotted_order, (\n                        f\"{name} not after {name_order[i - 1]}\"\n                    )\n                prev_dotted_order = dotted_order\n                if name in dotted_order_map:\n                    msg = f\"Duplicate name {name}\"\n                    raise ValueError(msg)\n                dotted_order_map[name] = dotted_order\n                id_map[name] = posts[i][\"id\"]\n                parent_id_map[name] = posts[i].get(\"parent_run_id\")\n                i += 1\n\n        # Now check the dotted orders\n        for name, parent_ in expected_parents.items():\n            dotted_order = dotted_order_map[name]\n            if parent_ is not None:\n                parent_dotted_order = dotted_order_map[parent_]\n                assert dotted_order.startswith(parent_dotted_order), (\n                    f\"{name}, {parent_dotted_order} not in {dotted_order}\"\n                )\n                assert str(parent_id_map[name]) == str(id_map[parent_])\n            else:\n                assert dotted_order.split(\".\")[0] == dotted_order\n\n    @pytest.mark.parametrize(\n        \"method\",\n        [\n            lambda parent, cb: parent.invoke(1, {\"callbacks\": cb}),\n            lambda parent, cb: list(parent.stream(1, {\"callbacks\": cb}))[-1],\n            lambda parent, cb: parent.batch([1], {\"callbacks\": cb})[0],\n        ],\n        ids=[\"invoke\", \"stream\", \"batch\"],\n    )\n    def test_sync(\n        self, method: Callable[[RunnableLambda, list[BaseCallbackHandler]], int]\n    ) -> None:\n        def other_thing(a: int) -> Generator[int, None, None]:  # type: ignore\n            yield 1\n\n        parent = self._create_parent(other_thing)\n\n        # Now run the chain and check the resulting posts\n        assert method(parent, [self.tracer]) == 3\n\n        self._check_posts()\n\n    @staticmethod\n    async def ainvoke(parent: RunnableLambda, cb: list[BaseCallbackHandler]) -> int:\n        return await parent.ainvoke(1, {\"callbacks\": cb})\n\n    @staticmethod\n    async def astream(parent: RunnableLambda, cb: list[BaseCallbackHandler]) -> int:\n        return [res async for res in parent.astream(1, {\"callbacks\": cb})][-1]\n\n    @staticmethod\n    async def abatch(parent: RunnableLambda, cb: list[BaseCallbackHandler]) -> int:\n        return (await parent.abatch([1], {\"callbacks\": cb}))[0]\n\n    @pytest.mark.skipif(\n        sys.version_info < (3, 11), reason=\"Asyncio context vars require Python 3.11+\"\n    )\n    @pytest.mark.parametrize(\"method\", [ainvoke, astream, abatch])\n    async def test_async(\n        self,\n        method: Callable[\n            [RunnableLambda, list[BaseCallbackHandler]], Coroutine[Any, Any, int]\n        ],\n    ) -> None:\n        async def other_thing(a: int) -> AsyncGenerator[int, None]:\n            yield 1\n\n        parent = self._create_parent(other_thing)\n\n        # Now run the chain and check the resulting posts\n        assert await method(parent, [self.tracer]) == 3\n\n        self._check_posts()\n\n\n@pytest.mark.parametrize(\"parent_type\", (\"ls\", \"lc\"))\ndef test_tree_is_constructed(parent_type: Literal[\"ls\", \"lc\"]) -> None:\n    mock_session = MagicMock()\n    mock_client_ = Client(\n        session=mock_session, api_key=\"test\", auto_batch_tracing=False\n    )\n\n    @traceable\n    def kitten(x: str) -> str:\n        return x\n\n    @RunnableLambda\n    def grandchild(x: str) -> str:\n        return kitten(x)\n\n    @RunnableLambda\n    def child(x: str) -> str:\n        return grandchild.invoke(x)\n\n    rid = uuid.uuid4()\n    with tracing_context(\n        client=mock_client_,\n        enabled=True,\n        metadata={\"some_foo\": \"some_bar\"},\n        tags=[\"afoo\"],\n    ):\n        if parent_type == \"ls\":\n            collected: dict[str, RunTree] = {}  # noqa\n\n            def collect_run(run: RunTree) -> None:\n                collected[str(run.id)] = run\n\n            @traceable\n            def parent() -> str:\n                return child.invoke(\"foo\")\n\n            assert (\n                parent(langsmith_extra={\"on_end\": collect_run, \"run_id\": rid}) == \"foo\"\n            )\n            assert collected\n            run = collected.get(str(rid))\n\n        else:\n\n            @RunnableLambda\n            def parent(_) -> str:  # type: ignore\n                return child.invoke(\"foo\")\n\n            tracer = LangChainTracer()\n            assert parent.invoke(..., {\"run_id\": rid, \"callbacks\": [tracer]}) == \"foo\"  # type: ignore\n            run = tracer.latest_run\n\n    assert run is not None\n    assert run.name == \"parent\"\n    assert run.child_runs\n    child_run = run.child_runs[0]\n    assert child_run.name == \"child\"\n    assert child_run.child_runs\n    grandchild_run = child_run.child_runs[0]\n    assert grandchild_run.name == \"grandchild\"\n    assert grandchild_run.child_runs\n    assert grandchild_run.metadata.get(\"some_foo\") == \"some_bar\"\n    assert \"afoo\" in grandchild_run.tags  # type: ignore\n    kitten_run = grandchild_run.child_runs[0]\n    assert kitten_run.name == \"kitten\"\n    assert not kitten_run.child_runs\n    assert kitten_run.metadata.get(\"some_foo\") == \"some_bar\"\n    assert \"afoo\" in kitten_run.tags  # type: ignore\n",
        "patch": "@@ -465,7 +465,7 @@ def parent() -> str:\n         else:\n \n             @RunnableLambda\n-            def parent(_) -> str:  # type: ignore\n+            def parent(_: Any) -> str:\n                 return child.invoke(\"foo\")\n \n             tracer = LangChainTracer()"
      },
      {
        "filename": "libs/core/tests/unit_tests/test_tools.py",
        "content_before": "\"\"\"Test the base tool implementation.\"\"\"\n\nimport inspect\nimport json\nimport sys\nimport textwrap\nimport threading\nfrom datetime import datetime\nfrom enum import Enum\nfrom functools import partial\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Generic,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n)\n\nimport pytest\nfrom pydantic import BaseModel, Field, ValidationError\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom pydantic.v1 import ValidationError as ValidationErrorV1\nfrom typing_extensions import TypedDict\n\nfrom langchain_core import tools\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.callbacks.manager import (\n    CallbackManagerForRetrieverRun,\n)\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import ToolCall, ToolMessage\nfrom langchain_core.messages.tool import ToolOutputMixin\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableConfig,\n    RunnableLambda,\n    ensure_config,\n)\nfrom langchain_core.tools import (\n    BaseTool,\n    StructuredTool,\n    Tool,\n    ToolException,\n    tool,\n)\nfrom langchain_core.tools.base import (\n    InjectedToolArg,\n    InjectedToolCallId,\n    SchemaAnnotationError,\n    _is_message_content_block,\n    _is_message_content_type,\n    get_all_basemodel_annotations,\n)\nfrom langchain_core.utils.function_calling import convert_to_openai_function\nfrom langchain_core.utils.pydantic import PYDANTIC_MAJOR_VERSION, _create_subset_model\nfrom tests.unit_tests.fake.callbacks import FakeCallbackHandler\nfrom tests.unit_tests.pydantic_utils import _schema\n\n\ndef test_unnamed_decorator() -> None:\n    \"\"\"Test functionality with unnamed decorator.\"\"\"\n\n    @tool\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search_api\"\n    assert not search_api.return_direct\n    assert search_api.invoke(\"test\") == \"API result\"\n\n\nclass _MockSchema(BaseModel):\n    \"\"\"Return the arguments directly.\"\"\"\n\n    arg1: int\n    arg2: bool\n    arg3: Optional[dict] = None\n\n\nclass _MockSchemaV1(BaseModelV1):\n    \"\"\"Return the arguments directly.\"\"\"\n\n    arg1: int\n    arg2: bool\n    arg3: Optional[dict] = None\n\n\nclass _MockStructuredTool(BaseTool):\n    name: str = \"structured_api\"\n    args_schema: type[BaseModel] = _MockSchema\n    description: str = \"A Structured Tool\"\n\n    def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    async def _arun(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        raise NotImplementedError\n\n\ndef test_structured_args() -> None:\n    \"\"\"Test functionality with structured arguments.\"\"\"\n    structured_api = _MockStructuredTool()\n    assert isinstance(structured_api, BaseTool)\n    assert structured_api.name == \"structured_api\"\n    expected_result = \"1 True {'foo': 'bar'}\"\n    args = {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"foo\": \"bar\"}}\n    assert structured_api.run(args) == expected_result\n\n\ndef test_misannotated_base_tool_raises_error() -> None:\n    \"\"\"Test that a BaseTool with the incorrect typehint raises an exception.\"\"\" \"\"\n    with pytest.raises(SchemaAnnotationError):\n\n        class _MisAnnotatedTool(BaseTool):\n            name: str = \"structured_api\"\n            # This would silently be ignored without the custom metaclass\n            args_schema: BaseModel = _MockSchema  # type: ignore\n            description: str = \"A Structured Tool\"\n\n            def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n                return f\"{arg1} {arg2} {arg3}\"\n\n            async def _arun(\n                self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n            ) -> str:\n                raise NotImplementedError\n\n\ndef test_forward_ref_annotated_base_tool_accepted() -> None:\n    \"\"\"Test that a using forward ref annotation syntax is accepted.\"\"\" \"\"\n\n    class _ForwardRefAnnotatedTool(BaseTool):\n        name: str = \"structured_api\"\n        args_schema: \"type[BaseModel]\" = _MockSchema\n        description: str = \"A Structured Tool\"\n\n        def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n            return f\"{arg1} {arg2} {arg3}\"\n\n        async def _arun(\n            self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n        ) -> str:\n            raise NotImplementedError\n\n\ndef test_subclass_annotated_base_tool_accepted() -> None:\n    \"\"\"Test BaseTool child w/ custom schema isn't overwritten.\"\"\"\n\n    class _ForwardRefAnnotatedTool(BaseTool):\n        name: str = \"structured_api\"\n        args_schema: type[_MockSchema] = _MockSchema\n        description: str = \"A Structured Tool\"\n\n        def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n            return f\"{arg1} {arg2} {arg3}\"\n\n        async def _arun(\n            self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n        ) -> str:\n            raise NotImplementedError\n\n    assert issubclass(_ForwardRefAnnotatedTool, BaseTool)\n    tool = _ForwardRefAnnotatedTool()\n    assert tool.args_schema == _MockSchema\n\n\ndef test_decorator_with_specified_schema() -> None:\n    \"\"\"Test that manually specified schemata are passed through to the tool.\"\"\"\n\n    @tool(args_schema=_MockSchema)\n    def tool_func(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(tool_func, BaseTool)\n    assert tool_func.args_schema == _MockSchema\n\n    @tool(args_schema=_MockSchemaV1)\n    def tool_func_v1(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(tool_func_v1, BaseTool)\n    assert tool_func_v1.args_schema == _MockSchemaV1\n\n\ndef test_decorated_function_schema_equivalent() -> None:\n    \"\"\"Test that a BaseTool without a schema meets expectations.\"\"\"\n\n    @tool\n    def structured_tool_input(\n        arg1: int, arg2: bool, arg3: Optional[dict] = None\n    ) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(structured_tool_input, BaseTool)\n    assert structured_tool_input.args_schema is not None\n    assert (\n        _schema(structured_tool_input.args_schema)[\"properties\"]\n        == _schema(_MockSchema)[\"properties\"]\n        == structured_tool_input.args\n    )\n\n\ndef test_args_kwargs_filtered() -> None:\n    class _SingleArgToolWithKwargs(BaseTool):\n        name: str = \"single_arg_tool\"\n        description: str = \"A  single arged tool with kwargs\"\n\n        def _run(\n            self,\n            some_arg: str,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            return \"foo\"\n\n        async def _arun(\n            self,\n            some_arg: str,\n            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            raise NotImplementedError\n\n    tool = _SingleArgToolWithKwargs()\n    assert tool.is_single_input\n\n    class _VarArgToolWithKwargs(BaseTool):\n        name: str = \"single_arg_tool\"\n        description: str = \"A single arged tool with kwargs\"\n\n        def _run(\n            self,\n            *args: Any,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            return \"foo\"\n\n        async def _arun(\n            self,\n            *args: Any,\n            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            raise NotImplementedError\n\n    tool2 = _VarArgToolWithKwargs()\n    assert tool2.is_single_input\n\n\ndef test_structured_args_decorator_no_infer_schema() -> None:\n    \"\"\"Test functionality with structured arguments parsed as a decorator.\"\"\"\n\n    @tool(infer_schema=False)\n    def structured_tool_input(\n        arg1: int, arg2: Union[float, datetime], opt_arg: Optional[dict] = None\n    ) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        return f\"{arg1}, {arg2}, {opt_arg}\"\n\n    assert isinstance(structured_tool_input, BaseTool)\n    assert structured_tool_input.name == \"structured_tool_input\"\n    args = {\"arg1\": 1, \"arg2\": 0.001, \"opt_arg\": {\"foo\": \"bar\"}}\n    with pytest.raises(ToolException):\n        assert structured_tool_input.run(args)\n\n\ndef test_structured_single_str_decorator_no_infer_schema() -> None:\n    \"\"\"Test functionality with structured arguments parsed as a decorator.\"\"\"\n\n    @tool(infer_schema=False)\n    def unstructured_tool_input(tool_input: str) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        assert isinstance(tool_input, str)\n        return f\"{tool_input}\"\n\n    assert isinstance(unstructured_tool_input, BaseTool)\n    assert unstructured_tool_input.args_schema is None\n    assert unstructured_tool_input.run(\"foo\") == \"foo\"\n\n\ndef test_structured_tool_types_parsed() -> None:\n    \"\"\"Test the non-primitive types are correctly passed to structured tools.\"\"\"\n\n    class SomeEnum(Enum):\n        A = \"a\"\n        B = \"b\"\n\n    class SomeBaseModel(BaseModel):\n        foo: str\n\n    @tool\n    def structured_tool(\n        some_enum: SomeEnum,\n        some_base_model: SomeBaseModel,\n    ) -> dict:\n        \"\"\"Return the arguments directly.\"\"\"\n        return {\n            \"some_enum\": some_enum,\n            \"some_base_model\": some_base_model,\n        }\n\n    assert isinstance(structured_tool, StructuredTool)\n    args = {\n        \"some_enum\": SomeEnum.A.value,\n        \"some_base_model\": SomeBaseModel(foo=\"bar\").model_dump(),\n    }\n    result = structured_tool.run(json.loads(json.dumps(args)))\n    expected = {\n        \"some_enum\": SomeEnum.A,\n        \"some_base_model\": SomeBaseModel(foo=\"bar\"),\n    }\n    assert result == expected\n\n\ndef test_structured_tool_types_parsed_pydantic_v1() -> None:\n    \"\"\"Test the non-primitive types are correctly passed to structured tools.\"\"\"\n\n    class SomeBaseModel(BaseModelV1):\n        foo: str\n\n    class AnotherBaseModel(BaseModelV1):\n        bar: str\n\n    @tool\n    def structured_tool(some_base_model: SomeBaseModel) -> AnotherBaseModel:\n        \"\"\"Return the arguments directly.\"\"\"\n        return AnotherBaseModel(bar=some_base_model.foo)\n\n    assert isinstance(structured_tool, StructuredTool)\n\n    expected = AnotherBaseModel(bar=\"baz\")\n    for arg in [\n        SomeBaseModel(foo=\"baz\"),\n        SomeBaseModel(foo=\"baz\").dict(),\n    ]:\n        args = {\"some_base_model\": arg}\n        result = structured_tool.run(args)\n        assert result == expected\n\n\ndef test_structured_tool_types_parsed_pydantic_mixed() -> None:\n    \"\"\"Test handling of tool with mixed Pydantic version arguments.\"\"\"\n\n    class SomeBaseModel(BaseModelV1):\n        foo: str\n\n    class AnotherBaseModel(BaseModel):\n        bar: str\n\n    with pytest.raises(NotImplementedError):\n\n        @tool\n        def structured_tool(\n            some_base_model: SomeBaseModel, another_base_model: AnotherBaseModel\n        ) -> None:\n            \"\"\"Return the arguments directly.\"\"\"\n\n\ndef test_base_tool_inheritance_base_schema() -> None:\n    \"\"\"Test schema is correctly inferred when inheriting from BaseTool.\"\"\"\n\n    class _MockSimpleTool(BaseTool):\n        name: str = \"simple_tool\"\n        description: str = \"A Simple Tool\"\n\n        def _run(self, tool_input: str) -> str:\n            return f\"{tool_input}\"\n\n        async def _arun(self, tool_input: str) -> str:\n            raise NotImplementedError\n\n    simple_tool = _MockSimpleTool()\n    assert simple_tool.args_schema is None\n    expected_args = {\"tool_input\": {\"title\": \"Tool Input\", \"type\": \"string\"}}\n    assert simple_tool.args == expected_args\n\n\ndef test_tool_lambda_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a lambda function.\"\"\"\n    tool = Tool(\n        name=\"tool\",\n        description=\"A tool\",\n        func=lambda tool_input: tool_input,\n    )\n    assert tool.args_schema is None\n    expected_args = {\"tool_input\": {\"type\": \"string\"}}\n    assert tool.args == expected_args\n\n\ndef test_structured_tool_from_function_docstring() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: str) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: the bar value\n            baz: the baz value\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__.strip())\n\n\ndef test_structured_tool_from_function_docstring_complex_args() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: list[str]) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: int\n            baz: List[str]\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\n            \"title\": \"Baz\",\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n        },\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\n                \"title\": \"Baz\",\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n            },\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__).strip()\n\n\ndef test_structured_tool_lambda_multi_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a lambda function.\"\"\"\n    tool = StructuredTool.from_function(\n        name=\"tool\",\n        description=\"A tool\",\n        func=lambda tool_input, other_arg: f\"{tool_input}{other_arg}\",  # type: ignore\n    )\n    assert tool.args_schema is not None\n    expected_args = {\n        \"tool_input\": {\"title\": \"Tool Input\"},\n        \"other_arg\": {\"title\": \"Other Arg\"},\n    }\n    assert tool.args == expected_args\n\n\ndef test_tool_partial_function_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a partial function.\"\"\"\n\n    def func(tool_input: str, other_arg: str) -> str:\n        assert isinstance(tool_input, str)\n        assert isinstance(other_arg, str)\n        return tool_input + other_arg\n\n    tool = Tool(\n        name=\"tool\",\n        description=\"A tool\",\n        func=partial(func, other_arg=\"foo\"),\n    )\n    assert tool.run(\"bar\") == \"barfoo\"\n\n\ndef test_empty_args_decorator() -> None:\n    \"\"\"Test inferred schema of decorated fn with no args.\"\"\"\n\n    @tool\n    def empty_tool_input() -> str:\n        \"\"\"Return a constant.\"\"\"\n        return \"the empty result\"\n\n    assert isinstance(empty_tool_input, BaseTool)\n    assert empty_tool_input.name == \"empty_tool_input\"\n    assert empty_tool_input.args == {}\n    assert empty_tool_input.run({}) == \"the empty result\"\n\n\ndef test_tool_from_function_with_run_manager() -> None:\n    \"\"\"Test run of tool when using run_manager.\"\"\"\n\n    def foo(bar: str, callbacks: Optional[CallbackManagerForToolRun] = None) -> str:\n        \"\"\"Docstring\n        Args:\n            bar: str.\n        \"\"\"\n        assert callbacks is not None\n        return \"foo\" + bar\n\n    handler = FakeCallbackHandler()\n    tool = Tool.from_function(foo, name=\"foo\", description=\"Docstring\")\n\n    assert tool.run(tool_input={\"bar\": \"bar\"}, run_manager=[handler]) == \"foobar\"\n    assert tool.run(\"baz\", run_manager=[handler]) == \"foobaz\"\n\n\ndef test_structured_tool_from_function_with_run_manager() -> None:\n    \"\"\"Test args and schema of structured tool when using callbacks.\"\"\"\n\n    def foo(\n        bar: int, baz: str, callbacks: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: int\n            baz: str\n        \"\"\"\n        assert callbacks is not None\n        return str(bar) + baz\n\n    handler = FakeCallbackHandler()\n    structured_tool = StructuredTool.from_function(foo)\n\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert (\n        structured_tool.run(\n            tool_input={\"bar\": \"10\", \"baz\": \"baz\"}, run_manger=[handler]\n        )\n        == \"10baz\"\n    )\n\n\ndef test_structured_tool_from_parameterless_function() -> None:\n    \"\"\"Test parameterless function of structured tool.\"\"\"\n\n    def foo() -> str:\n        \"\"\"Docstring.\"\"\"\n        return \"invoke foo\"\n\n    structured_tool = StructuredTool.from_function(foo)\n\n    assert structured_tool.run({}) == \"invoke foo\"\n    assert structured_tool.run(\"\") == \"invoke foo\"\n\n\ndef test_named_tool_decorator() -> None:\n    \"\"\"Test functionality when arguments are provided as input to decorator.\"\"\"\n\n    @tool(\"search\")\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        assert isinstance(query, str)\n        return f\"API result - {query}\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search\"\n    assert not search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result - foo\"\n\n\ndef test_named_tool_decorator_return_direct() -> None:\n    \"\"\"Test functionality when arguments and return direct are provided as input.\"\"\"\n\n    @tool(\"search\", return_direct=True)\n    def search_api(query: str, *args: Any) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search\"\n    assert search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result\"\n\n\ndef test_unnamed_tool_decorator_return_direct() -> None:\n    \"\"\"Test functionality when only return direct is provided.\"\"\"\n\n    @tool(return_direct=True)\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        assert isinstance(query, str)\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search_api\"\n    assert search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result\"\n\n\ndef test_tool_with_kwargs() -> None:\n    \"\"\"Test functionality when only return direct is provided.\"\"\"\n\n    @tool(return_direct=True)\n    def search_api(\n        arg_0: str,\n        arg_1: float = 4.3,\n        ping: str = \"hi\",\n    ) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return f\"arg_0={arg_0}, arg_1={arg_1}, ping={ping}\"\n\n    assert isinstance(search_api, BaseTool)\n    result = search_api.run(\n        tool_input={\n            \"arg_0\": \"foo\",\n            \"arg_1\": 3.2,\n            \"ping\": \"pong\",\n        }\n    )\n    assert result == \"arg_0=foo, arg_1=3.2, ping=pong\"\n\n    result = search_api.run(\n        tool_input={\n            \"arg_0\": \"foo\",\n        }\n    )\n    assert result == \"arg_0=foo, arg_1=4.3, ping=hi\"\n    # For backwards compatibility, we still accept a single str arg\n    result = search_api.run(\"foobar\")\n    assert result == \"arg_0=foobar, arg_1=4.3, ping=hi\"\n\n\ndef test_missing_docstring() -> None:\n    \"\"\"Test error is raised when docstring is missing.\"\"\"\n    # expect to throw a value error if there's no docstring\n    with pytest.raises(ValueError, match=\"Function must have a docstring\"):\n\n        @tool\n        def search_api(query: str) -> str:\n            return \"API result\"\n\n\ndef test_create_tool_positional_args() -> None:\n    \"\"\"Test that positional arguments are allowed.\"\"\"\n    test_tool = Tool(\"test_name\", lambda x: x, \"test_description\")\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n    assert test_tool.is_single_input\n\n\ndef test_create_tool_keyword_args() -> None:\n    \"\"\"Test that keyword arguments are allowed.\"\"\"\n    test_tool = Tool(name=\"test_name\", func=lambda x: x, description=\"test_description\")\n    assert test_tool.is_single_input\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n\n\nasync def test_create_async_tool() -> None:\n    \"\"\"Test that async tools are allowed.\"\"\"\n\n    async def _test_func(x: str) -> str:\n        return x\n\n    test_tool = Tool(\n        name=\"test_name\",\n        func=lambda x: x,\n        description=\"test_description\",\n        coroutine=_test_func,\n    )\n    assert test_tool.is_single_input\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n    assert test_tool.coroutine is not None\n    assert await test_tool.arun(\"foo\") == \"foo\"\n\n\nclass _FakeExceptionTool(BaseTool):\n    name: str = \"exception\"\n    description: str = \"an exception-throwing tool\"\n    exception: Exception = ToolException()\n\n    def _run(self) -> str:\n        raise self.exception\n\n    async def _arun(self) -> str:\n        raise self.exception\n\n\ndef test_exception_handling_bool() -> None:\n    _tool = _FakeExceptionTool(handle_tool_error=True)\n    expected = \"Tool execution error\"\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_str() -> None:\n    expected = \"foo bar\"\n    _tool = _FakeExceptionTool(handle_tool_error=expected)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_callable() -> None:\n    expected = \"foo bar\"\n\n    def handling(e: ToolException) -> str:\n        return expected\n\n    _tool = _FakeExceptionTool(handle_tool_error=handling)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_non_tool_exception() -> None:\n    _tool = _FakeExceptionTool(exception=ValueError())\n    with pytest.raises(ValueError):\n        _tool.run({})\n\n\nasync def test_async_exception_handling_bool() -> None:\n    _tool = _FakeExceptionTool(handle_tool_error=True)\n    expected = \"Tool execution error\"\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_str() -> None:\n    expected = \"foo bar\"\n    _tool = _FakeExceptionTool(handle_tool_error=expected)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_callable() -> None:\n    expected = \"foo bar\"\n\n    def handling(e: ToolException) -> str:\n        return expected\n\n    _tool = _FakeExceptionTool(handle_tool_error=handling)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_non_tool_exception() -> None:\n    _tool = _FakeExceptionTool(exception=ValueError())\n    with pytest.raises(ValueError):\n        await _tool.arun({})\n\n\ndef test_structured_tool_from_function() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: str) -> str:\n        \"\"\"Docstring thing.\n\n        Args:\n            bar: the bar value\n            baz: the baz value\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__.strip())\n\n\ndef test_validation_error_handling_bool() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"Tool input validation error\"\n    _tool = _MockStructuredTool(handle_validation_error=True)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_validation_error_handling_str() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n    _tool = _MockStructuredTool(handle_validation_error=expected)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_validation_error_handling_callable() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n\n    def handling(e: Union[ValidationError, ValidationErrorV1]) -> str:\n        return expected\n\n    _tool = _MockStructuredTool(handle_validation_error=handling)\n    actual = _tool.run({})\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\n    \"handler\",\n    [\n        True,\n        \"foo bar\",\n        lambda _: \"foo bar\",\n    ],\n)\ndef test_validation_error_handling_non_validation_error(\n    handler: Union[\n        bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n\n    class _RaiseNonValidationErrorTool(BaseTool):\n        name: str = \"raise_non_validation_error_tool\"\n        description: str = \"A tool that raises a non-validation error\"\n\n        def _parse_input(\n            self,\n            tool_input: Union[str, dict],\n            tool_call_id: Optional[str],\n        ) -> Union[str, dict[str, Any]]:\n            raise NotImplementedError\n\n        def _run(self) -> str:\n            return \"dummy\"\n\n        async def _arun(self) -> str:\n            return \"dummy\"\n\n    _tool = _RaiseNonValidationErrorTool(handle_validation_error=handler)  # type: ignore[call-arg]\n    with pytest.raises(NotImplementedError):\n        _tool.run({})\n\n\nasync def test_async_validation_error_handling_bool() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"Tool input validation error\"\n    _tool = _MockStructuredTool(handle_validation_error=True)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_validation_error_handling_str() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n    _tool = _MockStructuredTool(handle_validation_error=expected)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_validation_error_handling_callable() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n\n    def handling(e: Union[ValidationError, ValidationErrorV1]) -> str:\n        return expected\n\n    _tool = _MockStructuredTool(handle_validation_error=handling)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\n    \"handler\",\n    [\n        True,\n        \"foo bar\",\n        lambda _: \"foo bar\",\n    ],\n)\nasync def test_async_validation_error_handling_non_validation_error(\n    handler: Union[\n        bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n\n    class _RaiseNonValidationErrorTool(BaseTool):\n        name: str = \"raise_non_validation_error_tool\"\n        description: str = \"A tool that raises a non-validation error\"\n\n        def _parse_input(\n            self,\n            tool_input: Union[str, dict],\n            tool_call_id: Optional[str],\n        ) -> Union[str, dict[str, Any]]:\n            raise NotImplementedError\n\n        def _run(self) -> str:\n            return \"dummy\"\n\n        async def _arun(self) -> str:\n            return \"dummy\"\n\n    _tool = _RaiseNonValidationErrorTool(handle_validation_error=handler)  # type: ignore[call-arg]\n    with pytest.raises(NotImplementedError):\n        await _tool.arun({})\n\n\ndef test_optional_subset_model_rewrite() -> None:\n    class MyModel(BaseModel):\n        a: Optional[str] = None\n        b: str\n        c: Optional[list[Optional[str]]] = None\n\n    model2 = _create_subset_model(\"model2\", MyModel, [\"a\", \"b\", \"c\"])\n\n    assert set(_schema(model2)[\"required\"]) == {\"b\"}\n\n\n@pytest.mark.parametrize(\n    \"inputs, expected\",\n    [\n        # Check not required\n        ({\"bar\": \"bar\"}, {\"bar\": \"bar\", \"baz\": 3, \"buzz\": \"buzz\"}),\n        # Check overwritten\n        (\n            {\"bar\": \"bar\", \"baz\": 4, \"buzz\": \"not-buzz\"},\n            {\"bar\": \"bar\", \"baz\": 4, \"buzz\": \"not-buzz\"},\n        ),\n        # Check validation error when missing\n        ({}, None),\n        # Check validation error when wrong type\n        ({\"bar\": \"bar\", \"baz\": \"not-an-int\"}, None),\n        # Check OK when None explicitly passed\n        ({\"bar\": \"bar\", \"baz\": None}, {\"bar\": \"bar\", \"baz\": None, \"buzz\": \"buzz\"}),\n    ],\n)\ndef test_tool_invoke_optional_args(inputs: dict, expected: Optional[dict]) -> None:\n    @tool\n    def foo(bar: str, baz: Optional[int] = 3, buzz: Optional[str] = \"buzz\") -> dict:\n        \"\"\"The foo.\"\"\"\n        return {\n            \"bar\": bar,\n            \"baz\": baz,\n            \"buzz\": buzz,\n        }\n\n    if expected is not None:\n        assert foo.invoke(inputs) == expected  # type: ignore\n    else:\n        with pytest.raises(ValidationError):\n            foo.invoke(inputs)  # type: ignore\n\n\ndef test_tool_pass_context() -> None:\n    @tool\n    def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        assert bar == \"baz\"\n        return bar\n\n    assert foo.invoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"  # type: ignore\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=\"requires python3.11 or higher\",\n)\nasync def test_async_tool_pass_context() -> None:\n    @tool\n    async def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        assert bar == \"baz\"\n        return bar\n\n    assert (\n        await foo.ainvoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"  # type: ignore\n    )\n\n\ndef assert_bar(bar: Any, bar_config: RunnableConfig) -> Any:\n    assert bar_config[\"configurable\"][\"foo\"] == \"not-bar\"\n    assert bar == \"baz\"\n    return bar\n\n\n@tool\ndef foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool\nasync def afoo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool(infer_schema=False)\ndef simple_foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool(infer_schema=False)\nasync def asimple_foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\nclass FooBase(BaseTool):\n    name: str = \"Foo\"\n    description: str = \"Foo\"\n\n    def _run(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return assert_bar(bar, bar_config)\n\n\nFooBase.model_rebuild()\n\n\nclass AFooBase(FooBase):\n    async def _arun(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return assert_bar(bar, bar_config)\n\n\n@pytest.mark.parametrize(\"tool\", [foo, simple_foo, FooBase(), AFooBase()])\ndef test_tool_pass_config(tool: BaseTool) -> None:\n    assert tool.invoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"\n\n    # Test we don't mutate tool calls\n    tool_call = {\n        \"name\": tool.name,\n        \"args\": {\"bar\": \"baz\"},\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    _ = tool.invoke(tool_call, {\"configurable\": {\"foo\": \"not-bar\"}})\n    assert tool_call[\"args\"] == {\"bar\": \"baz\"}\n\n\nclass FooBaseNonPickleable(FooBase):\n    def _run(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return True\n\n\ndef test_tool_pass_config_non_pickleable() -> None:\n    tool = FooBaseNonPickleable()\n\n    args = {\"bar\": threading.Lock()}\n    tool_call = {\n        \"name\": tool.name,\n        \"args\": args,\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    _ = tool.invoke(tool_call, {\"configurable\": {\"foo\": \"not-bar\"}})\n    assert tool_call[\"args\"] == args\n\n\n@pytest.mark.parametrize(\n    \"tool\", [foo, afoo, simple_foo, asimple_foo, FooBase(), AFooBase()]\n)\nasync def test_async_tool_pass_config(tool: BaseTool) -> None:\n    assert (\n        await tool.ainvoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}})\n        == \"baz\"\n    )\n\n\ndef test_tool_description() -> None:\n    def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    assert foo1.description == \"The foo.\"  # type: ignore\n\n    foo2 = StructuredTool.from_function(foo)\n    assert foo2.description == \"The foo.\"\n\n\ndef test_tool_arg_descriptions() -> None:\n    def foo(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    args_schema = _schema(foo1.args_schema)  # type: ignore\n    assert args_schema == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    # Test parses docstring\n    foo2 = tool(foo, parse_docstring=True)\n    args_schema = _schema(foo2.args_schema)  # type: ignore\n    expected = {\n        \"title\": \"foo\",\n        \"description\": \"The foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"description\": \"The bar.\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"description\": \"The baz.\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n    assert args_schema == expected\n\n    # Test parsing with run_manager does not raise error\n    def foo3(\n        bar: str, baz: int, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo3, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n    assert args_schema[\"properties\"] == expected[\"properties\"]\n\n    # Test parameterless tool does not raise error for missing Args section\n    # in docstring.\n    def foo4() -> str:\n        \"\"\"The foo.\"\"\"\n        return \"bar\"\n\n    as_tool = tool(foo4, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n\n    def foo5(run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n        \"\"\"The foo.\"\"\"\n        return \"bar\"\n\n    as_tool = tool(foo5, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n\n\ndef test_docstring_parsing() -> None:\n    expected = {\n        \"title\": \"foo\",\n        \"description\": \"The foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"description\": \"The bar.\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"description\": \"The baz.\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    # Simple case\n    def foo(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == \"The foo.\"\n    assert args_schema[\"properties\"] == expected[\"properties\"]\n\n    # Multi-line description\n    def foo2(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Additional description here.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo2, parse_docstring=True)\n    args_schema2 = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema2[\"description\"] == \"The foo. Additional description here.\"\n    assert args_schema2[\"properties\"] == expected[\"properties\"]\n\n    # Multi-line wth Returns block\n    def foo3(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Additional description here.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n\n        Returns:\n            str: description of returned value.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo3, parse_docstring=True)\n    args_schema3 = _schema(as_tool.args_schema)  # type: ignore\n    args_schema3[\"title\"] = \"foo2\"\n    assert args_schema2 == args_schema3\n\n    # Single argument\n    def foo4(bar: str) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo4, parse_docstring=True)\n    args_schema4 = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema4[\"description\"] == \"The foo.\"\n    assert args_schema4[\"properties\"] == {\n        \"bar\": {\"description\": \"The bar.\", \"title\": \"Bar\", \"type\": \"string\"}\n    }\n\n\ndef test_tool_invalid_docstrings() -> None:\n    # Test invalid docstrings\n    def foo3(bar: str, baz: int) -> str:\n        \"\"\"The foo.\"\"\"\n        return bar\n\n    def foo4(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    def foo5(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            banana: The bar.\n            monkey: The baz.\n        \"\"\"\n        return bar\n\n    for func in [foo3, foo4, foo5]:\n        with pytest.raises(ValueError):\n            _ = tool(func, parse_docstring=True)\n\n\ndef test_tool_annotated_descriptions() -> None:\n    def foo(\n        bar: Annotated[str, \"this is the bar\"], baz: Annotated[int, \"this is the baz\"]\n    ) -> str:\n        \"\"\"The foo.\n\n        Returns:\n            The bar only.\n        \"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    args_schema = _schema(foo1.args_schema)  # type: ignore\n    assert args_schema == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\", \"description\": \"this is the bar\"},\n            \"baz\": {\n                \"title\": \"Baz\",\n                \"type\": \"integer\",\n                \"description\": \"this is the baz\",\n            },\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n\ndef test_tool_call_input_tool_message_output() -> None:\n    tool_call = {\n        \"name\": \"structured_api\",\n        \"args\": {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"img\": \"base64string...\"}},\n        \"id\": \"123\",\n        \"type\": \"tool_call\",\n    }\n    tool = _MockStructuredTool()\n    expected = ToolMessage(\n        \"1 True {'img': 'base64string...'}\", tool_call_id=\"123\", name=\"structured_api\"\n    )\n    actual = tool.invoke(tool_call)\n    assert actual == expected\n\n    tool_call.pop(\"type\")\n    with pytest.raises(ValidationError):\n        tool.invoke(tool_call)\n\n\nclass _MockStructuredToolWithRawOutput(BaseTool):\n    name: str = \"structured_api\"\n    args_schema: type[BaseModel] = _MockSchema\n    description: str = \"A Structured Tool\"\n    response_format: Literal[\"content_and_artifact\"] = \"content_and_artifact\"\n\n    def _run(\n        self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n    ) -> tuple[str, dict]:\n        return f\"{arg1} {arg2}\", {\"arg1\": arg1, \"arg2\": arg2, \"arg3\": arg3}\n\n\n@tool(\"structured_api\", response_format=\"content_and_artifact\")\ndef _mock_structured_tool_with_artifact(\n    arg1: int, arg2: bool, arg3: Optional[dict] = None\n) -> tuple[str, dict]:\n    \"\"\"A Structured Tool.\"\"\"\n    return f\"{arg1} {arg2}\", {\"arg1\": arg1, \"arg2\": arg2, \"arg3\": arg3}\n\n\n@pytest.mark.parametrize(\n    \"tool\", [_MockStructuredToolWithRawOutput(), _mock_structured_tool_with_artifact]\n)\ndef test_tool_call_input_tool_message_with_artifact(tool: BaseTool) -> None:\n    tool_call: dict = {\n        \"name\": \"structured_api\",\n        \"args\": {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"img\": \"base64string...\"}},\n        \"id\": \"123\",\n        \"type\": \"tool_call\",\n    }\n    expected = ToolMessage(\n        \"1 True\", artifact=tool_call[\"args\"], tool_call_id=\"123\", name=\"structured_api\"\n    )\n    actual = tool.invoke(tool_call)\n    assert actual == expected\n\n    tool_call.pop(\"type\")\n    with pytest.raises(ValidationError):\n        tool.invoke(tool_call)\n\n    actual_content = tool.invoke(tool_call[\"args\"])\n    assert actual_content == expected.content\n\n\ndef test_convert_from_runnable_dict() -> None:\n    # Test with typed dict input\n    class Args(TypedDict):\n        a: int\n        b: list[int]\n\n    def f(x: Args) -> str:\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    runnable: Runnable = RunnableLambda(f)\n    as_tool = runnable.as_tool()\n    args_schema = as_tool.args_schema\n    assert args_schema is not None\n    assert _schema(args_schema) == {\n        \"title\": \"f\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"array\", \"items\": {\"type\": \"integer\"}},\n        },\n        \"required\": [\"a\", \"b\"],\n    }\n    assert as_tool.description\n    result = as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n    assert result == \"6\"\n\n    as_tool = runnable.as_tool(name=\"my tool\", description=\"test description\")\n    assert as_tool.name == \"my tool\"\n    assert as_tool.description == \"test description\"\n\n    # Dict without typed input-- must supply schema\n    def g(x: dict[str, Any]) -> str:\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    # Specify via args_schema:\n    class GSchema(BaseModel):\n        \"\"\"Apply a function to an integer and list of integers.\"\"\"\n\n        a: int = Field(..., description=\"Integer\")\n        b: list[int] = Field(..., description=\"List of ints\")\n\n    runnable = RunnableLambda(g)\n    as_tool = runnable.as_tool(GSchema)\n    as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n    # Specify via arg_types:\n    runnable = RunnableLambda(g)\n    as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n    result = as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n    assert result == \"6\"\n\n    # Test with config\n    def h(x: dict[str, Any]) -> str:\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    runnable = RunnableLambda(h)\n    as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n    result = as_tool.invoke(\n        {\"a\": 3, \"b\": [1, 2]}, config={\"configurable\": {\"foo\": \"not-bar\"}}\n    )\n    assert result == \"6\"\n\n\ndef test_convert_from_runnable_other() -> None:\n    # String input\n    def f(x: str) -> str:\n        return x + \"a\"\n\n    def g(x: str) -> str:\n        return x + \"z\"\n\n    runnable: Runnable = RunnableLambda(f) | g\n    as_tool = runnable.as_tool()\n    args_schema = as_tool.args_schema\n    assert args_schema is None\n    assert as_tool.description\n\n    result = as_tool.invoke(\"b\")\n    assert result == \"baz\"\n\n    # Test with config\n    def h(x: str) -> str:\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        return x + \"a\"\n\n    runnable = RunnableLambda(h)\n    as_tool = runnable.as_tool()\n    result = as_tool.invoke(\"b\", config={\"configurable\": {\"foo\": \"not-bar\"}})\n    assert result == \"ba\"\n\n\n@tool(\"foo\", parse_docstring=True)\ndef injected_tool(x: int, y: Annotated[str, InjectedToolArg]) -> str:\n    \"\"\"foo.\n\n    Args:\n        x: abc\n        y: 123\n    \"\"\"\n    return y\n\n\nclass InjectedTool(BaseTool):\n    name: str = \"foo\"\n    description: str = \"foo.\"\n\n    def _run(self, x: int, y: Annotated[str, InjectedToolArg]) -> Any:\n        \"\"\"foo.\n\n        Args:\n            x: abc\n            y: 123\n        \"\"\"\n        return y\n\n\nclass fooSchema(BaseModel):  # noqa: N801\n    \"\"\"foo.\"\"\"\n\n    x: int = Field(..., description=\"abc\")\n    y: Annotated[str, \"foobar comment\", InjectedToolArg()] = Field(\n        ..., description=\"123\"\n    )\n\n\nclass InjectedToolWithSchema(BaseTool):\n    name: str = \"foo\"\n    description: str = \"foo.\"\n    args_schema: type[BaseModel] = fooSchema\n\n    def _run(self, x: int, y: str) -> Any:\n        return y\n\n\n@tool(\"foo\", args_schema=fooSchema)\ndef injected_tool_with_schema(x: int, y: str) -> str:\n    return y\n\n\n@pytest.mark.parametrize(\"tool_\", [InjectedTool()])\ndef test_tool_injected_arg_without_schema(tool_: BaseTool) -> None:\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\\n\\nArgs:\\n    x: abc\\n    y: 123\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\n@pytest.mark.parametrize(\n    \"tool_\",\n    [injected_tool_with_schema, InjectedToolWithSchema()],\n)\ndef test_tool_injected_arg_with_schema(tool_: BaseTool) -> None:\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"fooSchema\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef test_tool_injected_arg() -> None:\n    tool_ = injected_tool\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef test_tool_inherited_injected_arg() -> None:\n    class BarSchema(BaseModel):\n        \"\"\"bar.\"\"\"\n\n        y: Annotated[str, \"foobar comment\", InjectedToolArg()] = Field(\n            ..., description=\"123\"\n        )\n\n    class FooSchema(BarSchema):\n        \"\"\"foo.\"\"\"\n\n        x: int = Field(..., description=\"abc\")\n\n    class InheritedInjectedArgTool(BaseTool):\n        name: str = \"foo\"\n        description: str = \"foo.\"\n        args_schema: type[BaseModel] = FooSchema\n\n        def _run(self, x: int, y: str) -> Any:\n            return y\n\n    tool_ = InheritedInjectedArgTool()\n    assert tool_.get_input_schema().model_json_schema() == {\n        \"title\": \"FooSchema\",  # Matches the title from the provided schema\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"y\", \"x\"],\n    }\n    # Should not include `y` since it's annotated as an injected tool arg\n    assert tool_.tool_call_schema.model_json_schema() == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef _get_parametrized_tools() -> list:\n    def my_tool(x: int, y: str, some_tool: Annotated[Any, InjectedToolArg]) -> str:\n        \"\"\"my_tool.\"\"\"\n        return some_tool\n\n    async def my_async_tool(\n        x: int, y: str, *, some_tool: Annotated[Any, InjectedToolArg]\n    ) -> str:\n        \"\"\"my_tool.\"\"\"\n        return some_tool\n\n    return [my_tool, my_async_tool]\n\n\n@pytest.mark.parametrize(\"tool_\", _get_parametrized_tools())\ndef test_fn_injected_arg_with_schema(tool_: Callable) -> None:\n    assert convert_to_openai_function(tool_) == {\n        \"name\": tool_.__name__,\n        \"description\": \"my_tool.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"x\": {\"type\": \"integer\"},\n                \"y\": {\"type\": \"string\"},\n            },\n            \"required\": [\"x\", \"y\"],\n        },\n    }\n\n\ndef generate_models() -> list[Any]:\n    \"\"\"Generate a list of base models depending on the pydantic version.\"\"\"\n\n    class FooProper(BaseModel):\n        a: int\n        b: str\n\n    return [FooProper]\n\n\ndef generate_backwards_compatible_v1() -> list[Any]:\n    \"\"\"Generate a model with pydantic 2 from the v1 namespace.\"\"\"\n    from pydantic.v1 import BaseModel as BaseModelV1\n\n    class FooV1Namespace(BaseModelV1):\n        a: int\n        b: str\n\n    return [FooV1Namespace]\n\n\n# This generates a list of models that can be used for testing that our APIs\n# behave well with either pydantic 1 proper,\n# pydantic v1 from pydantic 2,\n# or pydantic 2 proper.\nTEST_MODELS = generate_models() + generate_backwards_compatible_v1()\n\n\n@pytest.mark.parametrize(\"pydantic_model\", TEST_MODELS)\ndef test_args_schema_as_pydantic(pydantic_model: Any) -> None:\n    class SomeTool(BaseTool):\n        args_schema: type[pydantic_model] = pydantic_model\n\n        def _run(self, *args: Any, **kwargs: Any) -> str:\n            return \"foo\"\n\n    tool = SomeTool(\n        name=\"some_tool\", description=\"some description\", args_schema=pydantic_model\n    )\n\n    input_schema = tool.get_input_schema()\n    input_json_schema = (\n        input_schema.model_json_schema()\n        if hasattr(input_schema, \"model_json_schema\")\n        else input_schema.schema()\n    )\n    assert input_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n    tool_schema = tool.tool_call_schema\n    tool_json_schema = (\n        tool_schema.model_json_schema()\n        if hasattr(tool_schema, \"model_json_schema\")\n        else tool_schema.schema()\n    )\n    assert tool_json_schema == {\n        \"description\": \"some description\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"some_tool\",\n        \"type\": \"object\",\n    }\n\n\ndef test_args_schema_explicitly_typed() -> None:\n    \"\"\"This should test that one can type the args schema as a pydantic model.\n\n    Please note that this will test using pydantic 2 even though BaseTool\n    is a pydantic 1 model!\n    \"\"\"\n    # Check with whatever pydantic model is passed in and not via v1 namespace\n    from pydantic import BaseModel\n\n    class Foo(BaseModel):\n        a: int\n        b: str\n\n    class SomeTool(BaseTool):\n        # type ignoring here since we're allowing overriding a type\n        # signature of pydantic.v1.BaseModel with pydantic.BaseModel\n        # for pydantic 2!\n        args_schema: type[BaseModel] = Foo  # type: ignore[assignment]\n\n        def _run(self, *args: Any, **kwargs: Any) -> str:\n            return \"foo\"\n\n    tool = SomeTool(name=\"some_tool\", description=\"some description\")\n\n    assert tool.get_input_schema().model_json_schema() == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"Foo\",\n        \"type\": \"object\",\n    }\n\n    assert tool.tool_call_schema.model_json_schema() == {\n        \"description\": \"some description\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"some_tool\",\n        \"type\": \"object\",\n    }\n\n\n@pytest.mark.parametrize(\"pydantic_model\", TEST_MODELS)\ndef test_structured_tool_with_different_pydantic_versions(pydantic_model: Any) -> None:\n    \"\"\"This should test that one can type the args schema as a pydantic model.\"\"\"\n    from langchain_core.tools import StructuredTool\n\n    def foo(a: int, b: str) -> str:\n        \"\"\"Hahaha.\"\"\"\n        return \"foo\"\n\n    foo_tool = StructuredTool.from_function(\n        func=foo,\n        args_schema=pydantic_model,\n    )\n\n    assert foo_tool.invoke({\"a\": 5, \"b\": \"hello\"}) == \"foo\"\n\n    args_schema = foo_tool.args_schema\n    args_json_schema = (\n        args_schema.model_json_schema()\n        if hasattr(args_schema, \"model_json_schema\")\n        else args_schema.schema()\n    )\n    assert args_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n    input_schema = foo_tool.get_input_schema()\n    input_json_schema = (\n        input_schema.model_json_schema()\n        if hasattr(input_schema, \"model_json_schema\")\n        else input_schema.schema()\n    )\n    assert input_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n\nvalid_tool_result_blocks = [\n    \"foo\",\n    {\"type\": \"text\", \"text\": \"foo\"},\n    {\"type\": \"text\", \"blah\": \"foo\"},  # note, only 'type' key is currently checked\n    {\"type\": \"image_url\", \"image_url\": {}},  # openai format\n    {\n        \"type\": \"image\",\n        \"source\": {\n            \"type\": \"base64\",\n            \"media_type\": \"image/jpeg\",\n            \"data\": \"123\",\n        },\n    },  # anthropic format\n    {\"type\": \"json\", \"json\": {}},  # bedrock format\n]\ninvalid_tool_result_blocks = [\n    {\"text\": \"foo\"},  # missing type\n    {\"results\": \"foo\"},  # not content blocks\n]\n\n\n@pytest.mark.parametrize(\n    (\"obj\", \"expected\"),\n    [\n        *([[block, True] for block in valid_tool_result_blocks]),\n        *([[block, False] for block in invalid_tool_result_blocks]),\n    ],\n)\ndef test__is_message_content_block(obj: Any, expected: bool) -> None:\n    assert _is_message_content_block(obj) is expected\n\n\n@pytest.mark.parametrize(\n    (\"obj\", \"expected\"),\n    [\n        [\"foo\", True],\n        [valid_tool_result_blocks, True],\n        [invalid_tool_result_blocks, False],\n    ],\n)\ndef test__is_message_content_type(obj: Any, expected: bool) -> None:\n    assert _is_message_content_type(obj) is expected\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 2, reason=\"Testing pydantic v2.\")\n@pytest.mark.parametrize(\"use_v1_namespace\", [True, False])\ndef test__get_all_basemodel_annotations_v2(use_v1_namespace: bool) -> None:\n    A = TypeVar(\"A\")\n\n    if use_v1_namespace:\n        from pydantic.v1 import BaseModel as BaseModel1\n\n        class ModelA(BaseModel1, Generic[A], extra=\"allow\"):\n            a: A\n\n    else:\n        from pydantic import BaseModel as BaseModel2\n        from pydantic import ConfigDict\n\n        class ModelA(BaseModel2, Generic[A]):  # type: ignore[no-redef]\n            a: A\n            model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"allow\")\n\n    class ModelB(ModelA[str]):\n        b: Annotated[ModelA[dict[str, Any]], \"foo\"]\n\n    class Mixin:\n        def foo(self) -> str:\n            return \"foo\"\n\n    class ModelC(Mixin, ModelB):\n        c: dict\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"], \"c\": dict}\n    actual = get_all_basemodel_annotations(ModelC)\n    assert actual == expected\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"]}\n    actual = get_all_basemodel_annotations(ModelB)\n    assert actual == expected\n\n    expected = {\"a\": Any}\n    actual = get_all_basemodel_annotations(ModelA)\n    assert actual == expected\n\n    expected = {\"a\": int}\n    actual = get_all_basemodel_annotations(ModelA[int])\n    assert actual == expected\n\n    D = TypeVar(\"D\", bound=Union[str, int])\n\n    class ModelD(ModelC, Generic[D]):\n        d: Optional[D]\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[str, int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD)\n    assert actual == expected\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD[int])\n    assert actual == expected\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 1, reason=\"Testing pydantic v1.\")\ndef test__get_all_basemodel_annotations_v1() -> None:\n    A = TypeVar(\"A\")\n\n    class ModelA(BaseModel, Generic[A], extra=\"allow\"):\n        a: A\n\n    class ModelB(ModelA[str]):\n        b: Annotated[ModelA[dict[str, Any]], \"foo\"]\n\n    class Mixin:\n        def foo(self) -> str:\n            return \"foo\"\n\n    class ModelC(Mixin, ModelB):\n        c: dict\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"], \"c\": dict}\n    actual = get_all_basemodel_annotations(ModelC)\n    assert actual == expected\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"]}\n    actual = get_all_basemodel_annotations(ModelB)\n    assert actual == expected\n\n    expected = {\"a\": Any}\n    actual = get_all_basemodel_annotations(ModelA)\n    assert actual == expected\n\n    expected = {\"a\": int}\n    actual = get_all_basemodel_annotations(ModelA[int])\n    assert actual == expected\n\n    D = TypeVar(\"D\", bound=Union[str, int])\n\n    class ModelD(ModelC, Generic[D]):\n        d: Optional[D]\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[str, int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD)\n    assert actual == expected\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD[int])\n    assert actual == expected\n\n\ndef test_tool_annotations_preserved() -> None:\n    \"\"\"Test that annotations are preserved when creating a tool.\"\"\"\n\n    @tool\n    def my_tool(val: int, other_val: Annotated[dict, \"my annotation\"]) -> str:\n        \"\"\"Tool docstring.\"\"\"\n        return \"foo\"\n\n    schema = my_tool.get_input_schema()  # type: ignore[attr-defined]\n\n    func = my_tool.func  # type: ignore[attr-defined]\n\n    expected_type_hints = {\n        name: hint\n        for name, hint in func.__annotations__.items()\n        if name in inspect.signature(func).parameters\n    }\n    assert schema.__annotations__ == expected_type_hints\n\n\ndef test_create_retriever_tool() -> None:\n    class MyRetriever(BaseRetriever):\n        def _get_relevant_documents(\n            self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n        ) -> list[Document]:\n            return [Document(page_content=f\"foo {query}\"), Document(page_content=\"bar\")]\n\n    retriever = MyRetriever()\n    retriever_tool = tools.create_retriever_tool(\n        retriever, \"retriever_tool_content\", \"Retriever Tool Content\"\n    )\n    assert isinstance(retriever_tool, BaseTool)\n    assert retriever_tool.name == \"retriever_tool_content\"\n    assert retriever_tool.description == \"Retriever Tool Content\"\n    assert retriever_tool.invoke(\"bar\") == \"foo bar\\n\\nbar\"\n    assert retriever_tool.invoke(\n        ToolCall(\n            name=\"retriever_tool_content\",\n            args={\"query\": \"bar\"},\n            id=\"123\",\n            type=\"tool_call\",\n        )\n    ) == ToolMessage(\n        \"foo bar\\n\\nbar\", tool_call_id=\"123\", name=\"retriever_tool_content\"\n    )\n\n    retriever_tool_artifact = tools.create_retriever_tool(\n        retriever,\n        \"retriever_tool_artifact\",\n        \"Retriever Tool Artifact\",\n        response_format=\"content_and_artifact\",\n    )\n    assert isinstance(retriever_tool_artifact, BaseTool)\n    assert retriever_tool_artifact.name == \"retriever_tool_artifact\"\n    assert retriever_tool_artifact.description == \"Retriever Tool Artifact\"\n    assert retriever_tool_artifact.invoke(\"bar\") == \"foo bar\\n\\nbar\"\n    assert retriever_tool_artifact.invoke(\n        ToolCall(\n            name=\"retriever_tool_artifact\",\n            args={\"query\": \"bar\"},\n            id=\"123\",\n            type=\"tool_call\",\n        )\n    ) == ToolMessage(\n        \"foo bar\\n\\nbar\",\n        artifact=[Document(page_content=\"foo bar\"), Document(page_content=\"bar\")],\n        tool_call_id=\"123\",\n        name=\"retriever_tool_artifact\",\n    )\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 2, reason=\"Testing pydantic v2.\")\ndef test_tool_args_schema_pydantic_v2_with_metadata() -> None:\n    from pydantic import BaseModel as BaseModelV2\n    from pydantic import Field as FieldV2\n    from pydantic import ValidationError as ValidationErrorV2\n\n    class Foo(BaseModelV2):\n        x: list[int] = FieldV2(\n            description=\"List of integers\", min_length=10, max_length=15\n        )\n\n    @tool(args_schema=Foo)\n    def foo(x):  # type: ignore[no-untyped-def]\n        \"\"\"Foo.\"\"\"\n        return x\n\n    assert foo.tool_call_schema.model_json_schema() == {\n        \"description\": \"Foo.\",\n        \"properties\": {\n            \"x\": {\n                \"description\": \"List of integers\",\n                \"items\": {\"type\": \"integer\"},\n                \"maxItems\": 15,\n                \"minItems\": 10,\n                \"title\": \"X\",\n                \"type\": \"array\",\n            }\n        },\n        \"required\": [\"x\"],\n        \"title\": \"foo\",\n        \"type\": \"object\",\n    }\n\n    assert foo.invoke({\"x\": [0] * 10})\n    with pytest.raises(ValidationErrorV2):\n        foo.invoke({\"x\": [0] * 9})\n\n\ndef test_imports() -> None:\n    expected_all = [\n        \"FILTERED_ARGS\",\n        \"SchemaAnnotationError\",\n        \"create_schema_from_function\",\n        \"ToolException\",\n        \"BaseTool\",\n        \"Tool\",\n        \"StructuredTool\",\n        \"tool\",\n        \"RetrieverInput\",\n        \"create_retriever_tool\",\n        \"ToolsRenderer\",\n        \"render_text_description\",\n        \"render_text_description_and_args\",\n        \"BaseToolkit\",\n        \"convert_runnable_to_tool\",\n        \"InjectedToolArg\",\n    ]\n    for module_name in expected_all:\n        assert hasattr(tools, module_name) and getattr(tools, module_name) is not None\n\n\ndef test_structured_tool_direct_init() -> None:\n    def foo(bar: str) -> str:\n        return bar\n\n    async def async_foo(bar: str) -> str:\n        return bar\n\n    class FooSchema(BaseModel):\n        bar: str = Field(..., description=\"The bar\")\n\n    tool = StructuredTool(name=\"foo\", args_schema=FooSchema, coroutine=async_foo)\n\n    with pytest.raises(NotImplementedError):\n        assert tool.invoke(\"hello\") == \"hello\"\n\n\ndef test_injected_arg_with_complex_type() -> None:\n    \"\"\"Test that an injected tool arg can be a complex type.\"\"\"\n\n    class Foo:\n        def __init__(self) -> None:\n            self.value = \"bar\"\n\n    @tool\n    def injected_tool(x: int, foo: Annotated[Foo, InjectedToolArg]) -> str:\n        \"\"\"Tool that has an injected tool arg.\"\"\"\n        return foo.value\n\n    assert injected_tool.invoke({\"x\": 5, \"foo\": Foo()}) == \"bar\"  # type: ignore\n\n\ndef test_tool_injected_tool_call_id() -> None:\n    @tool\n    def foo(x: int, tool_call_id: Annotated[str, InjectedToolCallId]) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"bar\")  # type: ignore\n\n    with pytest.raises(ValueError):\n        assert foo.invoke({\"x\": 0})\n\n    @tool\n    def foo2(x: int, tool_call_id: Annotated[str, InjectedToolCallId()]) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    assert foo2.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"bar\")  # type: ignore\n\n\ndef test_tool_uninjected_tool_call_id() -> None:\n    @tool\n    def foo(x: int, tool_call_id: str) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    with pytest.raises(ValueError):\n        foo.invoke({\"type\": \"tool_call\", \"args\": {\"x\": 0}, \"name\": \"foo\", \"id\": \"bar\"})\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0, \"tool_call_id\": \"zap\"},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"zap\")  # type: ignore\n\n\ndef test_tool_return_output_mixin() -> None:\n    class Bar(ToolOutputMixin):\n        def __init__(self, x: int) -> None:\n            self.x = x\n\n        def __eq__(self, other: Any) -> bool:\n            return isinstance(other, self.__class__) and self.x == other.x\n\n    @tool\n    def foo(x: int) -> Bar:\n        \"\"\"Foo.\"\"\"\n        return Bar(x=x)\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == Bar(x=0)\n\n\ndef test_tool_mutate_input() -> None:\n    class MyTool(BaseTool):\n        name: str = \"MyTool\"\n        description: str = \"a tool\"\n\n        def _run(\n            self,\n            x: str,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n        ) -> str:\n            return \"hi\"\n\n    my_input = {\"x\": \"hi\"}\n    MyTool().invoke(my_input)\n    assert my_input == {\"x\": \"hi\"}\n",
        "patch": "@@ -2206,7 +2206,7 @@ class Foo(BaseModelV2):\n         )\n \n     @tool(args_schema=Foo)\n-    def foo(x):  # type: ignore[no-untyped-def]\n+    def foo(x) -> list[int]:  # type: ignore[no-untyped-def] # noqa: ANN001\n         \"\"\"Foo.\"\"\"\n         return x\n "
      }
    ]
  },
  {
    "number": 28454,
    "title": "core[patch]: dont deep copy merge_message_runs",
    "body": "afaict no need to deep copy here, if we merge messages then we convert them to chunks first anyways",
    "issue_title": "core[patch]: dont deep copy merge_message_runs",
    "issue_body": "afaict no need to deep copy here, if we merge messages then we convert them to chunks first anyways",
    "files": [
      {
        "filename": "libs/core/langchain_core/messages/utils.py",
        "content_before": "\"\"\"Module contains utility functions for working with messages.\n\nSome examples of what you can do with these functions include:\n\n* Convert messages to strings (serialization)\n* Convert messages from dicts to Message objects (deserialization)\n* Filter messages from a list of messages based on name, type or id etc.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport base64\nimport inspect\nimport json\nfrom collections.abc import Iterable, Sequence\nfrom functools import partial\nfrom typing import (\n    TYPE_CHECKING,\n    Annotated,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    Union,\n    cast,\n    overload,\n)\n\nfrom pydantic import Discriminator, Field, Tag\n\nfrom langchain_core.exceptions import ErrorCode, create_message\nfrom langchain_core.messages.ai import AIMessage, AIMessageChunk\nfrom langchain_core.messages.base import BaseMessage, BaseMessageChunk\nfrom langchain_core.messages.chat import ChatMessage, ChatMessageChunk\nfrom langchain_core.messages.function import FunctionMessage, FunctionMessageChunk\nfrom langchain_core.messages.human import HumanMessage, HumanMessageChunk\nfrom langchain_core.messages.modifier import RemoveMessage\nfrom langchain_core.messages.system import SystemMessage, SystemMessageChunk\nfrom langchain_core.messages.tool import ToolCall, ToolMessage, ToolMessageChunk\n\nif TYPE_CHECKING:\n    from langchain_text_splitters import TextSplitter\n\n    from langchain_core.language_models import BaseLanguageModel\n    from langchain_core.prompt_values import PromptValue\n    from langchain_core.runnables.base import Runnable\n\n\ndef _get_type(v: Any) -> str:\n    \"\"\"Get the type associated with the object for serialization purposes.\"\"\"\n    if isinstance(v, dict) and \"type\" in v:\n        return v[\"type\"]\n    elif hasattr(v, \"type\"):\n        return v.type\n    else:\n        msg = (\n            f\"Expected either a dictionary with a 'type' key or an object \"\n            f\"with a 'type' attribute. Instead got type {type(v)}.\"\n        )\n        raise TypeError(msg)\n\n\nAnyMessage = Annotated[\n    Union[\n        Annotated[AIMessage, Tag(tag=\"ai\")],\n        Annotated[HumanMessage, Tag(tag=\"human\")],\n        Annotated[ChatMessage, Tag(tag=\"chat\")],\n        Annotated[SystemMessage, Tag(tag=\"system\")],\n        Annotated[FunctionMessage, Tag(tag=\"function\")],\n        Annotated[ToolMessage, Tag(tag=\"tool\")],\n        Annotated[AIMessageChunk, Tag(tag=\"AIMessageChunk\")],\n        Annotated[HumanMessageChunk, Tag(tag=\"HumanMessageChunk\")],\n        Annotated[ChatMessageChunk, Tag(tag=\"ChatMessageChunk\")],\n        Annotated[SystemMessageChunk, Tag(tag=\"SystemMessageChunk\")],\n        Annotated[FunctionMessageChunk, Tag(tag=\"FunctionMessageChunk\")],\n        Annotated[ToolMessageChunk, Tag(tag=\"ToolMessageChunk\")],\n    ],\n    Field(discriminator=Discriminator(_get_type)),\n]\n\n\ndef get_buffer_string(\n    messages: Sequence[BaseMessage], human_prefix: str = \"Human\", ai_prefix: str = \"AI\"\n) -> str:\n    \"\"\"Convert a sequence of Messages to strings and concatenate them into one string.\n\n    Args:\n        messages: Messages to be converted to strings.\n        human_prefix: The prefix to prepend to contents of HumanMessages.\n            Default is \"Human\".\n        ai_prefix: THe prefix to prepend to contents of AIMessages. Default is \"AI\".\n\n    Returns:\n        A single string concatenation of all input messages.\n\n    Raises:\n        ValueError: If an unsupported message type is encountered.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_core import AIMessage, HumanMessage\n\n            messages = [\n                HumanMessage(content=\"Hi, how are you?\"),\n                AIMessage(content=\"Good, how are you?\"),\n            ]\n            get_buffer_string(messages)\n            # -> \"Human: Hi, how are you?\\nAI: Good, how are you?\"\n    \"\"\"\n    string_messages = []\n    for m in messages:\n        if isinstance(m, HumanMessage):\n            role = human_prefix\n        elif isinstance(m, AIMessage):\n            role = ai_prefix\n        elif isinstance(m, SystemMessage):\n            role = \"System\"\n        elif isinstance(m, FunctionMessage):\n            role = \"Function\"\n        elif isinstance(m, ToolMessage):\n            role = \"Tool\"\n        elif isinstance(m, ChatMessage):\n            role = m.role\n        else:\n            msg = f\"Got unsupported message type: {m}\"\n            raise ValueError(msg)  # noqa: TRY004\n        message = f\"{role}: {m.content}\"\n        if isinstance(m, AIMessage) and \"function_call\" in m.additional_kwargs:\n            message += f\"{m.additional_kwargs['function_call']}\"\n        string_messages.append(message)\n\n    return \"\\n\".join(string_messages)\n\n\ndef _message_from_dict(message: dict) -> BaseMessage:\n    _type = message[\"type\"]\n    if _type == \"human\":\n        return HumanMessage(**message[\"data\"])\n    elif _type == \"ai\":\n        return AIMessage(**message[\"data\"])\n    elif _type == \"system\":\n        return SystemMessage(**message[\"data\"])\n    elif _type == \"chat\":\n        return ChatMessage(**message[\"data\"])\n    elif _type == \"function\":\n        return FunctionMessage(**message[\"data\"])\n    elif _type == \"tool\":\n        return ToolMessage(**message[\"data\"])\n    elif _type == \"remove\":\n        return RemoveMessage(**message[\"data\"])\n    elif _type == \"AIMessageChunk\":\n        return AIMessageChunk(**message[\"data\"])\n    elif _type == \"HumanMessageChunk\":\n        return HumanMessageChunk(**message[\"data\"])\n    elif _type == \"FunctionMessageChunk\":\n        return FunctionMessageChunk(**message[\"data\"])\n    elif _type == \"ToolMessageChunk\":\n        return ToolMessageChunk(**message[\"data\"])\n    elif _type == \"SystemMessageChunk\":\n        return SystemMessageChunk(**message[\"data\"])\n    elif _type == \"ChatMessageChunk\":\n        return ChatMessageChunk(**message[\"data\"])\n    else:\n        msg = f\"Got unexpected message type: {_type}\"\n        raise ValueError(msg)\n\n\ndef messages_from_dict(messages: Sequence[dict]) -> list[BaseMessage]:\n    \"\"\"Convert a sequence of messages from dicts to Message objects.\n\n    Args:\n        messages: Sequence of messages (as dicts) to convert.\n\n    Returns:\n        list of messages (BaseMessages).\n    \"\"\"\n    return [_message_from_dict(m) for m in messages]\n\n\ndef message_chunk_to_message(chunk: BaseMessageChunk) -> BaseMessage:\n    \"\"\"Convert a message chunk to a message.\n\n    Args:\n        chunk: Message chunk to convert.\n\n    Returns:\n        Message.\n    \"\"\"\n    if not isinstance(chunk, BaseMessageChunk):\n        return chunk\n    # chunk classes always have the equivalent non-chunk class as their first parent\n    ignore_keys = [\"type\"]\n    if isinstance(chunk, AIMessageChunk):\n        ignore_keys.append(\"tool_call_chunks\")\n    return chunk.__class__.__mro__[1](\n        **{k: v for k, v in chunk.__dict__.items() if k not in ignore_keys}\n    )\n\n\nMessageLikeRepresentation = Union[\n    BaseMessage, list[str], tuple[str, str], str, dict[str, Any]\n]\n\n\ndef _create_message_from_message_type(\n    message_type: str,\n    content: str,\n    name: Optional[str] = None,\n    tool_call_id: Optional[str] = None,\n    tool_calls: Optional[list[dict[str, Any]]] = None,\n    id: Optional[str] = None,\n    **additional_kwargs: Any,\n) -> BaseMessage:\n    \"\"\"Create a message from a message type and content string.\n\n    Args:\n        message_type: (str) the type of the message (e.g., \"human\", \"ai\", etc.).\n        content: (str) the content string.\n        name: (str) the name of the message. Default is None.\n        tool_call_id: (str) the tool call id. Default is None.\n        tool_calls: (list[dict[str, Any]]) the tool calls. Default is None.\n        id: (str) the id of the message. Default is None.\n        additional_kwargs: (dict[str, Any]) additional keyword arguments.\n\n    Returns:\n        a message of the appropriate type.\n\n    Raises:\n        ValueError: if the message type is not one of \"human\", \"user\", \"ai\",\n            \"assistant\", \"function\", \"tool\", \"system\", or \"developer\".\n    \"\"\"\n    kwargs: dict[str, Any] = {}\n    if name is not None:\n        kwargs[\"name\"] = name\n    if tool_call_id is not None:\n        kwargs[\"tool_call_id\"] = tool_call_id\n    if additional_kwargs:\n        if response_metadata := additional_kwargs.pop(\"response_metadata\", None):\n            kwargs[\"response_metadata\"] = response_metadata\n        kwargs[\"additional_kwargs\"] = additional_kwargs  # type: ignore[assignment]\n        additional_kwargs.update(additional_kwargs.pop(\"additional_kwargs\", {}))\n    if id is not None:\n        kwargs[\"id\"] = id\n    if tool_calls is not None:\n        kwargs[\"tool_calls\"] = []\n        for tool_call in tool_calls:\n            # Convert OpenAI-format tool call to LangChain format.\n            if \"function\" in tool_call:\n                args = tool_call[\"function\"][\"arguments\"]\n                if isinstance(args, str):\n                    args = json.loads(args, strict=False)\n                kwargs[\"tool_calls\"].append(\n                    {\n                        \"name\": tool_call[\"function\"][\"name\"],\n                        \"args\": args,\n                        \"id\": tool_call[\"id\"],\n                        \"type\": \"tool_call\",\n                    }\n                )\n            else:\n                kwargs[\"tool_calls\"].append(tool_call)\n    if message_type in (\"human\", \"user\"):\n        if example := kwargs.get(\"additional_kwargs\", {}).pop(\"example\", False):\n            kwargs[\"example\"] = example\n        message: BaseMessage = HumanMessage(content=content, **kwargs)\n    elif message_type in (\"ai\", \"assistant\"):\n        if example := kwargs.get(\"additional_kwargs\", {}).pop(\"example\", False):\n            kwargs[\"example\"] = example\n        message = AIMessage(content=content, **kwargs)\n    elif message_type in (\"system\", \"developer\"):\n        if message_type == \"developer\":\n            kwargs[\"additional_kwargs\"] = kwargs.get(\"additional_kwargs\") or {}\n            kwargs[\"additional_kwargs\"][\"__openai_role__\"] = \"developer\"\n        message = SystemMessage(content=content, **kwargs)\n    elif message_type == \"function\":\n        message = FunctionMessage(content=content, **kwargs)\n    elif message_type == \"tool\":\n        artifact = kwargs.get(\"additional_kwargs\", {}).pop(\"artifact\", None)\n        message = ToolMessage(content=content, artifact=artifact, **kwargs)\n    elif message_type == \"remove\":\n        message = RemoveMessage(**kwargs)\n    else:\n        msg = (\n            f\"Unexpected message type: '{message_type}'. Use one of 'human',\"\n            f\" 'user', 'ai', 'assistant', 'function', 'tool', 'system', or 'developer'.\"\n        )\n        msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)\n        raise ValueError(msg)\n    return message\n\n\ndef _convert_to_message(message: MessageLikeRepresentation) -> BaseMessage:\n    \"\"\"Instantiate a message from a variety of message formats.\n\n    The message format can be one of the following:\n\n    - BaseMessagePromptTemplate\n    - BaseMessage\n    - 2-tuple of (role string, template); e.g., (\"human\", \"{user_input}\")\n    - dict: a message dict with role and content keys\n    - string: shorthand for (\"human\", template); e.g., \"{user_input}\"\n\n    Args:\n        message: a representation of a message in one of the supported formats.\n\n    Returns:\n        an instance of a message or a message template.\n\n    Raises:\n        NotImplementedError: if the message type is not supported.\n        ValueError: if the message dict does not contain the required keys.\n    \"\"\"\n    if isinstance(message, BaseMessage):\n        _message = message\n    elif isinstance(message, str):\n        _message = _create_message_from_message_type(\"human\", message)\n    elif isinstance(message, Sequence) and len(message) == 2:\n        # mypy doesn't realise this can't be a string given the previous branch\n        message_type_str, template = message  # type: ignore[misc]\n        _message = _create_message_from_message_type(message_type_str, template)\n    elif isinstance(message, dict):\n        msg_kwargs = message.copy()\n        try:\n            try:\n                msg_type = msg_kwargs.pop(\"role\")\n            except KeyError:\n                msg_type = msg_kwargs.pop(\"type\")\n            # None msg content is not allowed\n            msg_content = msg_kwargs.pop(\"content\") or \"\"\n        except KeyError as e:\n            msg = f\"Message dict must contain 'role' and 'content' keys, got {message}\"\n            msg = create_message(\n                message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE\n            )\n            raise ValueError(msg) from e\n        _message = _create_message_from_message_type(\n            msg_type, msg_content, **msg_kwargs\n        )\n    else:\n        msg = f\"Unsupported message type: {type(message)}\"\n        msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)\n        raise NotImplementedError(msg)\n\n    return _message\n\n\ndef convert_to_messages(\n    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],\n) -> list[BaseMessage]:\n    \"\"\"Convert a sequence of messages to a list of messages.\n\n    Args:\n        messages: Sequence of messages to convert.\n\n    Returns:\n        list of messages (BaseMessages).\n    \"\"\"\n    # Import here to avoid circular imports\n    from langchain_core.prompt_values import PromptValue\n\n    if isinstance(messages, PromptValue):\n        return messages.to_messages()\n    return [_convert_to_message(m) for m in messages]\n\n\ndef _runnable_support(func: Callable) -> Callable:\n    @overload\n    def wrapped(\n        messages: Literal[None] = None, **kwargs: Any\n    ) -> Runnable[Sequence[MessageLikeRepresentation], list[BaseMessage]]: ...\n\n    @overload\n    def wrapped(\n        messages: Sequence[MessageLikeRepresentation], **kwargs: Any\n    ) -> list[BaseMessage]: ...\n\n    def wrapped(\n        messages: Union[Sequence[MessageLikeRepresentation], None] = None,\n        **kwargs: Any,\n    ) -> Union[\n        list[BaseMessage],\n        Runnable[Sequence[MessageLikeRepresentation], list[BaseMessage]],\n    ]:\n        from langchain_core.runnables.base import RunnableLambda\n\n        if messages is not None:\n            return func(messages, **kwargs)\n        else:\n            return RunnableLambda(partial(func, **kwargs), name=func.__name__)\n\n    wrapped.__doc__ = func.__doc__\n    return wrapped\n\n\n@_runnable_support\ndef filter_messages(\n    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],\n    *,\n    include_names: Optional[Sequence[str]] = None,\n    exclude_names: Optional[Sequence[str]] = None,\n    include_types: Optional[Sequence[Union[str, type[BaseMessage]]]] = None,\n    exclude_types: Optional[Sequence[Union[str, type[BaseMessage]]]] = None,\n    include_ids: Optional[Sequence[str]] = None,\n    exclude_ids: Optional[Sequence[str]] = None,\n) -> list[BaseMessage]:\n    \"\"\"Filter messages based on name, type or id.\n\n    Args:\n        messages: Sequence Message-like objects to filter.\n        include_names: Message names to include. Default is None.\n        exclude_names: Messages names to exclude. Default is None.\n        include_types: Message types to include. Can be specified as string names (e.g.\n            \"system\", \"human\", \"ai\", ...) or as BaseMessage classes (e.g.\n            SystemMessage, HumanMessage, AIMessage, ...). Default is None.\n        exclude_types: Message types to exclude. Can be specified as string names (e.g.\n            \"system\", \"human\", \"ai\", ...) or as BaseMessage classes (e.g.\n            SystemMessage, HumanMessage, AIMessage, ...). Default is None.\n        include_ids: Message IDs to include. Default is None.\n        exclude_ids: Message IDs to exclude. Default is None.\n\n    Returns:\n        A list of Messages that meets at least one of the incl_* conditions and none\n        of the excl_* conditions. If not incl_* conditions are specified then\n        anything that is not explicitly excluded will be included.\n\n    Raises:\n        ValueError if two incompatible arguments are provided.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_core.messages import filter_messages, AIMessage, HumanMessage, SystemMessage\n\n            messages = [\n                SystemMessage(\"you're a good assistant.\"),\n                HumanMessage(\"what's your name\", id=\"foo\", name=\"example_user\"),\n                AIMessage(\"steve-o\", id=\"bar\", name=\"example_assistant\"),\n                HumanMessage(\"what's your favorite color\", id=\"baz\",),\n                AIMessage(\"silicon blue\", id=\"blah\",),\n            ]\n\n            filter_messages(\n                messages,\n                incl_names=(\"example_user\", \"example_assistant\"),\n                incl_types=(\"system\",),\n                excl_ids=(\"bar\",),\n            )\n\n        .. code-block:: python\n\n            [\n                SystemMessage(\"you're a good assistant.\"),\n                HumanMessage(\"what's your name\", id=\"foo\", name=\"example_user\"),\n            ]\n    \"\"\"  # noqa: E501\n    messages = convert_to_messages(messages)\n    filtered: list[BaseMessage] = []\n    for msg in messages:\n        if (\n            (exclude_names and msg.name in exclude_names)\n            or (exclude_types and _is_message_type(msg, exclude_types))\n            or (exclude_ids and msg.id in exclude_ids)\n        ):\n            continue\n        else:\n            pass\n\n        # default to inclusion when no inclusion criteria given.\n        if (\n            not (include_types or include_ids or include_names)\n            or (include_names and msg.name in include_names)\n            or (include_types and _is_message_type(msg, include_types))\n            or (include_ids and msg.id in include_ids)\n        ):\n            filtered.append(msg)\n        else:\n            pass\n\n    return filtered\n\n\n@_runnable_support\ndef merge_message_runs(\n    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],\n    *,\n    chunk_separator: str = \"\\n\",\n) -> list[BaseMessage]:\n    \"\"\"Merge consecutive Messages of the same type.\n\n    **NOTE**: ToolMessages are not merged, as each has a distinct tool call id that\n    can't be merged.\n\n    Args:\n        messages: Sequence Message-like objects to merge.\n        chunk_separator: Specify the string to be inserted between message chunks.\n        Default is \"\\n\".\n\n    Returns:\n        list of BaseMessages with consecutive runs of message types merged into single\n        messages. By default, if two messages being merged both have string contents,\n        the merged content is a concatenation of the two strings with a new-line separator.\n        The separator inserted between message chunks can be controlled by specifying\n        any string with ``chunk_separator``. If at least one of the messages has a list of\n        content blocks, the merged content is a list of content blocks.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_core.messages import (\n                merge_message_runs,\n                AIMessage,\n                HumanMessage,\n                SystemMessage,\n                ToolCall,\n            )\n\n            messages = [\n                SystemMessage(\"you're a good assistant.\"),\n                HumanMessage(\"what's your favorite color\", id=\"foo\",),\n                HumanMessage(\"wait your favorite food\", id=\"bar\",),\n                AIMessage(\n                    \"my favorite colo\",\n                    tool_calls=[ToolCall(name=\"blah_tool\", args={\"x\": 2}, id=\"123\", type=\"tool_call\")],\n                    id=\"baz\",\n                ),\n                AIMessage(\n                    [{\"type\": \"text\", \"text\": \"my favorite dish is lasagna\"}],\n                    tool_calls=[ToolCall(name=\"blah_tool\", args={\"x\": -10}, id=\"456\", type=\"tool_call\")],\n                    id=\"blur\",\n                ),\n            ]\n\n            merge_message_runs(messages)\n\n        .. code-block:: python\n\n            [\n                SystemMessage(\"you're a good assistant.\"),\n                HumanMessage(\"what's your favorite color\\\\nwait your favorite food\", id=\"foo\",),\n                AIMessage(\n                    [\n                        \"my favorite colo\",\n                        {\"type\": \"text\", \"text\": \"my favorite dish is lasagna\"}\n                    ],\n                    tool_calls=[\n                        ToolCall({\"name\": \"blah_tool\", \"args\": {\"x\": 2}, \"id\": \"123\", \"type\": \"tool_call\"}),\n                        ToolCall({\"name\": \"blah_tool\", \"args\": {\"x\": -10}, \"id\": \"456\", \"type\": \"tool_call\"})\n                    ]\n                    id=\"baz\"\n                ),\n            ]\n\n    \"\"\"  # noqa: E501\n    if not messages:\n        return []\n    messages = convert_to_messages(messages)\n    merged: list[BaseMessage] = []\n    for msg in messages:\n        curr = msg.model_copy(deep=True)\n        last = merged.pop() if merged else None\n        if not last:\n            merged.append(curr)\n        elif isinstance(curr, ToolMessage) or not isinstance(curr, last.__class__):\n            merged.extend([last, curr])\n        else:\n            last_chunk = _msg_to_chunk(last)\n            curr_chunk = _msg_to_chunk(curr)\n            if curr_chunk.response_metadata:\n                curr_chunk.response_metadata.clear()\n            if (\n                isinstance(last_chunk.content, str)\n                and isinstance(curr_chunk.content, str)\n                and last_chunk.content\n                and curr_chunk.content\n            ):\n                last_chunk.content += chunk_separator\n            merged.append(_chunk_to_msg(last_chunk + curr_chunk))\n    return merged\n\n\n# TODO: Update so validation errors (for token_counter, for example) are raised on\n# init not at runtime.\n@_runnable_support\ndef trim_messages(\n    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],\n    *,\n    max_tokens: int,\n    token_counter: Union[\n        Callable[[list[BaseMessage]], int],\n        Callable[[BaseMessage], int],\n        BaseLanguageModel,\n    ],\n    strategy: Literal[\"first\", \"last\"] = \"last\",\n    allow_partial: bool = False,\n    end_on: Optional[\n        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]\n    ] = None,\n    start_on: Optional[\n        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]\n    ] = None,\n    include_system: bool = False,\n    text_splitter: Optional[Union[Callable[[str], list[str]], TextSplitter]] = None,\n) -> list[BaseMessage]:\n    r\"\"\"Trim messages to be below a token count.\n\n    trim_messages can be used to reduce the size of a chat history to a specified token\n    count or specified message count.\n\n    In either case, if passing the trimmed chat history back into a chat model\n    directly, the resulting chat history should usually satisfy the following\n    properties:\n\n    1. The resulting chat history should be valid. Most chat models expect that chat\n       history starts with either (1) a `HumanMessage` or (2) a `SystemMessage` followed\n       by a `HumanMessage`. To achieve this, set `start_on=\"human\"`.\n       In addition, generally a `ToolMessage` can only appear after an `AIMessage`\n       that involved a tool call.\n       Please see the following link for more information about messages:\n       https://python.langchain.com/docs/concepts/#messages\n    2. It includes recent messages and drops old messages in the chat history.\n       To achieve this set the `strategy=\"last\"`.\n    3. Usually, the new chat history should include the `SystemMessage` if it\n       was present in the original chat history since the `SystemMessage` includes\n       special instructions to the chat model. The `SystemMessage` is almost always\n       the first message in the history if present. To achieve this set the\n       `include_system=True`.\n\n    **Note** The examples below show how to configure `trim_messages` to achieve\n        a behavior consistent with the above properties.\n\n    Args:\n        messages: Sequence of Message-like objects to trim.\n        max_tokens: Max token count of trimmed messages.\n        token_counter: Function or llm for counting tokens in a BaseMessage or a list of\n            BaseMessage. If a BaseLanguageModel is passed in then\n            BaseLanguageModel.get_num_tokens_from_messages() will be used.\n            Set to `len` to count the number of **messages** in the chat history.\n        strategy: Strategy for trimming.\n            - \"first\": Keep the first <= n_count tokens of the messages.\n            - \"last\": Keep the last <= n_count tokens of the messages.\n            Default is \"last\".\n        allow_partial: Whether to split a message if only part of the message can be\n            included. If ``strategy=\"last\"`` then the last partial contents of a message\n            are included. If ``strategy=\"first\"`` then the first partial contents of a\n            message are included.\n            Default is False.\n        end_on: The message type to end on. If specified then every message after the\n            last occurrence of this type is ignored. If ``strategy==\"last\"`` then this\n            is done before we attempt to get the last ``max_tokens``. If\n            ``strategy==\"first\"`` then this is done after we get the first\n            ``max_tokens``. Can be specified as string names (e.g. \"system\", \"human\",\n            \"ai\", ...) or as BaseMessage classes (e.g. SystemMessage, HumanMessage,\n            AIMessage, ...). Can be a single type or a list of types.\n            Default is None.\n        start_on: The message type to start on. Should only be specified if\n            ``strategy=\"last\"``. If specified then every message before\n            the first occurrence of this type is ignored. This is done after we trim\n            the initial messages to the last ``max_tokens``. Does not\n            apply to a SystemMessage at index 0 if ``include_system=True``. Can be\n            specified as string names (e.g. \"system\", \"human\", \"ai\", ...) or as\n            BaseMessage classes (e.g. SystemMessage, HumanMessage, AIMessage, ...). Can\n            be a single type or a list of types.\n            Default is None.\n        include_system: Whether to keep the SystemMessage if there is one at index 0.\n            Should only be specified if ``strategy=\"last\"``.\n            Default is False.\n        text_splitter: Function or ``langchain_text_splitters.TextSplitter`` for\n            splitting the string contents of a message. Only used if\n            ``allow_partial=True``. If ``strategy=\"last\"`` then the last split tokens\n            from a partial message will be included. if ``strategy==\"first\"`` then the\n            first split tokens from a partial message will be included. Token splitter\n            assumes that separators are kept, so that split contents can be directly\n            concatenated to recreate the original text. Defaults to splitting on\n            newlines.\n\n    Returns:\n        list of trimmed BaseMessages.\n\n    Raises:\n        ValueError: if two incompatible arguments are specified or an unrecognized\n            ``strategy`` is specified.\n\n    Example:\n        Trim chat history based on token count, keeping the SystemMessage if\n        present, and ensuring that the chat history starts with a HumanMessage (\n        or a SystemMessage followed by a HumanMessage).\n\n        .. code-block:: python\n\n            from typing import list\n\n            from langchain_core.messages import (\n                AIMessage,\n                HumanMessage,\n                BaseMessage,\n                SystemMessage,\n                trim_messages,\n            )\n\n            messages = [\n                SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n                HumanMessage(\"i wonder why it's called langchain\"),\n                AIMessage(\n                    'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n                ),\n                HumanMessage(\"and who is harrison chasing anyways\"),\n                AIMessage(\n                    \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n                ),\n                HumanMessage(\"what do you call a speechless parrot\"),\n            ]\n\n\n            trim_messages(\n                messages,\n                max_tokens=45,\n                strategy=\"last\",\n                token_counter=ChatOpenAI(model=\"gpt-4o\"),\n                # Most chat models expect that chat history starts with either:\n                # (1) a HumanMessage or\n                # (2) a SystemMessage followed by a HumanMessage\n                start_on=\"human\",\n                # Usually, we want to keep the SystemMessage\n                # if it's present in the original history.\n                # The SystemMessage has special instructions for the model.\n                include_system=True,\n                allow_partial=False,\n            )\n\n        .. code-block:: python\n\n            [\n                SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n                HumanMessage(content='what do you call a speechless parrot'),\n            ]\n\n        Trim chat history based on the message count, keeping the SystemMessage if\n        present, and ensuring that the chat history starts with a HumanMessage (\n        or a SystemMessage followed by a HumanMessage).\n\n            trim_messages(\n                messages,\n                # When `len` is passed in as the token counter function,\n                # max_tokens will count the number of messages in the chat history.\n                max_tokens=4,\n                strategy=\"last\",\n                # Passing in `len` as a token counter function will\n                # count the number of messages in the chat history.\n                token_counter=len,\n                # Most chat models expect that chat history starts with either:\n                # (1) a HumanMessage or\n                # (2) a SystemMessage followed by a HumanMessage\n                start_on=\"human\",\n                # Usually, we want to keep the SystemMessage\n                # if it's present in the original history.\n                # The SystemMessage has special instructions for the model.\n                include_system=True,\n                allow_partial=False,\n            )\n\n        .. code-block:: python\n\n            [\n                SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n                HumanMessage(content='and who is harrison chasing anyways'),\n                AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n                HumanMessage(content='what do you call a speechless parrot'),\n            ]\n\n\n        Trim chat history using a custom token counter function that counts the\n        number of tokens in each message.\n\n        .. code-block:: python\n\n            messages = [\n                SystemMessage(\"This is a 4 token text. The full message is 10 tokens.\"),\n                HumanMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"first\"),\n                AIMessage(\n                    [\n                        {\"type\": \"text\", \"text\": \"This is the FIRST 4 token block.\"},\n                        {\"type\": \"text\", \"text\": \"This is the SECOND 4 token block.\"},\n                    ],\n                    id=\"second\",\n                ),\n                HumanMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"third\"),\n                AIMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"fourth\"),\n            ]\n\n            def dummy_token_counter(messages: list[BaseMessage]) -> int:\n                # treat each message like it adds 3 default tokens at the beginning\n                # of the message and at the end of the message. 3 + 4 + 3 = 10 tokens\n                # per message.\n\n                default_content_len = 4\n                default_msg_prefix_len = 3\n                default_msg_suffix_len = 3\n\n                count = 0\n                for msg in messages:\n                    if isinstance(msg.content, str):\n                        count += default_msg_prefix_len + default_content_len + default_msg_suffix_len\n                    if isinstance(msg.content, list):\n                        count += default_msg_prefix_len + len(msg.content) *  default_content_len + default_msg_suffix_len\n                return count\n\n        First 30 tokens, allowing partial messages:\n            .. code-block:: python\n\n                trim_messages(\n                    messages,\n                    max_tokens=30,\n                    token_counter=dummy_token_counter,\n                    strategy=\"first\",\n                    allow_partial=True,\n                )\n\n            .. code-block:: python\n\n                [\n                    SystemMessage(\"This is a 4 token text. The full message is 10 tokens.\"),\n                    HumanMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"first\"),\n                    AIMessage( [{\"type\": \"text\", \"text\": \"This is the FIRST 4 token block.\"}], id=\"second\"),\n                ]\n    \"\"\"  # noqa: E501\n    if start_on and strategy == \"first\":\n        raise ValueError\n    if include_system and strategy == \"first\":\n        raise ValueError\n    messages = convert_to_messages(messages)\n    if hasattr(token_counter, \"get_num_tokens_from_messages\"):\n        list_token_counter = token_counter.get_num_tokens_from_messages\n    elif callable(token_counter):\n        if (\n            list(inspect.signature(token_counter).parameters.values())[0].annotation\n            is BaseMessage\n        ):\n\n            def list_token_counter(messages: Sequence[BaseMessage]) -> int:\n                return sum(token_counter(msg) for msg in messages)  # type: ignore[arg-type, misc]\n\n        else:\n            list_token_counter = token_counter  # type: ignore[assignment]\n    else:\n        msg = (\n            f\"'token_counter' expected to be a model that implements \"\n            f\"'get_num_tokens_from_messages()' or a function. Received object of type \"\n            f\"{type(token_counter)}.\"\n        )\n        raise ValueError(msg)\n\n    try:\n        from langchain_text_splitters import TextSplitter\n    except ImportError:\n        text_splitter_fn: Optional[Callable] = cast(Optional[Callable], text_splitter)\n    else:\n        if isinstance(text_splitter, TextSplitter):\n            text_splitter_fn = text_splitter.split_text\n        else:\n            text_splitter_fn = text_splitter\n\n    text_splitter_fn = text_splitter_fn or _default_text_splitter\n\n    if strategy == \"first\":\n        return _first_max_tokens(\n            messages,\n            max_tokens=max_tokens,\n            token_counter=list_token_counter,\n            text_splitter=text_splitter_fn,\n            partial_strategy=\"first\" if allow_partial else None,\n            end_on=end_on,\n        )\n    elif strategy == \"last\":\n        return _last_max_tokens(\n            messages,\n            max_tokens=max_tokens,\n            token_counter=list_token_counter,\n            allow_partial=allow_partial,\n            include_system=include_system,\n            start_on=start_on,\n            end_on=end_on,\n            text_splitter=text_splitter_fn,\n        )\n    else:\n        msg = f\"Unrecognized {strategy=}. Supported strategies are 'last' and 'first'.\"\n        raise ValueError(msg)\n\n\ndef convert_to_openai_messages(\n    messages: Union[MessageLikeRepresentation, Sequence[MessageLikeRepresentation]],\n    *,\n    text_format: Literal[\"string\", \"block\"] = \"string\",\n) -> Union[dict, list[dict]]:\n    \"\"\"Convert LangChain messages into OpenAI message dicts.\n\n    Args:\n        messages: Message-like object or iterable of objects whose contents are\n            in OpenAI, Anthropic, Bedrock Converse, or VertexAI formats.\n        text_format: How to format string or text block contents:\n\n                - \"string\":\n                    If a message has a string content, this is left as a string. If\n                    a message has content blocks that are all of type 'text', these are\n                    joined with a newline to make a single string. If a message has\n                    content blocks and at least one isn't of type 'text', then\n                    all blocks are left as dicts.\n                - \"block\":\n                    If a message has a string content, this is turned into a list\n                    with a single content block of type 'text'. If a message has content\n                    blocks these are left as is.\n\n    Returns:\n        The return type depends on the input type:\n            - dict:\n                If a single message-like object is passed in, a single OpenAI message\n                dict is returned.\n            - list[dict]:\n                If a sequence of message-like objects are passed in, a list of OpenAI\n                message dicts is returned.\n\n    Example:\n\n        .. code-block:: python\n\n            from langchain_core.messages import (\n                convert_to_openai_messages,\n                AIMessage,\n                SystemMessage,\n                ToolMessage,\n            )\n\n            messages = [\n                SystemMessage([{\"type\": \"text\", \"text\": \"foo\"}]),\n                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"whats in this\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/png;base64,'/9j/4AAQSk'\"}}]},\n                AIMessage(\"\", tool_calls=[{\"name\": \"analyze\", \"args\": {\"baz\": \"buz\"}, \"id\": \"1\", \"type\": \"tool_call\"}]),\n                ToolMessage(\"foobar\", tool_call_id=\"1\", name=\"bar\"),\n                {\"role\": \"assistant\", \"content\": \"thats nice\"},\n            ]\n            oai_messages = convert_to_openai_messages(messages)\n            # -> [\n            #   {'role': 'system', 'content': 'foo'},\n            #   {'role': 'user', 'content': [{'type': 'text', 'text': 'whats in this'}, {'type': 'image_url', 'image_url': {'url': \"data:image/png;base64,'/9j/4AAQSk'\"}}]},\n            #   {'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '1','function': {'name': 'analyze', 'arguments': '{\"baz\": \"buz\"}'}}], 'content': ''},\n            #   {'role': 'tool', 'name': 'bar', 'content': 'foobar'},\n            #   {'role': 'assistant', 'content': 'thats nice'}\n            # ]\n\n    .. versionadded:: 0.3.11\n\n    \"\"\"  # noqa: E501\n    if text_format not in (\"string\", \"block\"):\n        err = f\"Unrecognized {text_format=}, expected one of 'string' or 'block'.\"\n        raise ValueError(err)\n\n    oai_messages: list = []\n\n    if is_single := isinstance(messages, (BaseMessage, dict, str)):\n        messages = [messages]\n\n    messages = convert_to_messages(messages)\n\n    for i, message in enumerate(messages):\n        oai_msg: dict = {\"role\": _get_message_openai_role(message)}\n        tool_messages: list = []\n        content: Union[str, list[dict]]\n\n        if message.name:\n            oai_msg[\"name\"] = message.name\n        if isinstance(message, AIMessage) and message.tool_calls:\n            oai_msg[\"tool_calls\"] = _convert_to_openai_tool_calls(message.tool_calls)\n        if message.additional_kwargs.get(\"refusal\"):\n            oai_msg[\"refusal\"] = message.additional_kwargs[\"refusal\"]\n        if isinstance(message, ToolMessage):\n            oai_msg[\"tool_call_id\"] = message.tool_call_id\n\n        if not message.content:\n            content = \"\" if text_format == \"string\" else []\n        elif isinstance(message.content, str):\n            if text_format == \"string\":\n                content = message.content\n            else:\n                content = [{\"type\": \"text\", \"text\": message.content}]\n        else:\n            if text_format == \"string\" and all(\n                isinstance(block, str) or block.get(\"type\") == \"text\"\n                for block in message.content\n            ):\n                content = \"\\n\".join(\n                    block if isinstance(block, str) else block[\"text\"]\n                    for block in message.content\n                )\n            else:\n                content = []\n                for j, block in enumerate(message.content):\n                    # OpenAI format\n                    if isinstance(block, str):\n                        content.append({\"type\": \"text\", \"text\": block})\n                    elif block.get(\"type\") == \"text\":\n                        if missing := [k for k in (\"text\",) if k not in block]:\n                            err = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': 'text' \"\n                                f\"but is missing expected key(s) \"\n                                f\"{missing}. Full content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(err)\n                        content.append({\"type\": block[\"type\"], \"text\": block[\"text\"]})\n                    elif block.get(\"type\") == \"image_url\":\n                        if missing := [k for k in (\"image_url\",) if k not in block]:\n                            err = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': 'image_url' \"\n                                f\"but is missing expected key(s) \"\n                                f\"{missing}. Full content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(err)\n                        content.append(\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": block[\"image_url\"],\n                            }\n                        )\n                    # Anthropic and Bedrock converse format\n                    elif (block.get(\"type\") == \"image\") or \"image\" in block:\n                        # Anthropic\n                        if source := block.get(\"source\"):\n                            if missing := [\n                                k\n                                for k in (\"media_type\", \"type\", \"data\")\n                                if k not in source\n                            ]:\n                                err = (\n                                    f\"Unrecognized content block at \"\n                                    f\"messages[{i}].content[{j}] has 'type': 'image' \"\n                                    f\"but 'source' is missing expected key(s) \"\n                                    f\"{missing}. Full content block:\\n\\n{block}\"\n                                )\n                                raise ValueError(err)\n                            content.append(\n                                {\n                                    \"type\": \"image_url\",\n                                    \"image_url\": {\n                                        \"url\": (\n                                            f\"data:{source['media_type']};\"\n                                            f\"{source['type']},{source['data']}\"\n                                        )\n                                    },\n                                }\n                            )\n                        # Bedrock converse\n                        elif image := block.get(\"image\"):\n                            if missing := [\n                                k for k in (\"source\", \"format\") if k not in image\n                            ]:\n                                err = (\n                                    f\"Unrecognized content block at \"\n                                    f\"messages[{i}].content[{j}] has key 'image', \"\n                                    f\"but 'image' is missing expected key(s) \"\n                                    f\"{missing}. Full content block:\\n\\n{block}\"\n                                )\n                                raise ValueError(err)\n                            b64_image = _bytes_to_b64_str(image[\"source\"][\"bytes\"])\n                            content.append(\n                                {\n                                    \"type\": \"image_url\",\n                                    \"image_url\": {\n                                        \"url\": (\n                                            f\"data:image/{image['format']};\"\n                                            f\"base64,{b64_image}\"\n                                        )\n                                    },\n                                }\n                            )\n                        else:\n                            err = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': 'image' \"\n                                f\"but does not have a 'source' or 'image' key. Full \"\n                                f\"content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(err)\n                    elif block.get(\"type\") == \"tool_use\":\n                        if missing := [\n                            k for k in (\"id\", \"name\", \"input\") if k not in block\n                        ]:\n                            err = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': \"\n                                f\"'tool_use', but is missing expected key(s) \"\n                                f\"{missing}. Full content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(err)\n                        if not any(\n                            tool_call[\"id\"] == block[\"id\"]\n                            for tool_call in cast(AIMessage, message).tool_calls\n                        ):\n                            oai_msg[\"tool_calls\"] = oai_msg.get(\"tool_calls\", [])\n                            oai_msg[\"tool_calls\"].append(\n                                {\n                                    \"type\": \"function\",\n                                    \"id\": block[\"id\"],\n                                    \"function\": {\n                                        \"name\": block[\"name\"],\n                                        \"arguments\": json.dumps(block[\"input\"]),\n                                    },\n                                }\n                            )\n                    elif block.get(\"type\") == \"tool_result\":\n                        if missing := [\n                            k for k in (\"content\", \"tool_use_id\") if k not in block\n                        ]:\n                            msg = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': \"\n                                f\"'tool_result', but is missing expected key(s) \"\n                                f\"{missing}. Full content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(msg)\n                        tool_message = ToolMessage(\n                            block[\"content\"],\n                            tool_call_id=block[\"tool_use_id\"],\n                            status=\"error\" if block.get(\"is_error\") else \"success\",\n                        )\n                        # Recurse to make sure tool message contents are OpenAI format.\n                        tool_messages.extend(\n                            convert_to_openai_messages(\n                                [tool_message], text_format=text_format\n                            )\n                        )\n                    elif (block.get(\"type\") == \"json\") or \"json\" in block:\n                        if \"json\" not in block:\n                            msg = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': 'json' \"\n                                f\"but does not have a 'json' key. Full \"\n                                f\"content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(msg)\n                        content.append(\n                            {\n                                \"type\": \"text\",\n                                \"text\": json.dumps(block[\"json\"]),\n                            }\n                        )\n                    elif (\n                        block.get(\"type\") == \"guard_content\"\n                    ) or \"guard_content\" in block:\n                        if (\n                            \"guard_content\" not in block\n                            or \"text\" not in block[\"guard_content\"]\n                        ):\n                            msg = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': \"\n                                f\"'guard_content' but does not have a \"\n                                f\"messages[{i}].content[{j}]['guard_content']['text'] \"\n                                f\"key. Full content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(msg)\n                        text = block[\"guard_content\"][\"text\"]\n                        if isinstance(text, dict):\n                            text = text[\"text\"]\n                        content.append({\"type\": \"text\", \"text\": text})\n                    # VertexAI format\n                    elif block.get(\"type\") == \"media\":\n                        if missing := [\n                            k for k in (\"mime_type\", \"data\") if k not in block\n                        ]:\n                            err = (\n                                f\"Unrecognized content block at \"\n                                f\"messages[{i}].content[{j}] has 'type': \"\n                                f\"'media' but does not have key(s) {missing}. Full \"\n                                f\"content block:\\n\\n{block}\"\n                            )\n                            raise ValueError(err)\n                        if \"image\" not in block[\"mime_type\"]:\n                            err = (\n                                f\"OpenAI messages can only support text and image data.\"\n                                f\" Received content block with media of type:\"\n                                f\" {block['mime_type']}\"\n                            )\n                            raise ValueError(err)\n                        b64_image = _bytes_to_b64_str(block[\"data\"])\n                        content.append(\n                            {\n                                \"type\": \"image_url\",\n                                \"image_url\": {\n                                    \"url\": (\n                                        f\"data:{block['mime_type']};base64,{b64_image}\"\n                                    )\n                                },\n                            }\n                        )\n                    else:\n                        err = (\n                            f\"Unrecognized content block at \"\n                            f\"messages[{i}].content[{j}] does not match OpenAI, \"\n                            f\"Anthropic, Bedrock Converse, or VertexAI format. Full \"\n                            f\"content block:\\n\\n{block}\"\n                        )\n                        raise ValueError(err)\n                if text_format == \"string\" and not any(\n                    block[\"type\"] != \"text\" for block in content\n                ):\n                    content = \"\\n\".join(block[\"text\"] for block in content)\n        oai_msg[\"content\"] = content\n        if message.content and not oai_msg[\"content\"] and tool_messages:\n            oai_messages.extend(tool_messages)\n        else:\n            oai_messages.extend([oai_msg, *tool_messages])\n\n    if is_single:\n        return oai_messages[0]\n    else:\n        return oai_messages\n\n\ndef _first_max_tokens(\n    messages: Sequence[BaseMessage],\n    *,\n    max_tokens: int,\n    token_counter: Callable[[list[BaseMessage]], int],\n    text_splitter: Callable[[str], list[str]],\n    partial_strategy: Optional[Literal[\"first\", \"last\"]] = None,\n    end_on: Optional[\n        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]\n    ] = None,\n) -> list[BaseMessage]:\n    messages = list(messages)\n    if not messages:\n        return messages\n    idx = 0\n    for i in range(len(messages)):\n        if token_counter(messages[:-i] if i else messages) <= max_tokens:\n            idx = len(messages) - i\n            break\n    if partial_strategy and (idx < len(messages) - 1 or idx == 0):\n        included_partial = False\n        if isinstance(messages[idx].content, list):\n            excluded = messages[idx].model_copy(deep=True)\n            num_block = len(excluded.content)\n            if partial_strategy == \"last\":\n                excluded.content = list(reversed(excluded.content))\n            for _ in range(1, num_block):\n                excluded.content = excluded.content[:-1]\n                if token_counter(messages[:idx] + [excluded]) <= max_tokens:\n                    messages = messages[:idx] + [excluded]\n                    idx += 1\n                    included_partial = True\n                    break\n            if included_partial and partial_strategy == \"last\":\n                excluded.content = list(reversed(excluded.content))\n        if not included_partial:\n            excluded = messages[idx].model_copy(deep=True)\n            if isinstance(excluded.content, list) and any(\n                isinstance(block, str) or block[\"type\"] == \"text\"\n                for block in messages[idx].content\n            ):\n                text_block = next(\n                    block\n                    for block in messages[idx].content\n                    if isinstance(block, str) or block[\"type\"] == \"text\"\n                )\n                text = (\n                    text_block[\"text\"] if isinstance(text_block, dict) else text_block\n                )\n            elif isinstance(excluded.content, str):\n                text = excluded.content\n            else:\n                text = None\n            if text:\n                split_texts = text_splitter(text)\n                num_splits = len(split_texts)\n                if partial_strategy == \"last\":\n                    split_texts = list(reversed(split_texts))\n                for _ in range(num_splits - 1):\n                    split_texts.pop()\n                    excluded.content = \"\".join(split_texts)\n                    if token_counter(messages[:idx] + [excluded]) <= max_tokens:\n                        if partial_strategy == \"last\":\n                            excluded.content = \"\".join(reversed(split_texts))\n                        messages = messages[:idx] + [excluded]\n                        idx += 1\n                        break\n\n    if end_on:\n        while idx > 0 and not _is_message_type(messages[idx - 1], end_on):\n            idx -= 1\n\n    return messages[:idx]\n\n\ndef _last_max_tokens(\n    messages: Sequence[BaseMessage],\n    *,\n    max_tokens: int,\n    token_counter: Callable[[list[BaseMessage]], int],\n    text_splitter: Callable[[str], list[str]],\n    allow_partial: bool = False,\n    include_system: bool = False,\n    start_on: Optional[\n        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]\n    ] = None,\n    end_on: Optional[\n        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]\n    ] = None,\n) -> list[BaseMessage]:\n    messages = list(messages)\n    if len(messages) == 0:\n        return []\n    if end_on:\n        while messages and not _is_message_type(messages[-1], end_on):\n            messages.pop()\n    swapped_system = include_system and isinstance(messages[0], SystemMessage)\n    reversed_ = messages[:1] + messages[1:][::-1] if swapped_system else messages[::-1]\n\n    reversed_ = _first_max_tokens(\n        reversed_,\n        max_tokens=max_tokens,\n        token_counter=token_counter,\n        text_splitter=text_splitter,\n        partial_strategy=\"last\" if allow_partial else None,\n        end_on=start_on,\n    )\n    if swapped_system:\n        return reversed_[:1] + reversed_[1:][::-1]\n    else:\n        return reversed_[::-1]\n\n\n_MSG_CHUNK_MAP: dict[type[BaseMessage], type[BaseMessageChunk]] = {\n    HumanMessage: HumanMessageChunk,\n    AIMessage: AIMessageChunk,\n    SystemMessage: SystemMessageChunk,\n    ToolMessage: ToolMessageChunk,\n    FunctionMessage: FunctionMessageChunk,\n    ChatMessage: ChatMessageChunk,\n}\n_CHUNK_MSG_MAP = {v: k for k, v in _MSG_CHUNK_MAP.items()}\n\n\ndef _msg_to_chunk(message: BaseMessage) -> BaseMessageChunk:\n    if message.__class__ in _MSG_CHUNK_MAP:\n        return _MSG_CHUNK_MAP[message.__class__](**message.model_dump(exclude={\"type\"}))\n\n    for msg_cls, chunk_cls in _MSG_CHUNK_MAP.items():\n        if isinstance(message, msg_cls):\n            return chunk_cls(**message.model_dump(exclude={\"type\"}))\n\n    msg = (\n        f\"Unrecognized message class {message.__class__}. Supported classes are \"\n        f\"{list(_MSG_CHUNK_MAP.keys())}\"\n    )\n    msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)\n    raise ValueError(msg)\n\n\ndef _chunk_to_msg(chunk: BaseMessageChunk) -> BaseMessage:\n    if chunk.__class__ in _CHUNK_MSG_MAP:\n        return _CHUNK_MSG_MAP[chunk.__class__](\n            **chunk.model_dump(exclude={\"type\", \"tool_call_chunks\"})\n        )\n    for chunk_cls, msg_cls in _CHUNK_MSG_MAP.items():\n        if isinstance(chunk, chunk_cls):\n            return msg_cls(**chunk.model_dump(exclude={\"type\", \"tool_call_chunks\"}))\n\n    msg = (\n        f\"Unrecognized message chunk class {chunk.__class__}. Supported classes are \"\n        f\"{list(_CHUNK_MSG_MAP.keys())}\"\n    )\n    msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)\n    raise ValueError(msg)\n\n\ndef _default_text_splitter(text: str) -> list[str]:\n    splits = text.split(\"\\n\")\n    return [s + \"\\n\" for s in splits[:-1]] + splits[-1:]\n\n\ndef _is_message_type(\n    message: BaseMessage,\n    type_: Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]],\n) -> bool:\n    types = [type_] if isinstance(type_, (str, type)) else type_\n    types_str = [t for t in types if isinstance(t, str)]\n    types_types = tuple(t for t in types if isinstance(t, type))\n\n    return message.type in types_str or isinstance(message, types_types)\n\n\ndef _bytes_to_b64_str(bytes_: bytes) -> str:\n    return base64.b64encode(bytes_).decode(\"utf-8\")\n\n\ndef _get_message_openai_role(message: BaseMessage) -> str:\n    if isinstance(message, AIMessage):\n        return \"assistant\"\n    elif isinstance(message, HumanMessage):\n        return \"user\"\n    elif isinstance(message, ToolMessage):\n        return \"tool\"\n    elif isinstance(message, SystemMessage):\n        return message.additional_kwargs.get(\"__openai_role__\", \"system\")\n    elif isinstance(message, FunctionMessage):\n        return \"function\"\n    elif isinstance(message, ChatMessage):\n        return message.role\n    else:\n        msg = f\"Unknown BaseMessage type {message.__class__}.\"\n        raise ValueError(msg)  # noqa: TRY004\n\n\ndef _convert_to_openai_tool_calls(tool_calls: list[ToolCall]) -> list[dict]:\n    return [\n        {\n            \"type\": \"function\",\n            \"id\": tool_call[\"id\"],\n            \"function\": {\n                \"name\": tool_call[\"name\"],\n                \"arguments\": json.dumps(tool_call[\"args\"]),\n            },\n        }\n        for tool_call in tool_calls\n    ]\n",
        "patch": "@@ -557,15 +557,14 @@ def merge_message_runs(\n     messages = convert_to_messages(messages)\n     merged: list[BaseMessage] = []\n     for msg in messages:\n-        curr = msg.model_copy(deep=True)\n         last = merged.pop() if merged else None\n         if not last:\n-            merged.append(curr)\n-        elif isinstance(curr, ToolMessage) or not isinstance(curr, last.__class__):\n-            merged.extend([last, curr])\n+            merged.append(msg)\n+        elif isinstance(msg, ToolMessage) or not isinstance(msg, last.__class__):\n+            merged.extend([last, msg])\n         else:\n             last_chunk = _msg_to_chunk(last)\n-            curr_chunk = _msg_to_chunk(curr)\n+            curr_chunk = _msg_to_chunk(msg)\n             if curr_chunk.response_metadata:\n                 curr_chunk.response_metadata.clear()\n             if ("
      }
    ]
  },
  {
    "number": 29927,
    "title": "`_wait_for_run` calling fix for `OpenAIAssistantRunnable`",
    "body": "- **Description:** Fixed the `OpenAIAssistantRunnable` call of `_wait_for_run`\r\n- **Issue:**  #29923",
    "issue_title": "`_wait_for_run` calling fix for `OpenAIAssistantRunnable`",
    "issue_body": "- **Description:** Fixed the `OpenAIAssistantRunnable` call of `_wait_for_run`\r\n- **Issue:**  #29923",
    "files": [
      {
        "filename": "libs/langchain/langchain/agents/openai_assistant/base.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport json\nfrom json import JSONDecodeError\nfrom time import sleep\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom langchain_core.agents import AgentAction, AgentFinish\nfrom langchain_core.callbacks import CallbackManager\nfrom langchain_core.load import dumpd\nfrom langchain_core.runnables import RunnableConfig, RunnableSerializable, ensure_config\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom pydantic import BaseModel, Field, model_validator\nfrom typing_extensions import Self\n\nif TYPE_CHECKING:\n    import openai\n    from openai.types.beta.threads import ThreadMessage  # type: ignore[attr-defined]\n    from openai.types.beta.threads.required_action_function_tool_call import (\n        RequiredActionFunctionToolCall,\n    )\n\n\nclass OpenAIAssistantFinish(AgentFinish):\n    \"\"\"AgentFinish with run and thread metadata.\n\n    Parameters:\n        run_id: Run id.\n        thread_id: Thread id.\n    \"\"\"\n\n    run_id: str\n    thread_id: str\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Check if the class is serializable by LangChain.\n\n        Returns:\n            False\n        \"\"\"\n        return False\n\n\nclass OpenAIAssistantAction(AgentAction):\n    \"\"\"AgentAction with info needed to submit custom tool output to existing run.\n\n    Parameters:\n        tool_call_id: Tool call id.\n        run_id: Run id.\n        thread_id: Thread id\n    \"\"\"\n\n    tool_call_id: str\n    run_id: str\n    thread_id: str\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Check if the class is serializable by LangChain.\n\n        Returns:\n            False\n        \"\"\"\n        return False\n\n\ndef _get_openai_client() -> openai.OpenAI:\n    try:\n        import openai\n\n        return openai.OpenAI()\n    except ImportError as e:\n        raise ImportError(\n            \"Unable to import openai, please install with `pip install openai`.\"\n        ) from e\n    except AttributeError as e:\n        raise AttributeError(\n            \"Please make sure you are using a v1.1-compatible version of openai. You \"\n            'can install with `pip install \"openai>=1.1\"`.'\n        ) from e\n\n\ndef _get_openai_async_client() -> openai.AsyncOpenAI:\n    try:\n        import openai\n\n        return openai.AsyncOpenAI()\n    except ImportError as e:\n        raise ImportError(\n            \"Unable to import openai, please install with `pip install openai`.\"\n        ) from e\n    except AttributeError as e:\n        raise AttributeError(\n            \"Please make sure you are using a v1.1-compatible version of openai. You \"\n            'can install with `pip install \"openai>=1.1\"`.'\n        ) from e\n\n\ndef _is_assistants_builtin_tool(\n    tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool],\n) -> bool:\n    \"\"\"Determine if tool corresponds to OpenAI Assistants built-in.\"\"\"\n    assistants_builtin_tools = (\"code_interpreter\", \"retrieval\")\n    return (\n        isinstance(tool, dict)\n        and (\"type\" in tool)\n        and (tool[\"type\"] in assistants_builtin_tools)\n    )\n\n\ndef _get_assistants_tool(\n    tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool],\n) -> Dict[str, Any]:\n    \"\"\"Convert a raw function/class to an OpenAI tool.\n\n    Note that OpenAI assistants supports several built-in tools,\n    such as \"code_interpreter\" and \"retrieval.\"\n    \"\"\"\n    if _is_assistants_builtin_tool(tool):\n        return tool  # type: ignore\n    else:\n        return convert_to_openai_tool(tool)\n\n\nOutputType = Union[\n    List[OpenAIAssistantAction],\n    OpenAIAssistantFinish,\n    List[\"ThreadMessage\"],\n    List[\"RequiredActionFunctionToolCall\"],\n]\n\n\nclass OpenAIAssistantRunnable(RunnableSerializable[Dict, OutputType]):\n    \"\"\"Run an OpenAI Assistant.\n\n    Example using OpenAI tools:\n        .. code-block:: python\n\n            from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\n\n            interpreter_assistant = OpenAIAssistantRunnable.create_assistant(\n                name=\"langchain assistant\",\n                instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n                tools=[{\"type\": \"code_interpreter\"}],\n                model=\"gpt-4-1106-preview\"\n            )\n            output = interpreter_assistant.invoke({\"content\": \"What's 10 - 4 raised to the 2.7\"})\n\n    Example using custom tools and AgentExecutor:\n        .. code-block:: python\n\n            from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\n            from langchain.agents import AgentExecutor\n            from langchain.tools import E2BDataAnalysisTool\n\n\n            tools = [E2BDataAnalysisTool(api_key=\"...\")]\n            agent = OpenAIAssistantRunnable.create_assistant(\n                name=\"langchain assistant e2b tool\",\n                instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n                tools=tools,\n                model=\"gpt-4-1106-preview\",\n                as_agent=True\n            )\n\n            agent_executor = AgentExecutor(agent=agent, tools=tools)\n            agent_executor.invoke({\"content\": \"What's 10 - 4 raised to the 2.7\"})\n\n\n    Example using custom tools and custom execution:\n        .. code-block:: python\n\n            from langchain_experimental.openai_assistant import OpenAIAssistantRunnable\n            from langchain.agents import AgentExecutor\n            from langchain_core.agents import AgentFinish\n            from langchain.tools import E2BDataAnalysisTool\n\n\n            tools = [E2BDataAnalysisTool(api_key=\"...\")]\n            agent = OpenAIAssistantRunnable.create_assistant(\n                name=\"langchain assistant e2b tool\",\n                instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n                tools=tools,\n                model=\"gpt-4-1106-preview\",\n                as_agent=True\n            )\n\n            def execute_agent(agent, tools, input):\n                tool_map = {tool.name: tool for tool in tools}\n                response = agent.invoke(input)\n                while not isinstance(response, AgentFinish):\n                    tool_outputs = []\n                    for action in response:\n                        tool_output = tool_map[action.tool].invoke(action.tool_input)\n                        tool_outputs.append({\"output\": tool_output, \"tool_call_id\": action.tool_call_id})\n                    response = agent.invoke(\n                        {\n                            \"tool_outputs\": tool_outputs,\n                            \"run_id\": action.run_id,\n                            \"thread_id\": action.thread_id\n                        }\n                    )\n\n                return response\n\n            response = execute_agent(agent, tools, {\"content\": \"What's 10 - 4 raised to the 2.7\"})\n            next_response = execute_agent(agent, tools, {\"content\": \"now add 17.241\", \"thread_id\": response.thread_id})\n\n    \"\"\"  # noqa: E501\n\n    client: Any = Field(default_factory=_get_openai_client)\n    \"\"\"OpenAI or AzureOpenAI client.\"\"\"\n    async_client: Any = None\n    \"\"\"OpenAI or AzureOpenAI async client.\"\"\"\n    assistant_id: str\n    \"\"\"OpenAI assistant id.\"\"\"\n    check_every_ms: float = 1_000.0\n    \"\"\"Frequency with which to check run progress in ms.\"\"\"\n    as_agent: bool = False\n    \"\"\"Use as a LangChain agent, compatible with the AgentExecutor.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_async_client(self) -> Self:\n        if self.async_client is None:\n            import openai\n\n            api_key = self.client.api_key\n            self.async_client = openai.AsyncOpenAI(api_key=api_key)\n        return self\n\n    @classmethod\n    def create_assistant(\n        cls,\n        name: str,\n        instructions: str,\n        tools: Sequence[Union[BaseTool, dict]],\n        model: str,\n        *,\n        client: Optional[Union[openai.OpenAI, openai.AzureOpenAI]] = None,\n        **kwargs: Any,\n    ) -> OpenAIAssistantRunnable:\n        \"\"\"Create an OpenAI Assistant and instantiate the Runnable.\n\n        Args:\n            name: Assistant name.\n            instructions: Assistant instructions.\n            tools: Assistant tools. Can be passed in OpenAI format or as BaseTools.\n            model: Assistant model to use.\n            client: OpenAI or AzureOpenAI client.\n                Will create a default OpenAI client if not specified.\n            kwargs: Additional arguments.\n\n        Returns:\n            OpenAIAssistantRunnable configured to run using the created assistant.\n        \"\"\"\n        client = client or _get_openai_client()\n        assistant = client.beta.assistants.create(\n            name=name,\n            instructions=instructions,\n            tools=[_get_assistants_tool(tool) for tool in tools],  # type: ignore\n            model=model,\n        )\n        return cls(assistant_id=assistant.id, client=client, **kwargs)\n\n    def invoke(\n        self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> OutputType:\n        \"\"\"Invoke assistant.\n\n        Args:\n            input: Runnable input dict that can have:\n                content: User message when starting a new run.\n                thread_id: Existing thread to use.\n                run_id: Existing run to use. Should only be supplied when providing\n                    the tool output for a required action after an initial invocation.\n                message_metadata: Metadata to associate with new message.\n                thread_metadata: Metadata to associate with new thread. Only relevant\n                    when new thread being created.\n                instructions: Additional run instructions.\n                model: Override Assistant model for this run.\n                tools: Override Assistant tools for this run.\n                parallel_tool_calls: Allow Assistant to set parallel_tool_calls\n                    for this run.\n                top_p: Override Assistant top_p for this run.\n                temperature: Override Assistant temperature for this run.\n                max_completion_tokens: Allow setting max_completion_tokens for this run.\n                max_prompt_tokens: Allow setting max_prompt_tokens for this run.\n                run_metadata: Metadata to associate with new run.\n            config: Runnable config. Defaults to None.\n\n        Return:\n            If self.as_agent, will return\n                Union[List[OpenAIAssistantAction], OpenAIAssistantFinish].\n                Otherwise, will return OpenAI types\n                Union[List[ThreadMessage], List[RequiredActionFunctionToolCall]].\n        \"\"\"\n\n        config = ensure_config(config)\n        callback_manager = CallbackManager.configure(\n            inheritable_callbacks=config.get(\"callbacks\"),\n            inheritable_tags=config.get(\"tags\"),\n            inheritable_metadata=config.get(\"metadata\"),\n        )\n        run_manager = callback_manager.on_chain_start(\n            dumpd(self), input, name=config.get(\"run_name\") or self.get_name()\n        )\n        try:\n            # Being run within AgentExecutor and there are tool outputs to submit.\n            if self.as_agent and input.get(\"intermediate_steps\"):\n                tool_outputs = self._parse_intermediate_steps(\n                    input[\"intermediate_steps\"]\n                )\n                run = self.client.beta.threads.runs.submit_tool_outputs(**tool_outputs)\n            # Starting a new thread and a new run.\n            elif \"thread_id\" not in input:\n                thread = {\n                    \"messages\": [\n                        {\n                            \"role\": \"user\",\n                            \"content\": input[\"content\"],\n                            \"metadata\": input.get(\"message_metadata\"),\n                        }\n                    ],\n                    \"metadata\": input.get(\"thread_metadata\"),\n                }\n                run = self._create_thread_and_run(input, thread)\n            # Starting a new run in an existing thread.\n            elif \"run_id\" not in input:\n                _ = self.client.beta.threads.messages.create(\n                    input[\"thread_id\"],\n                    content=input[\"content\"],\n                    role=\"user\",\n                    metadata=input.get(\"message_metadata\"),\n                )\n                run = self._create_run(input)\n            # Submitting tool outputs to an existing run, outside the AgentExecutor\n            # framework.\n            else:\n                run = self.client.beta.threads.runs.submit_tool_outputs(**input)\n            run = self._wait_for_run(run.id, run.thread_id)\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise e\n        try:\n            response = self._get_response(run)\n        except BaseException as e:\n            run_manager.on_chain_error(e, metadata=run.dict())\n            raise e\n        else:\n            run_manager.on_chain_end(response)\n            return response\n\n    @classmethod\n    async def acreate_assistant(\n        cls,\n        name: str,\n        instructions: str,\n        tools: Sequence[Union[BaseTool, dict]],\n        model: str,\n        *,\n        async_client: Optional[\n            Union[openai.AsyncOpenAI, openai.AsyncAzureOpenAI]\n        ] = None,\n        **kwargs: Any,\n    ) -> OpenAIAssistantRunnable:\n        \"\"\"Async create an AsyncOpenAI Assistant and instantiate the Runnable.\n\n        Args:\n            name: Assistant name.\n            instructions: Assistant instructions.\n            tools: Assistant tools. Can be passed in OpenAI format or as BaseTools.\n            model: Assistant model to use.\n            async_client: AsyncOpenAI client.\n                Will create default async_client if not specified.\n\n        Returns:\n            AsyncOpenAIAssistantRunnable configured to run using the created assistant.\n        \"\"\"\n        async_client = async_client or _get_openai_async_client()\n        openai_tools = [_get_assistants_tool(tool) for tool in tools]\n        assistant = await async_client.beta.assistants.create(\n            name=name,\n            instructions=instructions,\n            tools=openai_tools,  # type: ignore\n            model=model,\n        )\n        return cls(assistant_id=assistant.id, async_client=async_client, **kwargs)\n\n    async def ainvoke(\n        self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any\n    ) -> OutputType:\n        \"\"\"Async invoke assistant.\n\n        Args:\n            input: Runnable input dict that can have:\n                content: User message when starting a new run.\n                thread_id: Existing thread to use.\n                run_id: Existing run to use. Should only be supplied when providing\n                    the tool output for a required action after an initial invocation.\n                message_metadata: Metadata to associate with a new message.\n                thread_metadata: Metadata to associate with new thread. Only relevant\n                    when a new thread is created.\n                instructions: Overrides the instructions of the assistant.\n                additional_instructions: Appends additional instructions.\n                model: Override Assistant model for this run.\n                tools: Override Assistant tools for this run.\n                parallel_tool_calls: Allow Assistant to set parallel_tool_calls\n                    for this run.\n                top_p: Override Assistant top_p for this run.\n                temperature: Override Assistant temperature for this run.\n                max_completion_tokens: Allow setting max_completion_tokens for this run.\n                max_prompt_tokens: Allow setting max_prompt_tokens for this run.\n                run_metadata: Metadata to associate with new run.\n            config: Runnable config. Defaults to None.\n            kwargs: Additional arguments.\n\n        Return:\n            If self.as_agent, will return\n                Union[List[OpenAIAssistantAction], OpenAIAssistantFinish].\n                Otherwise, will return OpenAI types\n                Union[List[ThreadMessage], List[RequiredActionFunctionToolCall]].\n        \"\"\"\n\n        config = config or {}\n        callback_manager = CallbackManager.configure(\n            inheritable_callbacks=config.get(\"callbacks\"),\n            inheritable_tags=config.get(\"tags\"),\n            inheritable_metadata=config.get(\"metadata\"),\n        )\n        run_manager = callback_manager.on_chain_start(\n            dumpd(self), input, name=config.get(\"run_name\") or self.get_name()\n        )\n        try:\n            # Being run within AgentExecutor and there are tool outputs to submit.\n            if self.as_agent and input.get(\"intermediate_steps\"):\n                tool_outputs = await self._aparse_intermediate_steps(\n                    input[\"intermediate_steps\"]\n                )\n                run = await self.async_client.beta.threads.runs.submit_tool_outputs(\n                    **tool_outputs\n                )\n            # Starting a new thread and a new run.\n            elif \"thread_id\" not in input:\n                thread = {\n                    \"messages\": [\n                        {\n                            \"role\": \"user\",\n                            \"content\": input[\"content\"],\n                            \"metadata\": input.get(\"message_metadata\"),\n                        }\n                    ],\n                    \"metadata\": input.get(\"thread_metadata\"),\n                }\n                run = await self._acreate_thread_and_run(input, thread)\n            # Starting a new run in an existing thread.\n            elif \"run_id\" not in input:\n                _ = await self.async_client.beta.threads.messages.create(\n                    input[\"thread_id\"],\n                    content=input[\"content\"],\n                    role=\"user\",\n                    metadata=input.get(\"message_metadata\"),\n                )\n                run = await self._acreate_run(input)\n            # Submitting tool outputs to an existing run, outside the AgentExecutor\n            # framework.\n            else:\n                run = await self.async_client.beta.threads.runs.submit_tool_outputs(\n                    **input\n                )\n            run = await self._await_for_run(run.id, run.thread_id)\n        except BaseException as e:\n            run_manager.on_chain_error(e)\n            raise e\n        try:\n            response = self._get_response(run)\n        except BaseException as e:\n            run_manager.on_chain_error(e, metadata=run.dict())\n            raise e\n        else:\n            run_manager.on_chain_end(response)\n            return response\n\n    def _parse_intermediate_steps(\n        self, intermediate_steps: List[Tuple[OpenAIAssistantAction, str]]\n    ) -> dict:\n        last_action, last_output = intermediate_steps[-1]\n        run = self._wait_for_run(last_action.run_id, last_action.thread_id)\n        required_tool_call_ids = set()\n        if run.required_action:\n            required_tool_call_ids = {\n                tc.id for tc in run.required_action.submit_tool_outputs.tool_calls\n            }\n        tool_outputs = [\n            {\"output\": str(output), \"tool_call_id\": action.tool_call_id}\n            for action, output in intermediate_steps\n            if action.tool_call_id in required_tool_call_ids\n        ]\n        submit_tool_outputs = {\n            \"tool_outputs\": tool_outputs,\n            \"run_id\": last_action.run_id,\n            \"thread_id\": last_action.thread_id,\n        }\n        return submit_tool_outputs\n\n    def _create_run(self, input: dict) -> Any:\n        params = {\n            k: v\n            for k, v in input.items()\n            if k\n            in (\n                \"instructions\",\n                \"model\",\n                \"tools\",\n                \"additional_instructions\",\n                \"parallel_tool_calls\",\n                \"top_p\",\n                \"temperature\",\n                \"max_completion_tokens\",\n                \"max_prompt_tokens\",\n                \"run_metadata\",\n            )\n        }\n        return self.client.beta.threads.runs.create(\n            input[\"thread_id\"],\n            assistant_id=self.assistant_id,\n            **params,\n        )\n\n    def _create_thread_and_run(self, input: dict, thread: dict) -> Any:\n        params = {\n            k: v\n            for k, v in input.items()\n            if k\n            in (\n                \"instructions\",\n                \"model\",\n                \"tools\",\n                \"parallel_tool_calls\",\n                \"top_p\",\n                \"temperature\",\n                \"max_completion_tokens\",\n                \"max_prompt_tokens\",\n                \"run_metadata\",\n            )\n        }\n        run = self.client.beta.threads.create_and_run(\n            assistant_id=self.assistant_id,\n            thread=thread,\n            **params,\n        )\n        return run\n\n    def _get_response(self, run: Any) -> Any:\n        # TODO: Pagination\n\n        if run.status == \"completed\":\n            import openai\n\n            major_version = int(openai.version.VERSION.split(\".\")[0])\n            minor_version = int(openai.version.VERSION.split(\".\")[1])\n            version_gte_1_14 = (major_version > 1) or (\n                major_version == 1 and minor_version >= 14\n            )\n\n            messages = self.client.beta.threads.messages.list(\n                run.thread_id, order=\"asc\"\n            )\n            new_messages = [msg for msg in messages if msg.run_id == run.id]\n            if not self.as_agent:\n                return new_messages\n            answer: Any = [\n                msg_content for msg in new_messages for msg_content in msg.content\n            ]\n            if all(\n                (\n                    isinstance(content, openai.types.beta.threads.TextContentBlock)\n                    if version_gte_1_14\n                    else isinstance(\n                        content,\n                        openai.types.beta.threads.MessageContentText,  # type: ignore[attr-defined]\n                    )\n                )\n                for content in answer\n            ):\n                answer = \"\\n\".join(content.text.value for content in answer)\n            return OpenAIAssistantFinish(\n                return_values={\n                    \"output\": answer,\n                    \"thread_id\": run.thread_id,\n                    \"run_id\": run.id,\n                },\n                log=\"\",\n                run_id=run.id,\n                thread_id=run.thread_id,\n            )\n        elif run.status == \"requires_action\":\n            if not self.as_agent:\n                return run.required_action.submit_tool_outputs.tool_calls\n            actions = []\n            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n                function = tool_call.function\n                try:\n                    args = json.loads(function.arguments, strict=False)\n                except JSONDecodeError as e:\n                    raise ValueError(\n                        f\"Received invalid JSON function arguments: \"\n                        f\"{function.arguments} for function {function.name}\"\n                    ) from e\n                if len(args) == 1 and \"__arg1\" in args:\n                    args = args[\"__arg1\"]\n                actions.append(\n                    OpenAIAssistantAction(\n                        tool=function.name,\n                        tool_input=args,\n                        tool_call_id=tool_call.id,\n                        log=\"\",\n                        run_id=run.id,\n                        thread_id=run.thread_id,\n                    )\n                )\n            return actions\n        else:\n            run_info = json.dumps(run.dict(), indent=2)\n            raise ValueError(\n                f\"Unexpected run status: {run.status}. Full run info:\\n\\n{run_info})\"\n            )\n\n    def _wait_for_run(self, run_id: str, thread_id: str) -> Any:\n        in_progress = True\n        while in_progress:\n            run = self.client.beta.threads.runs.retrieve(run_id, thread_id=thread_id)\n            in_progress = run.status in (\"in_progress\", \"queued\")\n            if in_progress:\n                sleep(self.check_every_ms / 1000)\n        return run\n\n    async def _aparse_intermediate_steps(\n        self, intermediate_steps: List[Tuple[OpenAIAssistantAction, str]]\n    ) -> dict:\n        last_action, last_output = intermediate_steps[-1]\n        run = await self._wait_for_run(last_action.run_id, last_action.thread_id)\n        required_tool_call_ids = set()\n        if run.required_action:\n            required_tool_call_ids = {\n                tc.id for tc in run.required_action.submit_tool_outputs.tool_calls\n            }\n        tool_outputs = [\n            {\"output\": str(output), \"tool_call_id\": action.tool_call_id}\n            for action, output in intermediate_steps\n            if action.tool_call_id in required_tool_call_ids\n        ]\n        submit_tool_outputs = {\n            \"tool_outputs\": tool_outputs,\n            \"run_id\": last_action.run_id,\n            \"thread_id\": last_action.thread_id,\n        }\n        return submit_tool_outputs\n\n    async def _acreate_run(self, input: dict) -> Any:\n        params = {\n            k: v\n            for k, v in input.items()\n            if k\n            in (\n                \"instructions\",\n                \"model\",\n                \"tools\",\n                \"additional_instructions\",\n                \"parallel_tool_calls\",\n                \"top_p\",\n                \"temperature\",\n                \"max_completion_tokens\",\n                \"max_prompt_tokens\",\n                \"run_metadata\",\n            )\n        }\n        return await self.async_client.beta.threads.runs.create(\n            input[\"thread_id\"],\n            assistant_id=self.assistant_id,\n            **params,\n        )\n\n    async def _acreate_thread_and_run(self, input: dict, thread: dict) -> Any:\n        params = {\n            k: v\n            for k, v in input.items()\n            if k\n            in (\n                \"instructions\",\n                \"model\",\n                \"tools\",\n                \"parallel_tool_calls\",\n                \"top_p\",\n                \"temperature\",\n                \"max_completion_tokens\",\n                \"max_prompt_tokens\",\n                \"run_metadata\",\n            )\n        }\n        run = await self.async_client.beta.threads.create_and_run(\n            assistant_id=self.assistant_id,\n            thread=thread,\n            **params,\n        )\n        return run\n\n    async def _aget_response(self, run: Any) -> Any:\n        # TODO: Pagination\n\n        if run.status == \"completed\":\n            import openai\n\n            major_version = int(openai.version.VERSION.split(\".\")[0])\n            minor_version = int(openai.version.VERSION.split(\".\")[1])\n            version_gte_1_14 = (major_version > 1) or (\n                major_version == 1 and minor_version >= 14\n            )\n\n            messages = await self.async_client.beta.threads.messages.list(\n                run.thread_id, order=\"asc\"\n            )\n            new_messages = [msg for msg in messages if msg.run_id == run.id]\n            if not self.as_agent:\n                return new_messages\n            answer: Any = [\n                msg_content for msg in new_messages for msg_content in msg.content\n            ]\n            if all(\n                (\n                    isinstance(content, openai.types.beta.threads.TextContentBlock)\n                    if version_gte_1_14\n                    else isinstance(\n                        content,\n                        openai.types.beta.threads.MessageContentText,  # type: ignore[attr-defined]\n                    )\n                )\n                for content in answer\n            ):\n                answer = \"\\n\".join(content.text.value for content in answer)\n            return OpenAIAssistantFinish(\n                return_values={\n                    \"output\": answer,\n                    \"thread_id\": run.thread_id,\n                    \"run_id\": run.id,\n                },\n                log=\"\",\n                run_id=run.id,\n                thread_id=run.thread_id,\n            )\n        elif run.status == \"requires_action\":\n            if not self.as_agent:\n                return run.required_action.submit_tool_outputs.tool_calls\n            actions = []\n            for tool_call in run.required_action.submit_tool_outputs.tool_calls:\n                function = tool_call.function\n                try:\n                    args = json.loads(function.arguments, strict=False)\n                except JSONDecodeError as e:\n                    raise ValueError(\n                        f\"Received invalid JSON function arguments: \"\n                        f\"{function.arguments} for function {function.name}\"\n                    ) from e\n                if len(args) == 1 and \"__arg1\" in args:\n                    args = args[\"__arg1\"]\n                actions.append(\n                    OpenAIAssistantAction(\n                        tool=function.name,\n                        tool_input=args,\n                        tool_call_id=tool_call.id,\n                        log=\"\",\n                        run_id=run.id,\n                        thread_id=run.thread_id,\n                    )\n                )\n            return actions\n        else:\n            run_info = json.dumps(run.dict(), indent=2)\n            raise ValueError(\n                f\"Unexpected run status: {run.status}. Full run info:\\n\\n{run_info})\"\n            )\n\n    async def _await_for_run(self, run_id: str, thread_id: str) -> Any:\n        in_progress = True\n        while in_progress:\n            run = await self.async_client.beta.threads.runs.retrieve(\n                run_id, thread_id=thread_id\n            )\n            in_progress = run.status in (\"in_progress\", \"queued\")\n            if in_progress:\n                await asyncio.sleep(self.check_every_ms / 1000)\n        return run\n",
        "patch": "@@ -652,7 +652,7 @@ async def _aparse_intermediate_steps(\n         self, intermediate_steps: List[Tuple[OpenAIAssistantAction, str]]\n     ) -> dict:\n         last_action, last_output = intermediate_steps[-1]\n-        run = await self._wait_for_run(last_action.run_id, last_action.thread_id)\n+        run = self._wait_for_run(last_action.run_id, last_action.thread_id)\n         required_tool_call_ids = set()\n         if run.required_action:\n             required_tool_call_ids = {"
      }
    ]
  },
  {
    "number": 29921,
    "title": "core[patch]: return ToolMessage from tools when tool call ID is empty string",
    "body": null,
    "issue_title": "core[patch]: return ToolMessage from tools when tool call ID is empty string",
    "issue_body": null,
    "files": [
      {
        "filename": "libs/core/langchain_core/tools/base.py",
        "content_before": "from __future__ import annotations\n\nimport asyncio\nimport functools\nimport inspect\nimport json\nimport uuid\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom contextvars import copy_context\nfrom inspect import signature\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n    get_args,\n    get_origin,\n    get_type_hints,\n)\n\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    PydanticDeprecationWarning,\n    SkipValidation,\n    ValidationError,\n    model_validator,\n    validate_arguments,\n)\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom pydantic.v1 import ValidationError as ValidationErrorV1\nfrom pydantic.v1 import validate_arguments as validate_arguments_v1\n\nfrom langchain_core._api import deprecated\nfrom langchain_core.callbacks import (\n    AsyncCallbackManager,\n    BaseCallbackManager,\n    CallbackManager,\n    Callbacks,\n)\nfrom langchain_core.messages.tool import ToolCall, ToolMessage, ToolOutputMixin\nfrom langchain_core.runnables import (\n    RunnableConfig,\n    RunnableSerializable,\n    ensure_config,\n    patch_config,\n    run_in_executor,\n)\nfrom langchain_core.runnables.config import _set_config_context\nfrom langchain_core.runnables.utils import asyncio_accepts_context\nfrom langchain_core.utils.function_calling import (\n    _parse_google_docstring,\n    _py_38_safe_origin,\n)\nfrom langchain_core.utils.pydantic import (\n    TypeBaseModel,\n    _create_subset_model,\n    get_fields,\n    is_basemodel_subclass,\n    is_pydantic_v1_subclass,\n    is_pydantic_v2_subclass,\n)\n\nFILTERED_ARGS = (\"run_manager\", \"callbacks\")\n\n\nclass SchemaAnnotationError(TypeError):\n    \"\"\"Raised when 'args_schema' is missing or has an incorrect type annotation.\"\"\"\n\n\ndef _is_annotated_type(typ: type[Any]) -> bool:\n    return get_origin(typ) is Annotated\n\n\ndef _get_annotation_description(arg_type: type) -> str | None:\n    if _is_annotated_type(arg_type):\n        annotated_args = get_args(arg_type)\n        for annotation in annotated_args[1:]:\n            if isinstance(annotation, str):\n                return annotation\n    return None\n\n\ndef _get_filtered_args(\n    inferred_model: type[BaseModel],\n    func: Callable,\n    *,\n    filter_args: Sequence[str],\n    include_injected: bool = True,\n) -> dict:\n    \"\"\"Get the arguments from a function's signature.\"\"\"\n    schema = inferred_model.model_json_schema()[\"properties\"]\n    valid_keys = signature(func).parameters\n    return {\n        k: schema[k]\n        for i, (k, param) in enumerate(valid_keys.items())\n        if k not in filter_args\n        and (i > 0 or param.name not in (\"self\", \"cls\"))\n        and (include_injected or not _is_injected_arg_type(param.annotation))\n    }\n\n\ndef _parse_python_function_docstring(\n    function: Callable, annotations: dict, error_on_invalid_docstring: bool = False\n) -> tuple[str, dict]:\n    \"\"\"Parse the function and argument descriptions from the docstring of a function.\n\n    Assumes the function docstring follows Google Python style guide.\n    \"\"\"\n    docstring = inspect.getdoc(function)\n    return _parse_google_docstring(\n        docstring,\n        list(annotations),\n        error_on_invalid_docstring=error_on_invalid_docstring,\n    )\n\n\ndef _validate_docstring_args_against_annotations(\n    arg_descriptions: dict, annotations: dict\n) -> None:\n    \"\"\"Raise error if docstring arg is not in type annotations.\"\"\"\n    for docstring_arg in arg_descriptions:\n        if docstring_arg not in annotations:\n            msg = f\"Arg {docstring_arg} in docstring not found in function signature.\"\n            raise ValueError(msg)\n\n\ndef _infer_arg_descriptions(\n    fn: Callable,\n    *,\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = False,\n) -> tuple[str, dict]:\n    \"\"\"Infer argument descriptions from a function's docstring.\"\"\"\n    if hasattr(inspect, \"get_annotations\"):\n        # This is for python < 3.10\n        annotations = inspect.get_annotations(fn)  # type: ignore\n    else:\n        annotations = getattr(fn, \"__annotations__\", {})\n    if parse_docstring:\n        description, arg_descriptions = _parse_python_function_docstring(\n            fn, annotations, error_on_invalid_docstring=error_on_invalid_docstring\n        )\n    else:\n        description = inspect.getdoc(fn) or \"\"\n        arg_descriptions = {}\n    if parse_docstring:\n        _validate_docstring_args_against_annotations(arg_descriptions, annotations)\n    for arg, arg_type in annotations.items():\n        if arg in arg_descriptions:\n            continue\n        if desc := _get_annotation_description(arg_type):\n            arg_descriptions[arg] = desc\n    return description, arg_descriptions\n\n\ndef _is_pydantic_annotation(annotation: Any, pydantic_version: str = \"v2\") -> bool:\n    \"\"\"Determine if a type annotation is a Pydantic model.\"\"\"\n    base_model_class = BaseModelV1 if pydantic_version == \"v1\" else BaseModel\n    try:\n        return issubclass(annotation, base_model_class)\n    except TypeError:\n        return False\n\n\ndef _function_annotations_are_pydantic_v1(\n    signature: inspect.Signature, func: Callable\n) -> bool:\n    \"\"\"Determine if all Pydantic annotations in a function signature are from V1.\"\"\"\n    any_v1_annotations = any(\n        _is_pydantic_annotation(parameter.annotation, pydantic_version=\"v1\")\n        for parameter in signature.parameters.values()\n    )\n    any_v2_annotations = any(\n        _is_pydantic_annotation(parameter.annotation, pydantic_version=\"v2\")\n        for parameter in signature.parameters.values()\n    )\n    if any_v1_annotations and any_v2_annotations:\n        msg = (\n            f\"Function {func} contains a mix of Pydantic v1 and v2 annotations. \"\n            \"Only one version of Pydantic annotations per function is supported.\"\n        )\n        raise NotImplementedError(msg)\n    return any_v1_annotations and not any_v2_annotations\n\n\nclass _SchemaConfig:\n    \"\"\"Configuration for the pydantic model.\n\n    This is used to configure the pydantic model created from\n    a function's signature.\n\n    Parameters:\n        extra: Whether to allow extra fields in the model.\n        arbitrary_types_allowed: Whether to allow arbitrary types in the model.\n            Defaults to True.\n    \"\"\"\n\n    extra: str = \"forbid\"\n    arbitrary_types_allowed: bool = True\n\n\ndef create_schema_from_function(\n    model_name: str,\n    func: Callable,\n    *,\n    filter_args: Optional[Sequence[str]] = None,\n    parse_docstring: bool = False,\n    error_on_invalid_docstring: bool = False,\n    include_injected: bool = True,\n) -> type[BaseModel]:\n    \"\"\"Create a pydantic schema from a function's signature.\n\n    Args:\n        model_name: Name to assign to the generated pydantic schema.\n        func: Function to generate the schema from.\n        filter_args: Optional list of arguments to exclude from the schema.\n            Defaults to FILTERED_ARGS.\n        parse_docstring: Whether to parse the function's docstring for descriptions\n            for each argument. Defaults to False.\n        error_on_invalid_docstring: if ``parse_docstring`` is provided, configure\n            whether to raise ValueError on invalid Google Style docstrings.\n            Defaults to False.\n        include_injected: Whether to include injected arguments in the schema.\n            Defaults to True, since we want to include them in the schema\n            when *validating* tool inputs.\n\n    Returns:\n        A pydantic model with the same arguments as the function.\n    \"\"\"\n    sig = inspect.signature(func)\n\n    if _function_annotations_are_pydantic_v1(sig, func):\n        validated = validate_arguments_v1(func, config=_SchemaConfig)  # type: ignore\n    else:\n        # https://docs.pydantic.dev/latest/usage/validation_decorator/\n        with warnings.catch_warnings():\n            # We are using deprecated functionality here.\n            # This code should be re-written to simply construct a pydantic model\n            # using inspect.signature and create_model.\n            warnings.simplefilter(\"ignore\", category=PydanticDeprecationWarning)\n            validated = validate_arguments(func, config=_SchemaConfig)  # type: ignore\n\n    # Let's ignore `self` and `cls` arguments for class and instance methods\n    # If qualified name has a \".\", then it likely belongs in a class namespace\n    in_class = bool(func.__qualname__ and \".\" in func.__qualname__)\n\n    has_args = False\n    has_kwargs = False\n\n    for param in sig.parameters.values():\n        if param.kind == param.VAR_POSITIONAL:\n            has_args = True\n        elif param.kind == param.VAR_KEYWORD:\n            has_kwargs = True\n\n    inferred_model = validated.model  # type: ignore\n\n    if filter_args:\n        filter_args_ = filter_args\n    else:\n        # Handle classmethods and instance methods\n        existing_params: list[str] = list(sig.parameters.keys())\n        if existing_params and existing_params[0] in (\"self\", \"cls\") and in_class:\n            filter_args_ = [existing_params[0]] + list(FILTERED_ARGS)\n        else:\n            filter_args_ = list(FILTERED_ARGS)\n\n        for existing_param in existing_params:\n            if not include_injected and _is_injected_arg_type(\n                sig.parameters[existing_param].annotation\n            ):\n                filter_args_.append(existing_param)\n\n    description, arg_descriptions = _infer_arg_descriptions(\n        func,\n        parse_docstring=parse_docstring,\n        error_on_invalid_docstring=error_on_invalid_docstring,\n    )\n    # Pydantic adds placeholder virtual fields we need to strip\n    valid_properties = []\n    for field in get_fields(inferred_model):\n        if not has_args and field == \"args\":\n            continue\n        if not has_kwargs and field == \"kwargs\":\n            continue\n\n        if field == \"v__duplicate_kwargs\":  # Internal pydantic field\n            continue\n\n        if field not in filter_args_:\n            valid_properties.append(field)\n\n    return _create_subset_model(\n        model_name,\n        inferred_model,\n        list(valid_properties),\n        descriptions=arg_descriptions,\n        fn_description=description,\n    )\n\n\nclass ToolException(Exception):  # noqa: N818\n    \"\"\"Optional exception that tool throws when execution error occurs.\n\n    When this exception is thrown, the agent will not stop working,\n    but it will handle the exception according to the handle_tool_error\n    variable of the tool, and the processing result will be returned\n    to the agent as observation, and printed in red on the console.\n    \"\"\"\n\n\nArgsSchema = Union[TypeBaseModel, dict[str, Any]]\n\n\nclass BaseTool(RunnableSerializable[Union[str, dict, ToolCall], Any]):\n    \"\"\"Interface LangChain tools must implement.\"\"\"\n\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        \"\"\"Create the definition of the new tool class.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        args_schema_type = cls.__annotations__.get(\"args_schema\", None)\n\n        if args_schema_type is not None and args_schema_type == BaseModel:\n            # Throw errors for common mis-annotations.\n            # TODO: Use get_args / get_origin and fully\n            # specify valid annotations.\n            typehint_mandate = \"\"\"\nclass ChildTool(BaseTool):\n    ...\n    args_schema: Type[BaseModel] = SchemaClass\n    ...\"\"\"\n            name = cls.__name__\n            msg = (\n                f\"Tool definition for {name} must include valid type annotations\"\n                f\" for argument 'args_schema' to behave as expected.\\n\"\n                f\"Expected annotation of 'Type[BaseModel]'\"\n                f\" but got '{args_schema_type}'.\\n\"\n                f\"Expected class looks like:\\n\"\n                f\"{typehint_mandate}\"\n            )\n            raise SchemaAnnotationError(msg)\n\n    name: str\n    \"\"\"The unique name of the tool that clearly communicates its purpose.\"\"\"\n    description: str\n    \"\"\"Used to tell the model how/when/why to use the tool.\n\n    You can provide few-shot examples as a part of the description.\n    \"\"\"\n\n    args_schema: Annotated[Optional[ArgsSchema], SkipValidation()] = Field(\n        default=None, description=\"The tool schema.\"\n    )\n    \"\"\"Pydantic model class to validate and parse the tool's input arguments.\n\n    Args schema should be either:\n\n    - A subclass of pydantic.BaseModel.\n    or\n    - A subclass of pydantic.v1.BaseModel if accessing v1 namespace in pydantic 2\n    or\n    - a JSON schema dict\n    \"\"\"\n    return_direct: bool = False\n    \"\"\"Whether to return the tool's output directly.\n\n    Setting this to True means\n    that after the tool is called, the AgentExecutor will stop looping.\n    \"\"\"\n    verbose: bool = False\n    \"\"\"Whether to log the tool's progress.\"\"\"\n\n    callbacks: Callbacks = Field(default=None, exclude=True)\n    \"\"\"Callbacks to be called during tool execution.\"\"\"\n\n    callback_manager: Optional[BaseCallbackManager] = deprecated(\n        name=\"callback_manager\", since=\"0.1.7\", removal=\"1.0\", alternative=\"callbacks\"\n    )(\n        Field(\n            default=None,\n            exclude=True,\n            description=\"Callback manager to add to the run trace.\",\n        )\n    )\n    tags: Optional[list[str]] = None\n    \"\"\"Optional list of tags associated with the tool. Defaults to None.\n    These tags will be associated with each call to this tool,\n    and passed as arguments to the handlers defined in `callbacks`.\n    You can use these to eg identify a specific instance of a tool with its use case.\n    \"\"\"\n    metadata: Optional[dict[str, Any]] = None\n    \"\"\"Optional metadata associated with the tool. Defaults to None.\n    This metadata will be associated with each call to this tool,\n    and passed as arguments to the handlers defined in `callbacks`.\n    You can use these to eg identify a specific instance of a tool with its use case.\n    \"\"\"\n\n    handle_tool_error: Optional[Union[bool, str, Callable[[ToolException], str]]] = (\n        False\n    )\n    \"\"\"Handle the content of the ToolException thrown.\"\"\"\n\n    handle_validation_error: Optional[\n        Union[bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]]\n    ] = False\n    \"\"\"Handle the content of the ValidationError thrown.\"\"\"\n\n    response_format: Literal[\"content\", \"content_and_artifact\"] = \"content\"\n    \"\"\"The tool response format. Defaults to 'content'.\n\n    If \"content\" then the output of the tool is interpreted as the contents of a\n    ToolMessage. If \"content_and_artifact\" then the output is expected to be a\n    two-tuple corresponding to the (content, artifact) of a ToolMessage.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Initialize the tool.\"\"\"\n        if (\n            \"args_schema\" in kwargs\n            and kwargs[\"args_schema\"] is not None\n            and not is_basemodel_subclass(kwargs[\"args_schema\"])\n            and not isinstance(kwargs[\"args_schema\"], dict)\n        ):\n            msg = (\n                \"args_schema must be a subclass of pydantic BaseModel or \"\n                f\"a JSON schema dict. Got: {kwargs['args_schema']}.\"\n            )\n            raise TypeError(msg)\n        super().__init__(**kwargs)\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    def is_single_input(self) -> bool:\n        \"\"\"Whether the tool only accepts a single input.\"\"\"\n        keys = {k for k in self.args if k != \"kwargs\"}\n        return len(keys) == 1\n\n    @property\n    def args(self) -> dict:\n        if isinstance(self.args_schema, dict):\n            json_schema = self.args_schema\n        else:\n            input_schema = self.get_input_schema()\n            json_schema = input_schema.model_json_schema()\n        return json_schema[\"properties\"]\n\n    @property\n    def tool_call_schema(self) -> ArgsSchema:\n        if isinstance(self.args_schema, dict):\n            return self.args_schema\n\n        full_schema = self.get_input_schema()\n        fields = []\n        for name, type_ in get_all_basemodel_annotations(full_schema).items():\n            if not _is_injected_arg_type(type_):\n                fields.append(name)\n        return _create_subset_model(\n            self.name, full_schema, fields, fn_description=self.description\n        )\n\n    # --- Runnable ---\n\n    def get_input_schema(\n        self, config: Optional[RunnableConfig] = None\n    ) -> type[BaseModel]:\n        \"\"\"The tool's input schema.\n\n        Args:\n            config: The configuration for the tool.\n\n        Returns:\n            The input schema for the tool.\n        \"\"\"\n        if self.args_schema is not None:\n            if isinstance(self.args_schema, dict):\n                return super().get_input_schema(config)\n            return self.args_schema\n        else:\n            return create_schema_from_function(self.name, self._run)\n\n    def invoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n        return self.run(tool_input, **kwargs)\n\n    async def ainvoke(\n        self,\n        input: Union[str, dict, ToolCall],\n        config: Optional[RunnableConfig] = None,\n        **kwargs: Any,\n    ) -> Any:\n        tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n        return await self.arun(tool_input, **kwargs)\n\n    # --- Tool ---\n\n    def _parse_input(\n        self, tool_input: Union[str, dict], tool_call_id: Optional[str]\n    ) -> Union[str, dict[str, Any]]:\n        \"\"\"Convert tool input to a pydantic model.\n\n        Args:\n            tool_input: The input to the tool.\n        \"\"\"\n        input_args = self.args_schema\n        if isinstance(tool_input, str):\n            if input_args is not None:\n                if isinstance(input_args, dict):\n                    msg = (\n                        \"String tool inputs are not allowed when \"\n                        \"using tools with JSON schema args_schema.\"\n                    )\n                    raise ValueError(msg)\n                key_ = next(iter(get_fields(input_args).keys()))\n                if hasattr(input_args, \"model_validate\"):\n                    input_args.model_validate({key_: tool_input})\n                else:\n                    input_args.parse_obj({key_: tool_input})\n            return tool_input\n        else:\n            if input_args is not None:\n                if isinstance(input_args, dict):\n                    return tool_input\n                elif issubclass(input_args, BaseModel):\n                    for k, v in get_all_basemodel_annotations(input_args).items():\n                        if (\n                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)\n                            and k not in tool_input\n                        ):\n                            if tool_call_id is None:\n                                msg = (\n                                    \"When tool includes an InjectedToolCallId \"\n                                    \"argument, tool must always be invoked with a full \"\n                                    \"model ToolCall of the form: {'args': {...}, \"\n                                    \"'name': '...', 'type': 'tool_call', \"\n                                    \"'tool_call_id': '...'}\"\n                                )\n                                raise ValueError(msg)\n                            tool_input[k] = tool_call_id\n                    result = input_args.model_validate(tool_input)\n                    result_dict = result.model_dump()\n                elif issubclass(input_args, BaseModelV1):\n                    for k, v in get_all_basemodel_annotations(input_args).items():\n                        if (\n                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)\n                            and k not in tool_input\n                        ):\n                            if tool_call_id is None:\n                                msg = (\n                                    \"When tool includes an InjectedToolCallId \"\n                                    \"argument, tool must always be invoked with a full \"\n                                    \"model ToolCall of the form: {'args': {...}, \"\n                                    \"'name': '...', 'type': 'tool_call', \"\n                                    \"'tool_call_id': '...'}\"\n                                )\n                                raise ValueError(msg)\n                            tool_input[k] = tool_call_id\n                    result = input_args.parse_obj(tool_input)\n                    result_dict = result.dict()\n                else:\n                    msg = (\n                        \"args_schema must be a Pydantic BaseModel, \"\n                        f\"got {self.args_schema}\"\n                    )\n                    raise NotImplementedError(msg)\n                return {\n                    k: getattr(result, k)\n                    for k, v in result_dict.items()\n                    if k in tool_input\n                }\n            return tool_input\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def raise_deprecation(cls, values: dict) -> Any:\n        \"\"\"Raise deprecation warning if callback_manager is used.\n\n        Args:\n            values: The values to validate.\n\n        Returns:\n            The validated values.\n        \"\"\"\n        if values.get(\"callback_manager\") is not None:\n            warnings.warn(\n                \"callback_manager is deprecated. Please use callbacks instead.\",\n                DeprecationWarning,\n                stacklevel=6,\n            )\n            values[\"callbacks\"] = values.pop(\"callback_manager\", None)\n        return values\n\n    @abstractmethod\n    def _run(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Use the tool.\n\n        Add run_manager: Optional[CallbackManagerForToolRun] = None\n        to child implementations to enable tracing.\n        \"\"\"\n\n    async def _arun(self, *args: Any, **kwargs: Any) -> Any:\n        \"\"\"Use the tool asynchronously.\n\n        Add run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n        to child implementations to enable tracing.\n        \"\"\"\n        if kwargs.get(\"run_manager\") and signature(self._run).parameters.get(\n            \"run_manager\"\n        ):\n            kwargs[\"run_manager\"] = kwargs[\"run_manager\"].get_sync()\n        return await run_in_executor(None, self._run, *args, **kwargs)\n\n    def _to_args_and_kwargs(\n        self, tool_input: Union[str, dict], tool_call_id: Optional[str]\n    ) -> tuple[tuple, dict]:\n        if (\n            self.args_schema is not None\n            and isinstance(self.args_schema, type)\n            and is_basemodel_subclass(self.args_schema)\n            and not get_fields(self.args_schema)\n        ):\n            # StructuredTool with no args\n            return (), {}\n        tool_input = self._parse_input(tool_input, tool_call_id)\n        # For backwards compatibility, if run_input is a string,\n        # pass as a positional argument.\n        if isinstance(tool_input, str):\n            return (tool_input,), {}\n        else:\n            return (), tool_input\n\n    def run(\n        self,\n        tool_input: Union[str, dict[str, Any]],\n        verbose: Optional[bool] = None,\n        start_color: Optional[str] = \"green\",\n        color: Optional[str] = \"green\",\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        config: Optional[RunnableConfig] = None,\n        tool_call_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the tool.\n\n        Args:\n            tool_input: The input to the tool.\n            verbose: Whether to log the tool's progress. Defaults to None.\n            start_color: The color to use when starting the tool. Defaults to 'green'.\n            color: The color to use when ending the tool. Defaults to 'green'.\n            callbacks: Callbacks to be called during tool execution. Defaults to None.\n            tags: Optional list of tags associated with the tool. Defaults to None.\n            metadata: Optional metadata associated with the tool. Defaults to None.\n            run_name: The name of the run. Defaults to None.\n            run_id: The id of the run. Defaults to None.\n            config: The configuration for the tool. Defaults to None.\n            tool_call_id: The id of the tool call. Defaults to None.\n            kwargs: Keyword arguments to be passed to tool callbacks\n\n        Returns:\n            The output of the tool.\n\n        Raises:\n            ToolException: If an error occurs during tool execution.\n        \"\"\"\n        callback_manager = CallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose or bool(verbose),\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n\n        run_manager = callback_manager.on_tool_start(\n            {\"name\": self.name, \"description\": self.description},\n            tool_input if isinstance(tool_input, str) else str(tool_input),\n            color=start_color,\n            name=run_name,\n            run_id=run_id,\n            # Inputs by definition should always be dicts.\n            # For now, it's unclear whether this assumption is ever violated,\n            # but if it is we will send a `None` value to the callback instead\n            # TODO: will need to address issue via a patch.\n            inputs=tool_input if isinstance(tool_input, dict) else None,\n            **kwargs,\n        )\n\n        content = None\n        artifact = None\n        status = \"success\"\n        error_to_raise: Union[Exception, KeyboardInterrupt, None] = None\n        try:\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)\n            if signature(self._run).parameters.get(\"run_manager\"):\n                tool_kwargs = tool_kwargs | {\"run_manager\": run_manager}\n            if config_param := _get_runnable_config_param(self._run):\n                tool_kwargs = tool_kwargs | {config_param: config}\n            response = context.run(self._run, *tool_args, **tool_kwargs)\n            if self.response_format == \"content_and_artifact\":\n                if not isinstance(response, tuple) or len(response) != 2:\n                    msg = (\n                        \"Since response_format='content_and_artifact' \"\n                        \"a two-tuple of the message content and raw tool output is \"\n                        f\"expected. Instead generated response of type: \"\n                        f\"{type(response)}.\"\n                    )\n                    error_to_raise = ValueError(msg)\n                else:\n                    content, artifact = response\n            else:\n                content = response\n        except (ValidationError, ValidationErrorV1) as e:\n            if not self.handle_validation_error:\n                error_to_raise = e\n            else:\n                content = _handle_validation_error(e, flag=self.handle_validation_error)\n                status = \"error\"\n        except ToolException as e:\n            if not self.handle_tool_error:\n                error_to_raise = e\n            else:\n                content = _handle_tool_error(e, flag=self.handle_tool_error)\n                status = \"error\"\n        except (Exception, KeyboardInterrupt) as e:\n            error_to_raise = e\n\n        if error_to_raise:\n            run_manager.on_tool_error(error_to_raise)\n            raise error_to_raise\n        output = _format_output(content, artifact, tool_call_id, self.name, status)\n        run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n        return output\n\n    async def arun(\n        self,\n        tool_input: Union[str, dict],\n        verbose: Optional[bool] = None,\n        start_color: Optional[str] = \"green\",\n        color: Optional[str] = \"green\",\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        run_id: Optional[uuid.UUID] = None,\n        config: Optional[RunnableConfig] = None,\n        tool_call_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Run the tool asynchronously.\n\n        Args:\n            tool_input: The input to the tool.\n            verbose: Whether to log the tool's progress. Defaults to None.\n            start_color: The color to use when starting the tool. Defaults to 'green'.\n            color: The color to use when ending the tool. Defaults to 'green'.\n            callbacks: Callbacks to be called during tool execution. Defaults to None.\n            tags: Optional list of tags associated with the tool. Defaults to None.\n            metadata: Optional metadata associated with the tool. Defaults to None.\n            run_name: The name of the run. Defaults to None.\n            run_id: The id of the run. Defaults to None.\n            config: The configuration for the tool. Defaults to None.\n            tool_call_id: The id of the tool call. Defaults to None.\n            kwargs: Keyword arguments to be passed to tool callbacks\n\n        Returns:\n            The output of the tool.\n\n        Raises:\n            ToolException: If an error occurs during tool execution.\n        \"\"\"\n        callback_manager = AsyncCallbackManager.configure(\n            callbacks,\n            self.callbacks,\n            self.verbose or bool(verbose),\n            tags,\n            self.tags,\n            metadata,\n            self.metadata,\n        )\n        run_manager = await callback_manager.on_tool_start(\n            {\"name\": self.name, \"description\": self.description},\n            tool_input if isinstance(tool_input, str) else str(tool_input),\n            color=start_color,\n            name=run_name,\n            run_id=run_id,\n            # Inputs by definition should always be dicts.\n            # For now, it's unclear whether this assumption is ever violated,\n            # but if it is we will send a `None` value to the callback instead\n            # TODO: will need to address issue via a patch.\n            inputs=tool_input if isinstance(tool_input, dict) else None,\n            **kwargs,\n        )\n        content = None\n        artifact = None\n        status = \"success\"\n        error_to_raise: Optional[Union[Exception, KeyboardInterrupt]] = None\n        try:\n            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)\n            child_config = patch_config(config, callbacks=run_manager.get_child())\n            context = copy_context()\n            context.run(_set_config_context, child_config)\n            func_to_check = (\n                self._run if self.__class__._arun is BaseTool._arun else self._arun\n            )\n            if signature(func_to_check).parameters.get(\"run_manager\"):\n                tool_kwargs[\"run_manager\"] = run_manager\n            if config_param := _get_runnable_config_param(func_to_check):\n                tool_kwargs[config_param] = config\n\n            coro = context.run(self._arun, *tool_args, **tool_kwargs)\n            if asyncio_accepts_context():\n                response = await asyncio.create_task(coro, context=context)  # type: ignore\n            else:\n                response = await coro\n            if self.response_format == \"content_and_artifact\":\n                if not isinstance(response, tuple) or len(response) != 2:\n                    msg = (\n                        \"Since response_format='content_and_artifact' \"\n                        \"a two-tuple of the message content and raw tool output is \"\n                        f\"expected. Instead generated response of type: \"\n                        f\"{type(response)}.\"\n                    )\n                    error_to_raise = ValueError(msg)\n                else:\n                    content, artifact = response\n            else:\n                content = response\n        except ValidationError as e:\n            if not self.handle_validation_error:\n                error_to_raise = e\n            else:\n                content = _handle_validation_error(e, flag=self.handle_validation_error)\n                status = \"error\"\n        except ToolException as e:\n            if not self.handle_tool_error:\n                error_to_raise = e\n            else:\n                content = _handle_tool_error(e, flag=self.handle_tool_error)\n                status = \"error\"\n        except (Exception, KeyboardInterrupt) as e:\n            error_to_raise = e\n\n        if error_to_raise:\n            await run_manager.on_tool_error(error_to_raise)\n            raise error_to_raise\n\n        output = _format_output(content, artifact, tool_call_id, self.name, status)\n        await run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n        return output\n\n    @deprecated(\"0.1.47\", alternative=\"invoke\", removal=\"1.0\")\n    def __call__(self, tool_input: str, callbacks: Callbacks = None) -> str:\n        \"\"\"Make tool callable.\"\"\"\n        return self.run(tool_input, callbacks=callbacks)\n\n\ndef _is_tool_call(x: Any) -> bool:\n    return isinstance(x, dict) and x.get(\"type\") == \"tool_call\"\n\n\ndef _handle_validation_error(\n    e: Union[ValidationError, ValidationErrorV1],\n    *,\n    flag: Union[\n        Literal[True], str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> str:\n    if isinstance(flag, bool):\n        content = \"Tool input validation error\"\n    elif isinstance(flag, str):\n        content = flag\n    elif callable(flag):\n        content = flag(e)\n    else:\n        msg = (\n            f\"Got unexpected type of `handle_validation_error`. Expected bool, \"\n            f\"str or callable. Received: {flag}\"\n        )\n        raise ValueError(msg)  # noqa: TRY004\n    return content\n\n\ndef _handle_tool_error(\n    e: ToolException,\n    *,\n    flag: Optional[Union[Literal[True], str, Callable[[ToolException], str]]],\n) -> str:\n    if isinstance(flag, bool):\n        content = e.args[0] if e.args else \"Tool execution error\"\n    elif isinstance(flag, str):\n        content = flag\n    elif callable(flag):\n        content = flag(e)\n    else:\n        msg = (\n            f\"Got unexpected type of `handle_tool_error`. Expected bool, str \"\n            f\"or callable. Received: {flag}\"\n        )\n        raise ValueError(msg)  # noqa: TRY004\n    return content\n\n\ndef _prep_run_args(\n    input: Union[str, dict, ToolCall],\n    config: Optional[RunnableConfig],\n    **kwargs: Any,\n) -> tuple[Union[str, dict], dict]:\n    config = ensure_config(config)\n    if _is_tool_call(input):\n        tool_call_id: Optional[str] = cast(ToolCall, input)[\"id\"]\n        tool_input: Union[str, dict] = cast(ToolCall, input)[\"args\"].copy()\n    else:\n        tool_call_id = None\n        tool_input = cast(Union[str, dict], input)\n    return (\n        tool_input,\n        dict(\n            callbacks=config.get(\"callbacks\"),\n            tags=config.get(\"tags\"),\n            metadata=config.get(\"metadata\"),\n            run_name=config.get(\"run_name\"),\n            run_id=config.pop(\"run_id\", None),\n            config=config,\n            tool_call_id=tool_call_id,\n            **kwargs,\n        ),\n    )\n\n\ndef _format_output(\n    content: Any,\n    artifact: Any,\n    tool_call_id: Optional[str],\n    name: str,\n    status: str,\n) -> Union[ToolOutputMixin, Any]:\n    if isinstance(content, ToolOutputMixin) or not tool_call_id:\n        return content\n    if not _is_message_content_type(content):\n        content = _stringify(content)\n    return ToolMessage(\n        content,\n        artifact=artifact,\n        tool_call_id=tool_call_id,\n        name=name,\n        status=status,\n    )\n\n\ndef _is_message_content_type(obj: Any) -> bool:\n    \"\"\"Check for OpenAI or Anthropic format tool message content.\"\"\"\n    return (\n        isinstance(obj, str)\n        or isinstance(obj, list)\n        and all(_is_message_content_block(e) for e in obj)\n    )\n\n\ndef _is_message_content_block(obj: Any) -> bool:\n    \"\"\"Check for OpenAI or Anthropic format tool message content blocks.\"\"\"\n    if isinstance(obj, str):\n        return True\n    elif isinstance(obj, dict):\n        return obj.get(\"type\", None) in (\"text\", \"image_url\", \"image\", \"json\")\n    else:\n        return False\n\n\ndef _stringify(content: Any) -> str:\n    try:\n        return json.dumps(content, ensure_ascii=False)\n    except Exception:\n        return str(content)\n\n\ndef _get_type_hints(func: Callable) -> Optional[dict[str, type]]:\n    if isinstance(func, functools.partial):\n        func = func.func\n    try:\n        return get_type_hints(func)\n    except Exception:\n        return None\n\n\ndef _get_runnable_config_param(func: Callable) -> Optional[str]:\n    type_hints = _get_type_hints(func)\n    if not type_hints:\n        return None\n    for name, type_ in type_hints.items():\n        if type_ is RunnableConfig:\n            return name\n    return None\n\n\nclass InjectedToolArg:\n    \"\"\"Annotation for a Tool arg that is **not** meant to be generated by a model.\"\"\"\n\n\nclass InjectedToolCallId(InjectedToolArg):\n    r'''Annotation for injecting the tool_call_id.\n\n    Example:\n        ..code-block:: python\n\n            from typing_extensions import Annotated\n\n            from langchain_core.messages import ToolMessage\n            from langchain_core.tools import tool, InjectedToolCallID\n\n            @tool\n            def foo(x: int, tool_call_id: Annotated[str, InjectedToolCallID]) -> ToolMessage:\n                \"\"\"Return x.\"\"\"\n                return ToolMessage(str(x), artifact=x, name=\"foo\", tool_call_id=tool_call_id)\n    '''  # noqa: E501\n\n\ndef _is_injected_arg_type(\n    type_: type, injected_type: Optional[type[InjectedToolArg]] = None\n) -> bool:\n    injected_type = injected_type or InjectedToolArg\n    return any(\n        isinstance(arg, injected_type)\n        or (isinstance(arg, type) and issubclass(arg, injected_type))\n        for arg in get_args(type_)[1:]\n    )\n\n\ndef get_all_basemodel_annotations(\n    cls: Union[TypeBaseModel, Any], *, default_to_bound: bool = True\n) -> dict[str, type]:\n    # cls has no subscript: cls = FooBar\n    if isinstance(cls, type):\n        annotations: dict[str, type] = {}\n        for name, param in inspect.signature(cls).parameters.items():\n            # Exclude hidden init args added by pydantic Config. For example if\n            # BaseModel(extra=\"allow\") then \"extra_data\" will part of init sig.\n            if (\n                fields := getattr(cls, \"model_fields\", {})  # pydantic v2+\n                or getattr(cls, \"__fields__\", {})  # pydantic v1\n            ) and name not in fields:\n                continue\n            annotations[name] = param.annotation\n        orig_bases: tuple = getattr(cls, \"__orig_bases__\", ())\n    # cls has subscript: cls = FooBar[int]\n    else:\n        annotations = get_all_basemodel_annotations(\n            get_origin(cls), default_to_bound=False\n        )\n        orig_bases = (cls,)\n\n    # Pydantic v2 automatically resolves inherited generics, Pydantic v1 does not.\n    if not (isinstance(cls, type) and is_pydantic_v2_subclass(cls)):\n        # if cls = FooBar inherits from Baz[str], orig_bases will contain Baz[str]\n        # if cls = FooBar inherits from Baz, orig_bases will contain Baz\n        # if cls = FooBar[int], orig_bases will contain FooBar[int]\n        for parent in orig_bases:\n            # if class = FooBar inherits from Baz, parent = Baz\n            if isinstance(parent, type) and is_pydantic_v1_subclass(parent):\n                annotations.update(\n                    get_all_basemodel_annotations(parent, default_to_bound=False)\n                )\n                continue\n\n            parent_origin = get_origin(parent)\n\n            # if class = FooBar inherits from non-pydantic class\n            if not parent_origin:\n                continue\n\n            # if class = FooBar inherits from Baz[str]:\n            # parent = Baz[str],\n            # parent_origin = Baz,\n            # generic_type_vars = (type vars in Baz)\n            # generic_map = {type var in Baz: str}\n            generic_type_vars: tuple = getattr(parent_origin, \"__parameters__\", ())\n            generic_map = dict(zip(generic_type_vars, get_args(parent)))\n            for field in getattr(parent_origin, \"__annotations__\", {}):\n                annotations[field] = _replace_type_vars(\n                    annotations[field], generic_map, default_to_bound\n                )\n\n    return {\n        k: _replace_type_vars(v, default_to_bound=default_to_bound)\n        for k, v in annotations.items()\n    }\n\n\ndef _replace_type_vars(\n    type_: type,\n    generic_map: Optional[dict[TypeVar, type]] = None,\n    default_to_bound: bool = True,\n) -> type:\n    generic_map = generic_map or {}\n    if isinstance(type_, TypeVar):\n        if type_ in generic_map:\n            return generic_map[type_]\n        elif default_to_bound:\n            return type_.__bound__ or Any\n        else:\n            return type_\n    elif (origin := get_origin(type_)) and (args := get_args(type_)):\n        new_args = tuple(\n            _replace_type_vars(arg, generic_map, default_to_bound) for arg in args\n        )\n        return _py_38_safe_origin(origin)[new_args]  # type: ignore[index]\n    else:\n        return type_\n\n\nclass BaseToolkit(BaseModel, ABC):\n    \"\"\"Base Toolkit representing a collection of related tools.\"\"\"\n\n    @abstractmethod\n    def get_tools(self) -> list[BaseTool]:\n        \"\"\"Get the tools in the toolkit.\"\"\"\n",
        "patch": "@@ -960,7 +960,7 @@ def _format_output(\n     name: str,\n     status: str,\n ) -> Union[ToolOutputMixin, Any]:\n-    if isinstance(content, ToolOutputMixin) or not tool_call_id:\n+    if isinstance(content, ToolOutputMixin) or tool_call_id is None:\n         return content\n     if not _is_message_content_type(content):\n         content = _stringify(content)"
      },
      {
        "filename": "libs/core/tests/unit_tests/test_tools.py",
        "content_before": "\"\"\"Test the base tool implementation.\"\"\"\n\nimport inspect\nimport json\nimport sys\nimport textwrap\nimport threading\nfrom datetime import datetime\nfrom enum import Enum\nfrom functools import partial\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Generic,\n    Literal,\n    Optional,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport pytest\nfrom pydantic import BaseModel, Field, ValidationError\nfrom pydantic.v1 import BaseModel as BaseModelV1\nfrom pydantic.v1 import ValidationError as ValidationErrorV1\nfrom typing_extensions import TypedDict\n\nfrom langchain_core import tools\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForToolRun,\n    CallbackManagerForToolRun,\n)\nfrom langchain_core.callbacks.manager import (\n    CallbackManagerForRetrieverRun,\n)\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import ToolCall, ToolMessage\nfrom langchain_core.messages.tool import ToolOutputMixin\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.runnables import (\n    Runnable,\n    RunnableConfig,\n    RunnableLambda,\n    ensure_config,\n)\nfrom langchain_core.tools import (\n    BaseTool,\n    StructuredTool,\n    Tool,\n    ToolException,\n    tool,\n)\nfrom langchain_core.tools.base import (\n    InjectedToolArg,\n    InjectedToolCallId,\n    SchemaAnnotationError,\n    _is_message_content_block,\n    _is_message_content_type,\n    get_all_basemodel_annotations,\n)\nfrom langchain_core.utils.function_calling import convert_to_openai_function\nfrom langchain_core.utils.pydantic import (\n    PYDANTIC_MAJOR_VERSION,\n    _create_subset_model,\n    create_model_v2,\n)\nfrom tests.unit_tests.fake.callbacks import FakeCallbackHandler\nfrom tests.unit_tests.pydantic_utils import _schema\n\n\ndef _get_tool_call_json_schema(tool: BaseTool) -> dict:\n    tool_schema = tool.tool_call_schema\n    if isinstance(tool_schema, dict):\n        return tool_schema\n\n    if hasattr(tool_schema, \"model_json_schema\"):\n        return tool_schema.model_json_schema()\n    else:\n        return tool_schema.schema()\n\n\ndef test_unnamed_decorator() -> None:\n    \"\"\"Test functionality with unnamed decorator.\"\"\"\n\n    @tool\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search_api\"\n    assert not search_api.return_direct\n    assert search_api.invoke(\"test\") == \"API result\"\n\n\nclass _MockSchema(BaseModel):\n    \"\"\"Return the arguments directly.\"\"\"\n\n    arg1: int\n    arg2: bool\n    arg3: Optional[dict] = None\n\n\nclass _MockSchemaV1(BaseModelV1):\n    \"\"\"Return the arguments directly.\"\"\"\n\n    arg1: int\n    arg2: bool\n    arg3: Optional[dict] = None\n\n\nclass _MockStructuredTool(BaseTool):\n    name: str = \"structured_api\"\n    args_schema: type[BaseModel] = _MockSchema\n    description: str = \"A Structured Tool\"\n\n    def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    async def _arun(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        raise NotImplementedError\n\n\ndef test_structured_args() -> None:\n    \"\"\"Test functionality with structured arguments.\"\"\"\n    structured_api = _MockStructuredTool()\n    assert isinstance(structured_api, BaseTool)\n    assert structured_api.name == \"structured_api\"\n    expected_result = \"1 True {'foo': 'bar'}\"\n    args = {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"foo\": \"bar\"}}\n    assert structured_api.run(args) == expected_result\n\n\ndef test_misannotated_base_tool_raises_error() -> None:\n    \"\"\"Test that a BaseTool with the incorrect typehint raises an exception.\"\"\" \"\"\n    with pytest.raises(SchemaAnnotationError):\n\n        class _MisAnnotatedTool(BaseTool):\n            name: str = \"structured_api\"\n            # This would silently be ignored without the custom metaclass\n            args_schema: BaseModel = _MockSchema  # type: ignore\n            description: str = \"A Structured Tool\"\n\n            def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n                return f\"{arg1} {arg2} {arg3}\"\n\n            async def _arun(\n                self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n            ) -> str:\n                raise NotImplementedError\n\n\ndef test_forward_ref_annotated_base_tool_accepted() -> None:\n    \"\"\"Test that a using forward ref annotation syntax is accepted.\"\"\" \"\"\n\n    class _ForwardRefAnnotatedTool(BaseTool):\n        name: str = \"structured_api\"\n        args_schema: \"type[BaseModel]\" = _MockSchema\n        description: str = \"A Structured Tool\"\n\n        def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n            return f\"{arg1} {arg2} {arg3}\"\n\n        async def _arun(\n            self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n        ) -> str:\n            raise NotImplementedError\n\n\ndef test_subclass_annotated_base_tool_accepted() -> None:\n    \"\"\"Test BaseTool child w/ custom schema isn't overwritten.\"\"\"\n\n    class _ForwardRefAnnotatedTool(BaseTool):\n        name: str = \"structured_api\"\n        args_schema: type[_MockSchema] = _MockSchema\n        description: str = \"A Structured Tool\"\n\n        def _run(self, arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n            return f\"{arg1} {arg2} {arg3}\"\n\n        async def _arun(\n            self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n        ) -> str:\n            raise NotImplementedError\n\n    assert issubclass(_ForwardRefAnnotatedTool, BaseTool)\n    tool = _ForwardRefAnnotatedTool()\n    assert tool.args_schema == _MockSchema\n\n\ndef test_decorator_with_specified_schema() -> None:\n    \"\"\"Test that manually specified schemata are passed through to the tool.\"\"\"\n\n    @tool(args_schema=_MockSchema)\n    def tool_func(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(tool_func, BaseTool)\n    assert tool_func.args_schema == _MockSchema\n\n    @tool(args_schema=_MockSchemaV1)\n    def tool_func_v1(arg1: int, arg2: bool, arg3: Optional[dict] = None) -> str:\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(tool_func_v1, BaseTool)\n    assert tool_func_v1.args_schema == _MockSchemaV1\n\n\ndef test_decorated_function_schema_equivalent() -> None:\n    \"\"\"Test that a BaseTool without a schema meets expectations.\"\"\"\n\n    @tool\n    def structured_tool_input(\n        arg1: int, arg2: bool, arg3: Optional[dict] = None\n    ) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        return f\"{arg1} {arg2} {arg3}\"\n\n    assert isinstance(structured_tool_input, BaseTool)\n    assert structured_tool_input.args_schema is not None\n    assert (\n        _schema(structured_tool_input.args_schema)[\"properties\"]\n        == _schema(_MockSchema)[\"properties\"]\n        == structured_tool_input.args\n    )\n\n\ndef test_args_kwargs_filtered() -> None:\n    class _SingleArgToolWithKwargs(BaseTool):\n        name: str = \"single_arg_tool\"\n        description: str = \"A  single arged tool with kwargs\"\n\n        def _run(\n            self,\n            some_arg: str,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            return \"foo\"\n\n        async def _arun(\n            self,\n            some_arg: str,\n            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            raise NotImplementedError\n\n    tool = _SingleArgToolWithKwargs()\n    assert tool.is_single_input\n\n    class _VarArgToolWithKwargs(BaseTool):\n        name: str = \"single_arg_tool\"\n        description: str = \"A single arged tool with kwargs\"\n\n        def _run(\n            self,\n            *args: Any,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            return \"foo\"\n\n        async def _arun(\n            self,\n            *args: Any,\n            run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n            **kwargs: Any,\n        ) -> str:\n            raise NotImplementedError\n\n    tool2 = _VarArgToolWithKwargs()\n    assert tool2.is_single_input\n\n\ndef test_structured_args_decorator_no_infer_schema() -> None:\n    \"\"\"Test functionality with structured arguments parsed as a decorator.\"\"\"\n\n    @tool(infer_schema=False)\n    def structured_tool_input(\n        arg1: int, arg2: Union[float, datetime], opt_arg: Optional[dict] = None\n    ) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        return f\"{arg1}, {arg2}, {opt_arg}\"\n\n    assert isinstance(structured_tool_input, BaseTool)\n    assert structured_tool_input.name == \"structured_tool_input\"\n    args = {\"arg1\": 1, \"arg2\": 0.001, \"opt_arg\": {\"foo\": \"bar\"}}\n    with pytest.raises(ToolException):\n        assert structured_tool_input.run(args)\n\n\ndef test_structured_single_str_decorator_no_infer_schema() -> None:\n    \"\"\"Test functionality with structured arguments parsed as a decorator.\"\"\"\n\n    @tool(infer_schema=False)\n    def unstructured_tool_input(tool_input: str) -> str:\n        \"\"\"Return the arguments directly.\"\"\"\n        assert isinstance(tool_input, str)\n        return f\"{tool_input}\"\n\n    assert isinstance(unstructured_tool_input, BaseTool)\n    assert unstructured_tool_input.args_schema is None\n    assert unstructured_tool_input.run(\"foo\") == \"foo\"\n\n\ndef test_structured_tool_types_parsed() -> None:\n    \"\"\"Test the non-primitive types are correctly passed to structured tools.\"\"\"\n\n    class SomeEnum(Enum):\n        A = \"a\"\n        B = \"b\"\n\n    class SomeBaseModel(BaseModel):\n        foo: str\n\n    @tool\n    def structured_tool(\n        some_enum: SomeEnum,\n        some_base_model: SomeBaseModel,\n    ) -> dict:\n        \"\"\"Return the arguments directly.\"\"\"\n        return {\n            \"some_enum\": some_enum,\n            \"some_base_model\": some_base_model,\n        }\n\n    assert isinstance(structured_tool, StructuredTool)\n    args = {\n        \"some_enum\": SomeEnum.A.value,\n        \"some_base_model\": SomeBaseModel(foo=\"bar\").model_dump(),\n    }\n    result = structured_tool.run(json.loads(json.dumps(args)))\n    expected = {\n        \"some_enum\": SomeEnum.A,\n        \"some_base_model\": SomeBaseModel(foo=\"bar\"),\n    }\n    assert result == expected\n\n\ndef test_structured_tool_types_parsed_pydantic_v1() -> None:\n    \"\"\"Test the non-primitive types are correctly passed to structured tools.\"\"\"\n\n    class SomeBaseModel(BaseModelV1):\n        foo: str\n\n    class AnotherBaseModel(BaseModelV1):\n        bar: str\n\n    @tool\n    def structured_tool(some_base_model: SomeBaseModel) -> AnotherBaseModel:\n        \"\"\"Return the arguments directly.\"\"\"\n        return AnotherBaseModel(bar=some_base_model.foo)\n\n    assert isinstance(structured_tool, StructuredTool)\n\n    expected = AnotherBaseModel(bar=\"baz\")\n    for arg in [\n        SomeBaseModel(foo=\"baz\"),\n        SomeBaseModel(foo=\"baz\").dict(),\n    ]:\n        args = {\"some_base_model\": arg}\n        result = structured_tool.run(args)\n        assert result == expected\n\n\ndef test_structured_tool_types_parsed_pydantic_mixed() -> None:\n    \"\"\"Test handling of tool with mixed Pydantic version arguments.\"\"\"\n\n    class SomeBaseModel(BaseModelV1):\n        foo: str\n\n    class AnotherBaseModel(BaseModel):\n        bar: str\n\n    with pytest.raises(NotImplementedError):\n\n        @tool\n        def structured_tool(\n            some_base_model: SomeBaseModel, another_base_model: AnotherBaseModel\n        ) -> None:\n            \"\"\"Return the arguments directly.\"\"\"\n\n\ndef test_base_tool_inheritance_base_schema() -> None:\n    \"\"\"Test schema is correctly inferred when inheriting from BaseTool.\"\"\"\n\n    class _MockSimpleTool(BaseTool):\n        name: str = \"simple_tool\"\n        description: str = \"A Simple Tool\"\n\n        def _run(self, tool_input: str) -> str:\n            return f\"{tool_input}\"\n\n        async def _arun(self, tool_input: str) -> str:\n            raise NotImplementedError\n\n    simple_tool = _MockSimpleTool()\n    assert simple_tool.args_schema is None\n    expected_args = {\"tool_input\": {\"title\": \"Tool Input\", \"type\": \"string\"}}\n    assert simple_tool.args == expected_args\n\n\ndef test_tool_lambda_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a lambda function.\"\"\"\n    tool = Tool(\n        name=\"tool\",\n        description=\"A tool\",\n        func=lambda tool_input: tool_input,\n    )\n    assert tool.args_schema is None\n    expected_args = {\"tool_input\": {\"type\": \"string\"}}\n    assert tool.args == expected_args\n\n\ndef test_structured_tool_from_function_docstring() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: str) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: the bar value\n            baz: the baz value\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__.strip())\n\n\ndef test_structured_tool_from_function_docstring_complex_args() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: list[str]) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: int\n            baz: List[str]\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\n            \"title\": \"Baz\",\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n        },\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\n                \"title\": \"Baz\",\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n            },\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__).strip()\n\n\ndef test_structured_tool_lambda_multi_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a lambda function.\"\"\"\n    tool = StructuredTool.from_function(\n        name=\"tool\",\n        description=\"A tool\",\n        func=lambda tool_input, other_arg: f\"{tool_input}{other_arg}\",  # type: ignore\n    )\n    assert tool.args_schema is not None\n    expected_args = {\n        \"tool_input\": {\"title\": \"Tool Input\"},\n        \"other_arg\": {\"title\": \"Other Arg\"},\n    }\n    assert tool.args == expected_args\n\n\ndef test_tool_partial_function_args_schema() -> None:\n    \"\"\"Test args schema inference when the tool argument is a partial function.\"\"\"\n\n    def func(tool_input: str, other_arg: str) -> str:\n        assert isinstance(tool_input, str)\n        assert isinstance(other_arg, str)\n        return tool_input + other_arg\n\n    tool = Tool(\n        name=\"tool\",\n        description=\"A tool\",\n        func=partial(func, other_arg=\"foo\"),\n    )\n    assert tool.run(\"bar\") == \"barfoo\"\n\n\ndef test_empty_args_decorator() -> None:\n    \"\"\"Test inferred schema of decorated fn with no args.\"\"\"\n\n    @tool\n    def empty_tool_input() -> str:\n        \"\"\"Return a constant.\"\"\"\n        return \"the empty result\"\n\n    assert isinstance(empty_tool_input, BaseTool)\n    assert empty_tool_input.name == \"empty_tool_input\"\n    assert empty_tool_input.args == {}\n    assert empty_tool_input.run({}) == \"the empty result\"\n\n\ndef test_tool_from_function_with_run_manager() -> None:\n    \"\"\"Test run of tool when using run_manager.\"\"\"\n\n    def foo(bar: str, callbacks: Optional[CallbackManagerForToolRun] = None) -> str:\n        \"\"\"Docstring\n        Args:\n            bar: str.\n        \"\"\"\n        assert callbacks is not None\n        return \"foo\" + bar\n\n    handler = FakeCallbackHandler()\n    tool = Tool.from_function(foo, name=\"foo\", description=\"Docstring\")\n\n    assert tool.run(tool_input={\"bar\": \"bar\"}, run_manager=[handler]) == \"foobar\"\n    assert tool.run(\"baz\", run_manager=[handler]) == \"foobaz\"\n\n\ndef test_structured_tool_from_function_with_run_manager() -> None:\n    \"\"\"Test args and schema of structured tool when using callbacks.\"\"\"\n\n    def foo(\n        bar: int, baz: str, callbacks: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        \"\"\"Docstring.\n\n        Args:\n            bar: int\n            baz: str\n        \"\"\"\n        assert callbacks is not None\n        return str(bar) + baz\n\n    handler = FakeCallbackHandler()\n    structured_tool = StructuredTool.from_function(foo)\n\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"description\": inspect.getdoc(foo),\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert (\n        structured_tool.run(\n            tool_input={\"bar\": \"10\", \"baz\": \"baz\"}, run_manger=[handler]\n        )\n        == \"10baz\"\n    )\n\n\ndef test_structured_tool_from_parameterless_function() -> None:\n    \"\"\"Test parameterless function of structured tool.\"\"\"\n\n    def foo() -> str:\n        \"\"\"Docstring.\"\"\"\n        return \"invoke foo\"\n\n    structured_tool = StructuredTool.from_function(foo)\n\n    assert structured_tool.run({}) == \"invoke foo\"\n    assert structured_tool.run(\"\") == \"invoke foo\"\n\n\ndef test_named_tool_decorator() -> None:\n    \"\"\"Test functionality when arguments are provided as input to decorator.\"\"\"\n\n    @tool(\"search\")\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        assert isinstance(query, str)\n        return f\"API result - {query}\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search\"\n    assert not search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result - foo\"\n\n\ndef test_named_tool_decorator_return_direct() -> None:\n    \"\"\"Test functionality when arguments and return direct are provided as input.\"\"\"\n\n    @tool(\"search\", return_direct=True)\n    def search_api(query: str, *args: Any) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search\"\n    assert search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result\"\n\n\ndef test_unnamed_tool_decorator_return_direct() -> None:\n    \"\"\"Test functionality when only return direct is provided.\"\"\"\n\n    @tool(return_direct=True)\n    def search_api(query: str) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        assert isinstance(query, str)\n        return \"API result\"\n\n    assert isinstance(search_api, BaseTool)\n    assert search_api.name == \"search_api\"\n    assert search_api.return_direct\n    assert search_api.run({\"query\": \"foo\"}) == \"API result\"\n\n\ndef test_tool_with_kwargs() -> None:\n    \"\"\"Test functionality when only return direct is provided.\"\"\"\n\n    @tool(return_direct=True)\n    def search_api(\n        arg_0: str,\n        arg_1: float = 4.3,\n        ping: str = \"hi\",\n    ) -> str:\n        \"\"\"Search the API for the query.\"\"\"\n        return f\"arg_0={arg_0}, arg_1={arg_1}, ping={ping}\"\n\n    assert isinstance(search_api, BaseTool)\n    result = search_api.run(\n        tool_input={\n            \"arg_0\": \"foo\",\n            \"arg_1\": 3.2,\n            \"ping\": \"pong\",\n        }\n    )\n    assert result == \"arg_0=foo, arg_1=3.2, ping=pong\"\n\n    result = search_api.run(\n        tool_input={\n            \"arg_0\": \"foo\",\n        }\n    )\n    assert result == \"arg_0=foo, arg_1=4.3, ping=hi\"\n    # For backwards compatibility, we still accept a single str arg\n    result = search_api.run(\"foobar\")\n    assert result == \"arg_0=foobar, arg_1=4.3, ping=hi\"\n\n\ndef test_missing_docstring() -> None:\n    \"\"\"Test error is raised when docstring is missing.\"\"\"\n    # expect to throw a value error if there's no docstring\n    with pytest.raises(ValueError, match=\"Function must have a docstring\"):\n\n        @tool\n        def search_api(query: str) -> str:\n            return \"API result\"\n\n\ndef test_create_tool_positional_args() -> None:\n    \"\"\"Test that positional arguments are allowed.\"\"\"\n    test_tool = Tool(\"test_name\", lambda x: x, \"test_description\")\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n    assert test_tool.is_single_input\n\n\ndef test_create_tool_keyword_args() -> None:\n    \"\"\"Test that keyword arguments are allowed.\"\"\"\n    test_tool = Tool(name=\"test_name\", func=lambda x: x, description=\"test_description\")\n    assert test_tool.is_single_input\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n\n\nasync def test_create_async_tool() -> None:\n    \"\"\"Test that async tools are allowed.\"\"\"\n\n    async def _test_func(x: str) -> str:\n        return x\n\n    test_tool = Tool(\n        name=\"test_name\",\n        func=lambda x: x,\n        description=\"test_description\",\n        coroutine=_test_func,\n    )\n    assert test_tool.is_single_input\n    assert test_tool.invoke(\"foo\") == \"foo\"\n    assert test_tool.name == \"test_name\"\n    assert test_tool.description == \"test_description\"\n    assert test_tool.coroutine is not None\n    assert await test_tool.arun(\"foo\") == \"foo\"\n\n\nclass _FakeExceptionTool(BaseTool):\n    name: str = \"exception\"\n    description: str = \"an exception-throwing tool\"\n    exception: Exception = ToolException()\n\n    def _run(self) -> str:\n        raise self.exception\n\n    async def _arun(self) -> str:\n        raise self.exception\n\n\ndef test_exception_handling_bool() -> None:\n    _tool = _FakeExceptionTool(handle_tool_error=True)\n    expected = \"Tool execution error\"\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_str() -> None:\n    expected = \"foo bar\"\n    _tool = _FakeExceptionTool(handle_tool_error=expected)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_callable() -> None:\n    expected = \"foo bar\"\n\n    def handling(e: ToolException) -> str:\n        return expected\n\n    _tool = _FakeExceptionTool(handle_tool_error=handling)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_exception_handling_non_tool_exception() -> None:\n    _tool = _FakeExceptionTool(exception=ValueError())\n    with pytest.raises(ValueError):\n        _tool.run({})\n\n\nasync def test_async_exception_handling_bool() -> None:\n    _tool = _FakeExceptionTool(handle_tool_error=True)\n    expected = \"Tool execution error\"\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_str() -> None:\n    expected = \"foo bar\"\n    _tool = _FakeExceptionTool(handle_tool_error=expected)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_callable() -> None:\n    expected = \"foo bar\"\n\n    def handling(e: ToolException) -> str:\n        return expected\n\n    _tool = _FakeExceptionTool(handle_tool_error=handling)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_exception_handling_non_tool_exception() -> None:\n    _tool = _FakeExceptionTool(exception=ValueError())\n    with pytest.raises(ValueError):\n        await _tool.arun({})\n\n\ndef test_structured_tool_from_function() -> None:\n    \"\"\"Test that structured tools can be created from functions.\"\"\"\n\n    def foo(bar: int, baz: str) -> str:\n        \"\"\"Docstring thing.\n\n        Args:\n            bar: the bar value\n            baz: the baz value\n        \"\"\"\n        raise NotImplementedError\n\n    structured_tool = StructuredTool.from_function(foo)\n    assert structured_tool.name == \"foo\"\n    assert structured_tool.args == {\n        \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n        \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n    }\n\n    assert _schema(structured_tool.args_schema) == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"integer\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"string\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    assert foo.__doc__ is not None\n    assert structured_tool.description == textwrap.dedent(foo.__doc__.strip())\n\n\ndef test_validation_error_handling_bool() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"Tool input validation error\"\n    _tool = _MockStructuredTool(handle_validation_error=True)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_validation_error_handling_str() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n    _tool = _MockStructuredTool(handle_validation_error=expected)\n    actual = _tool.run({})\n    assert expected == actual\n\n\ndef test_validation_error_handling_callable() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n\n    def handling(e: Union[ValidationError, ValidationErrorV1]) -> str:\n        return expected\n\n    _tool = _MockStructuredTool(handle_validation_error=handling)\n    actual = _tool.run({})\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\n    \"handler\",\n    [\n        True,\n        \"foo bar\",\n        lambda _: \"foo bar\",\n    ],\n)\ndef test_validation_error_handling_non_validation_error(\n    handler: Union[\n        bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n\n    class _RaiseNonValidationErrorTool(BaseTool):\n        name: str = \"raise_non_validation_error_tool\"\n        description: str = \"A tool that raises a non-validation error\"\n\n        def _parse_input(\n            self,\n            tool_input: Union[str, dict],\n            tool_call_id: Optional[str],\n        ) -> Union[str, dict[str, Any]]:\n            raise NotImplementedError\n\n        def _run(self) -> str:\n            return \"dummy\"\n\n        async def _arun(self) -> str:\n            return \"dummy\"\n\n    _tool = _RaiseNonValidationErrorTool(handle_validation_error=handler)  # type: ignore[call-arg]\n    with pytest.raises(NotImplementedError):\n        _tool.run({})\n\n\nasync def test_async_validation_error_handling_bool() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"Tool input validation error\"\n    _tool = _MockStructuredTool(handle_validation_error=True)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_validation_error_handling_str() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n    _tool = _MockStructuredTool(handle_validation_error=expected)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\nasync def test_async_validation_error_handling_callable() -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n    expected = \"foo bar\"\n\n    def handling(e: Union[ValidationError, ValidationErrorV1]) -> str:\n        return expected\n\n    _tool = _MockStructuredTool(handle_validation_error=handling)\n    actual = await _tool.arun({})\n    assert expected == actual\n\n\n@pytest.mark.parametrize(\n    \"handler\",\n    [\n        True,\n        \"foo bar\",\n        lambda _: \"foo bar\",\n    ],\n)\nasync def test_async_validation_error_handling_non_validation_error(\n    handler: Union[\n        bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]\n    ],\n) -> None:\n    \"\"\"Test that validation errors are handled correctly.\"\"\"\n\n    class _RaiseNonValidationErrorTool(BaseTool):\n        name: str = \"raise_non_validation_error_tool\"\n        description: str = \"A tool that raises a non-validation error\"\n\n        def _parse_input(\n            self,\n            tool_input: Union[str, dict],\n            tool_call_id: Optional[str],\n        ) -> Union[str, dict[str, Any]]:\n            raise NotImplementedError\n\n        def _run(self) -> str:\n            return \"dummy\"\n\n        async def _arun(self) -> str:\n            return \"dummy\"\n\n    _tool = _RaiseNonValidationErrorTool(handle_validation_error=handler)  # type: ignore[call-arg]\n    with pytest.raises(NotImplementedError):\n        await _tool.arun({})\n\n\ndef test_optional_subset_model_rewrite() -> None:\n    class MyModel(BaseModel):\n        a: Optional[str] = None\n        b: str\n        c: Optional[list[Optional[str]]] = None\n\n    model2 = _create_subset_model(\"model2\", MyModel, [\"a\", \"b\", \"c\"])\n\n    assert set(_schema(model2)[\"required\"]) == {\"b\"}\n\n\n@pytest.mark.parametrize(\n    \"inputs, expected\",\n    [\n        # Check not required\n        ({\"bar\": \"bar\"}, {\"bar\": \"bar\", \"baz\": 3, \"buzz\": \"buzz\"}),\n        # Check overwritten\n        (\n            {\"bar\": \"bar\", \"baz\": 4, \"buzz\": \"not-buzz\"},\n            {\"bar\": \"bar\", \"baz\": 4, \"buzz\": \"not-buzz\"},\n        ),\n        # Check validation error when missing\n        ({}, None),\n        # Check validation error when wrong type\n        ({\"bar\": \"bar\", \"baz\": \"not-an-int\"}, None),\n        # Check OK when None explicitly passed\n        ({\"bar\": \"bar\", \"baz\": None}, {\"bar\": \"bar\", \"baz\": None, \"buzz\": \"buzz\"}),\n    ],\n)\ndef test_tool_invoke_optional_args(inputs: dict, expected: Optional[dict]) -> None:\n    @tool\n    def foo(bar: str, baz: Optional[int] = 3, buzz: Optional[str] = \"buzz\") -> dict:\n        \"\"\"The foo.\"\"\"\n        return {\n            \"bar\": bar,\n            \"baz\": baz,\n            \"buzz\": buzz,\n        }\n\n    if expected is not None:\n        assert foo.invoke(inputs) == expected  # type: ignore\n    else:\n        with pytest.raises(ValidationError):\n            foo.invoke(inputs)  # type: ignore\n\n\ndef test_tool_pass_context() -> None:\n    @tool\n    def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        assert bar == \"baz\"\n        return bar\n\n    assert foo.invoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"  # type: ignore\n\n\n@pytest.mark.skipif(\n    sys.version_info < (3, 11),\n    reason=\"requires python3.11 or higher\",\n)\nasync def test_async_tool_pass_context() -> None:\n    @tool\n    async def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        assert bar == \"baz\"\n        return bar\n\n    assert (\n        await foo.ainvoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"  # type: ignore\n    )\n\n\ndef assert_bar(bar: Any, bar_config: RunnableConfig) -> Any:\n    assert bar_config[\"configurable\"][\"foo\"] == \"not-bar\"\n    assert bar == \"baz\"\n    return bar\n\n\n@tool\ndef foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool\nasync def afoo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool(infer_schema=False)\ndef simple_foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\n@tool(infer_schema=False)\nasync def asimple_foo(bar: Any, bar_config: RunnableConfig) -> Any:\n    \"\"\"The foo.\"\"\"\n    return assert_bar(bar, bar_config)\n\n\nclass FooBase(BaseTool):\n    name: str = \"Foo\"\n    description: str = \"Foo\"\n\n    def _run(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return assert_bar(bar, bar_config)\n\n\nFooBase.model_rebuild()\n\n\nclass AFooBase(FooBase):\n    async def _arun(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return assert_bar(bar, bar_config)\n\n\n@pytest.mark.parametrize(\"tool\", [foo, simple_foo, FooBase(), AFooBase()])\ndef test_tool_pass_config(tool: BaseTool) -> None:\n    assert tool.invoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}}) == \"baz\"\n\n    # Test we don't mutate tool calls\n    tool_call = {\n        \"name\": tool.name,\n        \"args\": {\"bar\": \"baz\"},\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    _ = tool.invoke(tool_call, {\"configurable\": {\"foo\": \"not-bar\"}})\n    assert tool_call[\"args\"] == {\"bar\": \"baz\"}\n\n\nclass FooBaseNonPickleable(FooBase):\n    def _run(self, bar: Any, bar_config: RunnableConfig, **kwargs: Any) -> Any:\n        return True\n\n\ndef test_tool_pass_config_non_pickleable() -> None:\n    tool = FooBaseNonPickleable()\n\n    args = {\"bar\": threading.Lock()}\n    tool_call = {\n        \"name\": tool.name,\n        \"args\": args,\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    _ = tool.invoke(tool_call, {\"configurable\": {\"foo\": \"not-bar\"}})\n    assert tool_call[\"args\"] == args\n\n\n@pytest.mark.parametrize(\n    \"tool\", [foo, afoo, simple_foo, asimple_foo, FooBase(), AFooBase()]\n)\nasync def test_async_tool_pass_config(tool: BaseTool) -> None:\n    assert (\n        await tool.ainvoke({\"bar\": \"baz\"}, {\"configurable\": {\"foo\": \"not-bar\"}})\n        == \"baz\"\n    )\n\n\ndef test_tool_description() -> None:\n    def foo(bar: str) -> str:\n        \"\"\"The foo.\"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    assert foo1.description == \"The foo.\"  # type: ignore\n\n    foo2 = StructuredTool.from_function(foo)\n    assert foo2.description == \"The foo.\"\n\n\ndef test_tool_arg_descriptions() -> None:\n    def foo(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    args_schema = _schema(foo1.args_schema)  # type: ignore\n    assert args_schema == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    # Test parses docstring\n    foo2 = tool(foo, parse_docstring=True)\n    args_schema = _schema(foo2.args_schema)  # type: ignore\n    expected = {\n        \"title\": \"foo\",\n        \"description\": \"The foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"description\": \"The bar.\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"description\": \"The baz.\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n    assert args_schema == expected\n\n    # Test parsing with run_manager does not raise error\n    def foo3(\n        bar: str, baz: int, run_manager: Optional[CallbackManagerForToolRun] = None\n    ) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo3, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n    assert args_schema[\"properties\"] == expected[\"properties\"]\n\n    # Test parameterless tool does not raise error for missing Args section\n    # in docstring.\n    def foo4() -> str:\n        \"\"\"The foo.\"\"\"\n        return \"bar\"\n\n    as_tool = tool(foo4, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n\n    def foo5(run_manager: Optional[CallbackManagerForToolRun] = None) -> str:\n        \"\"\"The foo.\"\"\"\n        return \"bar\"\n\n    as_tool = tool(foo5, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == expected[\"description\"]\n\n\ndef test_docstring_parsing() -> None:\n    expected = {\n        \"title\": \"foo\",\n        \"description\": \"The foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"description\": \"The bar.\", \"type\": \"string\"},\n            \"baz\": {\"title\": \"Baz\", \"description\": \"The baz.\", \"type\": \"integer\"},\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n    # Simple case\n    def foo(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo, parse_docstring=True)\n    args_schema = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema[\"description\"] == \"The foo.\"\n    assert args_schema[\"properties\"] == expected[\"properties\"]\n\n    # Multi-line description\n    def foo2(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Additional description here.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo2, parse_docstring=True)\n    args_schema2 = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema2[\"description\"] == \"The foo. Additional description here.\"\n    assert args_schema2[\"properties\"] == expected[\"properties\"]\n\n    # Multi-line wth Returns block\n    def foo3(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Additional description here.\n\n        Args:\n            bar: The bar.\n            baz: The baz.\n\n        Returns:\n            str: description of returned value.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo3, parse_docstring=True)\n    args_schema3 = _schema(as_tool.args_schema)  # type: ignore\n    args_schema3[\"title\"] = \"foo2\"\n    assert args_schema2 == args_schema3\n\n    # Single argument\n    def foo4(bar: str) -> str:\n        \"\"\"The foo.\n\n        Args:\n            bar: The bar.\n        \"\"\"\n        return bar\n\n    as_tool = tool(foo4, parse_docstring=True)\n    args_schema4 = _schema(as_tool.args_schema)  # type: ignore\n    assert args_schema4[\"description\"] == \"The foo.\"\n    assert args_schema4[\"properties\"] == {\n        \"bar\": {\"description\": \"The bar.\", \"title\": \"Bar\", \"type\": \"string\"}\n    }\n\n\ndef test_tool_invalid_docstrings() -> None:\n    # Test invalid docstrings\n    def foo3(bar: str, baz: int) -> str:\n        \"\"\"The foo.\"\"\"\n        return bar\n\n    def foo4(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n        Args:\n            bar: The bar.\n            baz: The baz.\n        \"\"\"\n        return bar\n\n    def foo5(bar: str, baz: int) -> str:\n        \"\"\"The foo.\n\n        Args:\n            banana: The bar.\n            monkey: The baz.\n        \"\"\"\n        return bar\n\n    for func in [foo3, foo4, foo5]:\n        with pytest.raises(ValueError):\n            _ = tool(func, parse_docstring=True)\n\n\ndef test_tool_annotated_descriptions() -> None:\n    def foo(\n        bar: Annotated[str, \"this is the bar\"], baz: Annotated[int, \"this is the baz\"]\n    ) -> str:\n        \"\"\"The foo.\n\n        Returns:\n            The bar only.\n        \"\"\"\n        return bar\n\n    foo1 = tool(foo)\n    args_schema = _schema(foo1.args_schema)  # type: ignore\n    assert args_schema == {\n        \"title\": \"foo\",\n        \"type\": \"object\",\n        \"description\": inspect.getdoc(foo),\n        \"properties\": {\n            \"bar\": {\"title\": \"Bar\", \"type\": \"string\", \"description\": \"this is the bar\"},\n            \"baz\": {\n                \"title\": \"Baz\",\n                \"type\": \"integer\",\n                \"description\": \"this is the baz\",\n            },\n        },\n        \"required\": [\"bar\", \"baz\"],\n    }\n\n\ndef test_tool_call_input_tool_message_output() -> None:\n    tool_call = {\n        \"name\": \"structured_api\",\n        \"args\": {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"img\": \"base64string...\"}},\n        \"id\": \"123\",\n        \"type\": \"tool_call\",\n    }\n    tool = _MockStructuredTool()\n    expected = ToolMessage(\n        \"1 True {'img': 'base64string...'}\", tool_call_id=\"123\", name=\"structured_api\"\n    )\n    actual = tool.invoke(tool_call)\n    assert actual == expected\n\n    tool_call.pop(\"type\")\n    with pytest.raises(ValidationError):\n        tool.invoke(tool_call)\n\n\nclass _MockStructuredToolWithRawOutput(BaseTool):\n    name: str = \"structured_api\"\n    args_schema: type[BaseModel] = _MockSchema\n    description: str = \"A Structured Tool\"\n    response_format: Literal[\"content_and_artifact\"] = \"content_and_artifact\"\n\n    def _run(\n        self, arg1: int, arg2: bool, arg3: Optional[dict] = None\n    ) -> tuple[str, dict]:\n        return f\"{arg1} {arg2}\", {\"arg1\": arg1, \"arg2\": arg2, \"arg3\": arg3}\n\n\n@tool(\"structured_api\", response_format=\"content_and_artifact\")\ndef _mock_structured_tool_with_artifact(\n    arg1: int, arg2: bool, arg3: Optional[dict] = None\n) -> tuple[str, dict]:\n    \"\"\"A Structured Tool.\"\"\"\n    return f\"{arg1} {arg2}\", {\"arg1\": arg1, \"arg2\": arg2, \"arg3\": arg3}\n\n\n@pytest.mark.parametrize(\n    \"tool\", [_MockStructuredToolWithRawOutput(), _mock_structured_tool_with_artifact]\n)\ndef test_tool_call_input_tool_message_with_artifact(tool: BaseTool) -> None:\n    tool_call: dict = {\n        \"name\": \"structured_api\",\n        \"args\": {\"arg1\": 1, \"arg2\": True, \"arg3\": {\"img\": \"base64string...\"}},\n        \"id\": \"123\",\n        \"type\": \"tool_call\",\n    }\n    expected = ToolMessage(\n        \"1 True\", artifact=tool_call[\"args\"], tool_call_id=\"123\", name=\"structured_api\"\n    )\n    actual = tool.invoke(tool_call)\n    assert actual == expected\n\n    tool_call.pop(\"type\")\n    with pytest.raises(ValidationError):\n        tool.invoke(tool_call)\n\n    actual_content = tool.invoke(tool_call[\"args\"])\n    assert actual_content == expected.content\n\n\ndef test_convert_from_runnable_dict() -> None:\n    # Test with typed dict input\n    class Args(TypedDict):\n        a: int\n        b: list[int]\n\n    def f(x: Args) -> str:\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    runnable: Runnable = RunnableLambda(f)\n    as_tool = runnable.as_tool()\n    args_schema = as_tool.args_schema\n    assert args_schema is not None\n    assert _schema(args_schema) == {\n        \"title\": \"f\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"array\", \"items\": {\"type\": \"integer\"}},\n        },\n        \"required\": [\"a\", \"b\"],\n    }\n    assert as_tool.description\n    result = as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n    assert result == \"6\"\n\n    as_tool = runnable.as_tool(name=\"my tool\", description=\"test description\")\n    assert as_tool.name == \"my tool\"\n    assert as_tool.description == \"test description\"\n\n    # Dict without typed input-- must supply schema\n    def g(x: dict[str, Any]) -> str:\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    # Specify via args_schema:\n    class GSchema(BaseModel):\n        \"\"\"Apply a function to an integer and list of integers.\"\"\"\n\n        a: int = Field(..., description=\"Integer\")\n        b: list[int] = Field(..., description=\"List of ints\")\n\n    runnable = RunnableLambda(g)\n    as_tool = runnable.as_tool(GSchema)\n    as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n\n    # Specify via arg_types:\n    runnable = RunnableLambda(g)\n    as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n    result = as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n    assert result == \"6\"\n\n    # Test with config\n    def h(x: dict[str, Any]) -> str:\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        return str(x[\"a\"] * max(x[\"b\"]))\n\n    runnable = RunnableLambda(h)\n    as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n    result = as_tool.invoke(\n        {\"a\": 3, \"b\": [1, 2]}, config={\"configurable\": {\"foo\": \"not-bar\"}}\n    )\n    assert result == \"6\"\n\n\ndef test_convert_from_runnable_other() -> None:\n    # String input\n    def f(x: str) -> str:\n        return x + \"a\"\n\n    def g(x: str) -> str:\n        return x + \"z\"\n\n    runnable: Runnable = RunnableLambda(f) | g\n    as_tool = runnable.as_tool()\n    args_schema = as_tool.args_schema\n    assert args_schema is None\n    assert as_tool.description\n\n    result = as_tool.invoke(\"b\")\n    assert result == \"baz\"\n\n    # Test with config\n    def h(x: str) -> str:\n        config = ensure_config()\n        assert config[\"configurable\"][\"foo\"] == \"not-bar\"\n        return x + \"a\"\n\n    runnable = RunnableLambda(h)\n    as_tool = runnable.as_tool()\n    result = as_tool.invoke(\"b\", config={\"configurable\": {\"foo\": \"not-bar\"}})\n    assert result == \"ba\"\n\n\n@tool(\"foo\", parse_docstring=True)\ndef injected_tool(x: int, y: Annotated[str, InjectedToolArg]) -> str:\n    \"\"\"foo.\n\n    Args:\n        x: abc\n        y: 123\n    \"\"\"\n    return y\n\n\nclass InjectedTool(BaseTool):\n    name: str = \"foo\"\n    description: str = \"foo.\"\n\n    def _run(self, x: int, y: Annotated[str, InjectedToolArg]) -> Any:\n        \"\"\"foo.\n\n        Args:\n            x: abc\n            y: 123\n        \"\"\"\n        return y\n\n\nclass fooSchema(BaseModel):  # noqa: N801\n    \"\"\"foo.\"\"\"\n\n    x: int = Field(..., description=\"abc\")\n    y: Annotated[str, \"foobar comment\", InjectedToolArg()] = Field(\n        ..., description=\"123\"\n    )\n\n\nclass InjectedToolWithSchema(BaseTool):\n    name: str = \"foo\"\n    description: str = \"foo.\"\n    args_schema: type[BaseModel] = fooSchema\n\n    def _run(self, x: int, y: str) -> Any:\n        return y\n\n\n@tool(\"foo\", args_schema=fooSchema)\ndef injected_tool_with_schema(x: int, y: str) -> str:\n    return y\n\n\n@pytest.mark.parametrize(\"tool_\", [InjectedTool()])\ndef test_tool_injected_arg_without_schema(tool_: BaseTool) -> None:\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\\n\\nArgs:\\n    x: abc\\n    y: 123\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\n@pytest.mark.parametrize(\n    \"tool_\",\n    [injected_tool_with_schema, InjectedToolWithSchema()],\n)\ndef test_tool_injected_arg_with_schema(tool_: BaseTool) -> None:\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"fooSchema\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef test_tool_injected_arg() -> None:\n    tool_ = injected_tool\n    assert _schema(tool_.get_input_schema()) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"x\", \"y\"],\n    }\n    assert _schema(tool_.tool_call_schema) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef test_tool_inherited_injected_arg() -> None:\n    class BarSchema(BaseModel):\n        \"\"\"bar.\"\"\"\n\n        y: Annotated[str, \"foobar comment\", InjectedToolArg()] = Field(\n            ..., description=\"123\"\n        )\n\n    class FooSchema(BarSchema):\n        \"\"\"foo.\"\"\"\n\n        x: int = Field(..., description=\"abc\")\n\n    class InheritedInjectedArgTool(BaseTool):\n        name: str = \"foo\"\n        description: str = \"foo.\"\n        args_schema: type[BaseModel] = FooSchema\n\n        def _run(self, x: int, y: str) -> Any:\n            return y\n\n    tool_ = InheritedInjectedArgTool()\n    assert tool_.get_input_schema().model_json_schema() == {\n        \"title\": \"FooSchema\",  # Matches the title from the provided schema\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"},\n            \"y\": {\"description\": \"123\", \"title\": \"Y\", \"type\": \"string\"},\n        },\n        \"required\": [\"y\", \"x\"],\n    }\n    # Should not include `y` since it's annotated as an injected tool arg\n    assert _get_tool_call_json_schema(tool_) == {\n        \"title\": \"foo\",\n        \"description\": \"foo.\",\n        \"type\": \"object\",\n        \"properties\": {\"x\": {\"description\": \"abc\", \"title\": \"X\", \"type\": \"integer\"}},\n        \"required\": [\"x\"],\n    }\n    assert tool_.invoke({\"x\": 5, \"y\": \"bar\"}) == \"bar\"\n    assert tool_.invoke(\n        {\n            \"name\": \"foo\",\n            \"args\": {\"x\": 5, \"y\": \"bar\"},\n            \"id\": \"123\",\n            \"type\": \"tool_call\",\n        }\n    ) == ToolMessage(\"bar\", tool_call_id=\"123\", name=\"foo\")\n    expected_error = (\n        ValidationError if not isinstance(tool_, InjectedTool) else TypeError\n    )\n    with pytest.raises(expected_error):\n        tool_.invoke({\"x\": 5})\n\n    assert convert_to_openai_function(tool_) == {\n        \"name\": \"foo\",\n        \"description\": \"foo.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\"x\": {\"type\": \"integer\", \"description\": \"abc\"}},\n            \"required\": [\"x\"],\n        },\n    }\n\n\ndef _get_parametrized_tools() -> list:\n    def my_tool(x: int, y: str, some_tool: Annotated[Any, InjectedToolArg]) -> str:\n        \"\"\"my_tool.\"\"\"\n        return some_tool\n\n    async def my_async_tool(\n        x: int, y: str, *, some_tool: Annotated[Any, InjectedToolArg]\n    ) -> str:\n        \"\"\"my_tool.\"\"\"\n        return some_tool\n\n    return [my_tool, my_async_tool]\n\n\n@pytest.mark.parametrize(\"tool_\", _get_parametrized_tools())\ndef test_fn_injected_arg_with_schema(tool_: Callable) -> None:\n    assert convert_to_openai_function(tool_) == {\n        \"name\": tool_.__name__,\n        \"description\": \"my_tool.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"x\": {\"type\": \"integer\"},\n                \"y\": {\"type\": \"string\"},\n            },\n            \"required\": [\"x\", \"y\"],\n        },\n    }\n\n\ndef generate_models() -> list[Any]:\n    \"\"\"Generate a list of base models depending on the pydantic version.\"\"\"\n\n    class FooProper(BaseModel):\n        a: int\n        b: str\n\n    return [FooProper]\n\n\ndef generate_backwards_compatible_v1() -> list[Any]:\n    \"\"\"Generate a model with pydantic 2 from the v1 namespace.\"\"\"\n    from pydantic.v1 import BaseModel as BaseModelV1\n\n    class FooV1Namespace(BaseModelV1):\n        a: int\n        b: str\n\n    return [FooV1Namespace]\n\n\n# This generates a list of models that can be used for testing that our APIs\n# behave well with either pydantic 1 proper,\n# pydantic v1 from pydantic 2,\n# or pydantic 2 proper.\nTEST_MODELS = generate_models() + generate_backwards_compatible_v1()\n\n\n@pytest.mark.parametrize(\"pydantic_model\", TEST_MODELS)\ndef test_args_schema_as_pydantic(pydantic_model: Any) -> None:\n    class SomeTool(BaseTool):\n        args_schema: type[pydantic_model] = pydantic_model\n\n        def _run(self, *args: Any, **kwargs: Any) -> str:\n            return \"foo\"\n\n    tool = SomeTool(\n        name=\"some_tool\", description=\"some description\", args_schema=pydantic_model\n    )\n\n    input_schema = tool.get_input_schema()\n    input_json_schema = (\n        input_schema.model_json_schema()\n        if hasattr(input_schema, \"model_json_schema\")\n        else input_schema.schema()\n    )\n    assert input_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n    tool_json_schema = _get_tool_call_json_schema(tool)\n    assert tool_json_schema == {\n        \"description\": \"some description\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"some_tool\",\n        \"type\": \"object\",\n    }\n\n\ndef test_args_schema_explicitly_typed() -> None:\n    \"\"\"This should test that one can type the args schema as a pydantic model.\n\n    Please note that this will test using pydantic 2 even though BaseTool\n    is a pydantic 1 model!\n    \"\"\"\n    # Check with whatever pydantic model is passed in and not via v1 namespace\n    from pydantic import BaseModel\n\n    class Foo(BaseModel):\n        a: int\n        b: str\n\n    class SomeTool(BaseTool):\n        # type ignoring here since we're allowing overriding a type\n        # signature of pydantic.v1.BaseModel with pydantic.BaseModel\n        # for pydantic 2!\n        args_schema: type[BaseModel] = Foo  # type: ignore[assignment]\n\n        def _run(self, *args: Any, **kwargs: Any) -> str:\n            return \"foo\"\n\n    tool = SomeTool(name=\"some_tool\", description=\"some description\")\n\n    assert tool.get_input_schema().model_json_schema() == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"Foo\",\n        \"type\": \"object\",\n    }\n\n    assert _get_tool_call_json_schema(tool) == {\n        \"description\": \"some description\",\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"some_tool\",\n        \"type\": \"object\",\n    }\n\n\n@pytest.mark.parametrize(\"pydantic_model\", TEST_MODELS)\ndef test_structured_tool_with_different_pydantic_versions(pydantic_model: Any) -> None:\n    \"\"\"This should test that one can type the args schema as a pydantic model.\"\"\"\n    from langchain_core.tools import StructuredTool\n\n    def foo(a: int, b: str) -> str:\n        \"\"\"Hahaha.\"\"\"\n        return \"foo\"\n\n    foo_tool = StructuredTool.from_function(\n        func=foo,\n        args_schema=pydantic_model,\n    )\n\n    assert foo_tool.invoke({\"a\": 5, \"b\": \"hello\"}) == \"foo\"\n\n    args_schema = cast(BaseModel, foo_tool.args_schema)\n    args_json_schema = (\n        args_schema.model_json_schema()\n        if hasattr(args_schema, \"model_json_schema\")\n        else args_schema.schema()\n    )\n    assert args_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n    input_schema = foo_tool.get_input_schema()\n    input_json_schema = (\n        input_schema.model_json_schema()\n        if hasattr(input_schema, \"model_json_schema\")\n        else input_schema.schema()\n    )\n    assert input_json_schema == {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"string\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": pydantic_model.__name__,\n        \"type\": \"object\",\n    }\n\n\nvalid_tool_result_blocks = [\n    \"foo\",\n    {\"type\": \"text\", \"text\": \"foo\"},\n    {\"type\": \"text\", \"blah\": \"foo\"},  # note, only 'type' key is currently checked\n    {\"type\": \"image_url\", \"image_url\": {}},  # openai format\n    {\n        \"type\": \"image\",\n        \"source\": {\n            \"type\": \"base64\",\n            \"media_type\": \"image/jpeg\",\n            \"data\": \"123\",\n        },\n    },  # anthropic format\n    {\"type\": \"json\", \"json\": {}},  # bedrock format\n]\ninvalid_tool_result_blocks = [\n    {\"text\": \"foo\"},  # missing type\n    {\"results\": \"foo\"},  # not content blocks\n]\n\n\n@pytest.mark.parametrize(\n    (\"obj\", \"expected\"),\n    [\n        *([[block, True] for block in valid_tool_result_blocks]),\n        *([[block, False] for block in invalid_tool_result_blocks]),\n    ],\n)\ndef test__is_message_content_block(obj: Any, expected: bool) -> None:\n    assert _is_message_content_block(obj) is expected\n\n\n@pytest.mark.parametrize(\n    (\"obj\", \"expected\"),\n    [\n        [\"foo\", True],\n        [valid_tool_result_blocks, True],\n        [invalid_tool_result_blocks, False],\n    ],\n)\ndef test__is_message_content_type(obj: Any, expected: bool) -> None:\n    assert _is_message_content_type(obj) is expected\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 2, reason=\"Testing pydantic v2.\")\n@pytest.mark.parametrize(\"use_v1_namespace\", [True, False])\ndef test__get_all_basemodel_annotations_v2(use_v1_namespace: bool) -> None:\n    A = TypeVar(\"A\")\n\n    if use_v1_namespace:\n        from pydantic.v1 import BaseModel as BaseModel1\n\n        class ModelA(BaseModel1, Generic[A], extra=\"allow\"):\n            a: A\n\n    else:\n        from pydantic import BaseModel as BaseModel2\n        from pydantic import ConfigDict\n\n        class ModelA(BaseModel2, Generic[A]):  # type: ignore[no-redef]\n            a: A\n            model_config = ConfigDict(arbitrary_types_allowed=True, extra=\"allow\")\n\n    class ModelB(ModelA[str]):\n        b: Annotated[ModelA[dict[str, Any]], \"foo\"]\n\n    class Mixin:\n        def foo(self) -> str:\n            return \"foo\"\n\n    class ModelC(Mixin, ModelB):\n        c: dict\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"], \"c\": dict}\n    actual = get_all_basemodel_annotations(ModelC)\n    assert actual == expected\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"]}\n    actual = get_all_basemodel_annotations(ModelB)\n    assert actual == expected\n\n    expected = {\"a\": Any}\n    actual = get_all_basemodel_annotations(ModelA)\n    assert actual == expected\n\n    expected = {\"a\": int}\n    actual = get_all_basemodel_annotations(ModelA[int])\n    assert actual == expected\n\n    D = TypeVar(\"D\", bound=Union[str, int])\n\n    class ModelD(ModelC, Generic[D]):\n        d: Optional[D]\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[str, int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD)\n    assert actual == expected\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD[int])\n    assert actual == expected\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 1, reason=\"Testing pydantic v1.\")\ndef test__get_all_basemodel_annotations_v1() -> None:\n    A = TypeVar(\"A\")\n\n    class ModelA(BaseModel, Generic[A], extra=\"allow\"):\n        a: A\n\n    class ModelB(ModelA[str]):\n        b: Annotated[ModelA[dict[str, Any]], \"foo\"]\n\n    class Mixin:\n        def foo(self) -> str:\n            return \"foo\"\n\n    class ModelC(Mixin, ModelB):\n        c: dict\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"], \"c\": dict}\n    actual = get_all_basemodel_annotations(ModelC)\n    assert actual == expected\n\n    expected = {\"a\": str, \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"]}\n    actual = get_all_basemodel_annotations(ModelB)\n    assert actual == expected\n\n    expected = {\"a\": Any}\n    actual = get_all_basemodel_annotations(ModelA)\n    assert actual == expected\n\n    expected = {\"a\": int}\n    actual = get_all_basemodel_annotations(ModelA[int])\n    assert actual == expected\n\n    D = TypeVar(\"D\", bound=Union[str, int])\n\n    class ModelD(ModelC, Generic[D]):\n        d: Optional[D]\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[str, int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD)\n    assert actual == expected\n\n    expected = {\n        \"a\": str,\n        \"b\": Annotated[ModelA[dict[str, Any]], \"foo\"],\n        \"c\": dict,\n        \"d\": Union[int, None],\n    }\n    actual = get_all_basemodel_annotations(ModelD[int])\n    assert actual == expected\n\n\ndef test_tool_annotations_preserved() -> None:\n    \"\"\"Test that annotations are preserved when creating a tool.\"\"\"\n\n    @tool\n    def my_tool(val: int, other_val: Annotated[dict, \"my annotation\"]) -> str:\n        \"\"\"Tool docstring.\"\"\"\n        return \"foo\"\n\n    schema = my_tool.get_input_schema()  # type: ignore[attr-defined]\n\n    func = my_tool.func  # type: ignore[attr-defined]\n\n    expected_type_hints = {\n        name: hint\n        for name, hint in func.__annotations__.items()\n        if name in inspect.signature(func).parameters\n    }\n    assert schema.__annotations__ == expected_type_hints\n\n\ndef test_create_retriever_tool() -> None:\n    class MyRetriever(BaseRetriever):\n        def _get_relevant_documents(\n            self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n        ) -> list[Document]:\n            return [Document(page_content=f\"foo {query}\"), Document(page_content=\"bar\")]\n\n    retriever = MyRetriever()\n    retriever_tool = tools.create_retriever_tool(\n        retriever, \"retriever_tool_content\", \"Retriever Tool Content\"\n    )\n    assert isinstance(retriever_tool, BaseTool)\n    assert retriever_tool.name == \"retriever_tool_content\"\n    assert retriever_tool.description == \"Retriever Tool Content\"\n    assert retriever_tool.invoke(\"bar\") == \"foo bar\\n\\nbar\"\n    assert retriever_tool.invoke(\n        ToolCall(\n            name=\"retriever_tool_content\",\n            args={\"query\": \"bar\"},\n            id=\"123\",\n            type=\"tool_call\",\n        )\n    ) == ToolMessage(\n        \"foo bar\\n\\nbar\", tool_call_id=\"123\", name=\"retriever_tool_content\"\n    )\n\n    retriever_tool_artifact = tools.create_retriever_tool(\n        retriever,\n        \"retriever_tool_artifact\",\n        \"Retriever Tool Artifact\",\n        response_format=\"content_and_artifact\",\n    )\n    assert isinstance(retriever_tool_artifact, BaseTool)\n    assert retriever_tool_artifact.name == \"retriever_tool_artifact\"\n    assert retriever_tool_artifact.description == \"Retriever Tool Artifact\"\n    assert retriever_tool_artifact.invoke(\"bar\") == \"foo bar\\n\\nbar\"\n    assert retriever_tool_artifact.invoke(\n        ToolCall(\n            name=\"retriever_tool_artifact\",\n            args={\"query\": \"bar\"},\n            id=\"123\",\n            type=\"tool_call\",\n        )\n    ) == ToolMessage(\n        \"foo bar\\n\\nbar\",\n        artifact=[Document(page_content=\"foo bar\"), Document(page_content=\"bar\")],\n        tool_call_id=\"123\",\n        name=\"retriever_tool_artifact\",\n    )\n\n\n@pytest.mark.skipif(PYDANTIC_MAJOR_VERSION != 2, reason=\"Testing pydantic v2.\")\ndef test_tool_args_schema_pydantic_v2_with_metadata() -> None:\n    from pydantic import BaseModel as BaseModelV2\n    from pydantic import Field as FieldV2\n    from pydantic import ValidationError as ValidationErrorV2\n\n    class Foo(BaseModelV2):\n        x: list[int] = FieldV2(\n            description=\"List of integers\", min_length=10, max_length=15\n        )\n\n    @tool(args_schema=Foo)\n    def foo(x):  # type: ignore[no-untyped-def]\n        \"\"\"Foo.\"\"\"\n        return x\n\n    assert _get_tool_call_json_schema(foo) == {\n        \"description\": \"Foo.\",\n        \"properties\": {\n            \"x\": {\n                \"description\": \"List of integers\",\n                \"items\": {\"type\": \"integer\"},\n                \"maxItems\": 15,\n                \"minItems\": 10,\n                \"title\": \"X\",\n                \"type\": \"array\",\n            }\n        },\n        \"required\": [\"x\"],\n        \"title\": \"foo\",\n        \"type\": \"object\",\n    }\n\n    assert foo.invoke({\"x\": [0] * 10})\n    with pytest.raises(ValidationErrorV2):\n        foo.invoke({\"x\": [0] * 9})\n\n\ndef test_imports() -> None:\n    expected_all = [\n        \"FILTERED_ARGS\",\n        \"SchemaAnnotationError\",\n        \"create_schema_from_function\",\n        \"ToolException\",\n        \"BaseTool\",\n        \"Tool\",\n        \"StructuredTool\",\n        \"tool\",\n        \"RetrieverInput\",\n        \"create_retriever_tool\",\n        \"ToolsRenderer\",\n        \"render_text_description\",\n        \"render_text_description_and_args\",\n        \"BaseToolkit\",\n        \"convert_runnable_to_tool\",\n        \"InjectedToolArg\",\n    ]\n    for module_name in expected_all:\n        assert hasattr(tools, module_name) and getattr(tools, module_name) is not None\n\n\ndef test_structured_tool_direct_init() -> None:\n    def foo(bar: str) -> str:\n        return bar\n\n    async def async_foo(bar: str) -> str:\n        return bar\n\n    class FooSchema(BaseModel):\n        bar: str = Field(..., description=\"The bar\")\n\n    tool = StructuredTool(name=\"foo\", args_schema=FooSchema, coroutine=async_foo)\n\n    with pytest.raises(NotImplementedError):\n        assert tool.invoke(\"hello\") == \"hello\"\n\n\ndef test_injected_arg_with_complex_type() -> None:\n    \"\"\"Test that an injected tool arg can be a complex type.\"\"\"\n\n    class Foo:\n        def __init__(self) -> None:\n            self.value = \"bar\"\n\n    @tool\n    def injected_tool(x: int, foo: Annotated[Foo, InjectedToolArg]) -> str:\n        \"\"\"Tool that has an injected tool arg.\"\"\"\n        return foo.value\n\n    assert injected_tool.invoke({\"x\": 5, \"foo\": Foo()}) == \"bar\"  # type: ignore\n\n\ndef test_tool_injected_tool_call_id() -> None:\n    @tool\n    def foo(x: int, tool_call_id: Annotated[str, InjectedToolCallId]) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"bar\")  # type: ignore\n\n    with pytest.raises(ValueError):\n        assert foo.invoke({\"x\": 0})\n\n    @tool\n    def foo2(x: int, tool_call_id: Annotated[str, InjectedToolCallId()]) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    assert foo2.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"bar\")  # type: ignore\n\n\ndef test_tool_uninjected_tool_call_id() -> None:\n    @tool\n    def foo(x: int, tool_call_id: str) -> ToolMessage:\n        \"\"\"Foo.\"\"\"\n        return ToolMessage(x, tool_call_id=tool_call_id)  # type: ignore\n\n    with pytest.raises(ValueError):\n        foo.invoke({\"type\": \"tool_call\", \"args\": {\"x\": 0}, \"name\": \"foo\", \"id\": \"bar\"})\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0, \"tool_call_id\": \"zap\"},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == ToolMessage(0, tool_call_id=\"zap\")  # type: ignore\n\n\ndef test_tool_return_output_mixin() -> None:\n    class Bar(ToolOutputMixin):\n        def __init__(self, x: int) -> None:\n            self.x = x\n\n        def __eq__(self, other: Any) -> bool:\n            return isinstance(other, self.__class__) and self.x == other.x\n\n    @tool\n    def foo(x: int) -> Bar:\n        \"\"\"Foo.\"\"\"\n        return Bar(x=x)\n\n    assert foo.invoke(\n        {\n            \"type\": \"tool_call\",\n            \"args\": {\"x\": 0},\n            \"name\": \"foo\",\n            \"id\": \"bar\",\n        }\n    ) == Bar(x=0)\n\n\ndef test_tool_mutate_input() -> None:\n    class MyTool(BaseTool):\n        name: str = \"MyTool\"\n        description: str = \"a tool\"\n\n        def _run(\n            self,\n            x: str,\n            run_manager: Optional[CallbackManagerForToolRun] = None,\n        ) -> str:\n            return \"hi\"\n\n    my_input = {\"x\": \"hi\"}\n    MyTool().invoke(my_input)\n    assert my_input == {\"x\": \"hi\"}\n\n\ndef test_structured_tool_args_schema_dict() -> None:\n    args_schema = {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n            \"b\": {\"title\": \"B\", \"type\": \"integer\"},\n        },\n        \"required\": [\"a\", \"b\"],\n        \"title\": \"add\",\n        \"type\": \"object\",\n    }\n    tool = StructuredTool(\n        name=\"add\",\n        description=\"add two numbers\",\n        args_schema=args_schema,\n        func=lambda a, b: a + b,\n    )\n    assert tool.invoke({\"a\": 1, \"b\": 2}) == 3\n    assert tool.args_schema == args_schema\n    # test that the tool call schema is the same as the args schema\n    assert _get_tool_call_json_schema(tool) == args_schema\n    # test that the input schema is the same as the parent (Runnable) input schema\n    assert (\n        tool.get_input_schema().model_json_schema()\n        == create_model_v2(\n            tool.get_name(\"Input\"),\n            root=tool.InputType,\n            module_name=tool.__class__.__module__,\n        ).model_json_schema()\n    )\n    # test that args are extracted correctly\n    assert tool.args == {\n        \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n        \"b\": {\"title\": \"B\", \"type\": \"integer\"},\n    }\n\n\ndef test_simple_tool_args_schema_dict() -> None:\n    args_schema = {\n        \"properties\": {\n            \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n        },\n        \"required\": [\"a\"],\n        \"title\": \"square\",\n        \"type\": \"object\",\n    }\n    tool = Tool(\n        name=\"square\",\n        description=\"square a number\",\n        args_schema=args_schema,\n        func=lambda a: a * a,\n    )\n    assert tool.invoke({\"a\": 2}) == 4\n    assert tool.args_schema == args_schema\n    # test that the tool call schema is the same as the args schema\n    assert _get_tool_call_json_schema(tool) == args_schema\n    # test that the input schema is the same as the parent (Runnable) input schema\n    assert (\n        tool.get_input_schema().model_json_schema()\n        == create_model_v2(\n            tool.get_name(\"Input\"),\n            root=tool.InputType,\n            module_name=tool.__class__.__module__,\n        ).model_json_schema()\n    )\n    # test that args are extracted correctly\n    assert tool.args == {\n        \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n    }\n",
        "patch": "@@ -2457,3 +2457,14 @@ def test_simple_tool_args_schema_dict() -> None:\n     assert tool.args == {\n         \"a\": {\"title\": \"A\", \"type\": \"integer\"},\n     }\n+\n+\n+def test_empty_string_tool_call_id() -> None:\n+    @tool\n+    def foo(x: int) -> str:\n+        \"\"\"Foo.\"\"\"\n+        return \"hi\"\n+\n+    assert foo.invoke({\"type\": \"tool_call\", \"args\": {\"x\": 0}, \"id\": \"\"}) == ToolMessage(\n+        content=\"hi\", name=\"foo\", tool_call_id=\"\"\n+    )"
      }
    ]
  },
  {
    "number": 29915,
    "title": "community/mlx_pipeline: fix crash at mlx call",
    "body": "\r\n- **Description:** \r\nSince mlx_lm 0.20, all calls to mlx crash due to deprecation of the way parameters are passed to methods generate and generate_step.\r\nParameters top_p, temp, repetition_penalty and repetition_context_size are not passed directly to those method anymore but wrapped into \"sampler\" and \"logit_processor\".\r\n\r\n\r\n- **Dependencies:** mlx_lm (optional)\r\n\r\n-  **Tests:** \r\nI've had a new test to existing test file: tests/integration_tests/llms/test_mlx_pipeline.py\r\n",
    "issue_title": "community/mlx_pipeline: fix crash at mlx call",
    "issue_body": "\r\n- **Description:** \r\nSince mlx_lm 0.20, all calls to mlx crash due to deprecation of the way parameters are passed to methods generate and generate_step.\r\nParameters top_p, temp, repetition_penalty and repetition_context_size are not passed directly to those method anymore but wrapped into \"sampler\" and \"logit_processor\".\r\n\r\n\r\n- **Dependencies:** mlx_lm (optional)\r\n\r\n-  **Tests:** \r\nI've had a new test to existing test file: tests/integration_tests/llms/test_mlx_pipeline.py\r\n",
    "files": [
      {
        "filename": "libs/community/langchain_community/chat_models/mlx.py",
        "content_before": "\"\"\"MLX Chat Wrapper.\"\"\"\n\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Type,\n    Union,\n)\n\nfrom langchain_core.callbacks.manager import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import BaseChatModel\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n)\nfrom langchain_core.outputs import (\n    ChatGeneration,\n    ChatGenerationChunk,\n    ChatResult,\n    LLMResult,\n)\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\n\nfrom langchain_community.llms.mlx_pipeline import MLXPipeline\n\nDEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful, and honest assistant.\"\"\"\n\n\nclass ChatMLX(BaseChatModel):\n    \"\"\"MLX chat models.\n\n    Works with `MLXPipeline` LLM.\n\n    To use, you should have the ``mlx-lm`` python package installed.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_community.chat_models import chatMLX\n            from langchain_community.llms import MLXPipeline\n\n            llm = MLXPipeline.from_model_id(\n                model_id=\"mlx-community/quantized-gemma-2b-it\",\n            )\n            chat = chatMLX(llm=llm)\n\n    \"\"\"\n\n    llm: MLXPipeline\n    system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n    tokenizer: Any = None\n\n    def __init__(self, **kwargs: Any):\n        super().__init__(**kwargs)\n        self.tokenizer = self.llm.tokenizer\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        llm_input = self._to_chat_prompt(messages)\n        llm_result = self.llm._generate(\n            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs\n        )\n        return self._to_chat_result(llm_result)\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        llm_input = self._to_chat_prompt(messages)\n        llm_result = await self.llm._agenerate(\n            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs\n        )\n        return self._to_chat_result(llm_result)\n\n    def _to_chat_prompt(\n        self,\n        messages: List[BaseMessage],\n        tokenize: bool = False,\n        return_tensors: Optional[str] = None,\n    ) -> str:\n        \"\"\"Convert a list of messages into a prompt format expected by wrapped LLM.\"\"\"\n        if not messages:\n            raise ValueError(\"At least one HumanMessage must be provided!\")\n\n        if not isinstance(messages[-1], HumanMessage):\n            raise ValueError(\"Last message must be a HumanMessage!\")\n\n        messages_dicts = [self._to_chatml_format(m) for m in messages]\n        return self.tokenizer.apply_chat_template(\n            messages_dicts,\n            tokenize=tokenize,\n            add_generation_prompt=True,\n            return_tensors=return_tensors,\n        )\n\n    def _to_chatml_format(self, message: BaseMessage) -> dict:\n        \"\"\"Convert LangChain message to ChatML format.\"\"\"\n\n        if isinstance(message, SystemMessage):\n            role = \"system\"\n        elif isinstance(message, AIMessage):\n            role = \"assistant\"\n        elif isinstance(message, HumanMessage):\n            role = \"user\"\n        else:\n            raise ValueError(f\"Unknown message type: {type(message)}\")\n\n        return {\"role\": role, \"content\": message.content}\n\n    @staticmethod\n    def _to_chat_result(llm_result: LLMResult) -> ChatResult:\n        chat_generations = []\n\n        for g in llm_result.generations[0]:\n            chat_generation = ChatGeneration(\n                message=AIMessage(content=g.text), generation_info=g.generation_info\n            )\n            chat_generations.append(chat_generation)\n\n        return ChatResult(\n            generations=chat_generations, llm_output=llm_result.llm_output\n        )\n\n    @property\n    def _llm_type(self) -> str:\n        return \"mlx-chat-wrapper\"\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        import mlx.core as mx\n        from mlx_lm.utils import generate_step\n\n        try:\n            import mlx.core as mx\n            from mlx_lm.utils import generate_step\n\n        except ImportError:\n            raise ImportError(\n                \"Could not import mlx_lm python package. \"\n                \"Please install it with `pip install mlx_lm`.\"\n            )\n        model_kwargs = kwargs.get(\"model_kwargs\", self.llm.pipeline_kwargs)\n        temp: float = model_kwargs.get(\"temp\", 0.0)\n        max_new_tokens: int = model_kwargs.get(\"max_tokens\", 100)\n        repetition_penalty: Optional[float] = model_kwargs.get(\n            \"repetition_penalty\", None\n        )\n        repetition_context_size: Optional[int] = model_kwargs.get(\n            \"repetition_context_size\", None\n        )\n\n        llm_input = self._to_chat_prompt(messages, tokenize=True, return_tensors=\"np\")\n\n        prompt_tokens = mx.array(llm_input[0])\n\n        eos_token_id = self.tokenizer.eos_token_id\n\n        for (token, prob), n in zip(\n            generate_step(\n                prompt_tokens,\n                self.llm.model,\n                temp=temp,\n                repetition_penalty=repetition_penalty,\n                repetition_context_size=repetition_context_size,\n            ),\n            range(max_new_tokens),\n        ):\n            # identify text to yield\n            text: Optional[str] = None\n            if not isinstance(token, int):\n                text = self.tokenizer.decode(token.item())\n            else:\n                text = self.tokenizer.decode(token)\n\n            # yield text, if any\n            if text:\n                chunk = ChatGenerationChunk(message=AIMessageChunk(content=text))\n                if run_manager:\n                    run_manager.on_llm_new_token(text, chunk=chunk)\n                yield chunk\n\n            # break if stop sequence found\n            if token == eos_token_id or (stop is not None and text in stop):\n                break\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        *,\n        tool_choice: Optional[Union[dict, str, Literal[\"auto\", \"none\"], bool]] = None,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        Assumes model is compatible with OpenAI tool-calling API.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports any tool definition handled by\n                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call.\n                Must be the name of the single provided function or\n                \"auto\" to automatically determine which function to call\n                (if any), or a dict of the form:\n                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n            **kwargs: Any additional parameters to pass to the\n                :class:`~langchain.runnable.Runnable` constructor.\n        \"\"\"\n\n        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n        if tool_choice is not None and tool_choice:\n            if len(formatted_tools) != 1:\n                raise ValueError(\n                    \"When specifying `tool_choice`, you must provide exactly one \"\n                    f\"tool. Received {len(formatted_tools)} tools.\"\n                )\n            if isinstance(tool_choice, str):\n                if tool_choice not in (\"auto\", \"none\"):\n                    tool_choice = {\n                        \"type\": \"function\",\n                        \"function\": {\"name\": tool_choice},\n                    }\n            elif isinstance(tool_choice, bool):\n                tool_choice = formatted_tools[0]\n            elif isinstance(tool_choice, dict):\n                if (\n                    formatted_tools[0][\"function\"][\"name\"]\n                    != tool_choice[\"function\"][\"name\"]\n                ):\n                    raise ValueError(\n                        f\"Tool choice {tool_choice} was specified, but the only \"\n                        f\"provided tool was {formatted_tools[0]['function']['name']}.\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n                    f\"Received: {tool_choice}\"\n                )\n            kwargs[\"tool_choice\"] = tool_choice\n        return super().bind(tools=formatted_tools, **kwargs)\n",
        "patch": "@@ -160,6 +160,7 @@ def _stream(\n \n         try:\n             import mlx.core as mx\n+            from mlx_lm.sample_utils import make_logits_processors, make_sampler\n             from mlx_lm.utils import generate_step\n \n         except ImportError:\n@@ -176,20 +177,28 @@ def _stream(\n         repetition_context_size: Optional[int] = model_kwargs.get(\n             \"repetition_context_size\", None\n         )\n+        top_p: float = model_kwargs.get(\"top_p\", 1.0)\n+        min_p: float = model_kwargs.get(\"min_p\", 0.0)\n+        min_tokens_to_keep: int = model_kwargs.get(\"min_tokens_to_keep\", 1)\n \n         llm_input = self._to_chat_prompt(messages, tokenize=True, return_tensors=\"np\")\n \n         prompt_tokens = mx.array(llm_input[0])\n \n         eos_token_id = self.tokenizer.eos_token_id\n \n+        sampler = make_sampler(temp or 0.0, top_p, min_p, min_tokens_to_keep)\n+\n+        logits_processors = make_logits_processors(\n+            None, repetition_penalty, repetition_context_size\n+        )\n+\n         for (token, prob), n in zip(\n             generate_step(\n                 prompt_tokens,\n                 self.llm.model,\n-                temp=temp,\n-                repetition_penalty=repetition_penalty,\n-                repetition_context_size=repetition_context_size,\n+                sampler=sampler,\n+                logits_processors=logits_processors,\n             ),\n             range(max_new_tokens),\n         ):"
      },
      {
        "filename": "libs/community/langchain_community/llms/mlx_pipeline.py",
        "content_before": "from __future__ import annotations\n\nimport logging\nfrom typing import Any, Callable, Iterator, List, Mapping, Optional\n\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.language_models.llms import LLM\nfrom langchain_core.outputs import GenerationChunk\nfrom pydantic import ConfigDict\n\nDEFAULT_MODEL_ID = \"mlx-community/quantized-gemma-2b\"\n\nlogger = logging.getLogger(__name__)\n\n\nclass MLXPipeline(LLM):\n    \"\"\"MLX Pipeline API.\n\n    To use, you should have the ``mlx-lm`` python package installed.\n\n    Example using from_model_id:\n        .. code-block:: python\n\n            from langchain_community.llms import MLXPipeline\n            pipe = MLXPipeline.from_model_id(\n                model_id=\"mlx-community/quantized-gemma-2b\",\n                pipeline_kwargs={\"max_tokens\": 10, \"temp\": 0.7},\n            )\n    Example passing model and tokenizer in directly:\n        .. code-block:: python\n\n            from langchain_community.llms import MLXPipeline\n            from mlx_lm import load\n            model_id=\"mlx-community/quantized-gemma-2b\"\n            model, tokenizer = load(model_id)\n            pipe = MLXPipeline(model=model, tokenizer=tokenizer)\n    \"\"\"\n\n    model_id: str = DEFAULT_MODEL_ID\n    \"\"\"Model name to use.\"\"\"\n    model: Any = None  #: :meta private:\n    \"\"\"Model.\"\"\"\n    tokenizer: Any = None  #: :meta private:\n    \"\"\"Tokenizer.\"\"\"\n    tokenizer_config: Optional[dict] = None\n    \"\"\"\n        Configuration parameters specifically for the tokenizer.\n        Defaults to an empty dictionary.\n    \"\"\"\n    adapter_file: Optional[str] = None\n    \"\"\"\n        Path to the adapter file. If provided, applies LoRA layers to the model.\n        Defaults to None.\n    \"\"\"\n    lazy: bool = False\n    \"\"\"\n        If False eval the model parameters to make sure they are\n        loaded in memory before returning, otherwise they will be loaded\n        when needed. Default: ``False``\n    \"\"\"\n    pipeline_kwargs: Optional[dict] = None\n    \"\"\"\n    Keyword arguments passed to the pipeline. Defaults include:\n        - temp (float): Temperature for generation, default is 0.0.\n        - max_tokens (int): Maximum tokens to generate, default is 100.\n        - verbose (bool): Whether to output verbose logging, default is False.\n        - formatter (Optional[Callable]): A callable to format the output.\n          Default is None.\n        - repetition_penalty (Optional[float]): The penalty factor for\n          repeated sequences, default is None.\n        - repetition_context_size (Optional[int]): Size of the context\n          for applying repetition penalty, default is None.\n        - top_p (float): The cumulative probability threshold for\n          top-p filtering, default is 1.0.\n\n    \"\"\"\n\n    model_config = ConfigDict(\n        extra=\"forbid\",\n    )\n\n    @classmethod\n    def from_model_id(\n        cls,\n        model_id: str,\n        tokenizer_config: Optional[dict] = None,\n        adapter_file: Optional[str] = None,\n        lazy: bool = False,\n        pipeline_kwargs: Optional[dict] = None,\n        **kwargs: Any,\n    ) -> MLXPipeline:\n        \"\"\"Construct the pipeline object from model_id and task.\"\"\"\n        try:\n            from mlx_lm import load\n\n        except ImportError:\n            raise ImportError(\n                \"Could not import mlx_lm python package. \"\n                \"Please install it with `pip install mlx_lm`.\"\n            )\n\n        tokenizer_config = tokenizer_config or {}\n        if adapter_file:\n            model, tokenizer = load(model_id, tokenizer_config, adapter_file, lazy)\n        else:\n            model, tokenizer = load(model_id, tokenizer_config, lazy=lazy)\n\n        _pipeline_kwargs = pipeline_kwargs or {}\n        return cls(\n            model_id=model_id,\n            model=model,\n            tokenizer=tokenizer,\n            tokenizer_config=tokenizer_config,\n            adapter_file=adapter_file,\n            lazy=lazy,\n            pipeline_kwargs=_pipeline_kwargs,\n            **kwargs,\n        )\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\n            \"model_id\": self.model_id,\n            \"tokenizer_config\": self.tokenizer_config,\n            \"adapter_file\": self.adapter_file,\n            \"lazy\": self.lazy,\n            \"pipeline_kwargs\": self.pipeline_kwargs,\n        }\n\n    @property\n    def _llm_type(self) -> str:\n        return \"mlx_pipeline\"\n\n    def _call(\n        self,\n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> str:\n        try:\n            from mlx_lm import generate\n\n        except ImportError:\n            raise ImportError(\n                \"Could not import mlx_lm python package. \"\n                \"Please install it with `pip install mlx_lm`.\"\n            )\n\n        pipeline_kwargs = kwargs.get(\"pipeline_kwargs\", self.pipeline_kwargs)\n\n        temp: float = pipeline_kwargs.get(\"temp\", 0.0)\n        max_tokens: int = pipeline_kwargs.get(\"max_tokens\", 100)\n        verbose: bool = pipeline_kwargs.get(\"verbose\", False)\n        formatter: Optional[Callable] = pipeline_kwargs.get(\"formatter\", None)\n        repetition_penalty: Optional[float] = pipeline_kwargs.get(\n            \"repetition_penalty\", None\n        )\n        repetition_context_size: Optional[int] = pipeline_kwargs.get(\n            \"repetition_context_size\", None\n        )\n        top_p: float = pipeline_kwargs.get(\"top_p\", 1.0)\n\n        return generate(\n            model=self.model,\n            tokenizer=self.tokenizer,\n            prompt=prompt,\n            temp=temp,\n            max_tokens=max_tokens,\n            verbose=verbose,\n            formatter=formatter,\n            repetition_penalty=repetition_penalty,\n            repetition_context_size=repetition_context_size,\n            top_p=top_p,\n        )\n\n    def _stream(\n        self,\n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[GenerationChunk]:\n        try:\n            import mlx.core as mx\n            from mlx_lm.utils import generate_step\n\n        except ImportError:\n            raise ImportError(\n                \"Could not import mlx_lm python package. \"\n                \"Please install it with `pip install mlx_lm`.\"\n            )\n\n        pipeline_kwargs = kwargs.get(\"pipeline_kwargs\", self.pipeline_kwargs)\n\n        temp: float = pipeline_kwargs.get(\"temp\", 0.0)\n        max_new_tokens: int = pipeline_kwargs.get(\"max_tokens\", 100)\n        repetition_penalty: Optional[float] = pipeline_kwargs.get(\n            \"repetition_penalty\", None\n        )\n        repetition_context_size: Optional[int] = pipeline_kwargs.get(\n            \"repetition_context_size\", None\n        )\n        top_p: float = pipeline_kwargs.get(\"top_p\", 1.0)\n\n        prompt = self.tokenizer.encode(prompt, return_tensors=\"np\")\n\n        prompt_tokens = mx.array(prompt[0])\n\n        eos_token_id = self.tokenizer.eos_token_id\n        detokenizer = self.tokenizer.detokenizer\n        detokenizer.reset()\n\n        for (token, prob), n in zip(\n            generate_step(\n                prompt=prompt_tokens,\n                model=self.model,\n                temp=temp,\n                repetition_penalty=repetition_penalty,\n                repetition_context_size=repetition_context_size,\n                top_p=top_p,\n            ),\n            range(max_new_tokens),\n        ):\n            # identify text to yield\n            text: Optional[str] = None\n            detokenizer.add_token(token)\n            detokenizer.finalize()\n            text = detokenizer.last_segment\n\n            # yield text, if any\n            if text:\n                chunk = GenerationChunk(text=text)\n                if run_manager:\n                    run_manager.on_llm_new_token(chunk.text)\n                yield chunk\n\n            # break if stop sequence found\n            if token == eos_token_id or (stop is not None and text in stop):\n                break\n",
        "patch": "@@ -101,7 +101,9 @@ def from_model_id(\n \n         tokenizer_config = tokenizer_config or {}\n         if adapter_file:\n-            model, tokenizer = load(model_id, tokenizer_config, adapter_file, lazy)\n+            model, tokenizer = load(\n+                model_id, tokenizer_config, adapter_path=adapter_file, lazy=lazy\n+            )\n         else:\n             model, tokenizer = load(model_id, tokenizer_config, lazy=lazy)\n \n@@ -141,6 +143,7 @@ def _call(\n     ) -> str:\n         try:\n             from mlx_lm import generate\n+            from mlx_lm.sample_utils import make_logits_processors, make_sampler\n \n         except ImportError:\n             raise ImportError(\n@@ -161,18 +164,23 @@ def _call(\n             \"repetition_context_size\", None\n         )\n         top_p: float = pipeline_kwargs.get(\"top_p\", 1.0)\n+        min_p: float = pipeline_kwargs.get(\"min_p\", 0.0)\n+        min_tokens_to_keep: int = pipeline_kwargs.get(\"min_tokens_to_keep\", 1)\n+\n+        sampler = make_sampler(temp, top_p, min_p, min_tokens_to_keep)\n+        logits_processors = make_logits_processors(\n+            None, repetition_penalty, repetition_context_size\n+        )\n \n         return generate(\n             model=self.model,\n             tokenizer=self.tokenizer,\n             prompt=prompt,\n-            temp=temp,\n             max_tokens=max_tokens,\n             verbose=verbose,\n             formatter=formatter,\n-            repetition_penalty=repetition_penalty,\n-            repetition_context_size=repetition_context_size,\n-            top_p=top_p,\n+            sampler=sampler,\n+            logits_processors=logits_processors,\n         )\n \n     def _stream(\n@@ -184,6 +192,7 @@ def _stream(\n     ) -> Iterator[GenerationChunk]:\n         try:\n             import mlx.core as mx\n+            from mlx_lm.sample_utils import make_logits_processors, make_sampler\n             from mlx_lm.utils import generate_step\n \n         except ImportError:\n@@ -203,6 +212,8 @@ def _stream(\n             \"repetition_context_size\", None\n         )\n         top_p: float = pipeline_kwargs.get(\"top_p\", 1.0)\n+        min_p: float = pipeline_kwargs.get(\"min_p\", 0.0)\n+        min_tokens_to_keep: int = pipeline_kwargs.get(\"min_tokens_to_keep\", 1)\n \n         prompt = self.tokenizer.encode(prompt, return_tensors=\"np\")\n \n@@ -212,14 +223,18 @@ def _stream(\n         detokenizer = self.tokenizer.detokenizer\n         detokenizer.reset()\n \n+        sampler = make_sampler(temp or 0.0, top_p, min_p, min_tokens_to_keep)\n+\n+        logits_processors = make_logits_processors(\n+            None, repetition_penalty, repetition_context_size\n+        )\n+\n         for (token, prob), n in zip(\n             generate_step(\n                 prompt=prompt_tokens,\n                 model=self.model,\n-                temp=temp,\n-                repetition_penalty=repetition_penalty,\n-                repetition_context_size=repetition_context_size,\n-                top_p=top_p,\n+                sampler=sampler,\n+                logits_processors=logits_processors,\n             ),\n             range(max_new_tokens),\n         ):"
      },
      {
        "filename": "libs/community/tests/integration_tests/llms/test_mlx_pipeline.py",
        "content_before": "\"\"\"Test MLX Pipeline wrapper.\"\"\"\n\nfrom langchain_community.llms.mlx_pipeline import MLXPipeline\n\n\ndef test_mlx_pipeline_text_generation() -> None:\n    \"\"\"Test valid call to MLX text generation model.\"\"\"\n    llm = MLXPipeline.from_model_id(\n        model_id=\"mlx-community/quantized-gemma-2b\",\n        pipeline_kwargs={\"max_tokens\": 10},\n    )\n    output = llm.invoke(\"Say foo:\")\n    assert isinstance(output, str)\n\n\ndef test_init_with_model_and_tokenizer() -> None:\n    \"\"\"Test initialization with a HF pipeline.\"\"\"\n    from mlx_lm import load\n\n    model, tokenizer = load(\"mlx-community/quantized-gemma-2b\")\n    llm = MLXPipeline(model=model, tokenizer=tokenizer)\n    output = llm.invoke(\"Say foo:\")\n    assert isinstance(output, str)\n\n\ndef test_huggingface_pipeline_runtime_kwargs() -> None:\n    \"\"\"Test pipelines specifying the device map parameter.\"\"\"\n    llm = MLXPipeline.from_model_id(\n        model_id=\"mlx-community/quantized-gemma-2b\",\n    )\n    prompt = \"Say foo:\"\n    output = llm.invoke(prompt, pipeline_kwargs={\"max_tokens\": 2})\n    assert len(output) < 10\n",
        "patch": "@@ -1,8 +1,11 @@\n \"\"\"Test MLX Pipeline wrapper.\"\"\"\n \n+import pytest\n+\n from langchain_community.llms.mlx_pipeline import MLXPipeline\n \n \n+@pytest.mark.requires(\"mlx_lm\")\n def test_mlx_pipeline_text_generation() -> None:\n     \"\"\"Test valid call to MLX text generation model.\"\"\"\n     llm = MLXPipeline.from_model_id(\n@@ -13,6 +16,7 @@ def test_mlx_pipeline_text_generation() -> None:\n     assert isinstance(output, str)\n \n \n+@pytest.mark.requires(\"mlx_lm\")\n def test_init_with_model_and_tokenizer() -> None:\n     \"\"\"Test initialization with a HF pipeline.\"\"\"\n     from mlx_lm import load\n@@ -23,6 +27,7 @@ def test_init_with_model_and_tokenizer() -> None:\n     assert isinstance(output, str)\n \n \n+@pytest.mark.requires(\"mlx_lm\")\n def test_huggingface_pipeline_runtime_kwargs() -> None:\n     \"\"\"Test pipelines specifying the device map parameter.\"\"\"\n     llm = MLXPipeline.from_model_id(\n@@ -31,3 +36,21 @@ def test_huggingface_pipeline_runtime_kwargs() -> None:\n     prompt = \"Say foo:\"\n     output = llm.invoke(prompt, pipeline_kwargs={\"max_tokens\": 2})\n     assert len(output) < 10\n+\n+\n+@pytest.mark.requires(\"mlx_lm\")\n+def test_mlx_pipeline_with_params() -> None:\n+    \"\"\"Test valid call to MLX text generation model.\"\"\"\n+    llm = MLXPipeline.from_model_id(\n+        model_id=\"mlx-community/quantized-gemma-2b\",\n+        pipeline_kwargs={\n+            \"max_tokens\": 10,\n+            \"temp\": 0.8,\n+            \"verbose\": False,\n+            \"repetition_penalty\": 1.1,\n+            \"repetition_context_size\": 64,\n+            \"top_p\": 0.95,\n+        },\n+    )\n+    output = llm.invoke(\"Say foo:\")\n+    assert isinstance(output, str)"
      }
    ]
  },
  {
    "number": 29838,
    "title": "mistral[patch]: support model_kwargs",
    "body": "- **Description:** Frequency_penalty added as a client parameter\r\n- **Issue:** #29803\r\n",
    "issue_title": "mistral[patch]: support model_kwargs",
    "issue_body": "- **Description:** Frequency_penalty added as a client parameter\r\n- **Issue:** #29803\r\n",
    "files": [
      {
        "filename": "libs/partners/mistralai/langchain_mistralai/chat_models.py",
        "content_before": "from __future__ import annotations\n\nimport hashlib\nimport json\nimport logging\nimport os\nimport re\nimport uuid\nfrom operator import itemgetter\nfrom typing import (\n    Any,\n    AsyncContextManager,\n    AsyncIterator,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n    cast,\n)\n\nimport httpx\nfrom httpx_sse import EventSource, aconnect_sse, connect_sse\nfrom langchain_core.callbacks import (\n    AsyncCallbackManagerForLLMRun,\n    CallbackManagerForLLMRun,\n)\nfrom langchain_core.language_models import LanguageModelInput\nfrom langchain_core.language_models.chat_models import (\n    BaseChatModel,\n    LangSmithParams,\n    agenerate_from_stream,\n    generate_from_stream,\n)\nfrom langchain_core.language_models.llms import create_base_retry_decorator\nfrom langchain_core.messages import (\n    AIMessage,\n    AIMessageChunk,\n    BaseMessage,\n    BaseMessageChunk,\n    ChatMessage,\n    ChatMessageChunk,\n    HumanMessage,\n    HumanMessageChunk,\n    InvalidToolCall,\n    SystemMessage,\n    SystemMessageChunk,\n    ToolCall,\n    ToolMessage,\n)\nfrom langchain_core.messages.tool import tool_call_chunk\nfrom langchain_core.output_parsers import (\n    JsonOutputParser,\n    PydanticOutputParser,\n)\nfrom langchain_core.output_parsers.base import OutputParserLike\nfrom langchain_core.output_parsers.openai_tools import (\n    JsonOutputKeyToolsParser,\n    PydanticToolsParser,\n    make_invalid_tool_call,\n    parse_tool_call,\n)\nfrom langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\nfrom langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\nfrom langchain_core.tools import BaseTool\nfrom langchain_core.utils import secret_from_env\nfrom langchain_core.utils.function_calling import convert_to_openai_tool\nfrom langchain_core.utils.pydantic import is_basemodel_subclass\nfrom pydantic import (\n    BaseModel,\n    ConfigDict,\n    Field,\n    SecretStr,\n    model_validator,\n)\nfrom typing_extensions import Self\n\nlogger = logging.getLogger(__name__)\n\n# Mistral enforces a specific pattern for tool call IDs\nTOOL_CALL_ID_PATTERN = re.compile(r\"^[a-zA-Z0-9]{9}$\")\n\n\ndef _create_retry_decorator(\n    llm: ChatMistralAI,\n    run_manager: Optional[\n        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n    ] = None,\n) -> Callable[[Any], Any]:\n    \"\"\"Returns a tenacity retry decorator, preconfigured to handle exceptions\"\"\"\n\n    errors = [httpx.RequestError, httpx.StreamError]\n    return create_base_retry_decorator(\n        error_types=errors, max_retries=llm.max_retries, run_manager=run_manager\n    )\n\n\ndef _is_valid_mistral_tool_call_id(tool_call_id: str) -> bool:\n    \"\"\"Check if tool call ID is nine character string consisting of a-z, A-Z, 0-9\"\"\"\n    return bool(TOOL_CALL_ID_PATTERN.match(tool_call_id))\n\n\ndef _base62_encode(num: int) -> str:\n    \"\"\"Encodes a number in base62 and ensures result is of a specified length.\"\"\"\n    base62 = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    if num == 0:\n        return base62[0]\n    arr = []\n    base = len(base62)\n    while num:\n        num, rem = divmod(num, base)\n        arr.append(base62[rem])\n    arr.reverse()\n    return \"\".join(arr)\n\n\ndef _convert_tool_call_id_to_mistral_compatible(tool_call_id: str) -> str:\n    \"\"\"Convert a tool call ID to a Mistral-compatible format\"\"\"\n    if _is_valid_mistral_tool_call_id(tool_call_id):\n        return tool_call_id\n    else:\n        hash_bytes = hashlib.sha256(tool_call_id.encode()).digest()\n        hash_int = int.from_bytes(hash_bytes, byteorder=\"big\")\n        base62_str = _base62_encode(hash_int)\n        if len(base62_str) >= 9:\n            return base62_str[:9]\n        else:\n            return base62_str.rjust(9, \"0\")\n\n\ndef _convert_mistral_chat_message_to_message(\n    _message: Dict,\n) -> BaseMessage:\n    role = _message[\"role\"]\n    assert role == \"assistant\", f\"Expected role to be 'assistant', got {role}\"\n    content = cast(str, _message[\"content\"])\n\n    additional_kwargs: Dict = {}\n    tool_calls = []\n    invalid_tool_calls = []\n    if raw_tool_calls := _message.get(\"tool_calls\"):\n        additional_kwargs[\"tool_calls\"] = raw_tool_calls\n        for raw_tool_call in raw_tool_calls:\n            try:\n                parsed: dict = cast(\n                    dict, parse_tool_call(raw_tool_call, return_id=True)\n                )\n                if not parsed[\"id\"]:\n                    parsed[\"id\"] = uuid.uuid4().hex[:]\n                tool_calls.append(parsed)\n            except Exception as e:\n                invalid_tool_calls.append(make_invalid_tool_call(raw_tool_call, str(e)))\n    return AIMessage(\n        content=content,\n        additional_kwargs=additional_kwargs,\n        tool_calls=tool_calls,\n        invalid_tool_calls=invalid_tool_calls,\n    )\n\n\ndef _raise_on_error(response: httpx.Response) -> None:\n    \"\"\"Raise an error if the response is an error.\"\"\"\n    if httpx.codes.is_error(response.status_code):\n        error_message = response.read().decode(\"utf-8\")\n        raise httpx.HTTPStatusError(\n            f\"Error response {response.status_code} \"\n            f\"while fetching {response.url}: {error_message}\",\n            request=response.request,\n            response=response,\n        )\n\n\nasync def _araise_on_error(response: httpx.Response) -> None:\n    \"\"\"Raise an error if the response is an error.\"\"\"\n    if httpx.codes.is_error(response.status_code):\n        error_message = (await response.aread()).decode(\"utf-8\")\n        raise httpx.HTTPStatusError(\n            f\"Error response {response.status_code} \"\n            f\"while fetching {response.url}: {error_message}\",\n            request=response.request,\n            response=response,\n        )\n\n\nasync def _aiter_sse(\n    event_source_mgr: AsyncContextManager[EventSource],\n) -> AsyncIterator[Dict]:\n    \"\"\"Iterate over the server-sent events.\"\"\"\n    async with event_source_mgr as event_source:\n        await _araise_on_error(event_source.response)\n        async for event in event_source.aiter_sse():\n            if event.data == \"[DONE]\":\n                return\n            yield event.json()\n\n\nasync def acompletion_with_retry(\n    llm: ChatMistralAI,\n    run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n    **kwargs: Any,\n) -> Any:\n    \"\"\"Use tenacity to retry the async completion call.\"\"\"\n    retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)\n\n    @retry_decorator\n    async def _completion_with_retry(**kwargs: Any) -> Any:\n        if \"stream\" not in kwargs:\n            kwargs[\"stream\"] = False\n        stream = kwargs[\"stream\"]\n        if stream:\n            event_source = aconnect_sse(\n                llm.async_client, \"POST\", \"/chat/completions\", json=kwargs\n            )\n            return _aiter_sse(event_source)\n        else:\n            response = await llm.async_client.post(url=\"/chat/completions\", json=kwargs)\n            await _araise_on_error(response)\n            return response.json()\n\n    return await _completion_with_retry(**kwargs)\n\n\ndef _convert_chunk_to_message_chunk(\n    chunk: Dict, default_class: Type[BaseMessageChunk]\n) -> BaseMessageChunk:\n    _delta = chunk[\"choices\"][0][\"delta\"]\n    role = _delta.get(\"role\")\n    content = _delta.get(\"content\") or \"\"\n    if role == \"user\" or default_class == HumanMessageChunk:\n        return HumanMessageChunk(content=content)\n    elif role == \"assistant\" or default_class == AIMessageChunk:\n        additional_kwargs: Dict = {}\n        if raw_tool_calls := _delta.get(\"tool_calls\"):\n            additional_kwargs[\"tool_calls\"] = raw_tool_calls\n            try:\n                tool_call_chunks = []\n                for raw_tool_call in raw_tool_calls:\n                    if not raw_tool_call.get(\"index\") and not raw_tool_call.get(\"id\"):\n                        tool_call_id = uuid.uuid4().hex[:]\n                    else:\n                        tool_call_id = raw_tool_call.get(\"id\")\n                    tool_call_chunks.append(\n                        tool_call_chunk(\n                            name=raw_tool_call[\"function\"].get(\"name\"),\n                            args=raw_tool_call[\"function\"].get(\"arguments\"),\n                            id=tool_call_id,\n                            index=raw_tool_call.get(\"index\"),\n                        )\n                    )\n            except KeyError:\n                pass\n        else:\n            tool_call_chunks = []\n        if token_usage := chunk.get(\"usage\"):\n            usage_metadata = {\n                \"input_tokens\": token_usage.get(\"prompt_tokens\", 0),\n                \"output_tokens\": token_usage.get(\"completion_tokens\", 0),\n                \"total_tokens\": token_usage.get(\"total_tokens\", 0),\n            }\n        else:\n            usage_metadata = None\n        return AIMessageChunk(\n            content=content,\n            additional_kwargs=additional_kwargs,\n            tool_call_chunks=tool_call_chunks,  # type: ignore[arg-type]\n            usage_metadata=usage_metadata,  # type: ignore[arg-type]\n        )\n    elif role == \"system\" or default_class == SystemMessageChunk:\n        return SystemMessageChunk(content=content)\n    elif role or default_class == ChatMessageChunk:\n        return ChatMessageChunk(content=content, role=role)\n    else:\n        return default_class(content=content)  # type: ignore[call-arg]\n\n\ndef _format_tool_call_for_mistral(tool_call: ToolCall) -> dict:\n    \"\"\"Format Langchain ToolCall to dict expected by Mistral.\"\"\"\n    result: Dict[str, Any] = {\n        \"function\": {\n            \"name\": tool_call[\"name\"],\n            \"arguments\": json.dumps(tool_call[\"args\"]),\n        }\n    }\n    if _id := tool_call.get(\"id\"):\n        result[\"id\"] = _convert_tool_call_id_to_mistral_compatible(_id)\n\n    return result\n\n\ndef _format_invalid_tool_call_for_mistral(invalid_tool_call: InvalidToolCall) -> dict:\n    \"\"\"Format Langchain InvalidToolCall to dict expected by Mistral.\"\"\"\n    result: Dict[str, Any] = {\n        \"function\": {\n            \"name\": invalid_tool_call[\"name\"],\n            \"arguments\": invalid_tool_call[\"args\"],\n        }\n    }\n    if _id := invalid_tool_call.get(\"id\"):\n        result[\"id\"] = _convert_tool_call_id_to_mistral_compatible(_id)\n\n    return result\n\n\ndef _convert_message_to_mistral_chat_message(\n    message: BaseMessage,\n) -> Dict:\n    if isinstance(message, ChatMessage):\n        return dict(role=message.role, content=message.content)\n    elif isinstance(message, HumanMessage):\n        return dict(role=\"user\", content=message.content)\n    elif isinstance(message, AIMessage):\n        message_dict: Dict[str, Any] = {\"role\": \"assistant\"}\n        tool_calls = []\n        if message.tool_calls or message.invalid_tool_calls:\n            for tool_call in message.tool_calls:\n                tool_calls.append(_format_tool_call_for_mistral(tool_call))\n            for invalid_tool_call in message.invalid_tool_calls:\n                tool_calls.append(\n                    _format_invalid_tool_call_for_mistral(invalid_tool_call)\n                )\n        elif \"tool_calls\" in message.additional_kwargs:\n            for tc in message.additional_kwargs[\"tool_calls\"]:\n                chunk = {\n                    \"function\": {\n                        \"name\": tc[\"function\"][\"name\"],\n                        \"arguments\": tc[\"function\"][\"arguments\"],\n                    }\n                }\n                if _id := tc.get(\"id\"):\n                    chunk[\"id\"] = _id\n                tool_calls.append(chunk)\n        else:\n            pass\n        if tool_calls:  # do not populate empty list tool_calls\n            message_dict[\"tool_calls\"] = tool_calls\n        if tool_calls and message.content:\n            # Assistant message must have either content or tool_calls, but not both.\n            # Some providers may not support tool_calls in the same message as content.\n            # This is done to ensure compatibility with messages from other providers.\n            message_dict[\"content\"] = \"\"\n        else:\n            message_dict[\"content\"] = message.content\n        if \"prefix\" in message.additional_kwargs:\n            message_dict[\"prefix\"] = message.additional_kwargs[\"prefix\"]\n        return message_dict\n    elif isinstance(message, SystemMessage):\n        return dict(role=\"system\", content=message.content)\n    elif isinstance(message, ToolMessage):\n        return {\n            \"role\": \"tool\",\n            \"content\": message.content,\n            \"name\": message.name,\n            \"tool_call_id\": _convert_tool_call_id_to_mistral_compatible(\n                message.tool_call_id\n            ),\n        }\n    else:\n        raise ValueError(f\"Got unknown type {message}\")\n\n\nclass ChatMistralAI(BaseChatModel):\n    \"\"\"A chat model that uses the MistralAI API.\"\"\"\n\n    # The type for client and async_client is ignored because the type is not\n    # an Optional after the model is initialized and the model_validator\n    # is run.\n    client: httpx.Client = Field(  # type: ignore # : meta private:\n        default=None, exclude=True\n    )\n    async_client: httpx.AsyncClient = Field(  # type: ignore # : meta private:\n        default=None, exclude=True\n    )  #: :meta private:\n    mistral_api_key: Optional[SecretStr] = Field(\n        alias=\"api_key\",\n        default_factory=secret_from_env(\"MISTRAL_API_KEY\", default=None),\n    )\n    endpoint: Optional[str] = Field(default=None, alias=\"base_url\")\n    max_retries: int = 5\n    timeout: int = 120\n    max_concurrent_requests: int = 64\n    model: str = Field(default=\"mistral-small\", alias=\"model_name\")\n    temperature: float = 0.7\n    max_tokens: Optional[int] = None\n    top_p: float = 1\n    \"\"\"Decode using nucleus sampling: consider the smallest set of tokens whose\n       probability sum is at least top_p. Must be in the closed interval [0.0, 1.0].\"\"\"\n    random_seed: Optional[int] = None\n    safe_mode: Optional[bool] = None\n    streaming: bool = False\n\n    model_config = ConfigDict(\n        populate_by_name=True,\n        arbitrary_types_allowed=True,\n    )\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling the API.\"\"\"\n        defaults = {\n            \"model\": self.model,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"top_p\": self.top_p,\n            \"random_seed\": self.random_seed,\n            \"safe_prompt\": self.safe_mode,\n        }\n        filtered = {k: v for k, v in defaults.items() if v is not None}\n        return filtered\n\n    def _get_ls_params(\n        self, stop: Optional[List[str]] = None, **kwargs: Any\n    ) -> LangSmithParams:\n        \"\"\"Get standard params for tracing.\"\"\"\n        params = self._get_invocation_params(stop=stop, **kwargs)\n        ls_params = LangSmithParams(\n            ls_provider=\"mistral\",\n            ls_model_name=self.model,\n            ls_model_type=\"chat\",\n            ls_temperature=params.get(\"temperature\", self.temperature),\n        )\n        if ls_max_tokens := params.get(\"max_tokens\", self.max_tokens):\n            ls_params[\"ls_max_tokens\"] = ls_max_tokens\n        if ls_stop := stop or params.get(\"stop\", None):\n            ls_params[\"ls_stop\"] = ls_stop\n        return ls_params\n\n    @property\n    def _client_params(self) -> Dict[str, Any]:\n        \"\"\"Get the parameters used for the client.\"\"\"\n        return self._default_params\n\n    def completion_with_retry(\n        self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Use tenacity to retry the completion call.\"\"\"\n        # retry_decorator = _create_retry_decorator(self, run_manager=run_manager)\n\n        # @retry_decorator\n        def _completion_with_retry(**kwargs: Any) -> Any:\n            if \"stream\" not in kwargs:\n                kwargs[\"stream\"] = False\n            stream = kwargs[\"stream\"]\n            if stream:\n\n                def iter_sse() -> Iterator[Dict]:\n                    with connect_sse(\n                        self.client, \"POST\", \"/chat/completions\", json=kwargs\n                    ) as event_source:\n                        _raise_on_error(event_source.response)\n                        for event in event_source.iter_sse():\n                            if event.data == \"[DONE]\":\n                                return\n                            yield event.json()\n\n                return iter_sse()\n            else:\n                response = self.client.post(url=\"/chat/completions\", json=kwargs)\n                _raise_on_error(response)\n                return response.json()\n\n        rtn = _completion_with_retry(**kwargs)\n        return rtn\n\n    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:\n        overall_token_usage: dict = {}\n        for output in llm_outputs:\n            if output is None:\n                # Happens in streaming\n                continue\n            token_usage = output[\"token_usage\"]\n            if token_usage is not None:\n                for k, v in token_usage.items():\n                    if k in overall_token_usage:\n                        overall_token_usage[k] += v\n                    else:\n                        overall_token_usage[k] = v\n        combined = {\"token_usage\": overall_token_usage, \"model_name\": self.model}\n        return combined\n\n    @model_validator(mode=\"after\")\n    def validate_environment(self) -> Self:\n        \"\"\"Validate api key, python package exists, temperature, and top_p.\"\"\"\n        if isinstance(self.mistral_api_key, SecretStr):\n            api_key_str: Optional[str] = self.mistral_api_key.get_secret_value()\n        else:\n            api_key_str = self.mistral_api_key\n\n        # todo: handle retries\n        base_url_str = (\n            self.endpoint\n            or os.environ.get(\"MISTRAL_BASE_URL\")\n            or \"https://api.mistral.ai/v1\"\n        )\n        self.endpoint = base_url_str\n        if not self.client:\n            self.client = httpx.Client(\n                base_url=base_url_str,\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Accept\": \"application/json\",\n                    \"Authorization\": f\"Bearer {api_key_str}\",\n                },\n                timeout=self.timeout,\n            )\n        # todo: handle retries and max_concurrency\n        if not self.async_client:\n            self.async_client = httpx.AsyncClient(\n                base_url=base_url_str,\n                headers={\n                    \"Content-Type\": \"application/json\",\n                    \"Accept\": \"application/json\",\n                    \"Authorization\": f\"Bearer {api_key_str}\",\n                },\n                timeout=self.timeout,\n            )\n\n        if self.temperature is not None and not 0 <= self.temperature <= 1:\n            raise ValueError(\"temperature must be in the range [0.0, 1.0]\")\n\n        if self.top_p is not None and not 0 <= self.top_p <= 1:\n            raise ValueError(\"top_p must be in the range [0.0, 1.0]\")\n\n        return self\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._stream(\n                messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return generate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs}\n        response = self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        )\n        return self._create_chat_result(response)\n\n    def _create_chat_result(self, response: Dict) -> ChatResult:\n        generations = []\n        token_usage = response.get(\"usage\", {})\n        for res in response[\"choices\"]:\n            finish_reason = res.get(\"finish_reason\")\n            message = _convert_mistral_chat_message_to_message(res[\"message\"])\n            if token_usage and isinstance(message, AIMessage):\n                message.usage_metadata = {\n                    \"input_tokens\": token_usage.get(\"prompt_tokens\", 0),\n                    \"output_tokens\": token_usage.get(\"completion_tokens\", 0),\n                    \"total_tokens\": token_usage.get(\"total_tokens\", 0),\n                }\n            gen = ChatGeneration(\n                message=message,\n                generation_info={\"finish_reason\": finish_reason},\n            )\n            generations.append(gen)\n\n        llm_output = {\"token_usage\": token_usage, \"model\": self.model}\n        return ChatResult(generations=generations, llm_output=llm_output)\n\n    def _create_message_dicts(\n        self, messages: List[BaseMessage], stop: Optional[List[str]]\n    ) -> Tuple[List[Dict], Dict[str, Any]]:\n        params = self._client_params\n        if stop is not None or \"stop\" in params:\n            if \"stop\" in params:\n                params.pop(\"stop\")\n            logger.warning(\n                \"Parameter `stop` not yet supported (https://docs.mistral.ai/api)\"\n            )\n        message_dicts = [_convert_message_to_mistral_chat_message(m) for m in messages]\n        return message_dicts, params\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs, \"stream\": True}\n\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        for chunk in self.completion_with_retry(\n            messages=message_dicts, run_manager=run_manager, **params\n        ):\n            if len(chunk.get(\"choices\", [])) == 0:\n                continue\n            new_chunk = _convert_chunk_to_message_chunk(chunk, default_chunk_class)\n            # make future chunks same type as first chunk\n            default_chunk_class = new_chunk.__class__\n            gen_chunk = ChatGenerationChunk(message=new_chunk)\n            if run_manager:\n                run_manager.on_llm_new_token(\n                    token=cast(str, new_chunk.content), chunk=gen_chunk\n                )\n            yield gen_chunk\n\n    async def _astream(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        **kwargs: Any,\n    ) -> AsyncIterator[ChatGenerationChunk]:\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs, \"stream\": True}\n\n        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk\n        async for chunk in await acompletion_with_retry(\n            self, messages=message_dicts, run_manager=run_manager, **params\n        ):\n            if len(chunk.get(\"choices\", [])) == 0:\n                continue\n            new_chunk = _convert_chunk_to_message_chunk(chunk, default_chunk_class)\n            # make future chunks same type as first chunk\n            default_chunk_class = new_chunk.__class__\n            gen_chunk = ChatGenerationChunk(message=new_chunk)\n            if run_manager:\n                await run_manager.on_llm_new_token(\n                    token=cast(str, new_chunk.content), chunk=gen_chunk\n                )\n            yield gen_chunk\n\n    async def _agenerate(\n        self,\n        messages: List[BaseMessage],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        should_stream = stream if stream is not None else self.streaming\n        if should_stream:\n            stream_iter = self._astream(\n                messages=messages, stop=stop, run_manager=run_manager, **kwargs\n            )\n            return await agenerate_from_stream(stream_iter)\n\n        message_dicts, params = self._create_message_dicts(messages, stop)\n        params = {**params, **kwargs}\n        response = await acompletion_with_retry(\n            self, messages=message_dicts, run_manager=run_manager, **params\n        )\n        return self._create_chat_result(response)\n\n    def bind_tools(\n        self,\n        tools: Sequence[Union[Dict[str, Any], Type, Callable, BaseTool]],\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, BaseMessage]:\n        \"\"\"Bind tool-like objects to this chat model.\n\n        Assumes model is compatible with OpenAI tool-calling API.\n\n        Args:\n            tools: A list of tool definitions to bind to this chat model.\n                Supports any tool definition handled by\n                :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`.\n            tool_choice: Which tool to require the model to call.\n                Must be the name of the single provided function or\n                \"auto\" to automatically determine which function to call\n                (if any), or a dict of the form:\n                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n            kwargs: Any additional parameters are passed directly to\n                ``self.bind(**kwargs)``.\n        \"\"\"\n\n        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n        return super().bind(tools=formatted_tools, **kwargs)\n\n    def with_structured_output(\n        self,\n        schema: Optional[Union[Dict, Type]] = None,\n        *,\n        method: Literal[\n            \"function_calling\", \"json_mode\", \"json_schema\"\n        ] = \"function_calling\",\n        include_raw: bool = False,\n        **kwargs: Any,\n    ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n        \"\"\"Model wrapper that returns outputs formatted to match the given schema.\n\n        Args:\n            schema:\n                The output schema. Can be passed in as:\n                    - an OpenAI function/tool schema,\n                    - a JSON Schema,\n                    - a TypedDict class (support added in 0.1.12),\n                    - or a Pydantic class.\n                If ``schema`` is a Pydantic class then the model output will be a\n                Pydantic instance of that class, and the model-generated fields will be\n                validated by the Pydantic class. Otherwise the model output will be a\n                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n                for more on how to properly specify types and descriptions of\n                schema fields when specifying a Pydantic or TypedDict class.\n\n                .. versionchanged:: 0.1.12\n\n                        Added support for TypedDict class.\n\n            method: The method for steering model generation, one of:\n\n                - \"function_calling\":\n                    Uses Mistral's\n                    `function-calling feature <https://docs.mistral.ai/capabilities/function_calling/>`_.\n                - \"json_schema\":\n                    Uses Mistral's\n                    `structured output feature <https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/>`_.\n                - \"json_mode\":\n                    Uses Mistral's\n                    `JSON mode <https://docs.mistral.ai/capabilities/structured-output/json_mode/>`_.\n                    Note that if using JSON mode then you\n                    must include instructions for formatting the output into the\n                    desired schema into the model call.\n\n                .. versionchanged:: 0.2.5\n\n                        Added method=\"json_schema\"\n\n            include_raw:\n                If False then only the parsed structured output is returned. If\n                an error occurs during model output parsing it will be raised. If True\n                then both the raw model response (a BaseMessage) and the parsed model\n                response will be returned. If an error occurs during output parsing it\n                will be caught and returned as well. The final output is always a dict\n                with keys \"raw\", \"parsed\", and \"parsing_error\".\n\n        Returns:\n            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n\n            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n            an instance of ``schema`` (i.e., a Pydantic object).\n\n            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n\n            If ``include_raw`` is True, then Runnable outputs a dict with keys:\n                - ``\"raw\"``: BaseMessage\n                - ``\"parsed\"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n                - ``\"parsing_error\"``: Optional[BaseException]\n\n        Example: schema=Pydantic class, method=\"function_calling\", include_raw=False:\n            .. code-block:: python\n\n                from typing import Optional\n\n                from langchain_mistralai import ChatMistralAI\n                from pydantic import BaseModel, Field\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    # If we provide default values and/or descriptions for fields, these will be passed\n                    # to the model. This is an important part of improving a model's ability to\n                    # correctly return structured outputs.\n                    justification: Optional[str] = Field(\n                        default=None, description=\"A justification for the answer.\"\n                    )\n\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n\n                # -> AnswerWithJustification(\n                #     answer='They weigh the same',\n                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n                # )\n\n        Example: schema=Pydantic class, method=\"function_calling\", include_raw=True:\n            .. code-block:: python\n\n                from langchain_mistralai import ChatMistralAI\n                from pydantic import BaseModel\n\n\n                class AnswerWithJustification(BaseModel):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: str\n\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification, include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n                #     'parsing_error': None\n                # }\n\n        Example: schema=TypedDict class, method=\"function_calling\", include_raw=False:\n            .. code-block:: python\n\n                # IMPORTANT: If you are using Python <=3.8, you need to import Annotated\n                # from typing_extensions, not from typing.\n                from typing_extensions import Annotated, TypedDict\n\n                from langchain_mistralai import ChatMistralAI\n\n\n                class AnswerWithJustification(TypedDict):\n                    '''An answer to the user question along with justification for the answer.'''\n\n                    answer: str\n                    justification: Annotated[\n                        Optional[str], None, \"A justification for the answer.\"\n                    ]\n\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(AnswerWithJustification)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        Example: schema=OpenAI function schema, method=\"function_calling\", include_raw=False:\n            .. code-block:: python\n\n                from langchain_mistralai import ChatMistralAI\n\n                oai_schema = {\n                    'name': 'AnswerWithJustification',\n                    'description': 'An answer to the user question along with justification for the answer.',\n                    'parameters': {\n                        'type': 'object',\n                        'properties': {\n                            'answer': {'type': 'string'},\n                            'justification': {'description': 'A justification for the answer.', 'type': 'string'}\n                        },\n                       'required': ['answer']\n                   }\n               }\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(oai_schema)\n\n                structured_llm.invoke(\n                    \"What weighs more a pound of bricks or a pound of feathers\"\n                )\n                # -> {\n                #     'answer': 'They weigh the same',\n                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n                # }\n\n        Example: schema=Pydantic class, method=\"json_mode\", include_raw=True:\n            .. code-block::\n\n                from langchain_mistralai import ChatMistralAI\n                from pydantic import BaseModel\n\n                class AnswerWithJustification(BaseModel):\n                    answer: str\n                    justification: str\n\n                llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n                structured_llm = llm.with_structured_output(\n                    AnswerWithJustification,\n                    method=\"json_mode\",\n                    include_raw=True\n                )\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': AnswerWithJustification(answer='They are both the same weight.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'),\n                #     'parsing_error': None\n                # }\n\n        Example: schema=None, method=\"json_mode\", include_raw=True:\n            .. code-block::\n\n                structured_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n\n                structured_llm.invoke(\n                    \"Answer the following question. \"\n                    \"Make sure to return a JSON blob with keys 'answer' and 'justification'.\\\\n\\\\n\"\n                    \"What's heavier a pound of bricks or a pound of feathers?\"\n                )\n                # -> {\n                #     'raw': AIMessage(content='{\\\\n    \"answer\": \"They are both the same weight.\",\\\\n    \"justification\": \"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.\" \\\\n}'),\n                #     'parsed': {\n                #         'answer': 'They are both the same weight.',\n                #         'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not the weight.'\n                #     },\n                #     'parsing_error': None\n                # }\n\n        \"\"\"  # noqa: E501\n        if kwargs:\n            raise ValueError(f\"Received unsupported arguments {kwargs}\")\n        is_pydantic_schema = isinstance(schema, type) and is_basemodel_subclass(schema)\n        if method == \"function_calling\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is 'function_calling'. \"\n                    \"Received None.\"\n                )\n            # TODO: Update to pass in tool name as tool_choice if/when Mistral supports\n            # specifying a tool.\n            llm = self.bind_tools(\n                [schema],\n                tool_choice=\"any\",\n                structured_output_format={\n                    \"kwargs\": {\"method\": \"function_calling\"},\n                    \"schema\": schema,\n                },\n            )\n            if is_pydantic_schema:\n                output_parser: OutputParserLike = PydanticToolsParser(\n                    tools=[schema],  # type: ignore[list-item]\n                    first_tool_only=True,  # type: ignore[list-item]\n                )\n            else:\n                key_name = convert_to_openai_tool(schema)[\"function\"][\"name\"]\n                output_parser = JsonOutputKeyToolsParser(\n                    key_name=key_name, first_tool_only=True\n                )\n        elif method == \"json_mode\":\n            llm = self.bind(\n                response_format={\"type\": \"json_object\"},\n                structured_output_format={\n                    \"kwargs\": {\n                        # this is correct - name difference with mistral api\n                        \"method\": \"json_mode\"\n                    },\n                    \"schema\": schema,\n                },\n            )\n            output_parser = (\n                PydanticOutputParser(pydantic_object=schema)  # type: ignore[type-var, arg-type]\n                if is_pydantic_schema\n                else JsonOutputParser()\n            )\n        elif method == \"json_schema\":\n            if schema is None:\n                raise ValueError(\n                    \"schema must be specified when method is 'json_schema'. \"\n                    \"Received None.\"\n                )\n            response_format = _convert_to_openai_response_format(schema, strict=True)\n            llm = self.bind(\n                response_format=response_format,\n                structured_output_format={\n                    \"kwargs\": {\"method\": \"json_schema\"},\n                    \"schema\": schema,\n                },\n            )\n\n            output_parser = (\n                PydanticOutputParser(pydantic_object=schema)  # type: ignore[arg-type]\n                if is_pydantic_schema\n                else JsonOutputParser()\n            )\n        if include_raw:\n            parser_assign = RunnablePassthrough.assign(\n                parsed=itemgetter(\"raw\") | output_parser, parsing_error=lambda _: None\n            )\n            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)\n            parser_with_fallback = parser_assign.with_fallbacks(\n                [parser_none], exception_key=\"parsing_error\"\n            )\n            return RunnableMap(raw=llm) | parser_with_fallback\n        else:\n            return llm | output_parser\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return self._default_params\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"mistralai-chat\"\n\n    @property\n    def lc_secrets(self) -> Dict[str, str]:\n        return {\"mistral_api_key\": \"MISTRAL_API_KEY\"}\n\n    @classmethod\n    def is_lc_serializable(cls) -> bool:\n        \"\"\"Return whether this model can be serialized by Langchain.\"\"\"\n        return True\n\n    @classmethod\n    def get_lc_namespace(cls) -> List[str]:\n        \"\"\"Get the namespace of the langchain object.\"\"\"\n        return [\"langchain\", \"chat_models\", \"mistralai\"]\n\n\ndef _convert_to_openai_response_format(\n    schema: Union[Dict[str, Any], Type], *, strict: Optional[bool] = None\n) -> Dict:\n    \"\"\"Same as in ChatOpenAI, but don't pass through Pydantic BaseModels.\"\"\"\n    if (\n        isinstance(schema, dict)\n        and \"json_schema\" in schema\n        and schema.get(\"type\") == \"json_schema\"\n    ):\n        response_format = schema\n    elif isinstance(schema, dict) and \"name\" in schema and \"schema\" in schema:\n        response_format = {\"type\": \"json_schema\", \"json_schema\": schema}\n    else:\n        if strict is None:\n            if isinstance(schema, dict) and isinstance(schema.get(\"strict\"), bool):\n                strict = schema[\"strict\"]\n            else:\n                strict = False\n        function = convert_to_openai_tool(schema, strict=strict)[\"function\"]\n        function[\"schema\"] = function.pop(\"parameters\")\n        response_format = {\"type\": \"json_schema\", \"json_schema\": function}\n\n    if strict is not None and strict is not response_format[\"json_schema\"].get(\n        \"strict\"\n    ):\n        msg = (\n            f\"Output schema already has 'strict' value set to \"\n            f\"{schema['json_schema']['strict']} but 'strict' also passed in to \"\n            f\"with_structured_output as {strict}. Please make sure that \"\n            f\"'strict' is only specified in one place.\"\n        )\n        raise ValueError(msg)\n    return response_format\n",
        "patch": "@@ -68,9 +68,10 @@\n from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n from langchain_core.tools import BaseTool\n-from langchain_core.utils import secret_from_env\n+from langchain_core.utils import get_pydantic_field_names, secret_from_env\n from langchain_core.utils.function_calling import convert_to_openai_tool\n from langchain_core.utils.pydantic import is_basemodel_subclass\n+from langchain_core.utils.utils import _build_model_kwargs\n from pydantic import (\n     BaseModel,\n     ConfigDict,\n@@ -392,12 +393,22 @@ class ChatMistralAI(BaseChatModel):\n     random_seed: Optional[int] = None\n     safe_mode: Optional[bool] = None\n     streaming: bool = False\n+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n+    \"\"\"Holds any invocation parameters not explicitly specified.\"\"\"\n \n     model_config = ConfigDict(\n         populate_by_name=True,\n         arbitrary_types_allowed=True,\n     )\n \n+    @model_validator(mode=\"before\")\n+    @classmethod\n+    def build_extra(cls, values: Dict[str, Any]) -> Any:\n+        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\n+        all_required_field_names = get_pydantic_field_names(cls)\n+        values = _build_model_kwargs(values, all_required_field_names)\n+        return values\n+\n     @property\n     def _default_params(self) -> Dict[str, Any]:\n         \"\"\"Get the default parameters for calling the API.\"\"\"\n@@ -408,6 +419,7 @@ def _default_params(self) -> Dict[str, Any]:\n             \"top_p\": self.top_p,\n             \"random_seed\": self.random_seed,\n             \"safe_prompt\": self.safe_mode,\n+            **self.model_kwargs,\n         }\n         filtered = {k: v for k, v in defaults.items() if v is not None}\n         return filtered"
      },
      {
        "filename": "libs/partners/mistralai/tests/unit_tests/test_chat_models.py",
        "content_before": "\"\"\"Test MistralAI Chat API wrapper.\"\"\"\n\nimport os\nfrom typing import Any, AsyncGenerator, Dict, Generator, List, cast\nfrom unittest.mock import patch\n\nimport pytest\nfrom langchain_core.callbacks.base import BaseCallbackHandler\nfrom langchain_core.messages import (\n    AIMessage,\n    BaseMessage,\n    ChatMessage,\n    HumanMessage,\n    InvalidToolCall,\n    SystemMessage,\n    ToolCall,\n)\nfrom pydantic import SecretStr\n\nfrom langchain_mistralai.chat_models import (  # type: ignore[import]\n    ChatMistralAI,\n    _convert_message_to_mistral_chat_message,\n    _convert_mistral_chat_message_to_message,\n    _convert_tool_call_id_to_mistral_compatible,\n    _is_valid_mistral_tool_call_id,\n)\n\nos.environ[\"MISTRAL_API_KEY\"] = \"foo\"\n\n\ndef test_mistralai_model_param() -> None:\n    llm = ChatMistralAI(model=\"foo\")  # type: ignore[call-arg]\n    assert llm.model == \"foo\"\n\n\ndef test_mistralai_initialization() -> None:\n    \"\"\"Test ChatMistralAI initialization.\"\"\"\n    # Verify that ChatMistralAI can be initialized using a secret key provided\n    # as a parameter rather than an environment variable.\n    for model in [\n        ChatMistralAI(model=\"test\", mistral_api_key=\"test\"),  # type: ignore[call-arg, call-arg]\n        ChatMistralAI(model=\"test\", api_key=\"test\"),  # type: ignore[call-arg, arg-type]\n    ]:\n        assert cast(SecretStr, model.mistral_api_key).get_secret_value() == \"test\"\n\n\n@pytest.mark.parametrize(\n    \"model,expected_url\",\n    [\n        (ChatMistralAI(model=\"test\"), \"https://api.mistral.ai/v1\"),  # type: ignore[call-arg, arg-type]\n        (ChatMistralAI(model=\"test\", endpoint=\"baz\"), \"baz\"),  # type: ignore[call-arg, arg-type]\n    ],\n)\ndef test_mistralai_initialization_baseurl(\n    model: ChatMistralAI, expected_url: str\n) -> None:\n    \"\"\"Test ChatMistralAI initialization.\"\"\"\n    # Verify that ChatMistralAI can be initialized providing endpoint, but also\n    # with default\n\n    assert model.endpoint == expected_url\n\n\n@pytest.mark.parametrize(\n    \"env_var_name\",\n    [\n        (\"MISTRAL_BASE_URL\"),\n    ],\n)\ndef test_mistralai_initialization_baseurl_env(env_var_name: str) -> None:\n    \"\"\"Test ChatMistralAI initialization.\"\"\"\n    # Verify that ChatMistralAI can be initialized using env variable\n    import os\n\n    os.environ[env_var_name] = \"boo\"\n    model = ChatMistralAI(model=\"test\")  # type: ignore[call-arg]\n    assert model.endpoint == \"boo\"\n\n\n@pytest.mark.parametrize(\n    (\"message\", \"expected\"),\n    [\n        (\n            SystemMessage(content=\"Hello\"),\n            dict(role=\"system\", content=\"Hello\"),\n        ),\n        (\n            HumanMessage(content=\"Hello\"),\n            dict(role=\"user\", content=\"Hello\"),\n        ),\n        (\n            AIMessage(content=\"Hello\"),\n            dict(role=\"assistant\", content=\"Hello\"),\n        ),\n        (\n            AIMessage(content=\"{\", additional_kwargs={\"prefix\": True}),\n            dict(role=\"assistant\", content=\"{\", prefix=True),\n        ),\n        (\n            ChatMessage(role=\"assistant\", content=\"Hello\"),\n            dict(role=\"assistant\", content=\"Hello\"),\n        ),\n    ],\n)\ndef test_convert_message_to_mistral_chat_message(\n    message: BaseMessage, expected: Dict\n) -> None:\n    result = _convert_message_to_mistral_chat_message(message)\n    assert result == expected\n\n\ndef _make_completion_response_from_token(token: str) -> Dict:\n    return dict(\n        id=\"abc123\",\n        model=\"fake_model\",\n        choices=[\n            dict(\n                index=0,\n                delta=dict(content=token),\n                finish_reason=None,\n            )\n        ],\n    )\n\n\ndef mock_chat_stream(*args: Any, **kwargs: Any) -> Generator:\n    def it() -> Generator:\n        for token in [\"Hello\", \" how\", \" can\", \" I\", \" help\", \"?\"]:\n            yield _make_completion_response_from_token(token)\n\n    return it()\n\n\nasync def mock_chat_astream(*args: Any, **kwargs: Any) -> AsyncGenerator:\n    async def it() -> AsyncGenerator:\n        for token in [\"Hello\", \" how\", \" can\", \" I\", \" help\", \"?\"]:\n            yield _make_completion_response_from_token(token)\n\n    return it()\n\n\nclass MyCustomHandler(BaseCallbackHandler):\n    last_token: str = \"\"\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n        self.last_token = token\n\n\n@patch(\n    \"langchain_mistralai.chat_models.ChatMistralAI.completion_with_retry\",\n    new=mock_chat_stream,\n)\ndef test_stream_with_callback() -> None:\n    callback = MyCustomHandler()\n    chat = ChatMistralAI(callbacks=[callback])\n    for token in chat.stream(\"Hello\"):\n        assert callback.last_token == token.content\n\n\n@patch(\"langchain_mistralai.chat_models.acompletion_with_retry\", new=mock_chat_astream)\nasync def test_astream_with_callback() -> None:\n    callback = MyCustomHandler()\n    chat = ChatMistralAI(callbacks=[callback])\n    async for token in chat.astream(\"Hello\"):\n        assert callback.last_token == token.content\n\n\ndef test__convert_dict_to_message_tool_call() -> None:\n    raw_tool_call = {\n        \"id\": \"ssAbar4Dr\",\n        \"function\": {\n            \"arguments\": '{\"name\": \"Sally\", \"hair_color\": \"green\"}',\n            \"name\": \"GenerateUsername\",\n        },\n    }\n    message = {\"role\": \"assistant\", \"content\": \"\", \"tool_calls\": [raw_tool_call]}\n    result = _convert_mistral_chat_message_to_message(message)\n    expected_output = AIMessage(\n        content=\"\",\n        additional_kwargs={\"tool_calls\": [raw_tool_call]},\n        tool_calls=[\n            ToolCall(\n                name=\"GenerateUsername\",\n                args={\"name\": \"Sally\", \"hair_color\": \"green\"},\n                id=\"ssAbar4Dr\",\n                type=\"tool_call\",\n            )\n        ],\n    )\n    assert result == expected_output\n    assert _convert_message_to_mistral_chat_message(expected_output) == message\n\n    # Test malformed tool call\n    raw_tool_calls = [\n        {\n            \"id\": \"pL5rEGzxe\",\n            \"function\": {\n                \"arguments\": '{\"name\": \"Sally\", \"hair_color\": \"green\"}',\n                \"name\": \"GenerateUsername\",\n            },\n        },\n        {\n            \"id\": \"ssAbar4Dr\",\n            \"function\": {\n                \"arguments\": \"oops\",\n                \"name\": \"GenerateUsername\",\n            },\n        },\n    ]\n    message = {\"role\": \"assistant\", \"content\": \"\", \"tool_calls\": raw_tool_calls}\n    result = _convert_mistral_chat_message_to_message(message)\n    expected_output = AIMessage(\n        content=\"\",\n        additional_kwargs={\"tool_calls\": raw_tool_calls},\n        invalid_tool_calls=[\n            InvalidToolCall(\n                name=\"GenerateUsername\",\n                args=\"oops\",\n                error=\"Function GenerateUsername arguments:\\n\\noops\\n\\nare not valid JSON. Received JSONDecodeError Expecting value: line 1 column 1 (char 0)\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \",  # noqa: E501\n                id=\"ssAbar4Dr\",\n                type=\"invalid_tool_call\",\n            ),\n        ],\n        tool_calls=[\n            ToolCall(\n                name=\"GenerateUsername\",\n                args={\"name\": \"Sally\", \"hair_color\": \"green\"},\n                id=\"pL5rEGzxe\",\n                type=\"tool_call\",\n            ),\n        ],\n    )\n    assert result == expected_output\n    assert _convert_message_to_mistral_chat_message(expected_output) == message\n\n\ndef test_custom_token_counting() -> None:\n    def token_encoder(text: str) -> List[int]:\n        return [1, 2, 3]\n\n    llm = ChatMistralAI(custom_get_token_ids=token_encoder)\n    assert llm.get_token_ids(\"foo\") == [1, 2, 3]\n\n\ndef test_tool_id_conversion() -> None:\n    assert _is_valid_mistral_tool_call_id(\"ssAbar4Dr\")\n    assert not _is_valid_mistral_tool_call_id(\"abc123\")\n    assert not _is_valid_mistral_tool_call_id(\"call_JIIjI55tTipFFzpcP8re3BpM\")\n\n    result_map = {\n        \"ssAbar4Dr\": \"ssAbar4Dr\",\n        \"abc123\": \"pL5rEGzxe\",\n        \"call_JIIjI55tTipFFzpcP8re3BpM\": \"8kxAQvoED\",\n    }\n    for input_id, expected_output in result_map.items():\n        assert _convert_tool_call_id_to_mistral_compatible(input_id) == expected_output\n        assert _is_valid_mistral_tool_call_id(expected_output)\n",
        "patch": "@@ -255,3 +255,18 @@ def test_tool_id_conversion() -> None:\n     for input_id, expected_output in result_map.items():\n         assert _convert_tool_call_id_to_mistral_compatible(input_id) == expected_output\n         assert _is_valid_mistral_tool_call_id(expected_output)\n+\n+\n+def test_extra_kwargs() -> None:\n+    # Check that foo is saved in extra_kwargs.\n+    llm = ChatMistralAI(model=\"my-model\", foo=3, max_tokens=10)  # type: ignore[call-arg]\n+    assert llm.max_tokens == 10\n+    assert llm.model_kwargs == {\"foo\": 3}\n+\n+    # Test that if extra_kwargs are provided, they are added to it.\n+    llm = ChatMistralAI(model=\"my-model\", foo=3, model_kwargs={\"bar\": 2})  # type: ignore[call-arg]\n+    assert llm.model_kwargs == {\"foo\": 3, \"bar\": 2}\n+\n+    # Test that if provided twice it errors\n+    with pytest.raises(ValueError):\n+        ChatMistralAI(model=\"my-model\", foo=3, model_kwargs={\"foo\": 2})  # type: ignore[call-arg]"
      }
    ]
  }
]