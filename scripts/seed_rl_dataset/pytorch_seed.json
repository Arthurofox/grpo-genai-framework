[
  {
    "issue": "Issue: Added info for each artifact option, added a help option to TORCH_LOG\u2026\n\n\u2026S, and changed the error message (#107758)\r\n\r\nNew message when invalid option is provided\r\n<img width=\"1551\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6355099/8b61534a-ee55-431e-94fe-2ffa25b7fd5c\">\r\n\r\nTORCH_LOGS=\"help\"\r\n<img width=\"1558\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6355099/72e8939c-92fa-4141-8114-79db71451d42\">\r\n\r\nTORCH_LOGS=\"+help\"\r\n<img width=\"1551\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6355099/2cdc94ac-505a-478c-aa58-0175526075d2\">\r\n\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/107758\r\nApproved by: https://github.com/ezyang, https://github.com/mlazos\r\nghstack dependencies: #106192\r\n\r\nFixes #ISSUE_NUMBER\r\n\n\n",
    "ctx": {
      "torch/_logging/_internal.py": "import functools\nimport itertools\nimport logging\nimport os\nimport re\nfrom dataclasses import dataclass, field\nfrom importlib import __import__\nfrom typing import Dict, Optional, Set, Union\nfrom weakref import WeakSet\n\nlog = logging.getLogger(__name__)\n\nDEFAULT_LOG_LEVEL = logging.WARN\nLOG_ENV_VAR = \"TORCH_LOGS\"\n\n\n@dataclass\nclass LogRegistry:\n    # shorthand name to log qualified name\n    # Note: this only contains loggers registered\n    # from register_log\n    # e.g. \"dynamo\" -> \"torch._dynamo\"\n    log_alias_to_log_qname: Dict[str, str] = field(default_factory=dict)\n\n    # artifact logger qualified names,\n    # this is populated lazily, as calls to getArtifactLogger\n    # currently formatted as <module>.__<artifact_name>\n    # e.g. \"torch._dynamo.convert_frame.__guards\"\n    artifact_log_qnames: Set[str] = field(default_factory=set)\n\n    # child logs of registered logs if specified via open\n    # registration by the user (ie placing \"torch._dynamo.output_graph\" in the env var)\n    # these need to be tracked so their levels can be reset properly\n    # e.g. \"torch._dynamo.output_graph\"\n    child_log_qnames: Set[str] = field(default_factory=set)\n\n    # artifact names, populated by register_artifact\n    # e.g. \"guards\"\n    artifact_names: Set[str] = field(default_factory=set)\n\n    # artifacts which are not displayed unless explicitly named in the\n    # settings. Ex. output_code is NOT displayed even if the inductor\n    # log level is set to DEBUG. It must be explicitly named in the settings\n    off_by_default_artifact_names: Set[str] = field(default_factory=set)\n\n    # logging format string for artifacts\n    artifact_log_formatters: Dict[str, logging.Formatter] = field(default_factory=dict)\n\n    def is_artifact(self, name):\n        return name in self.artifact_names\n\n    def is_log(self, alias):\n        return alias in self.log_alias_to_log_qname\n\n    # register a log with an alias\n    def register_log(self, alias, log_qname):\n        self.log_alias_to_log_qname[alias] = log_qname\n\n    # register an artifact name\n    def register_artifact_name(self, name, off_by_default, log_format):\n        self.artifact_names.add(name)\n\n        # if off by default, don't enable it\n        # when log_name's log_level is set to DEBUG\n        if off_by_default:\n            self.off_by_default_artifact_names.add(name)\n\n        if log_format is not None:\n            self.artifact_log_formatters[name] = logging.Formatter(log_format)\n\n    # register the qualified name of an artifact log\n    # this is needed to know which logs need to be reset\n    # whenever the log_state is changed\n    def register_artifact_log(self, artifact_log_qname):\n        self.artifact_log_qnames.add(artifact_log_qname)\n\n    def register_child_log(self, log_qname):\n        self.child_log_qnames.add(log_qname)\n\n    def get_log_qnames(self):\n        return set(self.log_alias_to_log_qname.values())\n\n    def get_artifact_log_qnames(self):\n        return set(self.artifact_log_qnames)\n\n    def get_child_log_qnames(self):\n        return set(self.child_log_qnames)\n\n    def is_off_by_default(self, artifact_qname):\n        return artifact_qname in self.off_by_default_artifact_names\n\n\n@dataclass\nclass LogState:\n    # qualified log names -> currently set log level\n    log_qname_to_level: Dict[str, str] = field(default_factory=dict)\n\n    # the set of currently enabled artifacts\n    artifact_names: Set[str] = field(default_factory=set)\n\n    def enable_artifact(self, artifact_name):\n        self.artifact_names.add(artifact_name)\n\n    def is_artifact_enabled(self, name):\n        return name in self.artifact_names\n\n    def enable_log(self, log_qname, log_level):\n        self.log_qname_to_level[log_qname] = log_level\n\n    def get_log_level_pairs(self):\n        return self.log_qname_to_level.items()\n\n    def clear(self):\n        self.log_qname_to_level.clear()\n        self.artifact_names.clear()\n\n\nlog_registry = LogRegistry()\nlog_state = LogState()\n\n# sample usage: torch._logging.set_logs(**torch._logging.DEFAULT_LOGGING)\nDEFAULT_LOGGING = {\n    \"graph_breaks\": True,\n    \"recompiles\": True,\n    \"dynamic\": logging.INFO,\n    \"guards\": True,\n    \"trace_source\": True,\n}\n\n\ndef set_logs(\n    *,\n    all: Optional[int] = None,\n    dynamo: Optional[int] = None,\n    aot: Optional[int] = None,\n    dynamic: Optional[int] = None,\n    inductor: Optional[int] = None,\n    distributed: Optional[int] = None,\n    onnx: Optional[int] = None,\n    bytecode: bool = False,\n    aot_graphs: bool = False,\n    aot_joint_graph: bool = False,\n    ddp_graphs: bool = False,\n    graph: bool = False,\n    graph_code: bool = False,\n    graph_breaks: bool = False,\n    graph_sizes: bool = False,\n    guards: bool = False,\n    recompiles: bool = False,\n    trace_source: bool = False,\n    trace_call: bool = False,\n    output_code: bool = False,\n    schedule: bool = False,\n    perf_hints: bool = False,\n    onnx_diagnostics: bool = False,\n    modules: Optional[Dict[str, Union[int, bool]]] = None,\n):\n    \"\"\"\n    Sets the log level for individual components and toggles individual log\n    artifact types.\n\n    .. warning:: This feature is a prototype and may have compatibility\n        breaking changes in the future.\n\n    .. note:: The ``TORCH_LOGS`` environment variable has complete precedence\n        over this function, so if it was set, this function does nothing.\n\n    A component is a set of related features in PyTorch. All of the log\n    messages emitted from a given component have their own log levels. If the\n    log level of a particular message has priority greater than or equal to its\n    component's log level setting, it is emitted. Otherwise, it is supressed.\n    This allows you to, for instance, silence large groups of log messages that\n    are not relevant to you and increase verbosity of logs for components that\n    are relevant. The expected log level values, ordered from highest to lowest\n    priority, are:\n\n        * ``logging.CRITICAL``\n        * ``logging.ERROR``\n        * ``logging.WARNING``\n        * ``logging.INFO``\n        * ``logging.DEBUG``\n        * ``logging.NOTSET``\n\n    See documentation for the Python ``logging`` module for more information on\n    log levels: `<https://docs.python.org/3/library/logging.html#logging-levels>`_\n\n    An artifact is a particular type of log message. Each artifact is assigned\n    to a parent component. A component can emit many different kinds of\n    artifacts. In general, an artifact is emitted if either its corresponding\n    setting in the argument list below is turned on or if its parent component\n    is set to a log level less than or equal to the log level of the artifact.\n\n    Keyword args:\n        all (:class:`Optional[int]`):\n            The default log level for all components. Default: ``logging.WARN``\n\n        dynamo (:class:`Optional[int]`):\n            The log level for the TorchDynamo component. Default: ``logging.WARN``\n\n        aot (:class:`Optional[int]`):\n            The log level for the AOTAutograd component. Default: ``logging.WARN``\n\n        inductor (:class:`Optional[int]`):\n            The log level for the TorchInductor component. Default: ``logging.WARN``\n\n        dynamic (:class:`Optional[int]`):\n            The log level for dynamic shapes. Default: ``logging.WARN``\n\n        distributed (:class:`Optional[int]`):\n            Whether to log communication operations and other debug info from pytorch distributed components.\n            Default: ``logging.WARN``\n\n        onnx (:class:`Optional[int]`):\n            The log level for the ONNX exporter component. Default: ``logging.WARN``\n\n        bytecode (:class:`bool`):\n            Whether to emit the original and generated bytecode from TorchDynamo.\n            Default: ``False``\n\n        aot_graphs (:class:`bool`):\n            Whether to emit the graphs generated by AOTAutograd. Default: ``False``\n\n        aot_joint_graph (:class:`bool`):\n            Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: ``False``\n\n        ddp_graphs (:class:`bool`):\n            Whether to emit graphs generated by DDPOptimizer. Default: ``False``\n\n        graph (:class:`bool`):\n            Whether to emit the graph captured by TorchDynamo in tabular format.\n            Default: ``False``\n\n        graph_code (:class:`bool`):\n            Whether to emit the python source of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        graph_breaks (:class:`bool`):\n            Whether to emit the graph breaks encountered by TorchDynamo.\n            Default: ``False``\n\n        graph_sizes (:class:`bool`):\n            Whether to emit tensor sizes of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        guards (:class:`bool`):\n            Whether to emit the guards generated by TorchDynamo for each compiled\n            function. Default: ``False``\n\n        recompiles (:class:`bool`):\n            Whether to emit a guard failure reason and message every time\n            TorchDynamo recompiles a function. Default: ``False``\n\n        trace_source (:class:`bool`):\n            Whether to emit when TorchDynamo begins tracing a new line. Default: ``False``\n\n        trace_call (:class:`bool`):\n            Whether to emit detailed line location when TorchDynamo creates an FX node\n            corresponding to function call. Python 3.11+ only. Default: ``False``\n\n        output_code (:class:`bool`):\n            Whether to emit the TorchInductor output code. Default: ``False``\n\n        schedule (:class:`bool`):\n            Whether to emit the TorchInductor schedule. Default: ``False``\n\n        perf_hints (:class:`bool`):\n            Whether to emit the TorchInductor perf hints. Default: ``False``\n\n        onnx_diagnostics (:class:`bool`):\n            Whether to emit the ONNX exporter diagnostics in logging. Default: ``False``\n\n        modules (dict):\n            This argument provides an alternate way to specify the above log\n            component and artifact settings, in the format of a keyword args\n            dictionary given as a single argument. There are two cases\n            where this is useful (1) if a new log component or artifact has\n            been registered but a keyword argument for it has not been added\n            to this function and (2) if the log level for an unregistered module\n            needs to be set. This can be done by providing the fully-qualified module\n            name as the key, with the log level as the value. Default: ``None``\n\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> import logging\n\n        # The following changes the \"dynamo\" component to emit DEBUG-level\n        # logs, and to emit \"graph_code\" artifacts.\n\n        >>> torch._logging.set_logs(dynamo=logging.DEBUG, graph_code=True)\n\n        # The following enables the logs for a different module\n\n        >>> torch._logging.set_logs(modules={\"unregistered.module.name\": logging.DEBUG})\n    \"\"\"\n    # ignore if env var is set\n    if LOG_ENV_VAR in os.environ:\n        log.warning(\n            \"Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs\"\n        )\n        return\n\n    log_state.clear()\n\n    modules = modules or {}\n\n    def _set_logs(**kwargs):\n        default_level = kwargs.pop(\"all\", None)\n        if default_level:\n            if default_level not in logging._levelToName:\n                raise ValueError(\n                    f\"Unrecognized log level for kwarg all: {default_level}, valid level values \"\n                    f\"are: {','.join([str(k) for k in logging._levelToName.keys()])}\"\n                )\n\n            # add any missing aliases to kwargs\n            for alias in log_registry.log_alias_to_log_qname.keys():\n                if alias not in kwargs:\n                    kwargs[alias] = default_level\n        else:\n            default_level = DEFAULT_LOG_LEVEL\n\n        for alias, val in itertools.chain(kwargs.items(), modules.items()):  # type: ignore[union-attr]\n            if val is None:\n                val = default_level\n\n            if log_registry.is_artifact(alias):\n                if not isinstance(val, bool):\n                    raise ValueError(\n                        f\"Expected bool to enable artifact {alias}, received {val}\"\n                    )\n\n                if val:\n                    log_state.enable_artifact(alias)\n            elif log_registry.is_log(alias) or alias in log_registry.child_log_qnames:\n                if val not in logging._levelToName:\n                    raise ValueError(\n                        f\"Unrecognized log level for log {alias}: {val}, valid level values \"\n                        f\"are: {','.join([str(k) for k in logging._levelToName.keys()])}\"\n                    )\n\n                log_state.enable_log(\n                    log_registry.log_alias_to_log_qname.get(alias, alias), val\n                )\n            elif alias == \"all\":\n                continue\n            else:\n                raise ValueError(\n                    f\"Unrecognized log or artifact name passed to set_logs: {alias}\"\n                )\n\n        _init_logs()\n\n    _set_logs(\n        all=all,\n        dynamo=dynamo,\n        aot=aot,\n        inductor=inductor,\n        dynamic=dynamic,\n        bytecode=bytecode,\n        aot_graphs=aot_graphs,\n        aot_joint_graph=aot_joint_graph,\n        ddp_graphs=ddp_graphs,\n        distributed=distributed,\n        graph=graph,\n        graph_code=graph_code,\n        graph_breaks=graph_breaks,\n        graph_sizes=graph_sizes,\n        guards=guards,\n        recompiles=recompiles,\n        trace_source=trace_source,\n        trace_call=trace_call,\n        output_code=output_code,\n        schedule=schedule,\n        perf_hints=perf_hints,\n        onnx=onnx,\n        onnx_diagnostics=onnx_diagnostics,\n    )\n\n\ndef get_loggers():\n    \"\"\"\n    Returns: a list of all registered loggers\n    \"\"\"\n    return [logging.getLogger(qname) for qname in log_registry.get_log_qnames()]\n\n\ndef register_log(setting_name, log_name):\n    \"\"\"\n    Enables a log to be controlled by the env var and user API with the setting_name\n    Args:\n        setting_name:  the shorthand name used in the env var and user API\n        log_name:  the log name that the setting_name is associated with\n    \"\"\"\n    log_registry.register_log(setting_name, log_name)\n\n\ndef register_artifact(setting_name, off_by_default=False, log_format=None):\n    \"\"\"\n    Enables an artifact to be controlled by the env var and user API with name\n    Args:\n        setting_name: the shorthand name used in the env var and user API\n        off_by_default: whether this artifact should be logged when the ancestor loggers\n            are enabled at level DEBUG\n    \"\"\"\n    log_registry.register_artifact_name(setting_name, off_by_default, log_format)\n\n\ndef getArtifactLogger(module_qname, artifact_name):\n    if artifact_name not in log_registry.artifact_names:\n        raise ValueError(\n            f\"Artifact name: {repr(artifact_name)} not registered,\"\n            f\"please call register_artifact({repr(artifact_name)}) in torch._logging.registrations.\"\n        )\n    qname = module_qname + f\".__{artifact_name}\"\n    log = logging.getLogger(qname)\n    log.artifact_name = artifact_name  # type: ignore[attr-defined]\n    log_registry.register_artifact_log(qname)\n    configure_artifact_log(log)\n    return log\n\n\nINCR_VERBOSITY_CHAR = \"+\"\nDECR_VERBOSITY_CHAR = \"-\"\nVERBOSITY_REGEX = (\n    \"(\"\n    + \"|\".join([re.escape(INCR_VERBOSITY_CHAR), re.escape(DECR_VERBOSITY_CHAR)])\n    + \"?)\"\n)\n\n\ndef configure_artifact_log(log):\n    # If the artifact is off by default, then it should only be logged when explicitly\n    # enabled; set propagate to False so that this artifact is not propagated\n    # to its ancestor logger\n    if log_registry.is_off_by_default(log.artifact_name):\n        log.propagate = False\n\n    # enable artifact logging when explicitly enabled\n    if log_state.is_artifact_enabled(log.artifact_name):\n        log.setLevel(logging.DEBUG)\n        log.propagate = True\n\n\n# match a comma separated list of loggable names (whitespace allowed after commas)\ndef _gen_settings_regex():\n    return re.compile(r\"((\\+|-)?[\\w\\.]+,\\s*)*(\\+|-)?[\\w\\.]+?\")\n\n\ndef _validate_settings(settings):\n    return re.fullmatch(_gen_settings_regex(), settings) is not None\n\n\ndef _invalid_settings_err_msg(settings):\n    entities = \"\\n  \" + \"\\n  \".join(\n        itertools.chain(\n            [\"all\"],\n            log_registry.log_alias_to_log_qname.keys(),\n            log_registry.artifact_names,\n        )\n    )\n    msg = (\n        f\"Invalid log settings: {settings}, must be a comma separated list of fully qualified module names, \"\n        f\"registered log names or registered artifact names.\\nCurrently registered names: {entities}\"\n    )\n    return msg\n\n\n@functools.lru_cache\ndef _parse_log_settings(settings):\n    if settings == \"\":\n        return dict()\n\n    if not _validate_settings(settings):\n        raise ValueError(_invalid_settings_err_msg(settings))\n\n    settings = re.sub(r\"\\s+\", \"\", settings)\n    log_names = settings.split(\",\")\n\n    def get_name_level_pair(name):\n        clean_name = name.replace(INCR_VERBOSITY_CHAR, \"\")\n        clean_name = clean_name.replace(DECR_VERBOSITY_CHAR, \"\")\n\n        if name[0] == INCR_VERBOSITY_CHAR:\n            level = logging.DEBUG\n        elif name[0] == DECR_VERBOSITY_CHAR:\n            level = logging.ERROR\n        else:\n            level = logging.INFO\n\n        return clean_name, level\n\n    log_state = LogState()\n\n    for name in log_names:\n        name, level = get_name_level_pair(name)\n        if name == \"all\":\n            for log_qname in log_registry.get_log_qnames():\n                log_state.enable_log(log_qname, level)\n\n    for name in log_names:\n        name, level = get_name_level_pair(name)\n\n        if log_registry.is_log(name):\n            assert level is not None\n            log_qname = log_registry.log_alias_to_log_qname[name]\n            log_state.enable_log(log_qname, level)\n        elif log_registry.is_artifact(name):\n            log_state.enable_artifact(name)\n        elif name == \"all\":\n            continue\n        elif _is_valid_module(name):\n            if not _has_registered_parent(name):\n                log_registry.register_log(name, name)\n            else:\n                log_registry.register_child_log(name)\n            log_state.enable_log(name, level)\n        else:\n            raise ValueError(_invalid_settings_err_msg(settings))\n\n    return log_state\n\n\ndef _is_valid_module(qname):\n    try:\n        __import__(qname)\n        return True\n    except ImportError:\n        return False\n\n\ndef _update_log_state_from_env():\n    global log_state\n    log_setting = os.environ.get(LOG_ENV_VAR, None)\n    if log_setting is not None:\n        log_state = _parse_log_settings(log_setting)\n\n\ndef _has_registered_parent(log_qname):\n    cur_log = logging.getLogger(log_qname)\n\n    registered_log_qnames = log_registry.get_log_qnames()\n\n    while cur_log.parent:\n        if cur_log.name in registered_log_qnames:\n            return True\n        cur_log = cur_log.parent\n\n    return False\n\n\n# apply custom formats to artifacts when necessary\nclass TorchLogsFormatter(logging.Formatter):\n    def format(self, record):\n        artifact_name = getattr(logging.getLogger(record.name), \"artifact_name\", None)\n        if artifact_name is not None:\n            artifact_formatter = log_registry.artifact_log_formatters.get(\n                artifact_name, None\n            )\n            if artifact_formatter is not None:\n                return artifact_formatter.format(record)\n\n        record.message = record.getMessage()\n        record.asctime = self.formatTime(record, self.datefmt)\n\n        lines = record.message.split(\"\\n\")\n        record.rankprefix = \"\"\n        if dist.is_available() and dist.is_initialized():\n            record.rankprefix = f\"[rank{dist.get_rank()}]:\"\n\n        record.compileid = \"\"\n        if (\n            compile_id := torch._guards.CompileContext.current_compile_id()\n        ) is not None:\n            record.compileid = f\" [{compile_id}]\"\n\n        prefix = f\"{record.rankprefix}[{record.asctime}]{record.compileid} {record.name}: [{record.levelname}]\"\n        return \"\\n\".join(f\"{prefix} {l}\" for l in lines)\n\n\nDEFAULT_FORMATTER = TorchLogsFormatter()\n\n\ndef _setup_handlers(create_handler_fn, log):\n    debug_handler = _track_handler(create_handler_fn())\n    debug_handler.setFormatter(DEFAULT_FORMATTER)\n    debug_handler.setLevel(logging.DEBUG)\n    log.addHandler(debug_handler)\n\n\nhandlers = WeakSet()  # type: ignore[var-annotated]\n\n\n# mark handlers that we've created\n# so we don't modify user handlers\ndef _track_handler(handler):\n    handlers.add(handler)\n    return handler\n\n\ndef _is_torch_handler(handler):\n    return handler in handlers\n\n\n# clears all torch handlers on specified loggers\ndef _clear_handlers(log):\n    to_remove = [handler for handler in log.handlers if _is_torch_handler(handler)]\n    for handler in to_remove:\n        log.removeHandler(handler)\n\n\ndef _reset_logs():\n    # reset all registered logs\n    for log_qname in log_registry.get_log_qnames():\n        log = logging.getLogger(log_qname)\n        log.setLevel(logging.WARNING)\n        log.propagate = False\n        _clear_handlers(log)\n\n    # reset all artifact and child logs\n    for artifact_log_qname in itertools.chain(\n        log_registry.get_artifact_log_qnames(), log_registry.get_child_log_qnames()\n    ):\n        log = logging.getLogger(artifact_log_qname)\n        log.setLevel(logging.NOTSET)\n        log.propagate = True\n\n\ndef _get_log_state():\n    return log_state\n\n\ndef _set_log_state(state):\n    global log_state\n    log_state = state\n\n\ndef _init_logs(log_file_name=None):\n    _reset_logs()\n    _update_log_state_from_env()\n\n    for log_qname, level in log_state.get_log_level_pairs():\n        log = logging.getLogger(log_qname)\n        log.setLevel(level)\n\n    # setup handlers for all registered loggers\n    for log_qname in log_registry.get_log_qnames():\n        log = logging.getLogger(log_qname)\n        _setup_handlers(\n            logging.StreamHandler,\n            log,\n        )\n\n        if log_file_name is not None:\n            _setup_handlers(\n                lambda: logging.FileHandler(log_file_name),\n                log,\n            )\n\n    # configure artifact loggers, note: this must happen last\n    # since the levels of ancestor loggers are taken into account\n    for artifact_log_qname in log_registry.get_artifact_log_qnames():\n        log = logging.getLogger(artifact_log_qname)\n        configure_artifact_log(log)\n\n\n@functools.lru_cache(None)\ndef warning_once(logger_obj, *args, **kwargs):\n    \"\"\"\n    This function is similar to `logger.warning()`, but will emit the warning with the same message only once\n    Note: The cache is for the function arguments, so 2 different callers using the same arguments will hit the cache.\n    The assumption here is that all warning messages are unique across the code. If they aren't then need to switch to\n    another type of cache that includes the caller frame information in the hashing function.\n    \"\"\"\n    logger_obj.warning(*args, **kwargs)\n\n\nclass LazyString:\n    def __init__(self, func, *args, **kwargs):\n        self.func = func\n        self.args = args\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return self.func(*self.args, **self.kwargs)\n\n\nimport torch._guards\nimport torch.distributed as dist\n",
      "torch/_logging/_registrations.py": "from ._internal import register_artifact, register_log\n\nregister_log(\"dynamo\", \"torch._dynamo\")\nregister_log(\"aot\", \"torch._functorch.aot_autograd\")\nregister_log(\"inductor\", \"torch._inductor\")\nregister_log(\"dynamic\", \"torch.fx.experimental.symbolic_shapes\")\nregister_log(\"torch\", \"torch\")\nregister_log(\"distributed\", \"torch.distributed\")\nregister_log(\"onnx\", \"torch.onnx\")\n\nregister_artifact(\"guards\")\nregister_artifact(\"verbose_guards\", off_by_default=True)\nregister_artifact(\"bytecode\", off_by_default=True)\nregister_artifact(\"graph\")\nregister_artifact(\"graph_code\")\nregister_artifact(\"graph_sizes\")\nregister_artifact(\"trace_source\")\nregister_artifact(\"trace_call\")\nregister_artifact(\"aot_graphs\")\nregister_artifact(\"aot_joint_graph\")\nregister_artifact(\"ddp_graphs\")\nregister_artifact(\"recompiles\")\nregister_artifact(\"graph_breaks\")\nregister_artifact(\"not_implemented\")\nregister_artifact(\"output_code\", off_by_default=True)\nregister_artifact(\"schedule\", off_by_default=True)\nregister_artifact(\"perf_hints\", off_by_default=True)\nregister_artifact(\"onnx_diagnostics\", off_by_default=True)\n\nregister_artifact(\"custom_format_test_artifact\", log_format=\"\")\n",
      "torch/_VF.py": "\"\"\"\nThis makes the functions in torch._C._VariableFunctions available as\n    torch._VF.<funcname>\nwithout mypy being able to find them.\n\nA subset of those functions are mapped to ATen functions in\ntorch/jit/_builtins.py\n\nSee https://github.com/pytorch/pytorch/issues/21478 for the reason for\nintroducing torch._VF\n\n\"\"\"\nimport sys\nimport types\n\nimport torch\n\n\nclass VFModule(types.ModuleType):\n    vf: types.ModuleType\n\n    def __init__(self, name):\n        super().__init__(name)\n        self.vf = torch._C._VariableFunctions\n\n    def __getattr__(self, attr):\n        return getattr(self.vf, attr)\n\n\nsys.modules[__name__] = VFModule(__name__)\n",
      "torch/__config__.py": "import torch\n\n\ndef show():\n    \"\"\"\n    Return a human-readable string with descriptions of the\n    configuration of PyTorch.\n    \"\"\"\n    return torch._C._show_config()\n\n\n# TODO: In principle, we could provide more structured version/config\n# information here. For now only CXX_FLAGS is exposed, as Timer\n# uses them.\ndef _cxx_flags():\n    \"\"\"Returns the CXX_FLAGS used when building PyTorch.\"\"\"\n    return torch._C._cxx_flags()\n\n\ndef parallel_info():\n    r\"\"\"Returns detailed string with parallelization settings\"\"\"\n    return torch._C._parallel_info()\n",
      "torch/__future__.py": "\"\"\"\nThis global flag controls whether to assign new tensors to the parameters\ninstead of changing the existing parameters in-place when converting an `nn.Module`\nusing the following methods:\n1. `module.cuda()` / `.cpu()` (for moving `module` between devices)\n2. `module.float()` / `.double()` / `.half()` (for converting `module` to a different dtype)\n3. `module.to()` / `.type()` (for changing `module`'s device or dtype)\n4. `module._apply(fn)` (for generic functions applied to `module`)\n\nDefault: False\n\"\"\"\n_overwrite_module_params_on_conversion = False\n\n\ndef set_overwrite_module_params_on_conversion(value):\n    global _overwrite_module_params_on_conversion\n    _overwrite_module_params_on_conversion = value\n\n\ndef get_overwrite_module_params_on_conversion():\n    return _overwrite_module_params_on_conversion\n",
      "torch/__init__.py": "\nr\"\"\"\nThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\n\nIt has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0.\n\"\"\"\n\nimport math\nimport os\nimport sys\nimport platform\nimport textwrap\nimport ctypes\nimport inspect\n\n# multipy/deploy is setting this import before importing torch, this is the most\n# reliable way we have to detect if we're running within deploy.\n# https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137\ndef _running_with_deploy():\n    return sys.modules.get(\"torch._meta_registrations\", None) is object\n\nfrom ._utils import _import_dotted_name, classproperty\nfrom ._utils_internal import get_file_path, prepare_multiprocessing_environment, \\\n    USE_RTLD_GLOBAL_WITH_LIBTORCH, USE_GLOBAL_DEPS\n\n# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\nif _running_with_deploy():\n    __version__ = \"torch-deploy-1.8\"\nelse:\n    from .torch_version import __version__ as __version__\n\nfrom typing import Any, Callable, Dict, Optional, Set, Tuple, Type, TYPE_CHECKING, Union, List\nimport builtins\n\n__all__ = [\n    'typename', 'is_tensor', 'is_storage',\n    'set_default_tensor_type', 'set_default_device',\n    'set_rng_state', 'get_rng_state', 'manual_seed', 'initial_seed', 'seed',\n    'save', 'load', 'set_printoptions', 'chunk', 'split', 'stack', 'matmul',\n    'no_grad', 'enable_grad', 'rand', 'randn', 'inference_mode',\n    'DoubleStorage', 'FloatStorage', 'LongStorage', 'IntStorage',\n    'ShortStorage', 'CharStorage', 'ByteStorage', 'BoolStorage',\n    'TypedStorage', 'UntypedStorage',\n    'DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',\n    'ShortTensor', 'CharTensor', 'ByteTensor', 'BoolTensor', 'Tensor',\n    'lobpcg', 'use_deterministic_algorithms',\n    'are_deterministic_algorithms_enabled',\n    'is_deterministic_algorithms_warn_only_enabled',\n    'set_deterministic_debug_mode', 'get_deterministic_debug_mode',\n    'set_float32_matmul_precision', 'get_float32_matmul_precision',\n    'set_warn_always', 'is_warn_always_enabled', 'SymInt', 'SymFloat',\n    'SymBool', 'sym_not',\n    'sym_int', 'sym_float', 'sym_max', 'sym_min', 'compile', 'vmap',\n    'export',\n]\n\n################################################################################\n# Load the extension module\n################################################################################\n\nif sys.platform == 'win32':\n    pfiles_path = os.getenv('ProgramFiles', 'C:\\\\Program Files')\n    py_dll_path = os.path.join(sys.exec_prefix, 'Library', 'bin')\n    th_dll_path = os.path.join(os.path.dirname(__file__), 'lib')\n\n    # When users create a virtualenv that inherits the base environment,\n    # we will need to add the corresponding library directory into\n    # DLL search directories. Otherwise, it will rely on `PATH` which\n    # is dependent on user settings.\n    if sys.exec_prefix != sys.base_exec_prefix:\n        base_py_dll_path = os.path.join(sys.base_exec_prefix, 'Library', 'bin')\n    else:\n        base_py_dll_path = ''\n\n    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path]))\n\n    if all(not os.path.exists(os.path.join(p, 'nvToolsExt64_1.dll')) for p in dll_paths):\n        nvtoolsext_dll_path = os.path.join(\n            os.getenv('NVTOOLSEXT_PATH', os.path.join(pfiles_path, 'NVIDIA Corporation', 'NvToolsExt')), 'bin', 'x64')\n    else:\n        nvtoolsext_dll_path = ''\n\n    from .version import cuda as cuda_version\n    import glob\n    if cuda_version and all(not glob.glob(os.path.join(p, 'cudart64*.dll')) for p in dll_paths):\n        cuda_version_1 = cuda_version.replace('.', '_')\n        cuda_path_var = 'CUDA_PATH_V' + cuda_version_1\n        default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)\n        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')\n    else:\n        cuda_path = ''\n\n    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))\n\n    kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)\n    with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')\n    prev_error_mode = kernel32.SetErrorMode(0x0001)\n\n    kernel32.LoadLibraryW.restype = ctypes.c_void_p\n    if with_load_library_flags:\n        kernel32.LoadLibraryExW.restype = ctypes.c_void_p\n\n    for dll_path in dll_paths:\n        os.add_dll_directory(dll_path)\n\n    try:\n        ctypes.CDLL('vcruntime140.dll')\n        ctypes.CDLL('msvcp140.dll')\n        ctypes.CDLL('vcruntime140_1.dll')\n    except OSError:\n        print('''Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe''')\n\n    dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))\n    path_patched = False\n    for dll in dlls:\n        is_loaded = False\n        if with_load_library_flags:\n            res = kernel32.LoadLibraryExW(dll, None, 0x00001100)\n            last_error = ctypes.get_last_error()\n            if res is None and last_error != 126:\n                err = ctypes.WinError(last_error)\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n            elif res is not None:\n                is_loaded = True\n        if not is_loaded:\n            if not path_patched:\n                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])\n                path_patched = True\n            res = kernel32.LoadLibraryW(dll)\n            if res is None:\n                err = ctypes.WinError(ctypes.get_last_error())\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n\n    kernel32.SetErrorMode(prev_error_mode)\n\n\ndef _preload_cuda_deps(lib_folder, lib_name):\n    \"\"\"Preloads cuda deps if they could not be found otherwise.\"\"\"\n    # Should only be called on Linux if default path resolution have failed\n    assert platform.system() == 'Linux', 'Should only be called on Linux'\n    import glob\n    lib_path = None\n    for path in sys.path:\n        nvidia_path = os.path.join(path, 'nvidia')\n        if not os.path.exists(nvidia_path):\n            continue\n        candidate_lib_paths = glob.glob(os.path.join(nvidia_path, lib_folder, 'lib', lib_name))\n        if candidate_lib_paths and not lib_path:\n            lib_path = candidate_lib_paths[0]\n        if lib_path:\n            break\n    if not lib_path:\n        raise ValueError(f\"{lib_name} not found in the system path {sys.path}\")\n    ctypes.CDLL(lib_path)\n\n\n# See Note [Global dependencies]\ndef _load_global_deps() -> None:\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return\n\n    lib_name = 'libtorch_global_deps' + ('.dylib' if platform.system() == 'Darwin' else '.so')\n    here = os.path.abspath(__file__)\n    lib_path = os.path.join(os.path.dirname(here), 'lib', lib_name)\n\n    try:\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n    except OSError as err:\n        # Can only happen for wheel with cuda libs as PYPI deps\n        # As PyTorch is not purelib, but nvidia-*-cu11 is\n        cuda_libs: Dict[str, str] = {\n            'cublas': 'libcublas.so.*[0-9]',\n            'cudnn': 'libcudnn.so.*[0-9]',\n            'cuda_nvrtc': 'libnvrtc.so.*[0-9].*[0-9]',\n            'cuda_runtime': 'libcudart.so.*[0-9].*[0-9]',\n            'cuda_cupti': 'libcupti.so.*[0-9].*[0-9]',\n            'cufft': 'libcufft.so.*[0-9]',\n            'curand': 'libcurand.so.*[0-9]',\n            'cusolver': 'libcusolver.so.*[0-9]',\n            'cusparse': 'libcusparse.so.*[0-9]',\n            'nccl': 'libnccl.so.*[0-9]',\n            'nvtx': 'libnvToolsExt.so.*[0-9]',\n        }\n        is_cuda_lib_err = [lib for lib in cuda_libs.values() if(lib.split('.')[0] in err.args[0])]\n        if not is_cuda_lib_err:\n            raise err\n        for lib_folder, lib_name in cuda_libs.items():\n            _preload_cuda_deps(lib_folder, lib_name)\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n\n\nif (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv('TORCH_USE_RTLD_GLOBAL')) and \\\n        (_running_with_deploy() or platform.system() != 'Windows'):\n    # Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a\n    # few circumstances:\n    #\n    #   1. You're in a build environment (e.g., fbcode) where\n    #      libtorch_global_deps is not available, but you still need\n    #      to get mkl to link in with RTLD_GLOBAL or it will just\n    #      not work.\n    #\n    #   2. You're trying to run PyTorch under UBSAN and you need\n    #      to ensure that only one copy of libtorch is loaded, so\n    #      vptr checks work properly\n    #\n    # If you're using this setting, you must verify that all the libraries\n    # you load consistently use the same libstdc++, or you may have\n    # mysterious segfaults.\n    #\n    old_flags = sys.getdlopenflags()\n    sys.setdlopenflags(os.RTLD_GLOBAL | os.RTLD_LAZY)\n    from torch._C import *  # noqa: F403\n    sys.setdlopenflags(old_flags)\n    del old_flags\n\nelse:\n    # Easy way.  You want this most of the time, because it will prevent\n    # C++ symbols from libtorch clobbering C++ symbols from other\n    # libraries, leading to mysterious segfaults.\n    #\n    # If building in an environment where libtorch_global_deps isn't available\n    # like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will\n    # want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False\n    #\n    # See Note [Global dependencies]\n    if USE_GLOBAL_DEPS:\n        _load_global_deps()\n    from torch._C import *  # noqa: F403\n\n# Appease the type checker; ordinarily this binding is inserted by the\n# torch._C module initialization code in C\nif TYPE_CHECKING:\n    import torch._C as _C\n\nclass SymInt:\n    \"\"\"\n    Like an int (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return builtins.bool(self != 0)\n\n    def __int__(self):\n        return self.node.int_()\n\n    def __index__(self):\n        return self.node.int_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_float__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return str(self.node)\n\nclass SymFloat:\n    \"\"\"\n    Like an float (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_int__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\nclass SymBool:\n    \"\"\"\n    Like an bool (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n\n    Unlike regular bools, regular boolean operators will force extra guards instead\n    of symbolically evaluate.  Use the bitwise operators instead to handle this.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    def __int__(self):\n        return builtins.int(self.node.bool_())\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n    def __and__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __or__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    # We very carefully define __sym_not__, and not a number of other\n    # plausible alternatives:\n    #\n    #   - We do not override __not__ because this is not a real magic\n    #     method; you cannot override the meaning of the not builtin in\n    #     Python.  We use the name 'sym_not' to clarify that in user code you\n    #     cannot use the builtin not or operator.not_ or operator.__not__ and\n    #     hit this magic method; you must use our custom sym_not operator.\n    #\n    #   - We do not override the __invert__ method because SymBool is\n    #     meant to be usable in situations where bool is expected.  However,\n    #     bitwise negation ~a does the wrong thing with booleans (because\n    #     bool is a subclass of int, so ~1 = -2 which is not falseish.)\n    #     This would be a giant footgun, so we get around it by defining\n    #     our own operator.  Note that bitwise and/or do the right thing,\n    #     so we reuse the conventional operators there for readability.\n    #\n    def __sym_not__(self) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\ndef sym_not(a):\n    r\"\"\" SymInt-aware utility for logical negation.\n\n    Args:\n        a (SymBool or bool): Object to negate\n    \"\"\"\n    if hasattr(a, '__sym_not__'):\n        return a.__sym_not__()\n    return not a\n\ndef sym_float(a):\n    r\"\"\" SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymFloat):\n        return a\n    elif hasattr(a, '__sym_float__'):\n        return a.__sym_float__()\n    return py_float(a)  # type: ignore[operator]\n\n\ndef sym_int(a):\n    r\"\"\" SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymInt):\n        return a\n    elif isinstance(a, SymFloat):\n        return math.floor(a) if a >= 0 else math.ceil(a)  # type: ignore[arg-type, call-overload]\n    return py_int(a)  # type: ignore[operator]\n\ndef sym_max(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_max__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        # NB: If you actually care about preserving output type exactly\n        # if you do something like max(0, 0.0), it is NOT sound to treat\n        # min/max as commutative\n        return b.__sym_max__(a)\n    return builtins.max(a, b)  # type: ignore[operator]\n\ndef sym_min(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_min__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        return b.__sym_min__(a)\n    return builtins.min(a, b)  # type: ignore[operator]\n\n# Check to see if we can load C extensions, and if not provide some guidance\n# on what the problem might be.\ntry:\n    # _initExtension is chosen (arbitrarily) as a sentinel.\n    from torch._C import _initExtension\nexcept ImportError:\n    import torch._C as _C_for_compiled_check\n\n    # The __file__ check only works for Python 3.7 and above.\n    if _C_for_compiled_check.__file__ is None:\n        raise ImportError(textwrap.dedent('''\n            Failed to load PyTorch C extensions:\n                It appears that PyTorch has loaded the `torch/_C` folder\n                of the PyTorch repository rather than the C extensions which\n                are expected in the `torch._C` namespace. This can occur when\n                using the `install` workflow. e.g.\n                    $ python setup.py install && python -c \"import torch\"\n\n                This error can generally be solved using the `develop` workflow\n                    $ python setup.py develop && python -c \"import torch\"  # This should succeed\n                or by running Python from a different directory.\n            ''').strip()) from None\n    raise  # If __file__ is not None the cause is unknown, so just re-raise.\n\nfor name in dir(_C):\n    if name[0] != '_' and not name.endswith('Base'):\n        __all__.append(name)\n        obj = getattr(_C, name)\n        if (isinstance(obj, Callable) or inspect.isclass(obj)):  # type: ignore[arg-type]\n            if (obj.__module__ != 'torch'):\n                # TODO: fix their module from C++ side\n                if name not in ['DisableTorchFunctionSubclass', 'DisableTorchFunction', 'Generator']:\n                    obj.__module__ = 'torch'\n\nif not TYPE_CHECKING:\n    # issue 38137 and python issue 43367. Submodules of a C extension are\n    # non-standard, and attributes of those submodules cannot be pickled since\n    # pickle expect to be able to import them as \"from _C.sub import attr\"\n    # which fails with \"_C is not a package\n    for attr in dir(_C):\n        candidate = getattr(_C, attr)\n        if type(candidate) is type(_C):\n            # submodule\n            if f'torch._C.{attr}' not in sys.modules:\n                sys.modules[f'torch._C.{attr}'] = candidate\n\n\n################################################################################\n# Define basic utilities\n################################################################################\n\n\ndef typename(o):\n    if isinstance(o, torch.Tensor):\n        return o.type()\n\n    module = ''\n    class_name = ''\n    if hasattr(o, '__module__') and o.__module__ != 'builtins' \\\n            and o.__module__ != '__builtin__' and o.__module__ is not None:\n        module = o.__module__ + '.'\n\n    if hasattr(o, '__qualname__'):\n        class_name = o.__qualname__\n    elif hasattr(o, '__name__'):\n        class_name = o.__name__\n    else:\n        class_name = o.__class__.__name__\n\n    return module + class_name\n\n\ndef is_tensor(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch tensor.\n\n    Note that this function is simply doing ``isinstance(obj, Tensor)``.\n    Using that ``isinstance`` check is better for typechecking with mypy,\n    and more explicit - so it's recommended to use that instead of\n    ``is_tensor``.\n\n    Args:\n        obj (Object): Object to test\n    Example::\n\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.is_tensor(x)\n        True\n\n    \"\"\"\n    return isinstance(obj, torch.Tensor)\n\n\ndef is_storage(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch storage object.\n\n    Args:\n        obj (Object): Object to test\n    \"\"\"\n    return type(obj) in _storage_classes\n\n\n_GLOBAL_DEVICE_CONTEXT = None\n\ndef set_default_device(device):\n    \"\"\"Sets the default ``torch.Tensor`` to be allocated on ``device``.  This\n    does not affect factory function calls which are called with an explicit\n    ``device`` argument.  Factory calls will be performed as if they\n    were passed ``device`` as an argument.\n\n    To only temporarily change the default device instead of setting it\n    globally, use ``with torch.device(device):`` instead.\n\n    The default device is initially ``cpu``.  If you set the default tensor\n    device to another device (e.g., ``cuda``) without a device index, tensors\n    will be allocated on whatever the current device for the device type,\n    even after :func:`torch.cuda.set_device` is called.\n\n    .. warning::\n\n        This function imposes a slight performance cost on every Python\n        call to the torch API (not just factory functions).  If this\n        is causing problems for you, please comment on\n        https://github.com/pytorch/pytorch/issues/92701\n\n    Args:\n        device (device or string): the device to set as default\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"requires cuda, changes global state\")\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cpu')\n        >>> torch.set_default_device('cuda')  # current device is 0\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=0)\n        >>> torch.set_default_device('cuda:1')\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=1)\n\n    \"\"\"\n    global _GLOBAL_DEVICE_CONTEXT\n    if _GLOBAL_DEVICE_CONTEXT is not None:\n        _GLOBAL_DEVICE_CONTEXT.__exit__(None, None, None)\n    if device is None:\n        _GLOBAL_DEVICE_CONTEXT = None\n        return\n    from torch.utils._device import DeviceContext\n    _GLOBAL_DEVICE_CONTEXT = DeviceContext(device)\n    _GLOBAL_DEVICE_CONTEXT.__enter__()\n\n\ndef set_default_tensor_type(t):\n    r\"\"\"Sets the default ``torch.Tensor`` type to floating point tensor type\n    ``t``. This type will also be used as default floating point type for\n    type inference in :func:`torch.tensor`.\n\n    The default floating point tensor type is initially ``torch.FloatTensor``.\n\n    Args:\n        t (type or string): the floating point tensor type or its name\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\n        torch.float32\n        >>> torch.set_default_tensor_type(torch.DoubleTensor)\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n\n    \"\"\"\n    if isinstance(t, str):\n        t = _import_dotted_name(t)\n    _C._set_default_tensor_type(t)\n\n\ndef set_default_dtype(d):\n    r\"\"\"\n\n    Sets the default floating point dtype to :attr:`d`. Supports torch.float32\n    and torch.float64 as inputs. Other dtypes may be accepted without complaint\n    but are not supported and are unlikely to work as expected.\n\n    When PyTorch is initialized its default floating point dtype is torch.float32,\n    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like\n    type inference. The default floating point dtype is used to:\n\n    1. Implicitly determine the default complex dtype. When the default floating point\n       type is float32 the default complex dtype is complex64, and when the default\n       floating point type is float64 the default complex type is complex128.\n    2. Infer the dtype for tensors constructed using Python floats or complex Python\n       numbers. See examples below.\n    3. Determine the result of type promotion between bool and integer tensors and\n       Python floats and complex Python numbers.\n\n    Args:\n        d (:class:`torch.dtype`): the floating point dtype to make the default.\n                                  Either torch.float32 or torch.float64.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> # initial default for floating point is torch.float32\n        >>> # Python floats are interpreted as float32\n        >>> torch.tensor([1.2, 3]).dtype\n        torch.float32\n        >>> # initial default for floating point is torch.complex64\n        >>> # Complex Python numbers are interpreted as complex64\n        >>> torch.tensor([1.2, 3j]).dtype\n        torch.complex64\n\n        >>> torch.set_default_dtype(torch.float64)\n\n        >>> # Python floats are now interpreted as float64\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\n        torch.complex128\n\n    \"\"\"\n    _C._set_default_dtype(d)\n\ndef use_deterministic_algorithms(mode: builtins.bool, *, warn_only: builtins.bool = False) -> None:\n    r\"\"\" Sets whether PyTorch operations must use \"deterministic\"\n    algorithms. That is, algorithms which, given the same input, and when\n    run on the same software and hardware, always produce the same output.\n    When enabled, operations will use deterministic algorithms when available,\n    and if only nondeterministic algorithms are available they will throw a\n    :class:`RuntimeError` when called.\n\n    .. note:: This setting alone is not always enough to make an application\n        reproducible. Refer to :ref:`reproducibility` for more information.\n\n    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative\n        interface for this feature.\n\n    The following normally-nondeterministic operations will act\n    deterministically when ``mode=True``:\n\n        * :class:`torch.nn.Conv1d` when called on CUDA tensor\n        * :class:`torch.nn.Conv2d` when called on CUDA tensor\n        * :class:`torch.nn.Conv3d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor\n        * :func:`torch.bmm` when called on sparse-dense CUDA tensors\n        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor\n          and the index is a list of tensors\n        * :func:`torch.Tensor.index_put` with ``accumulate=False``\n        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor\n        * :func:`torch.gather` when called on a CUDA tensor that requires grad\n        * :func:`torch.index_add` when called on CUDA tensor\n        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor\n        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor\n        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor\n        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='sum'`` or ``reduce='mean'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_`, when called with a tensor that is not\n          quantized, sets new elements to a known value.  Floating point or\n          complex values are set to NaN. Integer values are set to the maximum\n          value.\n        * :func:`torch.empty`, :func:`torch.empty_like`, :func:`torch.empty_strided`,\n          and :func:`torch.empty_permuted` will fill the output tensor with a known\n          value. Floating point or complex dtype tensors are filled with NaN. Integer\n          dtype tensors are filled with the maximum value.\n\n    The following normally-nondeterministic operations will throw a\n    :class:`RuntimeError` when ``mode=True``:\n\n        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxUnpool1d`\n        * :class:`torch.nn.MaxUnpool2d`\n        * :class:`torch.nn.MaxUnpool3d`\n        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor\n          and one of the following modes is used:\n\n          - ``linear``\n          - ``bilinear``\n          - ``bicubic``\n          - ``trilinear``\n\n        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor\n        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when\n          ``mode='max'``\n        * :func:`torch.Tensor.put_` when ``accumulate=False``\n        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor\n        * :func:`torch.histc` when called on a CUDA tensor\n        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``\n          tensor is given\n        * :func:`torch.kthvalue` with called on a CUDA tensor\n        * :func:`torch.median` with indices output when called on a CUDA tensor\n        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor\n        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='prod'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_` when called with a quantized tensor\n\n    A handful of CUDA operations are nondeterministic if the CUDA version is\n    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``\n    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more\n    details: `<https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility>`_\n    If one of these environment variable configurations is not set, a :class:`RuntimeError`\n    will be raised from these operations when called with CUDA tensors:\n\n        * :func:`torch.mm`\n        * :func:`torch.mv`\n        * :func:`torch.bmm`\n\n    Note that deterministic operations tend to have worse performance than\n    nondeterministic operations.\n\n    .. note::\n\n        This flag does not detect or prevent nondeterministic behavior caused\n        by calling an inplace operation on a tensor with an internal memory\n        overlap or by giving such a tensor as the :attr:`out` argument for an\n        operation. In these cases, multiple writes of different data may target\n        a single memory location, and the order of writes is not guaranteed.\n\n    Args:\n        mode (:class:`bool`): If True, makes potentially nondeterministic\n            operations switch to a deterministic algorithm or throw a runtime\n            error. If False, allows nondeterministic operations.\n\n    Keyword args:\n        warn_only (:class:`bool`, optional): If True, operations that do not\n            have a deterministic implementation will throw a warning instead of\n            an error. Default: ``False``\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> torch.use_deterministic_algorithms(True)\n\n        # Forward mode nondeterministic error\n        >>> torch.randn(10, device='cuda').kthvalue(0)\n        ...\n        RuntimeError: kthvalue CUDA does not have a deterministic implementation...\n\n        # Backward mode nondeterministic error\n        >>> torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()\n        ...\n        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...\n    \"\"\"\n    _C._set_deterministic_algorithms(mode, warn_only=warn_only)\n\ndef are_deterministic_algorithms_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is turned on. Refer to\n    :func:`torch.use_deterministic_algorithms` documentation for more details.\n    \"\"\"\n    return _C._get_deterministic_algorithms()\n\ndef is_deterministic_algorithms_warn_only_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is set to warn only.\n    Refer to :func:`torch.use_deterministic_algorithms` documentation for more\n    details.\n    \"\"\"\n    return _C._get_deterministic_algorithms_warn_only()\n\ndef set_deterministic_debug_mode(debug_mode: Union[builtins.int, str]) -> None:\n    r\"\"\"Sets the debug mode for deterministic operations.\n\n    .. note:: This is an alternative interface for\n        :func:`torch.use_deterministic_algorithms`. Refer to that function's\n        documentation for details about affected operations.\n\n    Args:\n        debug_mode(str or int): If \"default\" or 0, don't error or warn on\n            nondeterministic operations. If \"warn\" or 1, warn on\n            nondeterministic operations. If \"error\" or 2, error on\n            nondeterministic operations.\n    \"\"\"\n\n    # NOTE: builtins.int is used here because int in this scope resolves\n    # to torch.int\n    if not isinstance(debug_mode, (builtins.int, str)):\n        raise TypeError(f'debug_mode must be str or int, but got {type(debug_mode)}')\n\n    if isinstance(debug_mode, str):\n        if debug_mode == 'default':\n            debug_mode = 0\n        elif debug_mode == 'warn':\n            debug_mode = 1\n        elif debug_mode == 'error':\n            debug_mode = 2\n        else:\n            raise RuntimeError(\n                'invalid value of debug_mode, expected one of `default`, '\n                f'`warn`, `error`, but got {debug_mode}')\n\n    if debug_mode == 0:\n        _C._set_deterministic_algorithms(False)\n    elif debug_mode == 1:\n        _C._set_deterministic_algorithms(True, warn_only=True)\n    elif debug_mode == 2:\n        _C._set_deterministic_algorithms(True)\n    else:\n        raise RuntimeError(\n            'invalid value of debug_mode, expected 0, 1, or 2, '\n            f'but got {debug_mode}')\n\ndef get_deterministic_debug_mode() -> builtins.int:\n    r\"\"\"Returns the current value of the debug mode for deterministic\n    operations. Refer to :func:`torch.set_deterministic_debug_mode`\n    documentation for more details.\n    \"\"\"\n\n    if _C._get_deterministic_algorithms():\n        if _C._get_deterministic_algorithms_warn_only():\n            return 1\n        else:\n            return 2\n    else:\n        return 0\n\ndef get_float32_matmul_precision() -> builtins.str:\n    r\"\"\"Returns the current value of float32 matrix multiplication precision. Refer to\n    :func:`torch.set_float32_matmul_precision` documentation for more details.\n    \"\"\"\n    return _C._get_float32_matmul_precision()\n\ndef set_float32_matmul_precision(precision: str) -> None:\n    r\"\"\"Sets the internal precision of float32 matrix multiplications.\n\n    Running float32 matrix multiplications in lower precision may significantly increase\n    performance, and in some programs the loss of precision has a negligible impact.\n\n    Supports three settings:\n\n        * \"highest\", float32 matrix multiplications use the float32 datatype (24 mantissa\n          bits) for internal computations.\n        * \"high\", float32 matrix multiplications either use the TensorFloat32 datatype (10\n          mantissa bits) or treat each float32 number as the sum of two bfloat16 numbers\n          (approximately 16 mantissa bits), if the appropriate fast matrix multiplication\n          algorithms are available.  Otherwise float32 matrix multiplications are computed\n          as if the precision is \"highest\".  See below for more information on the bfloat16\n          approach.\n        * \"medium\", float32 matrix multiplications use the bfloat16 datatype (8 mantissa\n          bits) for internal computations, if a fast matrix multiplication algorithm\n          using that datatype internally is available. Otherwise float32\n          matrix multiplications are computed as if the precision is \"high\".\n\n    When using \"high\" precision, float32 multiplications may use a bfloat16-based algorithm\n    that is more complicated than simply truncating to some smaller number mantissa bits\n    (e.g. 10 for TensorFloat32, 8 for bfloat16).  Refer to [Henry2019]_ for a complete\n    description of this algorithm.  To briefly explain here, the first step is to realize\n    that we can perfectly encode a single float32 number as the sum of three bfloat16\n    numbers (because float32 has 24 mantissa bits while bfloat16 has 8, and both have the\n    same number of exponent bits).  This means that the product of two float32 numbers can\n    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade\n    accuracy for speed by dropping some of these products.  The \"high\" precision algorithm\n    specifically keeps only the three most significant products, which conveniently excludes\n    all of the products involving the last 8 mantissa bits of either input.  This means that\n    we can represent our inputs as the sum of two bfloat16 numbers rather than three.\n    Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than\n    float32 ones, it's faster to do three multiplications and 2 additions with bfloat16\n    precision than it is to do a single multiplication with float32 precision.\n\n    .. [Henry2019] http://arxiv.org/abs/1904.06376\n\n    .. note::\n\n        This does not change the output dtype of float32 matrix multiplications,\n        it controls how the internal computation of the matrix multiplication is performed.\n\n    .. note::\n\n        This does not change the precision of convolution operations. Other flags,\n        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution\n        operations.\n\n    .. note::\n\n        This flag currently only affects one native device type: CUDA.\n        If \"high\" or \"medium\" are set then the TensorFloat32 datatype will be used\n        when computing float32 matrix multiplications, equivalent to setting\n        `torch.backends.cuda.matmul.allow_tf32 = True`. When \"highest\" (the default)\n        is set then the float32 datatype is used for internal computations, equivalent\n        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.\n\n    Args:\n        precision(str): can be set to \"highest\" (default), \"high\", or \"medium\" (see above).\n\n    \"\"\"\n    _C._set_float32_matmul_precision(precision)\n\ndef set_warn_always(b: builtins.bool) -> None:\n    r\"\"\"When this flag is False (default) then some PyTorch warnings may only\n    appear once per process. This helps avoid excessive warning information.\n    Setting it to True causes these warnings to always appear, which may be\n    helpful when debugging.\n\n    Args:\n        b (:class:`bool`): If True, force warnings to always be emitted\n                           If False, set to the default behaviour\n    \"\"\"\n    _C._set_warnAlways(b)\n\ndef is_warn_always_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global warn_always flag is turned on. Refer to\n    :func:`torch.set_warn_always` documentation for more details.\n    \"\"\"\n    return _C._get_warnAlways()\n\n################################################################################\n# Define error checking functions\n################################################################################\n\n# These error checking functions must be kept consistent with their C++\n# equivalents. Their C++ equivalents are mentioned where applicable.\n\ndef _check_with(error_type, cond: Union[builtins.bool, SymBool], message: Callable[[], str]):\n    if not isinstance(cond, (builtins.bool, torch.SymBool)):\n        raise TypeError(f'cond must be a bool, but got {type(cond)}')\n\n    if torch.fx.experimental.symbolic_shapes.expect_true(cond):\n        return\n\n    # error_type must be a subclass of Exception and not subclass of Warning\n    assert issubclass(error_type, Exception) and not issubclass(error_type, Warning)\n\n    if message is None:\n        message_evaluated = (\n            'Expected cond to be True, but got False. (Could this error '\n            'message be improved? If so, please report an enhancement request '\n            'to PyTorch.)')\n\n    else:\n        if not callable(message):\n            raise TypeError('message must be a callable')\n\n        message_evaluated = str(message())\n\n    raise error_type(message_evaluated)\n\ndef _check(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(RuntimeError, cond, message)\n\ndef _check_index(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``IndexError``\n\n    C++ equivalent: ``TORCH_CHECK_INDEX``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(IndexError, cond, message)\n\ndef _check_value(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``ValueError``\n\n    C++ equivalent: ``TORCH_CHECK_VALUE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(ValueError, cond, message)\n\ndef _check_type(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``TypeError``\n\n    C++ equivalent: ``TORCH_CHECK_TYPE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(TypeError, cond, message)\n\ndef _check_not_implemented(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``NotImplementedError``\n\n    C++ equivalent: ``TORCH_CHECK_NOT_IMPLEMENTED``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(NotImplementedError, cond, message)\n\ndef _check_tensor_all_with(error_type, cond, message=None):\n    if not torch.is_tensor(cond):\n        raise TypeError(f'cond must be a tensor, but got {type(cond)}')\n\n    if not cond.dtype == torch.bool:\n        raise TypeError(\n            f'cond tensor must have dtype torch.bool, but got {cond.dtype}')\n\n    _check_with(error_type, cond._is_all_true().item(), message)\n\n# C++ equivalent: `TORCH_CHECK_TENSOR_ALL`\ndef _check_tensor_all(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK_TENSOR_ALL``\n\n    Args:\n        cond (:class:`torch.Tensor`): Tensor of dtype ``torch.bool``. If any\n            element is ``False``, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_tensor_all_with(RuntimeError, cond, message)\n\n################################################################################\n# Define numeric constants\n################################################################################\n\n# For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and\n# NumPy consistency (https://numpy.org/devdocs/reference/constants.html)\nfrom math import e , nan , inf , pi\n__all__.extend(['e', 'pi', 'nan', 'inf'])\n\n################################################################################\n# Define Storage and Tensor classes\n################################################################################\n\nfrom ._tensor import Tensor\nfrom .storage import _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n\n# NOTE: New <type>Storage classes should never be added. When adding a new\n# dtype, use torch.storage.TypedStorage directly.\n\nclass ByteStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.uint8\n\nclass DoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.double\n\nclass FloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.float\n\nclass HalfStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.half\n\nclass LongStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.long\n\nclass IntStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int\n\nclass ShortStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.short\n\nclass CharStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int8\n\nclass BoolStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bool\n\nclass BFloat16Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bfloat16\n\nclass ComplexDoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cdouble\n\nclass ComplexFloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cfloat\n\nclass QUInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint8\n\nclass QInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint8\n\nclass QInt32Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint32\n\nclass QUInt4x2Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint4x2\n\nclass QUInt2x4Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint2x4\n\n_storage_classes = {\n    UntypedStorage, DoubleStorage, FloatStorage, LongStorage, IntStorage,\n    ShortStorage, CharStorage, ByteStorage, HalfStorage, BoolStorage,\n    QUInt8Storage, QInt8Storage, QInt32Storage, BFloat16Storage,\n    ComplexFloatStorage, ComplexDoubleStorage, QUInt4x2Storage, QUInt2x4Storage,\n    TypedStorage\n}\n\n# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()\n_tensor_classes: Set[Type] = set()\n\n# If you edit these imports, please update torch/__init__.py.in as well\nfrom .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed\nfrom .serialization import save, load\nfrom ._tensor_str import set_printoptions\n\n################################################################################\n# Initialize extension\n################################################################################\n\ndef manager_path():\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return b\"\"\n    path = get_file_path('torch', 'bin', 'torch_shm_manager')\n    prepare_multiprocessing_environment(get_file_path('torch'))\n    if not os.path.exists(path):\n        raise RuntimeError(\"Unable to find torch_shm_manager at \" + path)\n    return path.encode('utf-8')\n\nfrom torch.amp import autocast\n\n# Initializing the extension shadows the built-in python float / int classes;\n# store them for later use by SymInt / SymFloat.\npy_float = float\npy_int = int\n\n# Shared memory manager needs to know the exact location of manager executable\n_C._initExtension(manager_path())\ndel manager_path\n\n# Appease the type checker: it can't deal with direct setting of globals().\n# Note that we will see \"too many\" functions when reexporting this way; there\n# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\n# so that this import is good enough\nif TYPE_CHECKING:\n    # Some type signatures pulled in from _VariableFunctions here clash with\n    # signatures already imported. For now these clashes are ignored; see\n    # PR #43339 for details.\n    from torch._C._VariableFunctions import *  # type: ignore[assignment, misc] # noqa: F403\n    # Fixup segment_reduce visibility\n    _segment_reduce = segment_reduce\n    del segment_reduce\n\n# Ops not to be exposed in `torch` namespace,\n# mostly helper ops.\nPRIVATE_OPS = (\n    'unique_dim',\n)\n\nfor name in dir(_C._VariableFunctions):\n    if name.startswith('__') or name in PRIVATE_OPS:\n        continue\n    obj = getattr(_C._VariableFunctions, name)\n    obj.__module__ = 'torch'\n    # Hide some APIs that should not be public\n    if name == \"segment_reduce\":\n        # TODO: Once the undocumented FC window is passed, remove the line bellow\n        globals()[name] = obj\n        name = \"_\" + name\n    globals()[name] = obj\n    if not name.startswith(\"_\"):\n        __all__.append(name)\n\n\n\n################################################################################\n# Import TorchDynamo's lazy APIs to avoid circular dependenices\n################################################################################\n\n# needs to be before from .functional import * to avoid circular dependencies\nfrom ._compile import _disable_dynamo\n\n################################################################################\n# Import interface functions defined in Python\n################################################################################\n\n# needs to be after the above ATen bindings so we can overwrite from Python side\nfrom .functional import *  # noqa: F403\n\n\n################################################################################\n# Remove unnecessary members\n################################################################################\n\ndel _StorageBase\ndel _LegacyStorage\n\n################################################################################\n# Define _assert\n################################################################################\n\n# needs to be before the submodule imports to avoid circular dependencies\ndef _assert(condition, message):\n    r\"\"\"A wrapper around Python's assert which is symbolically traceable.\n    \"\"\"\n    from .overrides import has_torch_function, handle_torch_function\n\n    if type(condition) is not torch.Tensor and has_torch_function((condition,)):\n        return handle_torch_function(_assert, (condition,), condition, message)\n    assert condition, message\n\n################################################################################\n# Import most common subpackages\n################################################################################\n\n# Use the redundant form so that type checkers know that these are a part of\n# the public API. The \"regular\" import lines are there solely for the runtime\n# side effect of adding to the imported module's members for other users.\nfrom torch import cuda as cuda\nfrom torch import cpu as cpu\nfrom torch import mps as mps\nfrom torch import autograd as autograd\nfrom torch.autograd import (\n    no_grad as no_grad,\n    enable_grad as enable_grad,\n    set_grad_enabled as set_grad_enabled,\n    inference_mode as inference_mode,\n)\nfrom torch import fft as fft\nfrom torch import futures as futures\nfrom torch import _awaits as _awaits\nfrom torch import nested as nested\nfrom torch import nn as nn\nfrom torch.signal import windows as windows\nfrom torch import optim as optim\nimport torch.optim._multi_tensor\nfrom torch import multiprocessing as multiprocessing\nfrom torch import sparse as sparse\nfrom torch import special as special\nimport torch.utils.backcompat\nfrom torch import jit as jit\nfrom torch import linalg as linalg\nfrom torch import hub as hub\nfrom torch import random as random\nfrom torch import distributions as distributions\nfrom torch import testing as testing\nfrom torch import backends as backends\nimport torch.utils.data\nfrom torch import __config__ as __config__\nfrom torch import __future__ as __future__\nfrom torch import profiler as profiler\n\n# Quantized, sparse, AO, etc. should be last to get imported, as nothing\n# is expected to depend on them.\nfrom torch import ao as ao\n# nn.quant* depends on ao -- so should be after those.\nimport torch.nn.quantizable\nimport torch.nn.quantized\nimport torch.nn.qat\nimport torch.nn.intrinsic\n\n_C._init_names(list(torch._storage_classes))\n\n# attach docstrings to torch and tensor functions\nfrom . import _torch_docs, _tensor_docs, _storage_docs\ndel _torch_docs, _tensor_docs, _storage_docs\n\n\ndef compiled_with_cxx11_abi() -> builtins.bool:\n    r\"\"\"Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\"\"\"\n    return _C._GLIBCXX_USE_CXX11_ABI\n\n\n# Import the ops \"namespace\"\nfrom torch._ops import ops\nfrom torch._classes import classes\n\n# quantization depends on torch.fx\n# Import quantization\nfrom torch import quantization as quantization\n\n# Import the quasi random sampler\nfrom torch import quasirandom as quasirandom\n\n# If you are seeing this, it means that this call site was not checked if\n# the memory format could be preserved, and it was switched to old default\n# behaviour of contiguous\nlegacy_contiguous_format = contiguous_format\n\n# Register fork handler to initialize OpenMP in child processes (see gh-28389)\nfrom torch.multiprocessing._atfork import register_after_fork\nregister_after_fork(torch.get_num_threads)\ndel register_after_fork\n\n# Import tools that require fully imported torch (for applying\n# torch.jit.script as a decorator, for instance):\nfrom ._lobpcg import lobpcg as lobpcg\n\n# These were previously defined in native_functions.yaml and appeared on the\n# `torch` namespace, but we moved them to c10 dispatch to facilitate custom\n# class usage. We add these lines here to preserve backward compatibility.\nquantized_lstm = torch.ops.aten.quantized_lstm\nquantized_gru = torch.ops.aten.quantized_gru\n\nfrom torch.utils.dlpack import from_dlpack, to_dlpack\n\n# Import experimental masked operations support. See\n# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\n# information.\nfrom . import masked\n\n# Import removed ops with error message about removal\nfrom ._linalg_utils import (  # type: ignore[misc]\n    matrix_rank,\n    eig,\n    solve,\n    lstsq,\n)\nfrom ._linalg_utils import _symeig as symeig  # type: ignore[misc]\n\nclass _TorchCompileInductorWrapper:\n    compiler_name = \"inductor\"\n\n    def __init__(self, mode, options, dynamic):\n        self.config: Dict[str, Any] = dict()\n        self.dynamic = dynamic\n        self.apply_mode(mode)\n        self.apply_options(options)\n\n        # FIXME: CUPTI Lazy Re-init and CUDA Graph crashes with CUDA 11.\n        if self.config.get(\"triton.cudagraphs\", False):\n            os.environ[\"DISABLE_CUPTI_LAZY_REINIT\"] = \"1\"\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileInductorWrapper) and\n                self.config == other.config and\n                self.dynamic == other.dynamic)\n\n    def apply_mode(self, mode: Optional[str]):\n        if mode is None or mode == \"default\":\n            pass\n        elif mode in (\"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"):\n            from torch._inductor import list_mode_options\n            self.apply_options(list_mode_options(mode, self.dynamic))\n        else:\n            raise RuntimeError(\n                f\"Unrecognized mode={mode}, should be one of: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs\"\n            )\n\n    def apply_options(self, options: Optional[Dict[str, Any]]):\n        if not options:\n            return\n\n        from torch._inductor import config\n        current_config: Dict[str, Any] = config.to_dict()  # type: ignore[attr-defined]\n\n        for key, val in options.items():\n            attr_name = key.replace(\"-\", \"_\")\n            if attr_name not in current_config:\n                raise RuntimeError(\n                    f\"Unexpected optimization option {key}, known options are {list(current_config.keys())}\"\n                )\n            if type(val) is not type(current_config[attr_name]):\n                val_type_str = type(val).__name__\n                expected_type_str = type(current_config[attr_name]).__name__\n                raise RuntimeError(\n                    f\"Unexpected type of attr {key}, got {val_type_str} should be {expected_type_str}\"\n                )\n            self.config[attr_name] = val\n\n    def __call__(self, model_, inputs_):\n        from torch._inductor.compile_fx import compile_fx\n\n        return compile_fx(model_, inputs_, config_patches=self.config)\n\n    def get_compiler_config(self):\n        from torch._inductor.compile_fx import get_patched_config_dict\n        return get_patched_config_dict(config_patches=self.config)\n\n    def reset(self):\n        from torch._inductor import config\n        if \"triton.cudagraphs\" in self.config or config.triton.cudagraphs:\n            if self.config.get(\"triton.cudagraphs\", True):\n                from torch._inductor.cudagraph_trees import reset_cudagraph_trees\n                reset_cudagraph_trees()\n\nclass _TorchCompileWrapper:\n    def __init__(self, backend, mode, options, dynamic):\n        from torch._dynamo.backends.registry import lookup_backend\n\n        if isinstance(backend, str):\n            self.compiler_name = backend\n        elif hasattr(backend, \"__name__\"):\n            self.compiler_name = backend.__name__\n        else:\n            self.compiler_name = str(backend)\n        self.dynamic = dynamic\n        self.compiler_fn = lookup_backend(backend)\n        self.kwargs = {}\n        # only pass the args if they non-empty\n        if mode and mode != \"default\":\n            self.kwargs[\"mode\"] = mode\n        if options:\n            self.kwargs[\"options\"] = options\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileWrapper) and\n                self.compiler_fn == other.compiler_fn and\n                self.kwargs == other.kwargs and\n                self.dynamic == other.dynamic)\n\n    def __call__(self, model_, inputs_):\n        return self.compiler_fn(model_, inputs_, **self.kwargs)\n\n\ndef compile(model: Optional[Callable] = None, *,\n            fullgraph: builtins.bool = False,\n            dynamic: Optional[builtins.bool] = None,\n            backend: Union[str, Callable] = \"inductor\",\n            mode: Union[str, None] = None,\n            options: Optional[Dict[str, Union[str, builtins.int, builtins.bool]]] = None,\n            disable: builtins.bool = False) -> Callable:\n    \"\"\"\n    Optimizes given model/function using TorchDynamo and specified backend.\n\n    Concretely, for every frame executed within the compiled region, we will attempt\n    to compile it and cache the compiled result on the code object for future\n    use.  A single frame may be compiled multiple times if previous compiled\n    results are not applicable for subsequent calls (this is called a \"guard\n    failure), you can use TORCH_LOGS=guards to debug these situations.\n    Multiple compiled results can be associated with a frame up to\n    ``torch._dynamo.config.cache_size_limit``, which defaults to 64; at which\n    point we will fall back to eager.  Note that compile caches are per\n    *code object*, not frame; if you dynamically create multiple copies of a\n    function, they will all share the same code cache.\n\n    Args:\n       model (Callable): Module/function to optimize\n       fullgraph (bool): Whether it is ok to break model into several subgraphs\n       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\n        to generate a kernel that is as dynamic as possible to avoid recompilations when\n        sizes change.  This may not always work as some operations/optimizations will\n        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\n        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\n        By default (None), we automatically detect if dynamism has occurred and compile a more\n        dynamic kernel upon recompile.\n       backend (str or Callable): backend to be used\n\n        - \"inductor\" is the default backend, which is a good balance between performance and overhead\n\n        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\n\n        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\n\n        - To register an out-of-tree custom backend: https://pytorch.org/docs/main/compile/custom-backends.html\n       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\n\n        - \"default\" is the default mode, which is a good balance between performance and overhead\n\n        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\n          useful for small batches.  Reduction of overhead can come at the cost of more memory\n          usage, as we will cache the workspace memory required for the invocation so that we\n          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\n          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\n          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\n          to debug.\n\n        - \"max-autotune\" is a mode that leverages Triton based matrix multiplications and convolutions\n          It enables CUDA graphs by default.\n\n        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\n\n        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\n\n       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\n\n        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\n\n        - `max_autotune` which will profile to pick the best matmul configuration\n\n        - `fallback_random` which is useful when debugging accuracy issues\n\n        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\n\n        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\n\n        - `trace.enabled` which is the most useful debugging flag to turn on\n\n        - `trace.graph_diagram` which will show you a picture of your graph after fusion\n\n        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\n       disable (bool): Turn torch.compile() into a no-op for testing\n\n    Example::\n\n        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n        def foo(x):\n            return torch.sin(x) + torch.cos(x)\n\n    \"\"\"\n    _C._log_api_usage_once(\"torch.compile\")\n    # Temporary until we get proper support for python 3.12\n    if sys.version_info >= (3, 12):\n        raise RuntimeError(\"Dynamo is not supported on Python 3.12+\")\n\n    # Decorator mode\n    if model is None:\n        def fn(model: Callable):\n            if model is None:\n                raise RuntimeError(\"Model can't be None\")\n            return compile(model,\n                           fullgraph=fullgraph,\n                           dynamic=dynamic,\n                           backend=backend,\n                           mode=mode,\n                           options=options,\n                           disable=disable)\n        return fn\n\n    if mode is not None and options is not None:\n        raise RuntimeError(\"Either mode or options can be specified, but both can't be specified at the same time.\")\n    if mode is None and options is None:\n        mode = \"default\"\n    if backend == \"inductor\":\n        backend = _TorchCompileInductorWrapper(mode, options, dynamic)\n    else:\n        backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n\n    return torch._dynamo.optimize(backend=backend, nopython=fullgraph, dynamic=dynamic, disable=disable)(model)\n\n\nfrom torch import export as export\n\n\ndef _register_device_module(device_type, module):\n    r\"\"\"Register an external runtime module of the specific :attr:`device_type`\n    supported by torch.\n\n    After the :attr:`module` is registered correctly, the user can refer\n    the external runtime module as part of torch with attribute torch.xxx.\n    \"\"\"\n    # Make sure the device_type represent a supported device type for torch.\n    device_type = torch.device(device_type).type\n    m = sys.modules[__name__]\n    if hasattr(m, device_type):\n        raise RuntimeError(f\"The runtime module of '{device_type}' has already \"\n                           f\"been registered with '{getattr(m, device_type)}'\")\n    setattr(m, device_type, module)\n    torch_module_name = '.'.join([__name__, device_type])\n    sys.modules[torch_module_name] = module\n\n# expose return_types\nfrom . import return_types\nfrom . import library\nif not TYPE_CHECKING:\n    from . import _meta_registrations\n\n# Enable CUDA Sanitizer\nif 'TORCH_CUDA_SANITIZER' in os.environ:\n    import torch.cuda._sanitizer as csan\n\n    csan.enable_cuda_sanitizer()\n\n# Populate magic methods on SymInt and SymFloat\nimport torch.fx.experimental.symbolic_shapes\n\nfrom torch import func as func\nfrom torch.func import vmap\n\n\n# The function _sparse_coo_tensor_unsafe is removed from PyTorch\n# Python API (v. 1.13), here we temporarily provide its replacement\n# with a deprecation warning.\n# TODO: remove the function for PyTorch v 1.15.\ndef _sparse_coo_tensor_unsafe(*args, **kwargs):\n    import warnings\n    warnings.warn('torch._sparse_coo_tensor_unsafe is deprecated, '\n                  'use torch.sparse_coo_tensor(..., check_invariants=False) instead.')\n    kwargs['check_invariants'] = False\n    return torch.sparse_coo_tensor(*args, **kwargs)\n\n# Register MPS specific decomps\ntorch.backends.mps._init()\n\nif not _running_with_deploy():\n    from torch import compiler as compiler\n\n    class _TritonLibrary:\n        lib = torch.library.Library(\"triton\", \"DEF\")\n        ops_table: Dict[Tuple[str, str], Callable] = {}\n\n        @classmethod\n        def registerOp(cls, op_key, full_schema, op_impl, dispatch_key):\n            if (op_key, dispatch_key) not in cls.ops_table:\n                cls.lib.define(full_schema)\n                cls.lib.impl(\"triton::\" + op_key, op_impl, dispatch_key)\n                cls.ops_table[(op_key, dispatch_key)] = op_impl\n\n            return cls.ops_table[(op_key, dispatch_key)]\n\n\n# Deprecated attributes\n_deprecated_attrs = {\n    \"has_mps\": torch.backends.mps.is_built,\n    \"has_cuda\": torch.backends.cuda.is_built,\n    \"has_cudnn\": torch.backends.cudnn.is_available,\n    \"has_mkldnn\": torch.backends.mkldnn.is_available,\n}\n\nif TYPE_CHECKING:\n    # Import the following modules during type checking to enable code intelligence features,\n    # such as auto-completion in tools like pylance, even when these modules are not explicitly\n    # imported in user code.\n    from torch import _dynamo as _dynamo\n    from torch import _inductor as _inductor\n    from torch import onnx as onnx\n\n_lazy_modules = {\n    \"_dynamo\",\n    \"_inductor\",\n    \"_export\",\n    # ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit\n    \"onnx\",\n}\n\ndef __getattr__(name):\n    # Deprecated attrs\n    replacement = _deprecated_attrs.get(name)\n    if replacement is not None:\n        import warnings\n        warnings.warn(f\"'{name}' is deprecated, please use '{replacement.__module__}.{replacement.__name__}()'\", stacklevel=2)\n        return replacement()\n\n    # Lazy modules\n    if name in _lazy_modules:\n        import importlib\n        return importlib.import_module(f\".{name}\", __name__)\n\n    raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")\n\nfrom . import _logging\n_logging._init_logs()\n",
      "torch/_appdirs.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (c) 2005-2010 ActiveState Software Inc.\n# Copyright (c) 2013 Eddy Petri\u0219or\n\n# flake8: noqa\n\n\"\"\"\nThis file is directly from\nhttps://github.com/ActiveState/appdirs/blob/3fe6a83776843a46f20c2e5587afcffe05e03b39/appdirs.py\n\nThe license of https://github.com/ActiveState/appdirs copied below:\n\n\n# This is the MIT license\n\nCopyright (c) 2010 ActiveState Software Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\n\"\"\"Utilities for determining application-specific dirs.\n\nSee <https://github.com/ActiveState/appdirs> for details and usage.\n\"\"\"\n# Dev Notes:\n# - MSDN on where to store app data files:\n#   http://support.microsoft.com/default.aspx?scid=kb;en-us;310294#XSLTH3194121123120121120120\n# - Mac OS X: http://developer.apple.com/documentation/MacOSX/Conceptual/BPFileSystem/index.html\n# - XDG spec for Un*x: https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html\n\n__version__ = \"1.4.4\"\n__version_info__ = tuple(int(segment) for segment in __version__.split(\".\"))\n\n\nimport os\nimport sys\n\nunicode = str\n\nif sys.platform.startswith(\"java\"):\n    import platform\n\n    os_name = platform.java_ver()[3][0]\n    if os_name.startswith(\"Windows\"):  # \"Windows XP\", \"Windows 7\", etc.\n        system = \"win32\"\n    elif os_name.startswith(\"Mac\"):  # \"Mac OS X\", etc.\n        system = \"darwin\"\n    else:  # \"Linux\", \"SunOS\", \"FreeBSD\", etc.\n        # Setting this to \"linux2\" is not ideal, but only Windows or Mac\n        # are actually checked for and the rest of the module expects\n        # *sys.platform* style strings.\n        system = \"linux2\"\nelse:\n    system = sys.platform\n\n\ndef user_data_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user data directories are:\n        Mac OS X:               ~/Library/Application Support/<AppName>\n        Unix:                   ~/.local/share/<AppName>    # or in $XDG_DATA_HOME, if defined\n        Win XP (not roaming):   C:\\Documents and Settings\\<username>\\Application Data\\<AppAuthor>\\<AppName>\n        Win XP (roaming):       C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\n        Win 7  (not roaming):   C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\n        Win 7  (roaming):       C:\\Users\\<username>\\AppData\\Roaming\\<AppAuthor>\\<AppName>\n\n    For Unix, we follow the XDG spec and support $XDG_DATA_HOME.\n    That means, by default \"~/.local/share/<AppName>\".\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        const = roaming and \"CSIDL_APPDATA\" or \"CSIDL_LOCAL_APPDATA\"\n        path = os.path.normpath(_get_win_folder(const))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Application Support/\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_DATA_HOME\", os.path.expanduser(\"~/.local/share\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef site_data_dir(appname=None, appauthor=None, version=None, multipath=False):\n    r\"\"\"Return full path to the user-shared data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"multipath\" is an optional parameter only applicable to *nix\n            which indicates that the entire list of data dirs should be\n            returned. By default, the first item from XDG_DATA_DIRS is\n            returned, or '/usr/local/share/<AppName>',\n            if XDG_DATA_DIRS is not set\n\n    Typical site data directories are:\n        Mac OS X:   /Library/Application Support/<AppName>\n        Unix:       /usr/local/share/<AppName> or /usr/share/<AppName>\n        Win XP:     C:\\Documents and Settings\\All Users\\Application Data\\<AppAuthor>\\<AppName>\n        Vista:      (Fail! \"C:\\ProgramData\" is a hidden *system* directory on Vista.)\n        Win 7:      C:\\ProgramData\\<AppAuthor>\\<AppName>   # Hidden, but writeable on Win 7.\n\n    For Unix, this is using the $XDG_DATA_DIRS[0] default.\n\n    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        path = os.path.normpath(_get_win_folder(\"CSIDL_COMMON_APPDATA\"))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"/Library/Application Support\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        # XDG default for $XDG_DATA_DIRS\n        # only first, if multipath is False\n        path = os.getenv(\n            \"XDG_DATA_DIRS\", os.pathsep.join([\"/usr/local/share\", \"/usr/share\"])\n        )\n        pathlist = [\n            os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)\n        ]\n        if appname:\n            if version:\n                appname = os.path.join(appname, version)\n            pathlist = [os.sep.join([x, appname]) for x in pathlist]\n\n        if multipath:\n            path = os.pathsep.join(pathlist)\n        else:\n            path = pathlist[0]\n        return path\n\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_config_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific config dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user config directories are:\n        Mac OS X:               ~/Library/Preferences/<AppName>\n        Unix:                   ~/.config/<AppName>     # or in $XDG_CONFIG_HOME, if defined\n        Win *:                  same as user_data_dir\n\n    For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME.\n    That means, by default \"~/.config/<AppName>\".\n    \"\"\"\n    if system == \"win32\":\n        path = user_data_dir(appname, appauthor, None, roaming)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Preferences/\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_CONFIG_HOME\", os.path.expanduser(\"~/.config\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef site_config_dir(appname=None, appauthor=None, version=None, multipath=False):\n    r\"\"\"Return full path to the user-shared data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"multipath\" is an optional parameter only applicable to *nix\n            which indicates that the entire list of config dirs should be\n            returned. By default, the first item from XDG_CONFIG_DIRS is\n            returned, or '/etc/xdg/<AppName>', if XDG_CONFIG_DIRS is not set\n\n    Typical site config directories are:\n        Mac OS X:   same as site_data_dir\n        Unix:       /etc/xdg/<AppName> or $XDG_CONFIG_DIRS[i]/<AppName> for each value in\n                    $XDG_CONFIG_DIRS\n        Win *:      same as site_data_dir\n        Vista:      (Fail! \"C:\\ProgramData\" is a hidden *system* directory on Vista.)\n\n    For Unix, this is using the $XDG_CONFIG_DIRS[0] default, if multipath=False\n\n    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.\n    \"\"\"\n    if system == \"win32\":\n        path = site_data_dir(appname, appauthor)\n        if appname and version:\n            path = os.path.join(path, version)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"/Library/Preferences\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        # XDG default for $XDG_CONFIG_DIRS\n        # only first, if multipath is False\n        path = os.getenv(\"XDG_CONFIG_DIRS\", \"/etc/xdg\")\n        pathlist = [\n            os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)\n        ]\n        if appname:\n            if version:\n                appname = os.path.join(appname, version)\n            pathlist = [os.sep.join([x, appname]) for x in pathlist]\n\n        if multipath:\n            path = os.pathsep.join(pathlist)\n        else:\n            path = pathlist[0]\n    return path\n\n\ndef user_cache_dir(appname=None, appauthor=None, version=None, opinion=True):\n    r\"\"\"Return full path to the user-specific cache dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"opinion\" (boolean) can be False to disable the appending of\n            \"Cache\" to the base app data dir for Windows. See\n            discussion below.\n\n    Typical user cache directories are:\n        Mac OS X:   ~/Library/Caches/<AppName>\n        Unix:       ~/.cache/<AppName> (XDG default)\n        Win XP:     C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\\Cache\n        Vista:      C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\\Cache\n\n    On Windows the only suggestion in the MSDN docs is that local settings go in\n    the `CSIDL_LOCAL_APPDATA` directory. This is identical to the non-roaming\n    app data dir (the default returned by `user_data_dir` above). Apps typically\n    put cache data somewhere *under* the given dir here. Some examples:\n        ...\\Mozilla\\Firefox\\Profiles\\<ProfileName>\\Cache\n        ...\\Acme\\SuperApp\\Cache\\1.0\n    OPINION: This function appends \"Cache\" to the `CSIDL_LOCAL_APPDATA` value.\n    This can be disabled with the `opinion=False` option.\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        path = os.path.normpath(_get_win_folder(\"CSIDL_LOCAL_APPDATA\"))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n            if opinion:\n                path = os.path.join(path, \"Cache\")\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Caches\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_state_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific state dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user state directories are:\n        Mac OS X:  same as user_data_dir\n        Unix:      ~/.local/state/<AppName>   # or in $XDG_STATE_HOME, if defined\n        Win *:     same as user_data_dir\n\n    For Unix, we follow this Debian proposal <https://wiki.debian.org/XDGBaseDirectorySpecification#state>\n    to extend the XDG spec and support $XDG_STATE_HOME.\n\n    That means, by default \"~/.local/state/<AppName>\".\n    \"\"\"\n    if system in [\"win32\", \"darwin\"]:\n        path = user_data_dir(appname, appauthor, None, roaming)\n    else:\n        path = os.getenv(\"XDG_STATE_HOME\", os.path.expanduser(\"~/.local/state\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_log_dir(appname=None, appauthor=None, version=None, opinion=True):\n    r\"\"\"Return full path to the user-specific log dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"opinion\" (boolean) can be False to disable the appending of\n            \"Logs\" to the base app data dir for Windows, and \"log\" to the\n            base cache dir for Unix. See discussion below.\n\n    Typical user log directories are:\n        Mac OS X:   ~/Library/Logs/<AppName>\n        Unix:       ~/.cache/<AppName>/log  # or under $XDG_CACHE_HOME if defined\n        Win XP:     C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\\Logs\n        Vista:      C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\\Logs\n\n    On Windows the only suggestion in the MSDN docs is that local settings\n    go in the `CSIDL_LOCAL_APPDATA` directory. (Note: I'm interested in\n    examples of what some windows apps use for a logs dir.)\n\n    OPINION: This function appends \"Logs\" to the `CSIDL_LOCAL_APPDATA`\n    value for Windows and appends \"log\" to the user cache dir for Unix.\n    This can be disabled with the `opinion=False` option.\n    \"\"\"\n    if system == \"darwin\":\n        path = os.path.join(os.path.expanduser(\"~/Library/Logs\"), appname)\n    elif system == \"win32\":\n        path = user_data_dir(appname, appauthor, version)\n        version = False\n        if opinion:\n            path = os.path.join(path, \"Logs\")\n    else:\n        path = user_cache_dir(appname, appauthor, version)\n        version = False\n        if opinion:\n            path = os.path.join(path, \"log\")\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\nclass AppDirs(object):\n    \"\"\"Convenience wrapper for getting application dirs.\"\"\"\n\n    def __init__(\n        self, appname=None, appauthor=None, version=None, roaming=False, multipath=False\n    ):\n        self.appname = appname\n        self.appauthor = appauthor\n        self.version = version\n        self.roaming = roaming\n        self.multipath = multipath\n\n    @property\n    def user_data_dir(self):\n        return user_data_dir(\n            self.appname, self.appauthor, version=self.version, roaming=self.roaming\n        )\n\n    @property\n    def site_data_dir(self):\n        return site_data_dir(\n            self.appname, self.appauthor, version=self.version, multipath=self.multipath\n        )\n\n    @property\n    def user_config_dir(self):\n        return user_config_dir(\n            self.appname, self.appauthor, version=self.version, roaming=self.roaming\n        )\n\n    @property\n    def site_config_dir(self):\n        return site_config_dir(\n            self.appname, self.appauthor, version=self.version, multipath=self.multipath\n        )\n\n    @property\n    def user_cache_dir(self):\n        return user_cache_dir(self.appname, self.appauthor, version=self.version)\n\n    @property\n    def user_state_dir(self):\n        return user_state_dir(self.appname, self.appauthor, version=self.version)\n\n    @property\n    def user_log_dir(self):\n        return user_log_dir(self.appname, self.appauthor, version=self.version)\n\n\n# ---- internal support stuff\n\n\ndef _get_win_folder_from_registry(csidl_name):\n    \"\"\"This is a fallback technique at best. I'm not sure if using the\n    registry for this guarantees us the correct answer for all CSIDL_*\n    names.\n    \"\"\"\n    import winreg as _winreg\n\n    shell_folder_name = {\n        \"CSIDL_APPDATA\": \"AppData\",\n        \"CSIDL_COMMON_APPDATA\": \"Common AppData\",\n        \"CSIDL_LOCAL_APPDATA\": \"Local AppData\",\n    }[csidl_name]\n\n    key = _winreg.OpenKey(\n        _winreg.HKEY_CURRENT_USER,\n        r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\",\n    )\n    dir, type = _winreg.QueryValueEx(key, shell_folder_name)\n    return dir\n\n\ndef _get_win_folder_with_pywin32(csidl_name):\n    from win32com.shell import shell, shellcon\n\n    dir = shell.SHGetFolderPath(0, getattr(shellcon, csidl_name), 0, 0)\n    # Try to make this a unicode path because SHGetFolderPath does\n    # not return unicode strings when there is unicode data in the\n    # path.\n    try:\n        dir = unicode(dir)\n\n        # Downgrade to short path name if have highbit chars. See\n        # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n        has_high_char = False\n        for c in dir:\n            if ord(c) > 255:\n                has_high_char = True\n                break\n        if has_high_char:\n            try:\n                import win32api\n\n                dir = win32api.GetShortPathName(dir)\n            except ImportError:\n                pass\n    except UnicodeError:\n        pass\n    return dir\n\n\ndef _get_win_folder_with_ctypes(csidl_name):\n    import ctypes\n\n    csidl_const = {\n        \"CSIDL_APPDATA\": 26,\n        \"CSIDL_COMMON_APPDATA\": 35,\n        \"CSIDL_LOCAL_APPDATA\": 28,\n    }[csidl_name]\n\n    buf = ctypes.create_unicode_buffer(1024)\n    ctypes.windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)\n\n    # Downgrade to short path name if have highbit chars. See\n    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n    has_high_char = False\n    for c in buf:\n        if ord(c) > 255:\n            has_high_char = True\n            break\n    if has_high_char:\n        buf2 = ctypes.create_unicode_buffer(1024)\n        if ctypes.windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):\n            buf = buf2\n\n    return buf.value\n\n\ndef _get_win_folder_with_jna(csidl_name):\n    import array\n\n    from com.sun import jna\n    from com.sun.jna.platform import win32\n\n    buf_size = win32.WinDef.MAX_PATH * 2\n    buf = array.zeros(\"c\", buf_size)\n    shell = win32.Shell32.INSTANCE\n    shell.SHGetFolderPath(\n        None,\n        getattr(win32.ShlObj, csidl_name),\n        None,\n        win32.ShlObj.SHGFP_TYPE_CURRENT,\n        buf,\n    )\n    dir = jna.Native.toString(buf.tostring()).rstrip(\"\\0\")\n\n    # Downgrade to short path name if have highbit chars. See\n    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n    has_high_char = False\n    for c in dir:\n        if ord(c) > 255:\n            has_high_char = True\n            break\n    if has_high_char:\n        buf = array.zeros(\"c\", buf_size)\n        kernel = win32.Kernel32.INSTANCE\n        if kernel.GetShortPathName(dir, buf, buf_size):\n            dir = jna.Native.toString(buf.tostring()).rstrip(\"\\0\")\n\n    return dir\n\n\nif system == \"win32\":\n    try:\n        import win32com.shell\n\n        _get_win_folder = _get_win_folder_with_pywin32\n    except ImportError:\n        try:\n            from ctypes import windll\n\n            _get_win_folder = _get_win_folder_with_ctypes\n        except ImportError:\n            try:\n                import com.sun.jna\n\n                _get_win_folder = _get_win_folder_with_jna\n            except ImportError:\n                _get_win_folder = _get_win_folder_from_registry\n\n\n# ---- self test code\n\nif __name__ == \"__main__\":\n    appname = \"MyApp\"\n    appauthor = \"MyCompany\"\n\n    props = (\n        \"user_data_dir\",\n        \"user_config_dir\",\n        \"user_cache_dir\",\n        \"user_state_dir\",\n        \"user_log_dir\",\n        \"site_data_dir\",\n        \"site_config_dir\",\n    )\n\n    print(f\"-- app dirs {__version__} --\")\n\n    print(\"-- app dirs (with optional 'version')\")\n    dirs = AppDirs(appname, appauthor, version=\"1.0\")\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (without optional 'version')\")\n    dirs = AppDirs(appname, appauthor)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (without optional 'appauthor')\")\n    dirs = AppDirs(appname)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (with disabled 'appauthor')\")\n    dirs = AppDirs(appname, appauthor=False)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n"
    },
    "patchgt": {
      "torch/_logging/_internal.py": "@@ -38,6 +38,12 @@ class LogRegistry:\n     # e.g. \"guards\"\n     artifact_names: Set[str] = field(default_factory=set)\n \n+    # Artifacts that should be visible by default in the error message\n+    visible_artifacts: Set[str] = field(default_factory=set)\n+\n+    # A short description of each artifact\n+    artifact_descriptions: Dict[str, str] = field(default_factory=dict)\n+\n     # artifacts which are not displayed unless explicitly named in the\n     # settings. Ex. output_code is NOT displayed even if the inductor\n     # log level is set to DEBUG. It must be explicitly named in the settings\n@@ -57,8 +63,13 @@ def register_log(self, alias, log_qname):\n         self.log_alias_to_log_qname[alias] = log_qname\n \n     # register an artifact name\n-    def register_artifact_name(self, name, off_by_default, log_format):\n+    def register_artifact_name(\n+        self, name, description, visible, off_by_default, log_format\n+    ):\n         self.artifact_names.add(name)\n+        if visible:\n+            self.visible_artifacts.add(name)\n+        self.artifact_descriptions[name] = description\n \n         # if off by default, don't enable it\n         # when log_name's log_level is set to DEBUG\n@@ -396,15 +407,21 @@ def register_log(setting_name, log_name):\n     log_registry.register_log(setting_name, log_name)\n \n \n-def register_artifact(setting_name, off_by_default=False, log_format=None):\n+def register_artifact(\n+    setting_name, description, visible=False, off_by_default=False, log_format=None\n+):\n     \"\"\"\n     Enables an artifact to be controlled by the env var and user API with name\n     Args:\n         setting_name: the shorthand name used in the env var and user API\n+        description: A description of what this outputs\n+        visible: Whether it gets suggested to users by default\n         off_by_default: whether this artifact should be logged when the ancestor loggers\n             are enabled at level DEBUG\n     \"\"\"\n-    log_registry.register_artifact_name(setting_name, off_by_default, log_format)\n+    log_registry.register_artifact_name(\n+        setting_name, description, visible, off_by_default, log_format\n+    )\n \n \n def getArtifactLogger(module_qname, artifact_name):\n@@ -452,18 +469,68 @@ def _validate_settings(settings):\n     return re.fullmatch(_gen_settings_regex(), settings) is not None\n \n \n-def _invalid_settings_err_msg(settings):\n-    entities = \"\\n  \" + \"\\n  \".join(\n-        itertools.chain(\n-            [\"all\"],\n-            log_registry.log_alias_to_log_qname.keys(),\n-            log_registry.artifact_names,\n-        )\n+def help_message(verbose=False):\n+    def pad_to(s, length=30):\n+        assert len(s) <= length\n+        return s + \" \" * (length - len(s))\n+\n+    if verbose:\n+        printed_artifacts = log_registry.artifact_names\n+    else:\n+        printed_artifacts = log_registry.visible_artifacts\n+\n+    if verbose:\n+        heading = \"All registered names\"\n+    else:\n+        heading = \"Visible registered names (use TORCH_LOGS='+help' for full list)\"\n+    lines = (\n+        [\"all\"]\n+        + list(log_registry.log_alias_to_log_qname.keys())\n+        + [\n+            f\"{pad_to(name)}\\t{log_registry.artifact_descriptions[name]}\"\n+            for name in printed_artifacts\n+        ]\n     )\n-    msg = (\n-        f\"Invalid log settings: {settings}, must be a comma separated list of fully qualified module names, \"\n-        f\"registered log names or registered artifact names.\\nCurrently registered names: {entities}\"\n+    setting_info = \"  \" + \"\\n  \".join(lines)\n+    examples = \"\"\"\n+Examples:\n+  TORCH_LOGS=\"+dynamo,aot\" will set the log level of TorchDynamo to\n+  logging.DEBUG and AOT to logging.INFO\n+\n+  TORCH_LOGS=\"-dynamo,+inductor\" will set the log level of TorchDynamo to\n+  logging.ERROR and TorchInductor to logging.DEBUG\n+\n+  TORCH_LOGS=\"aot_graphs\" will enable the aot_graphs artifact\n+\n+  TORCH_LOGS=\"+dynamo,schedule\" will enable set the log level of TorchDynamo\n+  to logging.DEBUG and enable the schedule artifact\n+\n+  TORCH_LOGS=\"+some.random.module,schedule\" will set the log level of\n+  some.random.module to logging.DEBUG and enable the schedule artifact\n+\"\"\"  # flake8: noqa: B950\n+    msg = f\"\"\"\n+TORCH_LOGS Info\n+{examples}\n+\n+{heading}\n+{setting_info}\n+\"\"\"\n+    return msg\n+\n+\n+def _invalid_settings_err_msg(settings, verbose=False):\n+    valid_settings = \", \".join(\n+        [\"all\"]\n+        + list(log_registry.log_alias_to_log_qname.keys())\n+        + list(log_registry.artifact_names)\n     )\n+    msg = f\"\"\"\n+Invalid log settings: {settings}, must be a comma separated list of fully\n+qualified module names, registered log names or registered artifact names.\n+For more info on various settings, try TORCH_LOGS=\"help\"\n+Valid settings:\n+{valid_settings}\n+\"\"\"\n     return msg\n \n \n@@ -472,6 +539,10 @@ def _parse_log_settings(settings):\n     if settings == \"\":\n         return dict()\n \n+    if settings == \"help\":\n+        raise ValueError(help_message(verbose=False))\n+    elif settings == \"+help\":\n+        raise ValueError(help_message(verbose=True))\n     if not _validate_settings(settings):\n         raise ValueError(_invalid_settings_err_msg(settings))\n ",
      "torch/_logging/_registrations.py": "@@ -1,3 +1,4 @@\n+# flake8: noqa: B950\n from ._internal import register_artifact, register_log\n \n register_log(\"dynamo\", \"torch._dynamo\")\n@@ -8,23 +9,72 @@\n register_log(\"distributed\", \"torch.distributed\")\n register_log(\"onnx\", \"torch.onnx\")\n \n-register_artifact(\"guards\")\n-register_artifact(\"verbose_guards\", off_by_default=True)\n-register_artifact(\"bytecode\", off_by_default=True)\n-register_artifact(\"graph\")\n-register_artifact(\"graph_code\")\n-register_artifact(\"graph_sizes\")\n-register_artifact(\"trace_source\")\n-register_artifact(\"trace_call\")\n-register_artifact(\"aot_graphs\")\n-register_artifact(\"aot_joint_graph\")\n-register_artifact(\"ddp_graphs\")\n-register_artifact(\"recompiles\")\n-register_artifact(\"graph_breaks\")\n-register_artifact(\"not_implemented\")\n-register_artifact(\"output_code\", off_by_default=True)\n-register_artifact(\"schedule\", off_by_default=True)\n-register_artifact(\"perf_hints\", off_by_default=True)\n-register_artifact(\"onnx_diagnostics\", off_by_default=True)\n+register_artifact(\n+    \"guards\",\n+    \"This prints the guards for every compiled Dynamo frame. It does not tell you where the guards come from.\",\n+    visible=True,\n+)\n+register_artifact(\"verbose_guards\", \"\", off_by_default=True)\n+register_artifact(\n+    \"bytecode\",\n+    \"Prints the original and modified bytecode from Dynamo. Mostly useful if you're debugging our bytecode generation in Dynamo.\",\n+    off_by_default=True,\n+)\n+register_artifact(\n+    \"graph\",\n+    \"Prints the dynamo traced graph (prior to AOTDispatch) in a table. If you prefer python code use `graph_code` instead. \",\n+)\n+register_artifact(\"graph_code\", \"Like `graph`, but gives you the Python code instead.\")\n+register_artifact(\n+    \"graph_sizes\", \"Prints the sizes of all FX nodes in the dynamo graph.\"\n+)\n+register_artifact(\n+    \"trace_source\",\n+    \"As we execute bytecode, prints the file name / line number we are processing and the actual source code. Useful with `bytecode`\",\n+)\n+register_artifact(\n+    \"trace_call\",\n+    \"Like trace_source, but it will give you the per-expression blow-by-blow if your Python is recent enough.\",\n+)\n+register_artifact(\n+    \"aot_graphs\",\n+    \"Prints the FX forward and backward graph generated by AOTDispatch, after partitioning. Useful to understand what's being given to Inductor\",\n+    visible=True,\n+)\n+register_artifact(\n+    \"aot_joint_graph\",\n+    \"Print FX joint graph from AOTAutograd, prior to partitioning. Useful for debugging partitioning\",\n+)\n+register_artifact(\n+    \"ddp_graphs\",\n+    \"Only relevant for compiling DDP. DDP splits into multiple graphs to trigger comms early. This will print each individual graph here.\",\n+)\n+register_artifact(\n+    \"recompiles\",\n+    \"Prints the reason why we recompiled a graph. Very, very useful.\",\n+    visible=True,\n+)\n+register_artifact(\n+    \"graph_breaks\",\n+    \"Prints whenever Dynamo decides that it needs to graph break (i.e. create a new graph). Useful for debugging why torch.compile has poor performance\",\n+    visible=True,\n+)\n+register_artifact(\n+    \"not_implemented\",\n+    \"Prints log messages whenever we return NotImplemented in a multi-dispatch, letting you trace through each object we attempted to dispatch to\",\n+)\n+register_artifact(\n+    \"output_code\",\n+    \"Prints the code that Inductor generates (either Triton or C++)\",\n+    off_by_default=True,\n+    visible=True,\n+)\n+register_artifact(\n+    \"schedule\",\n+    \"Inductor scheduler information. Useful if working on Inductor fusion algo\",\n+    off_by_default=True,\n+)\n+register_artifact(\"perf_hints\", \"\", off_by_default=True)\n+register_artifact(\"onnx_diagnostics\", \"\", off_by_default=True)\n \n-register_artifact(\"custom_format_test_artifact\", log_format=\"\")\n+register_artifact(\"custom_format_test_artifact\", \"Testing only\", log_format=\"\")"
    },
    "repo": "pytorch",
    "pr_number": 108365
  },
  {
    "issue": "Issue: Error when someone calls train/eval on pre_autograd graph (#108143)\n\n(Originally by @tugsbayasgalan)\r\n\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108143\r\nApproved by: https://github.com/andrewor14\n\n",
    "ctx": {
      "test/export/test_export.py": "# Owner(s): [\"module: dynamo\"]\nimport dataclasses\nimport unittest\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\n\nimport torch\nimport torch._dynamo as torchdynamo\nfrom functorch.experimental.control_flow import map\nfrom torch import Tensor\nfrom torch.export import Constraint\nfrom torch._export import DEFAULT_EXPORT_DYNAMO_CONFIG, dynamic_dim, export\nfrom torch._export.constraints import constrain_as_size, constrain_as_value\nfrom torch._export.utils import (\n    get_buffer,\n    get_param,\n    is_buffer,\n    is_param,\n    register_dataclass_as_pytree_node,\n)\nfrom torch.fx.experimental.proxy_tensor import make_fx\nfrom torch.testing import FileCheck\nfrom torch.testing._internal.common_utils import run_tests, TestCase\nfrom torch.utils._pytree import (\n    LeafSpec,\n    tree_flatten,\n    tree_unflatten,\n    TreeSpec,\n    treespec_loads,\n    treespec_dumps\n)\n\n\n@unittest.skipIf(not torchdynamo.is_dynamo_supported(), \"dynamo isn't support\")\nclass TestDynamismExpression(TestCase):\n    def test_export_inline_constraints(self):\n\n        def f(x):\n            b = x.item()\n            constrain_as_size(b)\n            return torch.full((b, 1), 1)\n\n        inp = (torch.tensor([3]),)\n        ref = f(*inp)\n\n        gm = export(f, inp)\n        res = gm(*inp)\n\n        self.assertTrue(torchdynamo.utils.same(ref, res))\n\n        gm = make_fx(f, tracing_mode=\"symbolic\")(*inp)\n        res = gm(*inp)\n        self.assertTrue(torchdynamo.utils.same(ref, res))\n\n    def test_export_constraints_error(self):\n\n        def invalid_input_conflict_with_input_constraints(x):\n            return x + 1\n\n        inp = torch.zeros([3])\n        inp_constraints = [\n            dynamic_dim(inp, 0) > 5,\n        ]\n        with self.assertRaisesRegex(torchdynamo.exc.UserError, \"not in range\"):\n            export(\n                invalid_input_conflict_with_input_constraints,\n                (inp,),\n                constraints=inp_constraints,\n            )\n\n        def conflicting_constraints(x):\n            b = x.item()\n            constrain_as_size(b)\n            constrain_as_value(b, min=4, max=5)\n            return torch.full((b, 1), 1)\n\n        inp = (torch.tensor([3]),)\n        ep = export(conflicting_constraints, inp)\n\n        with self.assertRaisesRegex(RuntimeError, r\"is outside of inline constraint \\[4, 5\\]\"):\n            ep(torch.tensor([3]))\n\n    def test_export_assume_static_by_default(self):\n        def branch_on_shape(x: torch.Tensor):\n            if x.shape[0] == 4:\n                return x + 1\n            else:\n                return x\n\n        inp = (torch.rand(4, 5),)\n\n        # Being able to export means shape is preserved as static\n        export(branch_on_shape, inp)\n\n\n@unittest.skipIf(not torchdynamo.is_dynamo_supported(), \"dynamo isn't support\")\nclass TestExport(TestCase):\n\n    def _test_export_same_as_eager(self, f, args, kwargs=None):\n        kwargs = kwargs or {}\n        exported_program = export(f, args, kwargs)\n        reversed_kwargs = {key: kwargs[key] for key in reversed(kwargs)}\n        self.assertEqual(exported_program(*args, **kwargs), f(*args, **kwargs))\n        self.assertEqual(exported_program(*args, **reversed_kwargs), f(*args, **reversed_kwargs))\n\n    def test_basic(self):\n        def f(x, y):\n            return x[0] + y\n\n        inp = ([torch.ones(1, 3)], torch.ones(1, 3))\n        self._test_export_same_as_eager(f, inp)\n\n    def test_export_preserve_signature(self):\n        class NestedChild(torch.nn.Module):\n            def forward(self, zx, y):\n                return {\"x\": y[\"key\"] + zx[1], \"w\": y[\"key\"] * zx[1]}\n\n        class Child1(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.nested = NestedChild()\n\n            def forward(self, x, y):\n                z = torch.ones_like(x)\n                xw = self.nested((z, x), y={\"key\": y})\n                return xw[\"w\"] + z - xw[\"x\"]\n\n        class Child2(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n\n            def forward(self, x):\n                return x - 1\n\n        class MyModule(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.foo = Child1()\n                self.bar = Child2()\n\n            def forward(self, x, y):\n                x = self.foo(x, y)\n                x = self.bar(x)\n                return x\n\n        orig_eager = MyModule()\n        inps = torch.rand(2, 3), torch.rand(2, 3)\n        ep = export(\n            orig_eager,\n            inps,\n            {},\n            preserve_module_call_signature=(\"foo.nested\", \"foo\"),\n        )\n        ep._validate()\n        self.assertEqual(len(ep.module_call_graph), 2)\n        # TODO(zhxchen17) unflattener\n        # unflattened = unflatten(export_module)\n        # self.compare_outputs(export_module, unflattened, inps)\n        # unflattened.foo.nested = NestedChild()\n        # self.compare_outputs(export_module, unflattened, inps)\n\n    def test_raise_user_error_when_guard_on_data_dependent_operation(self):\n        def fn_ddo(x):\n            y = x.nonzero()\n            z = y.shape[0]\n            if z > 2:\n                return x.cos()\n            else:\n                return x.sin()\n\n        with self.assertRaisesRegex(\n            torchdynamo.exc.UserError,\n            \"trying to get a value out of symbolic int\"\n        ):\n            _ = export(fn_ddo, (torch.tensor([2, 3, 5]),), constraints=None)\n\n    def test_if_functional(self):\n        def foo(x):\n            z = x + 4\n            z.add_(4)\n            y = z.view(x.shape)\n            return x.cos() + y.cos()\n\n        gm = export(foo, (torch.tensor([2, 3, 5]),), constraints=None)\n\n        view_count = 0\n        for node in gm.graph.nodes:\n            if node.op == \"call_function\" and node.target == torch.ops.aten.add_.Tensor:\n                # No more inplace mutation\n                self.assertNotEqual(\n                    node.target,\n                    torch.ops.aten.add_.Tensor,\n                    \"There shouldn't be any inplace mutation node in the graph.\"\n                )\n            if node.op == \"call_function\" and node.target == torch.ops.aten.view.default:\n                view_count += 1\n\n        # There should be nonzero view nodes in the graph\n        self.assertTrue(view_count > 0)\n\n    def test_export_mod_constraints(self):\n        class BasicDynamiShapeModel(torch.nn.Module):\n            def forward(self, x: torch.Tensor) -> torch.Tensor:\n                return x.view(x.shape[0] - 1, -1)\n\n        m = BasicDynamiShapeModel()\n        a = torch.randn(3, 4)\n        constraints = [3 <= dynamic_dim(a, 0), dynamic_dim(a, 1)]\n        with self.assertRaisesRegex(\n            torch._dynamo.exc.UserError,\n            (\n                \"Some dynamic dimensions need to be specialized because \"\n                \"the constraints inferred for them are too complex to specify\"\n                \".*\\n.*\\\\[0\\\\], which was marked dynamic, must be specialized to 3\"\n                \".*\\n.*\\\\[1\\\\], which was marked dynamic, must be specialized to 4\"\n            ),\n        ):\n            torch._export.export(m, (a,), constraints=constraints)\n        em = torch._export.export(m, (a,))\n        x = torch.randn(3, 5)\n        with self.assertRaisesRegex(RuntimeError, \"\\\\[1\\\\] is specialized at 4\"):\n            em(x)\n\n    def test_not_correct_dim(self):\n        def f(x):\n            return x.cos()\n\n        def g(x):\n            return x + 4\n\n        inp_for_f = torch.tensor(5)\n        with self.assertRaisesRegex(torchdynamo.exc.UserError, \"Cannot mark 0-dimension tensors to be dynamic\"):\n            constraints = [dynamic_dim(inp_for_f, 0)]\n\n        inp_for_f_mul_dim = torch.ones(5, 5)\n        with self.assertRaisesRegex(\n            torchdynamo.exc.UserError,\n            \"Expected the dimension passed to dynamic_dim to be in the range \\\\[0:1\\\\]\"\n        ):\n            constraints = [dynamic_dim(inp_for_f_mul_dim, 2)]\n\n        inp_for_g = 4\n        with self.assertRaisesRegex(torchdynamo.exc.UserError, \"Expected tensor as input to dynamic_dim\"):\n            constraints = [dynamic_dim(inp_for_g, 0)]\n\n    def test_map(self):\n        def list_tensor_map(xs, y, z):\n            def body(x, y, z):\n                return x + y + z\n\n            return map(body, xs, y, z)\n\n        inps = (torch.ones(6, 4), torch.tensor(5), torch.tensor(4))\n        self._test_export_same_as_eager(list_tensor_map, inps)\n\n    def test_export_func_with_kwargs(self):\n        def kw_func(arg1, arg2, kw1, kw2):\n            return arg1 + arg2, kw1 + kw2\n\n        args = (torch.ones(6, 4), torch.ones(1, 1))\n        kwargs = {\"kw1\": torch.ones(1, 1), \"kw2\": torch.ones(6, 4)}\n        self._test_export_same_as_eager(kw_func, args, kwargs)\n\n    def test_export_func_with_pytree_kwargs(self):\n        def kw_func(arg1, arg2, a, b):\n            return arg1 + a[\"kw1\"] + b[0], arg2 + a[\"kw2\"] + b[1]\n\n        args = (torch.ones(2, 3), torch.ones(3, 4))\n        kwargs = {\"a\": {\"kw1\": torch.ones(2, 3), \"kw2\": torch.ones(3, 4)}, \"b\": [torch.ones(2, 3), torch.ones(3, 4)]}\n        self._test_export_same_as_eager(kw_func, args, kwargs)\n\n    def test_export_func_with_default_kwargs(self):\n        def kw_func(arg1, arg2, a, b=1):\n            return arg1 + arg2, a[\"kw1\"] + a[\"kw2\"] + b\n\n        def kw_func2(arg1, arg2, a=1, b=2):\n            return arg1 + a, arg2 + b\n\n\n        args = (torch.ones(6, 4), torch.ones(1, 1))\n        kwargs1 = {\"a\": {\"kw1\": torch.ones(1, 1), \"kw2\": torch.ones(6, 4)}}\n        kwargs2 = {\"a\": {\"kw1\": torch.ones(1, 1), \"kw2\": torch.ones(6, 4)}, \"b\": 2}\n        self._test_export_same_as_eager(kw_func, args, kwargs1)\n        self._test_export_same_as_eager(kw_func, args, kwargs2)\n        kwargs3 = {\"b\": 1}\n        self._test_export_same_as_eager(kw_func2, args, kwargs3)\n\n    def test_export_func_with_var_postional_args(self):\n        def kw_func(arg1, arg2, *args):\n            return arg1 + args[0], arg2 + args[1]\n\n        args = (torch.ones(2, 3), torch.ones(3, 4), torch.ones(2, 3), torch.ones(3, 4))\n        self._test_export_same_as_eager(kw_func, args)\n\n    def test_export_func_with_keyword_only_args(self):\n        def kw_func(arg1, arg2, *args, kw1, kw2):\n            return arg1 + args[0] + kw1, arg2 + args[1] + kw2\n\n        args = (torch.ones(2, 3), torch.ones(3, 4), torch.ones(2, 3), torch.ones(3, 4))\n        kwargs = {\"kw1\": torch.ones(2, 3), \"kw2\": torch.ones(3, 4)}\n        self._test_export_same_as_eager(kw_func, args, kwargs)\n\n    def test_export_func_with_var_keyword_args(self):\n        def kw_func(arg1, arg2, *args, kw1, kw2, **kwargs):\n            return arg1 + args[0] + kw1 + kwargs[\"kw3\"], arg2 + args[1] + kw2 + kwargs[\"kw4\"]\n\n        args = (torch.ones(2, 3), torch.ones(3, 4), torch.ones(2, 3), torch.ones(3, 4))\n        kwargs = {\"kw1\": torch.ones(2, 3), \"kw2\": torch.ones(3, 4), \"kw3\": torch.ones(2, 3), \"kw4\": torch.ones(3, 4)}\n        self._test_export_same_as_eager(kw_func, args, kwargs)\n\n    def test_export_func_with_var_keyword_pytree_args(self):\n        def kw_func(arg1, arg2, *args, kw1, kw2, **kwargs):\n            return arg1 + arg2[0][0] + args[0] + kw1[0] + kwargs[\"kw3\"][0], arg2[1] + args[1] + kw2 + kwargs[\"kw4\"]\n\n        args = (torch.ones(2, 3), [(torch.ones(2, 3), ), torch.ones(3, 4)], torch.ones(2, 3), torch.ones(3, 4))\n        kwargs = {\"kw1\": (torch.ones(2, 3), ), \"kw2\": torch.ones(3, 4),\n                  \"kw3\": (torch.ones(2, 3), torch.ones(3, 4)), \"kw4\": torch.ones(3, 4)}\n        self._test_export_same_as_eager(kw_func, args, kwargs)\n\n    def test_linear_conv(self):\n\n        class MyLinear(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.weight = torch.randn(20, 98)\n                self.bias = torch.randn(20)\n\n            def forward(self, x):\n                return torch.nn.functional.linear(x, self.weight, self.bias)\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(16, 33, 3)\n                self.linear = MyLinear()\n\n            def forward(self, x):\n                x_conv = self.conv(x)\n                x_linear = self.linear(x_conv)\n                return x_linear.cos()\n\n        ep = export(Foo(), (torch.randn(20, 16, 50, 100),))\n        for node in ep.graph.nodes:\n            if (\n                node.op == \"placeholder\" and\n                node.name in ep.graph_signature.inputs_to_buffers or\n                node.name in ep.graph_signature.inputs_to_parameters\n            ):\n                self.assertTrue(\"source_fn\" in node.meta)\n                self.assertTrue(\"nn_module_stack\" in node.meta)\n\n\n    def test_error_does_not_reference_eager_fallback(self):\n        def fn_ddo(x):\n            y = x.nonzero()\n            z = y.shape[0]\n            if z > 2:\n                return x.cos()\n            else:\n                return x.sin()\n\n        with self.assertRaisesRegex(\n            torchdynamo.exc.UserError,\n            r\"^(?!.*fall back to eager).*\"\n        ):\n            _ = export(fn_ddo, (torch.tensor([2, 3, 5]),), constraints=None)\n\n    def test_pytree_regster_data_class(self):\n\n        @dataclass\n        class MyDataClass:\n            x: int\n            y: int\n            z: int = None\n\n        dt = MyDataClass(x=3, y=4)\n        flat, spec = tree_flatten(dt)\n        self.assertTrue(spec, LeafSpec())\n        self.assertTrue(len(flat) == 1)\n\n        register_dataclass_as_pytree_node(MyDataClass)\n\n        flat, spec = tree_flatten(dt)\n        self.assertEqual(\n            spec,\n            TreeSpec(\n                MyDataClass,\n                (\n                    MyDataClass,\n                    ['x', 'y'],\n                    ['z']\n                ),\n                [LeafSpec(), LeafSpec()]\n            )\n        )\n        self.assertEqual(flat, [3, 4])\n\n        orig_dt = tree_unflatten(flat, spec)\n        self.assertTrue(isinstance(orig_dt, MyDataClass))\n        self.assertEqual(orig_dt.x, 3)\n        self.assertEqual(orig_dt.y, 4)\n        self.assertEqual(orig_dt.z, None)\n\n        roundtrip_spec = treespec_loads(treespec_dumps(spec))\n        self.assertEqual(roundtrip_spec, spec)\n\n        # Override the registration with keep none fields\n        register_dataclass_as_pytree_node(MyDataClass, return_none_fields=True)\n\n        flat, spec = tree_flatten(dt)\n        self.assertEqual(\n            spec,\n            TreeSpec(\n                MyDataClass,\n                (\n                    MyDataClass,\n                    ['x', 'y', 'z'],\n                    [],\n                ),\n                [LeafSpec(), LeafSpec(), LeafSpec()]\n            )\n        )\n        self.assertEqual(flat, [3, 4, None])\n\n        orig_dt = tree_unflatten(flat, spec)\n        self.assertTrue(isinstance(orig_dt, MyDataClass))\n        self.assertEqual(orig_dt.x, 3)\n        self.assertEqual(orig_dt.y, 4)\n        self.assertEqual(orig_dt.z, None)\n\n        roundtrip_spec = treespec_loads(treespec_dumps(spec))\n        self.assertEqual(roundtrip_spec, spec)\n\n    def test_pytree_regster_nested_data_class(self):\n\n        @dataclass\n        class Inner:\n            x: int\n            y: int\n\n        @dataclass\n        class Outer:\n            xy: Inner\n            ab: Inner\n\n        xy = Inner(1, 2)\n        ab = Inner(3, 4)\n        dt = Outer(xy, ab)\n        inp = {\"dt1\": (dt, ({},)), \"dt2\": ((torch.ones(1),), dt)}\n\n        register_dataclass_as_pytree_node(Inner)\n        register_dataclass_as_pytree_node(Outer)\n\n        flat, spec = tree_flatten(inp)\n        self.assertEqual(flat, [1, 2, 3, 4, torch.ones(1), 1, 2, 3, 4])\n\n        unflat = tree_unflatten(flat, spec)\n        self.assertEqual(unflat, inp)\n\n        roundtrip_spec = treespec_loads(treespec_dumps(spec))\n        self.assertEqual(roundtrip_spec, spec)\n\n    def test_param_util(self):\n        class Basic(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.lin = torch.nn.Linear(10, 1)\n\n            def forward(self, x):\n                return self.lin(x)\n\n        ep = export(Basic(), (torch.randn(5, 10),))\n        num_params = 0\n        params = []\n        for node in ep.graph.nodes:\n            if is_param(ep, node):\n                num_params += 1\n                params.append(get_param(ep, node))\n        self.assertEqual(num_params, 2)\n        self.assertEqual(params[0].shape, [1, 10])  # weight\n        self.assertEqual(params[1].shape, [1])  # bias\n\n    def test_buffer_util(self):\n        ep = export(torch.nn.BatchNorm2d(100, affine=False), (torch.ones(20, 100, 35, 45), ))\n        num_buffer = 0\n        buffer = []\n\n        for node in ep.graph.nodes:\n            if is_buffer(ep, node):\n                num_buffer += 1\n                buffer.append(get_buffer(ep, node))\n        self.assertEqual(num_buffer, 3)\n\n        self.assertEqual(buffer[0].shape, torch.Size([100]))  # running_mean\n        self.assertEqual(buffer[1].shape, torch.Size([100]))  # running_var\n        self.assertEqual(buffer[2].shape, torch.Size([]))  # num_batches_tracked\n\n\n    def test_export_dynamo_config(self):\n        class MyModule(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.lstm = torch.nn.LSTM(input_size=4, hidden_size=5, num_layers=1)\n\n            def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n                return self.lstm(inputs)\n\n\n        config = DEFAULT_EXPORT_DYNAMO_CONFIG\n        mod = MyModule()\n\n        @contextmanager\n        def _patch_config(kwargs):\n            orig_config_dict = dataclasses.asdict(config)\n\n            try:\n                for k, v in kwargs.items():\n                    setattr(config, k, v)\n                yield\n            finally:\n                for k, v in orig_config_dict.items():\n                    setattr(config, k, v)\n\n        inp = (torch.rand(5, 4), )\n        exported_program = export(mod, inp)\n\n        with _patch_config({\"allow_rnn\": False}):\n            with self.assertRaisesRegex(\n                torch._dynamo.exc.Unsupported,\n                \"TorchDynamo purposely graph breaks on RNN, GRU, LSTMs\"\n            ):\n                _ = export(mod, inp)\n\n    def test_module(self):\n\n        class MyLinear(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.weight = torch.randn(20, 98)\n                self.bias = torch.randn(20)\n\n            def forward(self, x):\n                return torch.nn.functional.linear(x, self.weight, self.bias)\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(16, 33, 3)\n                self.linear = MyLinear()\n\n            def forward(self, x):\n                a, b = x\n                a_conv = self.conv(a)\n                a_linear = self.linear(a_conv)\n                b_conv = self.conv(b)\n                b_linear = self.linear(b_conv)\n                return (a_linear.cos() + b_linear.sin(), a_linear.sin() + b_linear.cos())\n\n        inp_container = ((torch.randn(20, 16, 50, 100), torch.randn(20, 16, 50, 100)),)\n\n        ep = export(Foo(), inp_container)\n        ep_rexported = export(ep.module(), inp_container)\n\n        inp_test = ((torch.randn(20, 16, 50, 100), torch.randn(20, 16, 50, 100)),)\n\n        self.assertTrue(torch.allclose(ep(*inp_test)[0], ep_rexported(*inp_test)[0]))\n        self.assertTrue(torch.allclose(ep(*inp_test)[1], ep_rexported(*inp_test)[1]))\n\n    def test_module_with_dict_container_inp_out(self):\n\n        class MyLinear(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.weight = torch.randn(20, 98)\n                self.bias = torch.randn(20)\n\n            def forward(self, x):\n                return torch.nn.functional.linear(x, self.weight, self.bias)\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(16, 33, 3)\n                self.linear = MyLinear()\n\n            def forward(self, x):\n                a1, a2 = x[\"a\"]\n                b = x[\"b\"]\n                a1_conv = self.conv(a1)\n                a1_linear = self.linear(a1_conv)\n                a2_conv = self.conv(a2)\n                a2_linear = self.linear(a2_conv)\n                b_conv = self.conv(b)\n                b_linear = self.linear(b_conv)\n                return {\"a\": a1_linear.cos() + b_linear.sin(), \"b\": a2_linear.sin() + b_linear.cos()}\n\n        inp_container = ({\"a\": (torch.randn(20, 16, 50, 100), torch.randn(20, 16, 50, 100)), \"b\": torch.randn(20, 16, 50, 100)},)\n\n        ep = export(Foo(), inp_container)\n        ep_rexported = export(ep.module(), inp_container)\n\n        inp_test = ({\"a\": (torch.randn(20, 16, 50, 100), torch.randn(20, 16, 50, 100)), \"b\": torch.randn(20, 16, 50, 100)},)\n\n        self.assertTrue(torch.allclose(ep(*inp_test)[\"a\"], ep_rexported(*inp_test)[\"a\"]))\n        self.assertTrue(torch.allclose(ep(*inp_test)[\"b\"], ep_rexported(*inp_test)[\"b\"]))\n\n    def test_args_type_checked(self):\n        def fn(x):\n            return x + 1\n\n        inp = torch.rand(2, 2)\n        with self.assertRaisesRegex(torch._dynamo.exc.UserError, \"to be a tuple\"):\n            # Intentionally not wrapping `inp` in a tuple to trigger the error\n            _ = export(fn, inp)\n\n    def test_constrain_value_with_no_default(self):\n        def fn(x, y):\n            n = x.max().item()\n            constrain_as_value(n)\n            return y + n\n\n        ep = export(fn, (torch.randint(3, 5, (2, 2)), torch.randint(3, 5, (2, 3))))\n        test_inp = (torch.randint(3, 5, (2, 2)), torch.randint(3, 5, (2, 3)))\n        self.assertTrue(torch.allclose(ep(*test_inp), fn(*test_inp)))\n\n    def test_constrain_value_with_symfloat(self):\n        def fn(x, y):\n            n = x.max().item()\n            constrain_as_value(n)\n            return y + n\n\n        with self.assertRaisesRegex(torch._dynamo.exc.TorchRuntimeError, \"Constraining SymFloat or Symbool is nyi\"):\n            _ = export(fn, (torch.rand(2, 2), torch.rand(2, 3)))\n\n    def test_constrain_size_in_eager(self):\n        def fn(x, y):\n            n = x.max().item()\n            constrain_as_size(n)\n            return y + n\n\n        ep = export(fn, (torch.randint(1, 2, (2, 2)), torch.randint(3, 5, (2, 3))))\n        test_inp = (torch.randint(1, 2, (2, 2)), torch.randint(3, 5, (2, 3)))\n        self.assertTrue(torch.allclose(ep(*test_inp), fn(*test_inp)))\n\n    def test_constrain_size_with_constrain_value(self):\n        def fn(x, y):\n            n = x.max().item()\n            constrain_as_value(n, 2, 10)\n            constrain_as_size(n)\n            return y + n\n\n        with self.assertRaisesRegex(RuntimeError, r\"Invalid value range for 1 between \\[2, 10\\].\"):\n            _ = fn(torch.randint(1, 2, (2, 2)), torch.randint(3, 5, (2, 3)))\n\n        ep = export(fn, (torch.randint(3, 4, (2, 2)), torch.randint(3, 5, (2, 3))))\n        with self.assertRaisesRegex(RuntimeError, \"is outside of inline constraint\"):\n            test_inp = (torch.randint(1, 2, (2, 2)), torch.randint(3, 5, (2, 3)))\n            _ = ep(*test_inp)\n\n    def test_constrain_size_with_various_cases(self):\n\n        def case_1(x, y):\n            n = x.item()\n            constrain_as_size(n, min=0)\n            return y.sum() + torch.ones(n, 5).sum()\n\n        def case_2(x, y):\n            n = x.item()\n            constrain_as_size(n, min=0, max=6)\n            return y.sum() + torch.ones(n, 5).sum()\n\n        def case_3(x, y):\n            n = x.item()\n            constrain_as_size(n, min=0, max=1)\n            return y.sum() + torch.ones(n, 5).sum()\n\n        def case_4(x, y):\n            n = x.item()\n            constrain_as_size(n, min=2)\n            return y.sum() + torch.ones(n, 5).sum()\n\n        def case_5(x, y):\n            n = x.item()\n            constrain_as_size(n, min=1)\n            return y.sum() + torch.ones(n, 5).sum()\n\n        ep = export(case_1, (torch.tensor(1), torch.ones(4, 5)))\n\n        with self.assertRaisesRegex(RuntimeError, r\"Invalid value range for -1 between\"):\n            _ = case_1(torch.tensor(-1), torch.randn(4, 5))\n\n        self.assertTrue(\n            torch.allclose(\n                ep(torch.tensor(1), torch.ones(4, 5)),\n                case_1(torch.tensor(1), torch.ones(4, 5)),\n            )\n        )\n\n        ep = export(case_2, (torch.tensor(5), torch.randn(4, 5)))\n\n        with self.assertRaisesRegex(RuntimeError, r\"Invalid value range for 7 between\"):\n            _ = case_2(torch.tensor(7), torch.randn(4, 5))\n\n        with self.assertRaisesRegex(RuntimeError, r\"Invalid value range for 9 between\"):\n            _ = case_2(torch.tensor(9), torch.randn(4, 5))\n\n        self.assertTrue(\n            torch.allclose(\n                ep(torch.tensor(5), torch.ones(4, 5)),\n                case_2(torch.tensor(5), torch.ones(4, 5)),\n            )\n        )\n\n        with self.assertRaisesRegex(RuntimeError, \"Max value to constrain_range_for_size must be greater than 2. got: 1\"):\n            _ = case_3(torch.tensor(1), torch.randn(4, 5))\n\n        with self.assertRaisesRegex(RuntimeError, r\"Invalid value range for 1 between \\[2, 9223372036854775807\\].\"):\n            _ = case_4(torch.tensor(1), torch.randn(4, 5))\n\n        ep = export(case_4, (torch.tensor(5), torch.randn(4, 5)))\n\n        with self.assertRaisesRegex(RuntimeError, r\"Invalid value range for 1\"):\n            _ = case_4(torch.tensor(1), torch.randn(4, 5))\n\n        self.assertTrue(\n            torch.allclose(\n                ep(torch.tensor(5), torch.ones(4, 5)),\n                case_4(torch.tensor(5), torch.ones(4, 5)),\n            )\n        )\n\n        ep = export(case_5, (torch.tensor(5), torch.randn(4, 5)))\n\n        with self.assertRaisesRegex(RuntimeError, r\"Invalid value range for 0\"):\n            _ = case_5(torch.tensor(0), torch.randn(4, 5))\n\n        self.assertTrue(\n            torch.allclose(\n                ep(torch.tensor(5), torch.ones(4, 5)),\n                case_5(torch.tensor(5), torch.ones(4, 5)),\n            )\n        )\n\n    def test_mixed_input(self):\n        def func(a, b, alpha: int):\n            return torch.add(a, b, alpha=alpha)\n\n        a = torch.rand(1, 2)\n        b = torch.rand(1, 2)\n        alpha = 10\n\n        exported = torch._export.export(func, (a, b, alpha))\n        for node in exported.graph_module.graph.nodes:\n            if node.op == \"placeholder\":\n                self.assertTrue(isinstance(node.meta[\"val\"], (Tensor, int)))\n\n    def test_export_with_inline_constraints(self):\n        def f(x):\n            a = x.item()\n            constrain_as_value(a, 4, 7)\n            return torch.empty((a, 4))\n\n        ep = export(f, (torch.tensor([5]),))\n        self.assertEqual(ep(torch.tensor([6])).shape, (6, 4))\n\n        FileCheck().check_count(\n            \"torch.ops.aten.sym_constrain_range.default\", 1, exactly=True\n        ).run(ep.graph_module.code)\n\n        with self.assertRaisesRegex(\n            RuntimeError,\n            r\"_local_scalar_dense is outside of inline constraint \\[4, 7\\]\",\n        ) as cm:\n            ep(torch.tensor([30]))\n\n    def test_export_with_inline_constraints_complex(self):\n        def f(x):\n            a = x.item()\n            constrain_as_value(a, 4, 7)\n            empty = torch.empty((a, 4))\n\n            return torch.cat((empty.transpose(0, 1), torch.zeros(6, a)), 0)\n\n        ep = export(f, (torch.tensor([6]),))\n        self.assertEqual(ep(torch.tensor([5])).shape, (10, 5))\n        FileCheck().check_count(\n            \"torch.ops.aten.sym_constrain_range.default\", 1, exactly=True\n        ).run(ep.graph_module.code)\n\n    def test_to_module_with_mutated_buffer(self):\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"buf\", torch.zeros(1))\n\n            def forward(self, x):\n                self.buf.add_(1)\n                return x.sum() + self.buf.sum()\n\n        exported = torch._export.export(Foo(), (torch.ones(5, 5),))\n        stateful_gm = exported.module()\n        export_return_val = stateful_gm(torch.ones(5, 5))\n        eager = Foo()\n        eager_return_val = eager(torch.ones(5, 5))\n        self.assertTrue(torch.allclose(eager_return_val, export_return_val))\n\n        for name, buffer in stateful_gm.named_buffers():\n            self.assertTrue(torch.allclose(torch.ones(1), buffer))\n\n        changed = stateful_gm.graph.eliminate_dead_code()\n        self.assertFalse(changed)\n        self.assertTrue(torch.allclose(stateful_gm(torch.ones(5, 5)), eager(torch.ones(5, 5))))\n\n        for name, buffer in stateful_gm.named_buffers():\n            self.assertTrue(torch.allclose(torch.tensor(2, dtype=torch.float), buffer))\n\n    def test_to_module_with_mutated_buffer_multiple(self):\n\n        class Bar(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"buf\", torch.ones(1))\n\n            def forward(self, x):\n                self.buf.add_(1)\n                return x.sum() + self.buf.sum()\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"buf\", torch.zeros(1))\n                self.bar = Bar()\n\n            def forward(self, x):\n                self.buf.add_(1)\n                self.bar.buf.add_(2)\n                bar = self.bar(x)\n                return bar.sum() + self.buf.sum()\n\n        exported = torch._export.export(Foo(), (torch.ones(5, 5),))\n        stateful_gm = exported.module()\n        export_return_val = stateful_gm(torch.ones(5, 5))\n        eager = Foo()\n        eager_return_val = eager(torch.ones(5, 5))\n        self.assertTrue(torch.allclose(eager_return_val, export_return_val))\n\n        for name, buffer in stateful_gm.named_buffers():\n            if name == \"L__self___buf\":\n                self.assertTrue(torch.allclose(torch.ones(1), buffer))\n            if name == \"L__self___bar_buf\":\n                self.assertTrue(torch.allclose(torch.tensor(4, dtype=torch.float), buffer))\n\n        changed = stateful_gm.graph.eliminate_dead_code()\n        self.assertFalse(changed)\n        self.assertTrue(torch.allclose(stateful_gm(torch.ones(5, 5)), eager(torch.ones(5, 5))))\n\n        for name, buffer in stateful_gm.named_buffers():\n            if name == \"L__self___buf\":\n                self.assertTrue(torch.allclose(torch.tensor(2, dtype=torch.float), buffer))\n            if name == \"L__self___bar_buf\":\n                self.assertTrue(torch.allclose(torch.tensor(7, dtype=torch.float), buffer))\n\n    def test_runtime_assert_for_prim(self):\n\n        def f(x, y):\n            return x + y\n\n        tensor_inp = torch.ones(7, 5)\n        exported = torch._export.export(f, (tensor_inp, 5), constraints=[dynamic_dim(tensor_inp, 0) > 5])\n        self.assertTrue(torch.allclose(exported(torch.ones(8, 5), 5), f(torch.ones(8, 5), 5)))\n        with self.assertRaisesRegex(RuntimeError, \"Input arg1_1 is specialized to be 5 at tracing time\"):\n            _ = exported(torch.ones(8, 5), 6)\n\n        exported = torch._export.export(f, (tensor_inp, 5.0), constraints=[dynamic_dim(tensor_inp, 0) > 5])\n        with self.assertRaisesRegex(RuntimeError, \"Input arg1_1 is specialized to be 5.0 at tracing time\"):\n            _ = exported(torch.ones(7, 5), 6.0)\n\n    def test_runtime_assert_for_prm_str(self):\n\n        def g(a, b, mode):\n            return torch.div(a, b, rounding_mode=mode)\n\n        inps = (torch.randn(4, 4), torch.randn(4), \"trunc\")\n        exported = torch._export.export(g, inps)\n        with self.assertRaisesRegex(RuntimeError, \"Input arg2_1 is specialized to be trunc at\"):\n            _ = exported(torch.randn(4, 4), torch.randn(4), \"floor\")\n        self.assertTrue(torch.allclose(exported(*inps), g(*inps)))\n\n    def test_to_module_with_mutated_buffer_multiple_update_sub_later(self):\n\n        class Bar(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"buf\", torch.ones(1))\n\n            def forward(self, x):\n                self.buf.add_(1)\n                return x.sum() + self.buf.sum()\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"buf\", torch.zeros(1))\n                self.bar = Bar()\n\n            def forward(self, x):\n                self.buf.add_(1)\n                bar = self.bar(x)\n                self.bar.buf.add_(2)\n                return bar.sum() + self.buf.sum()\n\n        exported = torch._export.export(Foo(), (torch.ones(5, 5),))\n        stateful_gm = exported.module()\n        export_return_val = stateful_gm(torch.ones(5, 5))\n        eager = Foo()\n        eager_return_val = eager(torch.ones(5, 5))\n        self.assertTrue(torch.allclose(eager_return_val, export_return_val))\n\n        for name, buffer in stateful_gm.named_buffers():\n            if name == \"L__self___buf\":\n                self.assertTrue(torch.allclose(torch.ones(1), buffer))\n            if name == \"L__self___bar_buf\":\n                self.assertTrue(torch.allclose(torch.tensor(4, dtype=torch.float), buffer))\n\n        changed = stateful_gm.graph.eliminate_dead_code()\n        self.assertFalse(changed)\n        self.assertTrue(torch.allclose(stateful_gm(torch.ones(5, 5)), eager(torch.ones(5, 5))))\n\n        for name, buffer in stateful_gm.named_buffers():\n            if name == \"L__self___buf\":\n                self.assertTrue(torch.allclose(torch.tensor(2, dtype=torch.float), buffer))\n            if name == \"L__self___bar_buf\":\n                self.assertTrue(torch.allclose(torch.tensor(7, dtype=torch.float), buffer))\n\n    def test_retracable_ep(self):\n\n        class Bar(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"buf\", torch.ones(1))\n\n            def forward(self, x):\n                self.buf.add_(1)\n                return x.sum() + self.buf.sum()\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"buf\", torch.zeros(1))\n                self.bar = Bar()\n\n            def forward(self, x):\n                self.buf.add_(1)\n                bar = self.bar(x)\n                self.bar.buf.add_(2)\n                return bar.sum() + self.buf.sum()\n\n        inp = torch.ones(5, 5)\n        exported = torch._export.export(Foo(), (inp,))\n        reexported = torch._export.export(exported, (inp,))\n\n        self.assertTrue(torch.allclose(exported(inp), reexported(inp)))\n\n        inp = torch.ones(5, 5)\n        exported = torch._export.export(Foo(), (inp,), constraints=[dynamic_dim(inp, 0)])\n        reexported = torch._export.export(exported, (inp,))\n\n        self.assertTrue(torch.allclose(exported(torch.ones(7, 5)), reexported(torch.ones(7, 5))))\n\n        exported = torch._export.export(Foo(), (inp,), constraints=[dynamic_dim(inp, 0)])\n        # This seems fine because the exported program is generalized to work for dynamic shapes.\n        reexported = torch._export.export(exported, (inp,))\n        self.assertTrue(torch.allclose(exported(torch.ones(7, 5)), reexported(torch.ones(7, 5))))\n\n        exported = torch._export.export(Foo(), (inp,), constraints=[dynamic_dim(inp, 0)])\n        with self.assertRaisesRegex(torch._dynamo.exc.UserError, 'Cannot provide constraints for already exported program.'):\n            _ = torch._export.export(exported, (inp,), constraints=[dynamic_dim(inp, 0)])\n        # Reexported program should still work for dynamic shapes.\n        reexported = torch._export.export(exported, (inp,))\n        self.assertTrue(reexported(torch.ones(7, 5)), Foo()(torch.ones(7, 5)))\n\n    def test_retrace_graph_level_meta_preservation(self):\n\n        class Foo(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n\n            def forward(self, x):\n                if x.shape[0] > 4:\n                    return x.cos()\n                return x.sin()\n\n        inp = torch.ones(7, 5)\n        exported = torch._export.export(Foo(), (inp,), constraints=[dynamic_dim(inp, 0) > 5])\n        stateful_module = exported.module()\n        self.assertTrue(len(stateful_module.meta[\"input_shape_constraints\"]), 1)\n\n        re_exported = torch._export.export(stateful_module, (inp,))\n        self.assertTrue(len(re_exported.graph_module.meta[\"input_shape_constraints\"]), 1)\n        self.assertTrue(torch.allclose(exported(torch.ones(7, 5)), re_exported(torch.ones(7, 5))))\n\n        re_exported_v2 = torch._export.export(exported, (inp,))\n        self.assertTrue(len(re_exported_v2.graph_module.meta[\"input_shape_constraints\"]), 1)\n        self.assertTrue(torch.allclose(exported(torch.ones(7, 5)), re_exported_v2(torch.ones(7, 5))))\n\n    def test_constrain_as_size_error(self):\n\n        def f(x):\n            a = x.item()\n            return torch.full((a, 4), 0)\n\n        with self.assertRaisesRegex(\n            torch._dynamo.exc.UserError,\n            \"Tried to use data-dependent value in the subsequent computation\"\n        ):\n            _ = export(f, (torch.tensor(6),))\n\n    def test_constraint_directly_construct(self):\n        with self.assertRaisesRegex(\n            TypeError,\n            \"torch.export.Constraint has no public constructor. Please use torch.export.dynamic_dim\"\n        ):\n            _ = Constraint()\n\n\nif __name__ == '__main__':\n    run_tests()\n",
      "torch/_export/__init__.py": "import dataclasses\nimport io\nimport re\nimport pathlib\nimport weakref\nimport zipfile\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\nfrom unittest.mock import patch\n\nimport sympy\n\nimport torch\nimport torch._dynamo\nimport torch.fx\nimport torch.fx._pytree as fx_pytree\n\nimport torch.utils._pytree as pytree\nfrom torch._decomp import core_aten_decompositions, get_decompositions\nfrom torch._dispatch.python import enable_python_dispatcher\nfrom torch.export import Constraint, _create_constraint\nfrom torch._dynamo.exc import UserError, UserErrorType\nfrom torch._dynamo.source import ConstantSource\nfrom torch._export.exported_program import ModuleCallEntry, ModuleCallSignature\nfrom torch._export.passes.collect_tracepoints_pass import CollectTracepointsPass\nfrom torch._functorch.aot_autograd import aot_export_module\nfrom torch._functorch.eager_transforms import functionalize\nfrom torch._guards import detect_fake_mode\nfrom torch._ops import OpOverload\nfrom torch._subclasses.fake_tensor import FakeTensorMode\nfrom torch.fx import traceback as fx_traceback\nfrom torch.fx._compatibility import compatibility\nfrom torch.fx.experimental.proxy_tensor import make_fx\nfrom torch.fx.experimental.symbolic_shapes import (\n    ConstraintViolationError,\n    GuardOnDataDependentSymNode,\n    ShapeEnv,\n    StrictMinMaxConstraint,\n)\nfrom torch.fx.graph import _PyTreeCodeGen, _PyTreeInfo\nfrom torch.utils._sympy.value_ranges import ValueRangeError, ValueRanges\n\nfrom .exported_program import (\n    _process_constraints,\n    CallSpec,\n    combine_args_kwargs,\n    ExportBackwardSignature,\n    ExportedProgram,\n    ExportGraphSignature,\n)\nfrom .passes.add_runtime_assertions_for_constraints_pass import (\n    _AddRuntimeAssertionsForInlineConstraintsPass,\n)\nfrom .passes.replace_sym_size_ops_pass import _ReplaceSymSizeOpPass\nfrom .passes.replace_view_ops_with_view_copy_ops_pass import (\n    ReplaceViewOpsWithViewCopyOpsPass,\n)\nfrom .wrappers import _wrap_submodules\n\n\ndef dynamic_dim(t: torch.Tensor, index: int):\n    if not isinstance(t, torch.Tensor):\n        raise UserError(\n            UserErrorType.DYNAMIC_DIM,\n            f\"Expected tensor as input to dynamic_dim but got {type(t)}\"\n        )\n\n    if t.dim() < 1:\n        raise UserError(\n            UserErrorType.DYNAMIC_DIM,\n            \"Cannot mark 0-dimension tensors to be dynamic\"\n        )\n\n    if index >= t.dim():\n        raise UserError(\n            UserErrorType.DYNAMIC_DIM,\n            f\"Expected the dimension passed to dynamic_dim to be in the range [0:{t.dim()-1}]\"\n            f\" but got {index}, which is out of bounds for the given tensor.\"\n        )\n\n    return _create_constraint(\n        weakref.ref(t),\n        id(t),\n        index,\n        StrictMinMaxConstraint(\n            vr=ValueRanges(lower=2, upper=sympy.oo), warn_only=False\n        ),\n    )\n\n\n@dataclasses.dataclass\nclass ExportDynamoConfig:\n    \"\"\"\n    Manage Export-specific configurations of Dynamo.\n    \"\"\"\n    allow_rnn: bool = True\n\nDEFAULT_EXPORT_DYNAMO_CONFIG = ExportDynamoConfig()\n\n\nDECOMP_TABLE = core_aten_decompositions()\n\n\n# FIXME: actually migrate it to pre_autograd tracing\n@compatibility(is_backward_compatible=False)\ndef capture_pre_autograd_graph(\n    f: Callable,\n    args: Tuple[Any],\n    kwargs: Optional[Dict[str, Any]] = None,\n    constraints: Optional[List[Constraint]] = None,\n) -> torch.nn.Module:\n    \"\"\"\n    A helper function that is intended to trace a module before any pre-autograd\n    decomposition is run. The produced module will be \"non-functional\" and\n    composed of aten operators. Later this API will be deleted in favor of more general\n    torch.export API.\n\n    Args:\n      f: A callable to be traced\n\n      args: example positional inputs.\n\n      kwargs: optional example keyword inputs.\n\n      constraints: A optional list of constraints on the dynamic arguments specifying\n            their possible range of their shapes\n\n    Returns:\n        An nn.Module containing the traced method.\n\n    \"\"\"\n\n    decomp_table = {\n        torch.ops.aten.dropout.default: torch.ops.aten.dropout.default.decompose,\n        torch.ops.aten.batch_norm.default: torch.ops.aten.batch_norm.default.decompose,\n        torch.ops.aten._batch_norm_impl_index.default: torch.ops.aten._batch_norm_impl_index.default.decompose,\n        torch.ops.aten.native_batch_norm.default: torch.ops.aten.native_batch_norm.default.decompose,\n    }\n\n    if kwargs is None:\n        kwargs = {}\n\n    with torch._dynamo.config.patch(dataclasses.asdict(DEFAULT_EXPORT_DYNAMO_CONFIG)):  # type: ignore[attr-defined]\n        m = torch._dynamo.export(\n            f,\n            constraints=constraints,\n            assume_static_by_default=True,\n            tracing_mode=\"symbolic\",\n            decomposition_table=decomp_table,\n            pre_dispatch=True,\n            aten_graph=True,\n        )(\n            *args,\n            **kwargs,\n        )[0]\n\n        for n in m.graph.nodes:\n            n.meta[\"is_torch_exported\"] = True\n        return m\n\n\ndef _convert_input_to_fake(gm, args, kwargs):\n    fake_inps: List[torch.Tensor] = []\n    fake_mode = FakeTensorMode(\n        allow_fallback_kernels=False,\n        allow_non_fake_inputs=True,\n        shape_env=ShapeEnv(\n            assume_static_by_default=True,\n        ),\n    )\n\n    for node in gm.graph.nodes:\n        if node.op == \"placeholder\" and \"val\" in node.meta:\n            fake_val = node.meta[\"val\"]\n            if fake_val is not None and isinstance(fake_val, torch.Tensor):\n                fake_inps.append(fake_val)\n\n    if detected_fake_mode := detect_fake_mode(fake_inps):\n        fake_mode = detected_fake_mode\n\n    count = 0\n\n    def convert_to_fake(x):\n        nonlocal count\n        val = fake_inps[count]\n        count += 1\n        return val\n\n    fake_args = pytree.tree_map_only(torch.Tensor, convert_to_fake, args)\n    # TODO properly use the cached fake tensor\n    fake_kwargs = pytree.tree_map_only(torch.Tensor, fake_mode.from_tensor, kwargs)\n    return fake_args, fake_kwargs, fake_mode\n\n\ndef _safe_to_skip_dynamo(gm: torch.fx.GraphModule):\n    for node in gm.graph.nodes:\n        if \"is_torch_exported\" in node.meta:\n            return True\n    return False\n\n\ndef export(\n    f: Callable,\n    args: Tuple[Any, ...],\n    kwargs: Optional[Dict[str, Any]] = None,\n    constraints: Optional[List[Constraint]] = None,\n    *,\n    preserve_module_call_signature: Tuple[str, ...] = (),\n) -> ExportedProgram:\n    \"\"\"\n    Traces either an nn.Module's forward function or just a callable with PyTorch\n    operations inside and produce a ExportedProgram.\n\n    Args:\n        m: the `nn.Module` or callable to trace.\n\n        args: example positional inputs.\n\n        kwargs: optional example keyword inputs.\n\n        constraints: A optional list of constraints on the dynamic arguments specifying\n            their possible range of their shapes\n\n        preserve_module_call_signature: A list of submodule paths for which the original\n            calling conventions are preserved as metadata.\n\n    Returns:\n        An ExportedProgram containing the traced method.\n    \"\"\"\n    constraints = constraints or []\n    kwargs = kwargs or {}\n\n    if not isinstance(args, tuple):\n        raise UserError(UserErrorType.INVALID_INPUT,\n                        f\"Expecting `args` to be a tuple of example positional inputs, got {type(args)}\")\n\n    # We convert to nn.Module because __call__ of ExportedProgram\n    # is untracable right now.\n    if isinstance(f, ExportedProgram):\n        if len(constraints) > 0:\n            raise UserError(\n                UserErrorType.INVALID_INPUT,\n                \"Cannot provide constraints for already exported program.\"\n            )\n        f = f.module()\n\n    with torch._dynamo.config.patch(dataclasses.asdict(DEFAULT_EXPORT_DYNAMO_CONFIG)):  # type: ignore[attr-defined]\n        try:\n            module_call_signatures: Dict[str, ModuleCallSignature] = {}\n            # TODO Horrible hack to skip dynamo\n            if isinstance(f, torch.fx.GraphModule) and _safe_to_skip_dynamo(f):\n                if len(constraints) > 0:\n                    raise UserError(\n                        UserErrorType.INVALID_INPUT,\n                        \"Cannot provide constraints for already exported program.\"\n                    )\n                gm_torch_level = f\n            else:\n                with _wrap_submodules(f, preserve_module_call_signature, module_call_signatures):\n                    gm_torch_level, _ = torch._dynamo.export(\n                        f,\n                        constraints=constraints,\n                        assume_static_by_default=True,\n                        tracing_mode=\"symbolic\",\n                    )(\n                        *args,\n                        **kwargs,\n                    )\n        except (ConstraintViolationError, ValueRangeError) as e:\n            raise UserError(UserErrorType.CONSTRAIN_VIOLATION, str(e))\n        except GuardOnDataDependentSymNode as e:\n            raise UserError(\n                UserErrorType.ANTI_PATTERN,\n                f\"Consider annotating your code using constrain_as_*(). {str(e)}\")\n\n    params_buffers: OrderedDict[str, Union[torch.Tensor, torch.nn.Parameter]] = OrderedDict()\n    for name, param in gm_torch_level.named_parameters(recurse=True, remove_duplicate=False):\n        params_buffers[name] = param\n\n    for name, buffer in gm_torch_level.named_buffers(recurse=True, remove_duplicate=False):\n        params_buffers[name] = buffer\n\n    fake_args, fake_kwargs, fake_mode = _convert_input_to_fake(gm_torch_level, args, kwargs)\n\n    # First, we want to pass through the graph to try populating\n    # val field for getattr if there is anything missing.\n    # THis can happen when quantization adds extra params and forgets\n    # to update \"val\"\n    for node in gm_torch_level.graph.nodes:\n        if node.op == \"get_attr\" and \"val\" not in node.meta:\n            attr = getattr(gm_torch_level, node.target)\n            # Checks if it is not a HigherOrderOp branch or a module\n            if not isinstance(attr, torch.nn.Module):\n                node.meta[\"val\"] = fake_mode.from_tensor(attr, static_shapes=True)\n\n    # When aot_export lifts the params, we lose the nn_module_stack\n    # and source_fn from the param nodes as they are treated as fresh inputs\n    # Therefore, we manually extract them before calling into aot_export\n    params_buffers_to_node_meta = OrderedDict()\n    for node in gm_torch_level.graph.nodes:\n        target = node.target\n        meta = node.meta\n        if node.op == \"call_module\":\n            submodule = getattr(gm_torch_level, target)\n            if isinstance(submodule, torch.nn.Module):\n                for name, _ in submodule.named_parameters(recurse=True, remove_duplicate=False):\n                    params_buffers_to_node_meta[target + \".\" + name] = meta\n\n                for name, _ in submodule.named_buffers(recurse=True, remove_duplicate=False):\n                    params_buffers_to_node_meta[target + \".\" + name] = meta\n\n        if node.op == \"get_attr\":\n            submodule = getattr(gm_torch_level, target)\n            if not isinstance(submodule, torch.fx.GraphModule):\n                params_buffers_to_node_meta[target] = meta\n\n        # If the call_function uses param as input, we also need to update params' meta\n        # with this call_function node's meta.\n        # This is basically the same flow as torch.fx.traceback.preserve_meta()\n        if node.op == \"call_function\" and not isinstance(node.target, torch._ops.HigherOrderOperator):\n            for arg in node._input_nodes:\n                if arg.op == \"get_attr\":\n                    for entry in torch.fx.proxy._COPY_META_FIELDS:\n                        if entry in meta:\n                            params_buffers_to_node_meta[arg.target][entry] = meta[entry]\n\n    # Fix the graph output signature to be tuple if scalar\n    out_spec = orig_out_spec = gm_torch_level._out_spec\n    # aot_export expect the return type to always be a tuple.\n    if out_spec.type not in (list, tuple):\n        out_spec = pytree.TreeSpec(tuple, None, [out_spec])\n\n    orig_args = gm_torch_level.graph._codegen.pytree_info.orig_args  # type: ignore[attr-defined]\n\n    gm_torch_level.graph._codegen = _PyTreeCodeGen(\n        _PyTreeInfo(\n            orig_args,\n            gm_torch_level._in_spec,\n            out_spec,\n        )\n    )\n    gm_torch_level.recompile()\n\n    # Note: aot_export_module doesn't accept kwargs, we'd like to reorder the kwargs as an OrderedDict\n    # to follow the order in orig_args and correctly call gm_torch_level\n    gm, graph_signature = aot_export_module(\n        gm_torch_level,\n        (*fake_args, *_reorder_kwargs_by_names(orig_args, fake_args, fake_kwargs).values()),\n        decompositions=DECOMP_TABLE,\n        trace_joint=False\n    )\n\n    export_backward_signature = ExportBackwardSignature(\n        gradients_to_parameters=graph_signature.backward_signature.gradients_to_parameters,\n        gradients_to_user_inputs=graph_signature.backward_signature.gradients_to_user_inputs,\n        loss_output=graph_signature.backward_signature.loss_output\n    ) if graph_signature.backward_signature is not None else None\n\n    def to_str_list(sig_component: List[Any]):\n        return [str(v) for v in sig_component]\n\n    def to_str_dict(sig_component: Dict[Any, Any]):\n        return {str(k): str(v) for k, v in sig_component.items()}\n\n    export_graph_signature = ExportGraphSignature(\n        parameters=to_str_list(graph_signature.parameters),\n        buffers=to_str_list(graph_signature.buffers),\n        user_inputs=to_str_list(graph_signature.user_inputs),\n        user_outputs=to_str_list(graph_signature.user_outputs),\n        inputs_to_parameters=to_str_dict(graph_signature.inputs_to_parameters),\n        inputs_to_buffers=to_str_dict(graph_signature.inputs_to_buffers),\n        buffers_to_mutate=to_str_dict(graph_signature.buffers_to_mutate),\n        backward_signature=export_backward_signature\n    )\n\n    # NOTE: aot_export adds symint metadata for placeholders with int values;\n    # since these become specialized, we replace such metadata with the original values\n    flat_args, in_spec = pytree.tree_flatten(combine_args_kwargs(args, kwargs))\n    index = 0\n    total_param_buffers = len(graph_signature.parameters) + len(graph_signature.buffers)\n    for node in gm.graph.nodes:\n        if node.op == \"placeholder\":\n            if index >= total_param_buffers:\n                user_arg = flat_args[index - total_param_buffers]\n                if not isinstance(user_arg, torch.Tensor):\n                    node.meta[\"val\"] = user_arg\n            index += 1\n\n    # TODO unfortunately preserving graph-level metadata is not\n    # working well with aot_export. So we manually copy it.\n    # (The node-level meta is addressed above.)\n    gm.meta.update(gm_torch_level.meta)\n\n    # The unbacked symint symbols are updated in aot_export\n    # so we serialize them here instead of inside dynamo\n    gm.meta[\"inline_constraints\"] = {\n        k: v\n        for k, v in fake_mode.shape_env.runtime_var_to_range.items()\n        if re.match(r\"^[if]\\d+$\", str(k))\n    }\n\n    # After aot_export, set the param/buffer metadata back into placeholders\n    # Technically, users can still construct this data from param names\n    # without relying on this metadata\n    for node in gm.graph.nodes:\n        if node.op == \"placeholder\":\n            if node.target in export_graph_signature.inputs_to_parameters:\n                param_name = export_graph_signature.inputs_to_parameters[node.target]\n                if param_name in params_buffers_to_node_meta:\n                    for k, v in params_buffers_to_node_meta[param_name].items():\n                        node.meta[k] = v\n            if node.target in export_graph_signature.inputs_to_buffers:\n                buffer_name = export_graph_signature.inputs_to_buffers[node.target]\n                if buffer_name in params_buffers_to_node_meta:\n                    for k, v in params_buffers_to_node_meta[buffer_name].items():\n                        node.meta[k] = v\n\n        node.meta[\"is_torch_exported\"] = True\n\n    range_constraints, equality_constraints = _process_constraints(\n        gm,\n        export_graph_signature,\n        flat_args,\n    )\n    assert orig_out_spec is not None\n    exported_program = ExportedProgram(\n        gm,\n        gm.graph,\n        export_graph_signature,\n        CallSpec(in_spec, orig_out_spec),\n        params_buffers,\n        range_constraints,\n        equality_constraints,\n        [ModuleCallEntry(fqn, sig) for fqn, sig in module_call_signatures.items()],\n        (args, {}),\n    )\n\n    exported_program = exported_program._transform(\n        _AddRuntimeAssertionsForInlineConstraintsPass(range_constraints, equality_constraints)\n    )\n    if len(preserve_module_call_signature) > 0:\n        exported_program = exported_program._transform(CollectTracepointsPass(module_call_signatures))\n    return exported_program._transform(_ReplaceSymSizeOpPass())\n\n\ndef _reorder_kwargs_by_names(arg_names: List[str], args: Tuple[Any], kwargs: Dict[str, Any]):\n    assert len(arg_names) == len(args) + len(kwargs), (\n        f\"Total number of arg names is expected to be {len(arg_names)} \"\n        f\"but got {len(args)} positional args, {len(kwargs)} kwargs.\"\n    )\n    return OrderedDict({kw_name: kwargs[kw_name] for kw_name in arg_names[len(args):]})\n\n\ndef save(\n    ep: ExportedProgram,\n    f: Union[str, pathlib.Path, io.BytesIO],\n    *,\n    extra_files: Optional[Dict[str, Any]] = None,\n    opset_version: Optional[Dict[str, int]] = None,\n) -> None:\n    from .serde.serialize import serialize\n    from .serde.schema import SCHEMA_VERSION\n    serialized_program, serialized_state_dict = serialize(ep, opset_version)\n\n    if isinstance(f, (str, pathlib.Path)):\n        f = str(f)\n\n    with zipfile.ZipFile(f, 'w') as zipf:\n        # Save serialized_ep and serialized_state_dict to the zip file\n        zipf.writestr('serialized_exported_program.json', serialized_program)\n        zipf.writestr('serialized_state_dict.json', serialized_state_dict)\n        zipf.writestr('version', str(SCHEMA_VERSION))\n\n        # Add extra files if provided\n        if extra_files:\n            for extra_file_name, content in extra_files.items():\n                encoded_content = content.encode('utf-8')\n                zipf.writestr(f\"extra_files/{extra_file_name}\", encoded_content)\n\n\ndef load(\n    f: Union[str, pathlib.Path, io.BytesIO],\n    *,\n    extra_files: Optional[Dict[str, Any]] = None,\n    expected_opset_version: Optional[Dict[str, int]] = None,\n) -> ExportedProgram:\n    if isinstance(f, (str, pathlib.Path)):\n        f = str(f)\n\n    with zipfile.ZipFile(f, 'r') as zipf:\n        # Check the version\n        version = int(zipf.read('version'))\n        from .serde.schema import SCHEMA_VERSION\n\n        if version != SCHEMA_VERSION:\n            raise RuntimeError(\n                f\"Serialized version {version} does not match our current \"\n                f\"schema version {SCHEMA_VERSION}.\"\n            )\n\n        # Load serialized_ep and serialized_state_dict from the zip file\n        serialized_ep = zipf.read('serialized_exported_program.json')\n        serialized_state_dict = zipf.read('serialized_state_dict.json')\n\n        # Deserialize ExportedProgram\n        from .serde.serialize import deserialize\n        ep = deserialize(serialized_ep, serialized_state_dict, expected_opset_version)\n\n        # Populate extra_files map\n        if extra_files is not None:\n            for filename in extra_files.keys():\n                extra_files[filename] = zipf.read(f\"extra_files/{filename}\").decode('utf-8')\n\n        return ep\n\n\ndef aot_compile(\n    f: Callable,\n    args: Tuple[Any],\n    kwargs: Optional[Dict[str, Any]] = None,\n    constraints: Optional[List[Constraint]] = None,\n    options: Optional[Dict[str, Any]] = None,\n) -> Tuple[str, ExportedProgram]:\n    \"\"\"\n    Note: this function is not stable yet\n\n    Traces either an nn.Module's forward function or just a callable with PyTorch\n    operations inside, generates executable cpp code from the program, and returns\n    the path to the generated shared library\n\n    Args:\n        f: the `nn.Module` or callable to trace.\n\n        args: example positional inputs.\n\n        kwargs: optional example keyword inputs.\n\n        constraints: A optional list of constraints on the dynamic arguments specifying\n            their possible range of their shapes\n\n        options: A dictionary of options to control inductor\n\n    Returns:\n        Path to the generated shared library, and the exported program\n    \"\"\"\n    from torch._inductor.compile_fx import compile_fx_aot\n    from torch._inductor.decomposition import select_decomp_table\n\n    global DECOMP_TABLE\n    DECOMP_TABLE = select_decomp_table()\n    ep = export(f, args, kwargs, constraints)\n    # Reset the global value\n    DECOMP_TABLE = core_aten_decompositions()\n\n    param_buffer_values = list(ep.state_dict.values())\n    flat_example_inputs = fx_pytree.tree_flatten_spec(\n        combine_args_kwargs(args, kwargs), ep.call_spec.in_spec  # type: ignore[arg-type]\n    )\n    all_args = (*param_buffer_values, *flat_example_inputs)\n\n    so_path = torch._inductor.aot_compile(ep.graph_module, list(all_args), options)\n    return so_path, ep\n",
      "functorch/__init__.py": "# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the BSD-style license found in the\n# LICENSE file in the root directory of this source tree.\nimport torch\n\nfrom torch._functorch.deprecated import (\n    combine_state_for_ensemble,\n    functionalize,\n    grad,\n    grad_and_value,\n    hessian,\n    jacfwd,\n    jacrev,\n    jvp,\n    make_functional,\n    make_functional_with_buffers,\n    vjp,\n    vmap,\n)\n\n# utilities. Maybe these should go in their own namespace in the future?\nfrom torch._functorch.make_functional import (\n    FunctionalModule,\n    FunctionalModuleWithBuffers,\n)\n\n# Top-level APIs. Please think carefully before adding something to the\n# top-level namespace:\n# - private helper functions should go into torch._functorch\n# - very experimental things should go into functorch.experimental\n# - compilation related things should go into functorch.compile\n\n# Was never documented\nfrom torch._functorch.python_key import make_fx\n\n__version__ = torch.__version__\n",
      "torch/_VF.py": "\"\"\"\nThis makes the functions in torch._C._VariableFunctions available as\n    torch._VF.<funcname>\nwithout mypy being able to find them.\n\nA subset of those functions are mapped to ATen functions in\ntorch/jit/_builtins.py\n\nSee https://github.com/pytorch/pytorch/issues/21478 for the reason for\nintroducing torch._VF\n\n\"\"\"\nimport sys\nimport types\n\nimport torch\n\n\nclass VFModule(types.ModuleType):\n    vf: types.ModuleType\n\n    def __init__(self, name):\n        super().__init__(name)\n        self.vf = torch._C._VariableFunctions\n\n    def __getattr__(self, attr):\n        return getattr(self.vf, attr)\n\n\nsys.modules[__name__] = VFModule(__name__)\n",
      "torch/__config__.py": "import torch\n\n\ndef show():\n    \"\"\"\n    Return a human-readable string with descriptions of the\n    configuration of PyTorch.\n    \"\"\"\n    return torch._C._show_config()\n\n\n# TODO: In principle, we could provide more structured version/config\n# information here. For now only CXX_FLAGS is exposed, as Timer\n# uses them.\ndef _cxx_flags():\n    \"\"\"Returns the CXX_FLAGS used when building PyTorch.\"\"\"\n    return torch._C._cxx_flags()\n\n\ndef parallel_info():\n    r\"\"\"Returns detailed string with parallelization settings\"\"\"\n    return torch._C._parallel_info()\n",
      "torch/__future__.py": "\"\"\"\nThis global flag controls whether to assign new tensors to the parameters\ninstead of changing the existing parameters in-place when converting an `nn.Module`\nusing the following methods:\n1. `module.cuda()` / `.cpu()` (for moving `module` between devices)\n2. `module.float()` / `.double()` / `.half()` (for converting `module` to a different dtype)\n3. `module.to()` / `.type()` (for changing `module`'s device or dtype)\n4. `module._apply(fn)` (for generic functions applied to `module`)\n\nDefault: False\n\"\"\"\n_overwrite_module_params_on_conversion = False\n\n\ndef set_overwrite_module_params_on_conversion(value):\n    global _overwrite_module_params_on_conversion\n    _overwrite_module_params_on_conversion = value\n\n\ndef get_overwrite_module_params_on_conversion():\n    return _overwrite_module_params_on_conversion\n",
      "torch/__init__.py": "\nr\"\"\"\nThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\n\nIt has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0.\n\"\"\"\n\nimport math\nimport os\nimport sys\nimport platform\nimport textwrap\nimport ctypes\nimport inspect\n\n# multipy/deploy is setting this import before importing torch, this is the most\n# reliable way we have to detect if we're running within deploy.\n# https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137\ndef _running_with_deploy():\n    return sys.modules.get(\"torch._meta_registrations\", None) is object\n\nfrom ._utils import _import_dotted_name, classproperty\nfrom ._utils_internal import get_file_path, prepare_multiprocessing_environment, \\\n    USE_RTLD_GLOBAL_WITH_LIBTORCH, USE_GLOBAL_DEPS\n\n# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\nif _running_with_deploy():\n    __version__ = \"torch-deploy-1.8\"\nelse:\n    from .torch_version import __version__ as __version__\n\nfrom typing import Any, Callable, Dict, Optional, Set, Tuple, Type, TYPE_CHECKING, Union, List\nimport builtins\n\n__all__ = [\n    'typename', 'is_tensor', 'is_storage',\n    'set_default_tensor_type', 'set_default_device',\n    'set_rng_state', 'get_rng_state', 'manual_seed', 'initial_seed', 'seed',\n    'save', 'load', 'set_printoptions', 'chunk', 'split', 'stack', 'matmul',\n    'no_grad', 'enable_grad', 'rand', 'randn', 'inference_mode',\n    'DoubleStorage', 'FloatStorage', 'LongStorage', 'IntStorage',\n    'ShortStorage', 'CharStorage', 'ByteStorage', 'BoolStorage',\n    'TypedStorage', 'UntypedStorage',\n    'DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',\n    'ShortTensor', 'CharTensor', 'ByteTensor', 'BoolTensor', 'Tensor',\n    'lobpcg', 'use_deterministic_algorithms',\n    'are_deterministic_algorithms_enabled',\n    'is_deterministic_algorithms_warn_only_enabled',\n    'set_deterministic_debug_mode', 'get_deterministic_debug_mode',\n    'set_float32_matmul_precision', 'get_float32_matmul_precision',\n    'set_warn_always', 'is_warn_always_enabled', 'SymInt', 'SymFloat',\n    'SymBool', 'sym_not',\n    'sym_int', 'sym_float', 'sym_max', 'sym_min', 'compile', 'vmap',\n    'export',\n]\n\n################################################################################\n# Load the extension module\n################################################################################\n\nif sys.platform == 'win32':\n    pfiles_path = os.getenv('ProgramFiles', 'C:\\\\Program Files')\n    py_dll_path = os.path.join(sys.exec_prefix, 'Library', 'bin')\n    th_dll_path = os.path.join(os.path.dirname(__file__), 'lib')\n\n    # When users create a virtualenv that inherits the base environment,\n    # we will need to add the corresponding library directory into\n    # DLL search directories. Otherwise, it will rely on `PATH` which\n    # is dependent on user settings.\n    if sys.exec_prefix != sys.base_exec_prefix:\n        base_py_dll_path = os.path.join(sys.base_exec_prefix, 'Library', 'bin')\n    else:\n        base_py_dll_path = ''\n\n    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path]))\n\n    if all(not os.path.exists(os.path.join(p, 'nvToolsExt64_1.dll')) for p in dll_paths):\n        nvtoolsext_dll_path = os.path.join(\n            os.getenv('NVTOOLSEXT_PATH', os.path.join(pfiles_path, 'NVIDIA Corporation', 'NvToolsExt')), 'bin', 'x64')\n    else:\n        nvtoolsext_dll_path = ''\n\n    from .version import cuda as cuda_version\n    import glob\n    if cuda_version and all(not glob.glob(os.path.join(p, 'cudart64*.dll')) for p in dll_paths):\n        cuda_version_1 = cuda_version.replace('.', '_')\n        cuda_path_var = 'CUDA_PATH_V' + cuda_version_1\n        default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)\n        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')\n    else:\n        cuda_path = ''\n\n    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))\n\n    kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)\n    with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')\n    prev_error_mode = kernel32.SetErrorMode(0x0001)\n\n    kernel32.LoadLibraryW.restype = ctypes.c_void_p\n    if with_load_library_flags:\n        kernel32.LoadLibraryExW.restype = ctypes.c_void_p\n\n    for dll_path in dll_paths:\n        os.add_dll_directory(dll_path)\n\n    try:\n        ctypes.CDLL('vcruntime140.dll')\n        ctypes.CDLL('msvcp140.dll')\n        ctypes.CDLL('vcruntime140_1.dll')\n    except OSError:\n        print('''Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe''')\n\n    dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))\n    path_patched = False\n    for dll in dlls:\n        is_loaded = False\n        if with_load_library_flags:\n            res = kernel32.LoadLibraryExW(dll, None, 0x00001100)\n            last_error = ctypes.get_last_error()\n            if res is None and last_error != 126:\n                err = ctypes.WinError(last_error)\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n            elif res is not None:\n                is_loaded = True\n        if not is_loaded:\n            if not path_patched:\n                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])\n                path_patched = True\n            res = kernel32.LoadLibraryW(dll)\n            if res is None:\n                err = ctypes.WinError(ctypes.get_last_error())\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n\n    kernel32.SetErrorMode(prev_error_mode)\n\n\ndef _preload_cuda_deps(lib_folder, lib_name):\n    \"\"\"Preloads cuda deps if they could not be found otherwise.\"\"\"\n    # Should only be called on Linux if default path resolution have failed\n    assert platform.system() == 'Linux', 'Should only be called on Linux'\n    import glob\n    lib_path = None\n    for path in sys.path:\n        nvidia_path = os.path.join(path, 'nvidia')\n        if not os.path.exists(nvidia_path):\n            continue\n        candidate_lib_paths = glob.glob(os.path.join(nvidia_path, lib_folder, 'lib', lib_name))\n        if candidate_lib_paths and not lib_path:\n            lib_path = candidate_lib_paths[0]\n        if lib_path:\n            break\n    if not lib_path:\n        raise ValueError(f\"{lib_name} not found in the system path {sys.path}\")\n    ctypes.CDLL(lib_path)\n\n\n# See Note [Global dependencies]\ndef _load_global_deps() -> None:\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return\n\n    lib_name = 'libtorch_global_deps' + ('.dylib' if platform.system() == 'Darwin' else '.so')\n    here = os.path.abspath(__file__)\n    lib_path = os.path.join(os.path.dirname(here), 'lib', lib_name)\n\n    try:\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n    except OSError as err:\n        # Can only happen for wheel with cuda libs as PYPI deps\n        # As PyTorch is not purelib, but nvidia-*-cu11 is\n        cuda_libs: Dict[str, str] = {\n            'cublas': 'libcublas.so.*[0-9]',\n            'cudnn': 'libcudnn.so.*[0-9]',\n            'cuda_nvrtc': 'libnvrtc.so.*[0-9].*[0-9]',\n            'cuda_runtime': 'libcudart.so.*[0-9].*[0-9]',\n            'cuda_cupti': 'libcupti.so.*[0-9].*[0-9]',\n            'cufft': 'libcufft.so.*[0-9]',\n            'curand': 'libcurand.so.*[0-9]',\n            'cusolver': 'libcusolver.so.*[0-9]',\n            'cusparse': 'libcusparse.so.*[0-9]',\n            'nccl': 'libnccl.so.*[0-9]',\n            'nvtx': 'libnvToolsExt.so.*[0-9]',\n        }\n        is_cuda_lib_err = [lib for lib in cuda_libs.values() if(lib.split('.')[0] in err.args[0])]\n        if not is_cuda_lib_err:\n            raise err\n        for lib_folder, lib_name in cuda_libs.items():\n            _preload_cuda_deps(lib_folder, lib_name)\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n\n\nif (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv('TORCH_USE_RTLD_GLOBAL')) and \\\n        (_running_with_deploy() or platform.system() != 'Windows'):\n    # Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a\n    # few circumstances:\n    #\n    #   1. You're in a build environment (e.g., fbcode) where\n    #      libtorch_global_deps is not available, but you still need\n    #      to get mkl to link in with RTLD_GLOBAL or it will just\n    #      not work.\n    #\n    #   2. You're trying to run PyTorch under UBSAN and you need\n    #      to ensure that only one copy of libtorch is loaded, so\n    #      vptr checks work properly\n    #\n    # If you're using this setting, you must verify that all the libraries\n    # you load consistently use the same libstdc++, or you may have\n    # mysterious segfaults.\n    #\n    old_flags = sys.getdlopenflags()\n    sys.setdlopenflags(os.RTLD_GLOBAL | os.RTLD_LAZY)\n    from torch._C import *  # noqa: F403\n    sys.setdlopenflags(old_flags)\n    del old_flags\n\nelse:\n    # Easy way.  You want this most of the time, because it will prevent\n    # C++ symbols from libtorch clobbering C++ symbols from other\n    # libraries, leading to mysterious segfaults.\n    #\n    # If building in an environment where libtorch_global_deps isn't available\n    # like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will\n    # want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False\n    #\n    # See Note [Global dependencies]\n    if USE_GLOBAL_DEPS:\n        _load_global_deps()\n    from torch._C import *  # noqa: F403\n\n# Appease the type checker; ordinarily this binding is inserted by the\n# torch._C module initialization code in C\nif TYPE_CHECKING:\n    import torch._C as _C\n\nclass SymInt:\n    \"\"\"\n    Like an int (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return builtins.bool(self != 0)\n\n    def __int__(self):\n        return self.node.int_()\n\n    def __index__(self):\n        return self.node.int_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_float__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return str(self.node)\n\nclass SymFloat:\n    \"\"\"\n    Like an float (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_int__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\nclass SymBool:\n    \"\"\"\n    Like an bool (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n\n    Unlike regular bools, regular boolean operators will force extra guards instead\n    of symbolically evaluate.  Use the bitwise operators instead to handle this.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    def __int__(self):\n        return builtins.int(self.node.bool_())\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n    def __and__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __or__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    # We very carefully define __sym_not__, and not a number of other\n    # plausible alternatives:\n    #\n    #   - We do not override __not__ because this is not a real magic\n    #     method; you cannot override the meaning of the not builtin in\n    #     Python.  We use the name 'sym_not' to clarify that in user code you\n    #     cannot use the builtin not or operator.not_ or operator.__not__ and\n    #     hit this magic method; you must use our custom sym_not operator.\n    #\n    #   - We do not override the __invert__ method because SymBool is\n    #     meant to be usable in situations where bool is expected.  However,\n    #     bitwise negation ~a does the wrong thing with booleans (because\n    #     bool is a subclass of int, so ~1 = -2 which is not falseish.)\n    #     This would be a giant footgun, so we get around it by defining\n    #     our own operator.  Note that bitwise and/or do the right thing,\n    #     so we reuse the conventional operators there for readability.\n    #\n    def __sym_not__(self) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\ndef sym_not(a):\n    r\"\"\" SymInt-aware utility for logical negation.\n\n    Args:\n        a (SymBool or bool): Object to negate\n    \"\"\"\n    if hasattr(a, '__sym_not__'):\n        return a.__sym_not__()\n    return not a\n\ndef sym_float(a):\n    r\"\"\" SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymFloat):\n        return a\n    elif hasattr(a, '__sym_float__'):\n        return a.__sym_float__()\n    return py_float(a)  # type: ignore[operator]\n\n\ndef sym_int(a):\n    r\"\"\" SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymInt):\n        return a\n    elif isinstance(a, SymFloat):\n        return math.floor(a) if a >= 0 else math.ceil(a)  # type: ignore[arg-type, call-overload]\n    return py_int(a)  # type: ignore[operator]\n\ndef sym_max(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_max__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        # NB: If you actually care about preserving output type exactly\n        # if you do something like max(0, 0.0), it is NOT sound to treat\n        # min/max as commutative\n        return b.__sym_max__(a)\n    return builtins.max(a, b)  # type: ignore[operator]\n\ndef sym_min(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_min__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        return b.__sym_min__(a)\n    return builtins.min(a, b)  # type: ignore[operator]\n\n# Check to see if we can load C extensions, and if not provide some guidance\n# on what the problem might be.\ntry:\n    # _initExtension is chosen (arbitrarily) as a sentinel.\n    from torch._C import _initExtension\nexcept ImportError:\n    import torch._C as _C_for_compiled_check\n\n    # The __file__ check only works for Python 3.7 and above.\n    if _C_for_compiled_check.__file__ is None:\n        raise ImportError(textwrap.dedent('''\n            Failed to load PyTorch C extensions:\n                It appears that PyTorch has loaded the `torch/_C` folder\n                of the PyTorch repository rather than the C extensions which\n                are expected in the `torch._C` namespace. This can occur when\n                using the `install` workflow. e.g.\n                    $ python setup.py install && python -c \"import torch\"\n\n                This error can generally be solved using the `develop` workflow\n                    $ python setup.py develop && python -c \"import torch\"  # This should succeed\n                or by running Python from a different directory.\n            ''').strip()) from None\n    raise  # If __file__ is not None the cause is unknown, so just re-raise.\n\nfor name in dir(_C):\n    if name[0] != '_' and not name.endswith('Base'):\n        __all__.append(name)\n        obj = getattr(_C, name)\n        if (isinstance(obj, Callable) or inspect.isclass(obj)):  # type: ignore[arg-type]\n            if (obj.__module__ != 'torch'):\n                # TODO: fix their module from C++ side\n                if name not in ['DisableTorchFunctionSubclass', 'DisableTorchFunction', 'Generator']:\n                    obj.__module__ = 'torch'\n\nif not TYPE_CHECKING:\n    # issue 38137 and python issue 43367. Submodules of a C extension are\n    # non-standard, and attributes of those submodules cannot be pickled since\n    # pickle expect to be able to import them as \"from _C.sub import attr\"\n    # which fails with \"_C is not a package\n    for attr in dir(_C):\n        candidate = getattr(_C, attr)\n        if type(candidate) is type(_C):\n            # submodule\n            if f'torch._C.{attr}' not in sys.modules:\n                sys.modules[f'torch._C.{attr}'] = candidate\n\n\n################################################################################\n# Define basic utilities\n################################################################################\n\n\ndef typename(o):\n    if isinstance(o, torch.Tensor):\n        return o.type()\n\n    module = ''\n    class_name = ''\n    if hasattr(o, '__module__') and o.__module__ != 'builtins' \\\n            and o.__module__ != '__builtin__' and o.__module__ is not None:\n        module = o.__module__ + '.'\n\n    if hasattr(o, '__qualname__'):\n        class_name = o.__qualname__\n    elif hasattr(o, '__name__'):\n        class_name = o.__name__\n    else:\n        class_name = o.__class__.__name__\n\n    return module + class_name\n\n\ndef is_tensor(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch tensor.\n\n    Note that this function is simply doing ``isinstance(obj, Tensor)``.\n    Using that ``isinstance`` check is better for typechecking with mypy,\n    and more explicit - so it's recommended to use that instead of\n    ``is_tensor``.\n\n    Args:\n        obj (Object): Object to test\n    Example::\n\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.is_tensor(x)\n        True\n\n    \"\"\"\n    return isinstance(obj, torch.Tensor)\n\n\ndef is_storage(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch storage object.\n\n    Args:\n        obj (Object): Object to test\n    \"\"\"\n    return type(obj) in _storage_classes\n\n\n_GLOBAL_DEVICE_CONTEXT = None\n\ndef set_default_device(device):\n    \"\"\"Sets the default ``torch.Tensor`` to be allocated on ``device``.  This\n    does not affect factory function calls which are called with an explicit\n    ``device`` argument.  Factory calls will be performed as if they\n    were passed ``device`` as an argument.\n\n    To only temporarily change the default device instead of setting it\n    globally, use ``with torch.device(device):`` instead.\n\n    The default device is initially ``cpu``.  If you set the default tensor\n    device to another device (e.g., ``cuda``) without a device index, tensors\n    will be allocated on whatever the current device for the device type,\n    even after :func:`torch.cuda.set_device` is called.\n\n    .. warning::\n\n        This function imposes a slight performance cost on every Python\n        call to the torch API (not just factory functions).  If this\n        is causing problems for you, please comment on\n        https://github.com/pytorch/pytorch/issues/92701\n\n    Args:\n        device (device or string): the device to set as default\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"requires cuda, changes global state\")\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cpu')\n        >>> torch.set_default_device('cuda')  # current device is 0\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=0)\n        >>> torch.set_default_device('cuda:1')\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=1)\n\n    \"\"\"\n    global _GLOBAL_DEVICE_CONTEXT\n    if _GLOBAL_DEVICE_CONTEXT is not None:\n        _GLOBAL_DEVICE_CONTEXT.__exit__(None, None, None)\n    if device is None:\n        _GLOBAL_DEVICE_CONTEXT = None\n        return\n    from torch.utils._device import DeviceContext\n    _GLOBAL_DEVICE_CONTEXT = DeviceContext(device)\n    _GLOBAL_DEVICE_CONTEXT.__enter__()\n\n\ndef set_default_tensor_type(t):\n    r\"\"\"Sets the default ``torch.Tensor`` type to floating point tensor type\n    ``t``. This type will also be used as default floating point type for\n    type inference in :func:`torch.tensor`.\n\n    The default floating point tensor type is initially ``torch.FloatTensor``.\n\n    Args:\n        t (type or string): the floating point tensor type or its name\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\n        torch.float32\n        >>> torch.set_default_tensor_type(torch.DoubleTensor)\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n\n    \"\"\"\n    if isinstance(t, str):\n        t = _import_dotted_name(t)\n    _C._set_default_tensor_type(t)\n\n\ndef set_default_dtype(d):\n    r\"\"\"\n\n    Sets the default floating point dtype to :attr:`d`. Supports torch.float32\n    and torch.float64 as inputs. Other dtypes may be accepted without complaint\n    but are not supported and are unlikely to work as expected.\n\n    When PyTorch is initialized its default floating point dtype is torch.float32,\n    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like\n    type inference. The default floating point dtype is used to:\n\n    1. Implicitly determine the default complex dtype. When the default floating point\n       type is float32 the default complex dtype is complex64, and when the default\n       floating point type is float64 the default complex type is complex128.\n    2. Infer the dtype for tensors constructed using Python floats or complex Python\n       numbers. See examples below.\n    3. Determine the result of type promotion between bool and integer tensors and\n       Python floats and complex Python numbers.\n\n    Args:\n        d (:class:`torch.dtype`): the floating point dtype to make the default.\n                                  Either torch.float32 or torch.float64.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> # initial default for floating point is torch.float32\n        >>> # Python floats are interpreted as float32\n        >>> torch.tensor([1.2, 3]).dtype\n        torch.float32\n        >>> # initial default for floating point is torch.complex64\n        >>> # Complex Python numbers are interpreted as complex64\n        >>> torch.tensor([1.2, 3j]).dtype\n        torch.complex64\n\n        >>> torch.set_default_dtype(torch.float64)\n\n        >>> # Python floats are now interpreted as float64\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\n        torch.complex128\n\n    \"\"\"\n    _C._set_default_dtype(d)\n\ndef use_deterministic_algorithms(mode: builtins.bool, *, warn_only: builtins.bool = False) -> None:\n    r\"\"\" Sets whether PyTorch operations must use \"deterministic\"\n    algorithms. That is, algorithms which, given the same input, and when\n    run on the same software and hardware, always produce the same output.\n    When enabled, operations will use deterministic algorithms when available,\n    and if only nondeterministic algorithms are available they will throw a\n    :class:`RuntimeError` when called.\n\n    .. note:: This setting alone is not always enough to make an application\n        reproducible. Refer to :ref:`reproducibility` for more information.\n\n    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative\n        interface for this feature.\n\n    The following normally-nondeterministic operations will act\n    deterministically when ``mode=True``:\n\n        * :class:`torch.nn.Conv1d` when called on CUDA tensor\n        * :class:`torch.nn.Conv2d` when called on CUDA tensor\n        * :class:`torch.nn.Conv3d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor\n        * :func:`torch.bmm` when called on sparse-dense CUDA tensors\n        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor\n          and the index is a list of tensors\n        * :func:`torch.Tensor.index_put` with ``accumulate=False``\n        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor\n        * :func:`torch.gather` when called on a CUDA tensor that requires grad\n        * :func:`torch.index_add` when called on CUDA tensor\n        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor\n        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor\n        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor\n        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='sum'`` or ``reduce='mean'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_`, when called with a tensor that is not\n          quantized, sets new elements to a known value.  Floating point or\n          complex values are set to NaN. Integer values are set to the maximum\n          value.\n        * :func:`torch.empty`, :func:`torch.empty_like`, :func:`torch.empty_strided`,\n          and :func:`torch.empty_permuted` will fill the output tensor with a known\n          value. Floating point or complex dtype tensors are filled with NaN. Integer\n          dtype tensors are filled with the maximum value.\n\n    The following normally-nondeterministic operations will throw a\n    :class:`RuntimeError` when ``mode=True``:\n\n        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxUnpool1d`\n        * :class:`torch.nn.MaxUnpool2d`\n        * :class:`torch.nn.MaxUnpool3d`\n        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor\n          and one of the following modes is used:\n\n          - ``linear``\n          - ``bilinear``\n          - ``bicubic``\n          - ``trilinear``\n\n        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor\n        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when\n          ``mode='max'``\n        * :func:`torch.Tensor.put_` when ``accumulate=False``\n        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor\n        * :func:`torch.histc` when called on a CUDA tensor\n        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``\n          tensor is given\n        * :func:`torch.kthvalue` with called on a CUDA tensor\n        * :func:`torch.median` with indices output when called on a CUDA tensor\n        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor\n        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='prod'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_` when called with a quantized tensor\n\n    A handful of CUDA operations are nondeterministic if the CUDA version is\n    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``\n    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more\n    details: `<https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility>`_\n    If one of these environment variable configurations is not set, a :class:`RuntimeError`\n    will be raised from these operations when called with CUDA tensors:\n\n        * :func:`torch.mm`\n        * :func:`torch.mv`\n        * :func:`torch.bmm`\n\n    Note that deterministic operations tend to have worse performance than\n    nondeterministic operations.\n\n    .. note::\n\n        This flag does not detect or prevent nondeterministic behavior caused\n        by calling an inplace operation on a tensor with an internal memory\n        overlap or by giving such a tensor as the :attr:`out` argument for an\n        operation. In these cases, multiple writes of different data may target\n        a single memory location, and the order of writes is not guaranteed.\n\n    Args:\n        mode (:class:`bool`): If True, makes potentially nondeterministic\n            operations switch to a deterministic algorithm or throw a runtime\n            error. If False, allows nondeterministic operations.\n\n    Keyword args:\n        warn_only (:class:`bool`, optional): If True, operations that do not\n            have a deterministic implementation will throw a warning instead of\n            an error. Default: ``False``\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> torch.use_deterministic_algorithms(True)\n\n        # Forward mode nondeterministic error\n        >>> torch.randn(10, device='cuda').kthvalue(0)\n        ...\n        RuntimeError: kthvalue CUDA does not have a deterministic implementation...\n\n        # Backward mode nondeterministic error\n        >>> torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()\n        ...\n        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...\n    \"\"\"\n    _C._set_deterministic_algorithms(mode, warn_only=warn_only)\n\ndef are_deterministic_algorithms_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is turned on. Refer to\n    :func:`torch.use_deterministic_algorithms` documentation for more details.\n    \"\"\"\n    return _C._get_deterministic_algorithms()\n\ndef is_deterministic_algorithms_warn_only_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is set to warn only.\n    Refer to :func:`torch.use_deterministic_algorithms` documentation for more\n    details.\n    \"\"\"\n    return _C._get_deterministic_algorithms_warn_only()\n\ndef set_deterministic_debug_mode(debug_mode: Union[builtins.int, str]) -> None:\n    r\"\"\"Sets the debug mode for deterministic operations.\n\n    .. note:: This is an alternative interface for\n        :func:`torch.use_deterministic_algorithms`. Refer to that function's\n        documentation for details about affected operations.\n\n    Args:\n        debug_mode(str or int): If \"default\" or 0, don't error or warn on\n            nondeterministic operations. If \"warn\" or 1, warn on\n            nondeterministic operations. If \"error\" or 2, error on\n            nondeterministic operations.\n    \"\"\"\n\n    # NOTE: builtins.int is used here because int in this scope resolves\n    # to torch.int\n    if not isinstance(debug_mode, (builtins.int, str)):\n        raise TypeError(f'debug_mode must be str or int, but got {type(debug_mode)}')\n\n    if isinstance(debug_mode, str):\n        if debug_mode == 'default':\n            debug_mode = 0\n        elif debug_mode == 'warn':\n            debug_mode = 1\n        elif debug_mode == 'error':\n            debug_mode = 2\n        else:\n            raise RuntimeError(\n                'invalid value of debug_mode, expected one of `default`, '\n                f'`warn`, `error`, but got {debug_mode}')\n\n    if debug_mode == 0:\n        _C._set_deterministic_algorithms(False)\n    elif debug_mode == 1:\n        _C._set_deterministic_algorithms(True, warn_only=True)\n    elif debug_mode == 2:\n        _C._set_deterministic_algorithms(True)\n    else:\n        raise RuntimeError(\n            'invalid value of debug_mode, expected 0, 1, or 2, '\n            f'but got {debug_mode}')\n\ndef get_deterministic_debug_mode() -> builtins.int:\n    r\"\"\"Returns the current value of the debug mode for deterministic\n    operations. Refer to :func:`torch.set_deterministic_debug_mode`\n    documentation for more details.\n    \"\"\"\n\n    if _C._get_deterministic_algorithms():\n        if _C._get_deterministic_algorithms_warn_only():\n            return 1\n        else:\n            return 2\n    else:\n        return 0\n\ndef get_float32_matmul_precision() -> builtins.str:\n    r\"\"\"Returns the current value of float32 matrix multiplication precision. Refer to\n    :func:`torch.set_float32_matmul_precision` documentation for more details.\n    \"\"\"\n    return _C._get_float32_matmul_precision()\n\ndef set_float32_matmul_precision(precision: str) -> None:\n    r\"\"\"Sets the internal precision of float32 matrix multiplications.\n\n    Running float32 matrix multiplications in lower precision may significantly increase\n    performance, and in some programs the loss of precision has a negligible impact.\n\n    Supports three settings:\n\n        * \"highest\", float32 matrix multiplications use the float32 datatype (24 mantissa\n          bits) for internal computations.\n        * \"high\", float32 matrix multiplications either use the TensorFloat32 datatype (10\n          mantissa bits) or treat each float32 number as the sum of two bfloat16 numbers\n          (approximately 16 mantissa bits), if the appropriate fast matrix multiplication\n          algorithms are available.  Otherwise float32 matrix multiplications are computed\n          as if the precision is \"highest\".  See below for more information on the bfloat16\n          approach.\n        * \"medium\", float32 matrix multiplications use the bfloat16 datatype (8 mantissa\n          bits) for internal computations, if a fast matrix multiplication algorithm\n          using that datatype internally is available. Otherwise float32\n          matrix multiplications are computed as if the precision is \"high\".\n\n    When using \"high\" precision, float32 multiplications may use a bfloat16-based algorithm\n    that is more complicated than simply truncating to some smaller number mantissa bits\n    (e.g. 10 for TensorFloat32, 8 for bfloat16).  Refer to [Henry2019]_ for a complete\n    description of this algorithm.  To briefly explain here, the first step is to realize\n    that we can perfectly encode a single float32 number as the sum of three bfloat16\n    numbers (because float32 has 24 mantissa bits while bfloat16 has 8, and both have the\n    same number of exponent bits).  This means that the product of two float32 numbers can\n    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade\n    accuracy for speed by dropping some of these products.  The \"high\" precision algorithm\n    specifically keeps only the three most significant products, which conveniently excludes\n    all of the products involving the last 8 mantissa bits of either input.  This means that\n    we can represent our inputs as the sum of two bfloat16 numbers rather than three.\n    Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than\n    float32 ones, it's faster to do three multiplications and 2 additions with bfloat16\n    precision than it is to do a single multiplication with float32 precision.\n\n    .. [Henry2019] http://arxiv.org/abs/1904.06376\n\n    .. note::\n\n        This does not change the output dtype of float32 matrix multiplications,\n        it controls how the internal computation of the matrix multiplication is performed.\n\n    .. note::\n\n        This does not change the precision of convolution operations. Other flags,\n        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution\n        operations.\n\n    .. note::\n\n        This flag currently only affects one native device type: CUDA.\n        If \"high\" or \"medium\" are set then the TensorFloat32 datatype will be used\n        when computing float32 matrix multiplications, equivalent to setting\n        `torch.backends.cuda.matmul.allow_tf32 = True`. When \"highest\" (the default)\n        is set then the float32 datatype is used for internal computations, equivalent\n        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.\n\n    Args:\n        precision(str): can be set to \"highest\" (default), \"high\", or \"medium\" (see above).\n\n    \"\"\"\n    _C._set_float32_matmul_precision(precision)\n\ndef set_warn_always(b: builtins.bool) -> None:\n    r\"\"\"When this flag is False (default) then some PyTorch warnings may only\n    appear once per process. This helps avoid excessive warning information.\n    Setting it to True causes these warnings to always appear, which may be\n    helpful when debugging.\n\n    Args:\n        b (:class:`bool`): If True, force warnings to always be emitted\n                           If False, set to the default behaviour\n    \"\"\"\n    _C._set_warnAlways(b)\n\ndef is_warn_always_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global warn_always flag is turned on. Refer to\n    :func:`torch.set_warn_always` documentation for more details.\n    \"\"\"\n    return _C._get_warnAlways()\n\n################################################################################\n# Define error checking functions\n################################################################################\n\n# These error checking functions must be kept consistent with their C++\n# equivalents. Their C++ equivalents are mentioned where applicable.\n\ndef _check_with(error_type, cond: Union[builtins.bool, SymBool], message: Callable[[], str]):\n    if not isinstance(cond, (builtins.bool, torch.SymBool)):\n        raise TypeError(f'cond must be a bool, but got {type(cond)}')\n\n    if torch.fx.experimental.symbolic_shapes.expect_true(cond):\n        return\n\n    # error_type must be a subclass of Exception and not subclass of Warning\n    assert issubclass(error_type, Exception) and not issubclass(error_type, Warning)\n\n    if message is None:\n        message_evaluated = (\n            'Expected cond to be True, but got False. (Could this error '\n            'message be improved? If so, please report an enhancement request '\n            'to PyTorch.)')\n\n    else:\n        if not callable(message):\n            raise TypeError('message must be a callable')\n\n        message_evaluated = str(message())\n\n    raise error_type(message_evaluated)\n\ndef _check(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(RuntimeError, cond, message)\n\ndef _check_index(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``IndexError``\n\n    C++ equivalent: ``TORCH_CHECK_INDEX``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(IndexError, cond, message)\n\ndef _check_value(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``ValueError``\n\n    C++ equivalent: ``TORCH_CHECK_VALUE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(ValueError, cond, message)\n\ndef _check_type(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``TypeError``\n\n    C++ equivalent: ``TORCH_CHECK_TYPE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(TypeError, cond, message)\n\ndef _check_not_implemented(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``NotImplementedError``\n\n    C++ equivalent: ``TORCH_CHECK_NOT_IMPLEMENTED``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(NotImplementedError, cond, message)\n\ndef _check_tensor_all_with(error_type, cond, message=None):\n    if not torch.is_tensor(cond):\n        raise TypeError(f'cond must be a tensor, but got {type(cond)}')\n\n    if not cond.dtype == torch.bool:\n        raise TypeError(\n            f'cond tensor must have dtype torch.bool, but got {cond.dtype}')\n\n    _check_with(error_type, cond._is_all_true().item(), message)\n\n# C++ equivalent: `TORCH_CHECK_TENSOR_ALL`\ndef _check_tensor_all(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK_TENSOR_ALL``\n\n    Args:\n        cond (:class:`torch.Tensor`): Tensor of dtype ``torch.bool``. If any\n            element is ``False``, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_tensor_all_with(RuntimeError, cond, message)\n\n################################################################################\n# Define numeric constants\n################################################################################\n\n# For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and\n# NumPy consistency (https://numpy.org/devdocs/reference/constants.html)\nfrom math import e , nan , inf , pi\n__all__.extend(['e', 'pi', 'nan', 'inf'])\n\n################################################################################\n# Define Storage and Tensor classes\n################################################################################\n\nfrom ._tensor import Tensor\nfrom .storage import _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n\n# NOTE: New <type>Storage classes should never be added. When adding a new\n# dtype, use torch.storage.TypedStorage directly.\n\nclass ByteStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.uint8\n\nclass DoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.double\n\nclass FloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.float\n\nclass HalfStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.half\n\nclass LongStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.long\n\nclass IntStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int\n\nclass ShortStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.short\n\nclass CharStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int8\n\nclass BoolStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bool\n\nclass BFloat16Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bfloat16\n\nclass ComplexDoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cdouble\n\nclass ComplexFloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cfloat\n\nclass QUInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint8\n\nclass QInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint8\n\nclass QInt32Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint32\n\nclass QUInt4x2Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint4x2\n\nclass QUInt2x4Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint2x4\n\n_storage_classes = {\n    UntypedStorage, DoubleStorage, FloatStorage, LongStorage, IntStorage,\n    ShortStorage, CharStorage, ByteStorage, HalfStorage, BoolStorage,\n    QUInt8Storage, QInt8Storage, QInt32Storage, BFloat16Storage,\n    ComplexFloatStorage, ComplexDoubleStorage, QUInt4x2Storage, QUInt2x4Storage,\n    TypedStorage\n}\n\n# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()\n_tensor_classes: Set[Type] = set()\n\n# If you edit these imports, please update torch/__init__.py.in as well\nfrom .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed\nfrom .serialization import save, load\nfrom ._tensor_str import set_printoptions\n\n################################################################################\n# Initialize extension\n################################################################################\n\ndef manager_path():\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return b\"\"\n    path = get_file_path('torch', 'bin', 'torch_shm_manager')\n    prepare_multiprocessing_environment(get_file_path('torch'))\n    if not os.path.exists(path):\n        raise RuntimeError(\"Unable to find torch_shm_manager at \" + path)\n    return path.encode('utf-8')\n\nfrom torch.amp import autocast\n\n# Initializing the extension shadows the built-in python float / int classes;\n# store them for later use by SymInt / SymFloat.\npy_float = float\npy_int = int\n\n# Shared memory manager needs to know the exact location of manager executable\n_C._initExtension(manager_path())\ndel manager_path\n\n# Appease the type checker: it can't deal with direct setting of globals().\n# Note that we will see \"too many\" functions when reexporting this way; there\n# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\n# so that this import is good enough\nif TYPE_CHECKING:\n    # Some type signatures pulled in from _VariableFunctions here clash with\n    # signatures already imported. For now these clashes are ignored; see\n    # PR #43339 for details.\n    from torch._C._VariableFunctions import *  # type: ignore[assignment, misc] # noqa: F403\n    # Fixup segment_reduce visibility\n    _segment_reduce = segment_reduce\n    del segment_reduce\n\n# Ops not to be exposed in `torch` namespace,\n# mostly helper ops.\nPRIVATE_OPS = (\n    'unique_dim',\n)\n\nfor name in dir(_C._VariableFunctions):\n    if name.startswith('__') or name in PRIVATE_OPS:\n        continue\n    obj = getattr(_C._VariableFunctions, name)\n    obj.__module__ = 'torch'\n    # Hide some APIs that should not be public\n    if name == \"segment_reduce\":\n        # TODO: Once the undocumented FC window is passed, remove the line bellow\n        globals()[name] = obj\n        name = \"_\" + name\n    globals()[name] = obj\n    if not name.startswith(\"_\"):\n        __all__.append(name)\n\n\n\n################################################################################\n# Import TorchDynamo's lazy APIs to avoid circular dependenices\n################################################################################\n\n# needs to be before from .functional import * to avoid circular dependencies\nfrom ._compile import _disable_dynamo\n\n################################################################################\n# Import interface functions defined in Python\n################################################################################\n\n# needs to be after the above ATen bindings so we can overwrite from Python side\nfrom .functional import *  # noqa: F403\n\n\n################################################################################\n# Remove unnecessary members\n################################################################################\n\ndel _StorageBase\ndel _LegacyStorage\n\n################################################################################\n# Define _assert\n################################################################################\n\n# needs to be before the submodule imports to avoid circular dependencies\ndef _assert(condition, message):\n    r\"\"\"A wrapper around Python's assert which is symbolically traceable.\n    \"\"\"\n    from .overrides import has_torch_function, handle_torch_function\n\n    if type(condition) is not torch.Tensor and has_torch_function((condition,)):\n        return handle_torch_function(_assert, (condition,), condition, message)\n    assert condition, message\n\n################################################################################\n# Import most common subpackages\n################################################################################\n\n# Use the redundant form so that type checkers know that these are a part of\n# the public API. The \"regular\" import lines are there solely for the runtime\n# side effect of adding to the imported module's members for other users.\nfrom torch import cuda as cuda\nfrom torch import cpu as cpu\nfrom torch import mps as mps\nfrom torch import autograd as autograd\nfrom torch.autograd import (\n    no_grad as no_grad,\n    enable_grad as enable_grad,\n    set_grad_enabled as set_grad_enabled,\n    inference_mode as inference_mode,\n)\nfrom torch import fft as fft\nfrom torch import futures as futures\nfrom torch import _awaits as _awaits\nfrom torch import nested as nested\nfrom torch import nn as nn\nfrom torch.signal import windows as windows\nfrom torch import optim as optim\nimport torch.optim._multi_tensor\nfrom torch import multiprocessing as multiprocessing\nfrom torch import sparse as sparse\nfrom torch import special as special\nimport torch.utils.backcompat\nfrom torch import jit as jit\nfrom torch import linalg as linalg\nfrom torch import hub as hub\nfrom torch import random as random\nfrom torch import distributions as distributions\nfrom torch import testing as testing\nfrom torch import backends as backends\nimport torch.utils.data\nfrom torch import __config__ as __config__\nfrom torch import __future__ as __future__\nfrom torch import profiler as profiler\n\n# Quantized, sparse, AO, etc. should be last to get imported, as nothing\n# is expected to depend on them.\nfrom torch import ao as ao\n# nn.quant* depends on ao -- so should be after those.\nimport torch.nn.quantizable\nimport torch.nn.quantized\nimport torch.nn.qat\nimport torch.nn.intrinsic\n\n_C._init_names(list(torch._storage_classes))\n\n# attach docstrings to torch and tensor functions\nfrom . import _torch_docs, _tensor_docs, _storage_docs\ndel _torch_docs, _tensor_docs, _storage_docs\n\n\ndef compiled_with_cxx11_abi() -> builtins.bool:\n    r\"\"\"Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\"\"\"\n    return _C._GLIBCXX_USE_CXX11_ABI\n\n\n# Import the ops \"namespace\"\nfrom torch._ops import ops\nfrom torch._classes import classes\n\n# quantization depends on torch.fx\n# Import quantization\nfrom torch import quantization as quantization\n\n# Import the quasi random sampler\nfrom torch import quasirandom as quasirandom\n\n# If you are seeing this, it means that this call site was not checked if\n# the memory format could be preserved, and it was switched to old default\n# behaviour of contiguous\nlegacy_contiguous_format = contiguous_format\n\n# Register fork handler to initialize OpenMP in child processes (see gh-28389)\nfrom torch.multiprocessing._atfork import register_after_fork\nregister_after_fork(torch.get_num_threads)\ndel register_after_fork\n\n# Import tools that require fully imported torch (for applying\n# torch.jit.script as a decorator, for instance):\nfrom ._lobpcg import lobpcg as lobpcg\n\n# These were previously defined in native_functions.yaml and appeared on the\n# `torch` namespace, but we moved them to c10 dispatch to facilitate custom\n# class usage. We add these lines here to preserve backward compatibility.\nquantized_lstm = torch.ops.aten.quantized_lstm\nquantized_gru = torch.ops.aten.quantized_gru\n\nfrom torch.utils.dlpack import from_dlpack, to_dlpack\n\n# Import experimental masked operations support. See\n# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\n# information.\nfrom . import masked\n\n# Import removed ops with error message about removal\nfrom ._linalg_utils import (  # type: ignore[misc]\n    matrix_rank,\n    eig,\n    solve,\n    lstsq,\n)\nfrom ._linalg_utils import _symeig as symeig  # type: ignore[misc]\n\nclass _TorchCompileInductorWrapper:\n    compiler_name = \"inductor\"\n\n    def __init__(self, mode, options, dynamic):\n        self.config: Dict[str, Any] = dict()\n        self.dynamic = dynamic\n        self.apply_mode(mode)\n        self.apply_options(options)\n\n        # FIXME: CUPTI Lazy Re-init and CUDA Graph crashes with CUDA 11.\n        if self.config.get(\"triton.cudagraphs\", False):\n            os.environ[\"DISABLE_CUPTI_LAZY_REINIT\"] = \"1\"\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileInductorWrapper) and\n                self.config == other.config and\n                self.dynamic == other.dynamic)\n\n    def apply_mode(self, mode: Optional[str]):\n        if mode is None or mode == \"default\":\n            pass\n        elif mode in (\"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"):\n            from torch._inductor import list_mode_options\n            self.apply_options(list_mode_options(mode, self.dynamic))\n        else:\n            raise RuntimeError(\n                f\"Unrecognized mode={mode}, should be one of: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs\"\n            )\n\n    def apply_options(self, options: Optional[Dict[str, Any]]):\n        if not options:\n            return\n\n        from torch._inductor import config\n        current_config: Dict[str, Any] = config.to_dict()  # type: ignore[attr-defined]\n\n        for key, val in options.items():\n            attr_name = key.replace(\"-\", \"_\")\n            if attr_name not in current_config:\n                raise RuntimeError(\n                    f\"Unexpected optimization option {key}, known options are {list(current_config.keys())}\"\n                )\n            if type(val) is not type(current_config[attr_name]):\n                val_type_str = type(val).__name__\n                expected_type_str = type(current_config[attr_name]).__name__\n                raise RuntimeError(\n                    f\"Unexpected type of attr {key}, got {val_type_str} should be {expected_type_str}\"\n                )\n            self.config[attr_name] = val\n\n    def __call__(self, model_, inputs_):\n        from torch._inductor.compile_fx import compile_fx\n\n        return compile_fx(model_, inputs_, config_patches=self.config)\n\n    def get_compiler_config(self):\n        from torch._inductor.compile_fx import get_patched_config_dict\n        return get_patched_config_dict(config_patches=self.config)\n\n    def reset(self):\n        from torch._inductor import config\n        if \"triton.cudagraphs\" in self.config or config.triton.cudagraphs:\n            if self.config.get(\"triton.cudagraphs\", True):\n                from torch._inductor.cudagraph_trees import reset_cudagraph_trees\n                reset_cudagraph_trees()\n\nclass _TorchCompileWrapper:\n    def __init__(self, backend, mode, options, dynamic):\n        from torch._dynamo.backends.registry import lookup_backend\n\n        if isinstance(backend, str):\n            self.compiler_name = backend\n        elif hasattr(backend, \"__name__\"):\n            self.compiler_name = backend.__name__\n        else:\n            self.compiler_name = str(backend)\n        self.dynamic = dynamic\n        self.compiler_fn = lookup_backend(backend)\n        self.kwargs = {}\n        # only pass the args if they non-empty\n        if mode and mode != \"default\":\n            self.kwargs[\"mode\"] = mode\n        if options:\n            self.kwargs[\"options\"] = options\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileWrapper) and\n                self.compiler_fn == other.compiler_fn and\n                self.kwargs == other.kwargs and\n                self.dynamic == other.dynamic)\n\n    def __call__(self, model_, inputs_):\n        return self.compiler_fn(model_, inputs_, **self.kwargs)\n\n\ndef compile(model: Optional[Callable] = None, *,\n            fullgraph: builtins.bool = False,\n            dynamic: Optional[builtins.bool] = None,\n            backend: Union[str, Callable] = \"inductor\",\n            mode: Union[str, None] = None,\n            options: Optional[Dict[str, Union[str, builtins.int, builtins.bool]]] = None,\n            disable: builtins.bool = False) -> Callable:\n    \"\"\"\n    Optimizes given model/function using TorchDynamo and specified backend.\n\n    Concretely, for every frame executed within the compiled region, we will attempt\n    to compile it and cache the compiled result on the code object for future\n    use.  A single frame may be compiled multiple times if previous compiled\n    results are not applicable for subsequent calls (this is called a \"guard\n    failure), you can use TORCH_LOGS=guards to debug these situations.\n    Multiple compiled results can be associated with a frame up to\n    ``torch._dynamo.config.cache_size_limit``, which defaults to 64; at which\n    point we will fall back to eager.  Note that compile caches are per\n    *code object*, not frame; if you dynamically create multiple copies of a\n    function, they will all share the same code cache.\n\n    Args:\n       model (Callable): Module/function to optimize\n       fullgraph (bool): Whether it is ok to break model into several subgraphs\n       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\n        to generate a kernel that is as dynamic as possible to avoid recompilations when\n        sizes change.  This may not always work as some operations/optimizations will\n        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\n        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\n        By default (None), we automatically detect if dynamism has occurred and compile a more\n        dynamic kernel upon recompile.\n       backend (str or Callable): backend to be used\n\n        - \"inductor\" is the default backend, which is a good balance between performance and overhead\n\n        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\n\n        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\n\n        - To register an out-of-tree custom backend: https://pytorch.org/docs/main/compile/custom-backends.html\n       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\n\n        - \"default\" is the default mode, which is a good balance between performance and overhead\n\n        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\n          useful for small batches.  Reduction of overhead can come at the cost of more memory\n          usage, as we will cache the workspace memory required for the invocation so that we\n          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\n          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\n          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\n          to debug.\n\n        - \"max-autotune\" is a mode that leverages Triton based matrix multiplications and convolutions\n          It enables CUDA graphs by default.\n\n        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\n\n        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\n\n       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\n\n        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\n\n        - `max_autotune` which will profile to pick the best matmul configuration\n\n        - `fallback_random` which is useful when debugging accuracy issues\n\n        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\n\n        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\n\n        - `trace.enabled` which is the most useful debugging flag to turn on\n\n        - `trace.graph_diagram` which will show you a picture of your graph after fusion\n\n        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\n       disable (bool): Turn torch.compile() into a no-op for testing\n\n    Example::\n\n        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n        def foo(x):\n            return torch.sin(x) + torch.cos(x)\n\n    \"\"\"\n    _C._log_api_usage_once(\"torch.compile\")\n    # Temporary until we get proper support for python 3.12\n    if sys.version_info >= (3, 12):\n        raise RuntimeError(\"Dynamo is not supported on Python 3.12+\")\n\n    # Decorator mode\n    if model is None:\n        def fn(model: Callable):\n            if model is None:\n                raise RuntimeError(\"Model can't be None\")\n            return compile(model,\n                           fullgraph=fullgraph,\n                           dynamic=dynamic,\n                           backend=backend,\n                           mode=mode,\n                           options=options,\n                           disable=disable)\n        return fn\n\n    if mode is not None and options is not None:\n        raise RuntimeError(\"Either mode or options can be specified, but both can't be specified at the same time.\")\n    if mode is None and options is None:\n        mode = \"default\"\n    if backend == \"inductor\":\n        backend = _TorchCompileInductorWrapper(mode, options, dynamic)\n    else:\n        backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n\n    return torch._dynamo.optimize(backend=backend, nopython=fullgraph, dynamic=dynamic, disable=disable)(model)\n\n\nfrom torch import export as export\n\n\ndef _register_device_module(device_type, module):\n    r\"\"\"Register an external runtime module of the specific :attr:`device_type`\n    supported by torch.\n\n    After the :attr:`module` is registered correctly, the user can refer\n    the external runtime module as part of torch with attribute torch.xxx.\n    \"\"\"\n    # Make sure the device_type represent a supported device type for torch.\n    device_type = torch.device(device_type).type\n    m = sys.modules[__name__]\n    if hasattr(m, device_type):\n        raise RuntimeError(f\"The runtime module of '{device_type}' has already \"\n                           f\"been registered with '{getattr(m, device_type)}'\")\n    setattr(m, device_type, module)\n    torch_module_name = '.'.join([__name__, device_type])\n    sys.modules[torch_module_name] = module\n\n# expose return_types\nfrom . import return_types\nfrom . import library\nif not TYPE_CHECKING:\n    from . import _meta_registrations\n\n# Enable CUDA Sanitizer\nif 'TORCH_CUDA_SANITIZER' in os.environ:\n    import torch.cuda._sanitizer as csan\n\n    csan.enable_cuda_sanitizer()\n\n# Populate magic methods on SymInt and SymFloat\nimport torch.fx.experimental.symbolic_shapes\n\nfrom torch import func as func\nfrom torch.func import vmap\n\n\n# The function _sparse_coo_tensor_unsafe is removed from PyTorch\n# Python API (v. 1.13), here we temporarily provide its replacement\n# with a deprecation warning.\n# TODO: remove the function for PyTorch v 1.15.\ndef _sparse_coo_tensor_unsafe(*args, **kwargs):\n    import warnings\n    warnings.warn('torch._sparse_coo_tensor_unsafe is deprecated, '\n                  'use torch.sparse_coo_tensor(..., check_invariants=False) instead.')\n    kwargs['check_invariants'] = False\n    return torch.sparse_coo_tensor(*args, **kwargs)\n\n# Register MPS specific decomps\ntorch.backends.mps._init()\n\nif not _running_with_deploy():\n    from torch import compiler as compiler\n\n    class _TritonLibrary:\n        lib = torch.library.Library(\"triton\", \"DEF\")\n        ops_table: Dict[Tuple[str, str], Callable] = {}\n\n        @classmethod\n        def registerOp(cls, op_key, full_schema, op_impl, dispatch_key):\n            if (op_key, dispatch_key) not in cls.ops_table:\n                cls.lib.define(full_schema)\n                cls.lib.impl(\"triton::\" + op_key, op_impl, dispatch_key)\n                cls.ops_table[(op_key, dispatch_key)] = op_impl\n\n            return cls.ops_table[(op_key, dispatch_key)]\n\n\n# Deprecated attributes\n_deprecated_attrs = {\n    \"has_mps\": torch.backends.mps.is_built,\n    \"has_cuda\": torch.backends.cuda.is_built,\n    \"has_cudnn\": torch.backends.cudnn.is_available,\n    \"has_mkldnn\": torch.backends.mkldnn.is_available,\n}\n\nif TYPE_CHECKING:\n    # Import the following modules during type checking to enable code intelligence features,\n    # such as auto-completion in tools like pylance, even when these modules are not explicitly\n    # imported in user code.\n    from torch import _dynamo as _dynamo\n    from torch import _inductor as _inductor\n    from torch import onnx as onnx\n\n_lazy_modules = {\n    \"_dynamo\",\n    \"_inductor\",\n    \"_export\",\n    # ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit\n    \"onnx\",\n}\n\ndef __getattr__(name):\n    # Deprecated attrs\n    replacement = _deprecated_attrs.get(name)\n    if replacement is not None:\n        import warnings\n        warnings.warn(f\"'{name}' is deprecated, please use '{replacement.__module__}.{replacement.__name__}()'\", stacklevel=2)\n        return replacement()\n\n    # Lazy modules\n    if name in _lazy_modules:\n        import importlib\n        return importlib.import_module(f\".{name}\", __name__)\n\n    raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")\n\nfrom . import _logging\n_logging._init_logs()\n"
    },
    "patchgt": {
      "test/export/test_export.py": "@@ -9,7 +9,7 @@\n from functorch.experimental.control_flow import map\n from torch import Tensor\n from torch.export import Constraint\n-from torch._export import DEFAULT_EXPORT_DYNAMO_CONFIG, dynamic_dim, export\n+from torch._export import DEFAULT_EXPORT_DYNAMO_CONFIG, dynamic_dim, export, capture_pre_autograd_graph\n from torch._export.constraints import constrain_as_size, constrain_as_value\n from torch._export.utils import (\n     get_buffer,\n@@ -1023,6 +1023,23 @@ def test_constraint_directly_construct(self):\n         ):\n             _ = Constraint()\n \n+    def test_train_eval_on_exported_preautograd_module(self):\n+        class Foo(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+\n+            def forward(self, x):\n+                if x.shape[0] > 4:\n+                    return x.cos()\n+                return x.sin()\n+\n+        graph_module = capture_pre_autograd_graph(Foo(), (torch.ones(7, 5),))\n+        with self.assertRaisesRegex(NotImplementedError, r\"Calling train\\(\\) is not supported yet.\"):\n+            graph_module.train()\n+\n+        with self.assertRaisesRegex(NotImplementedError, r\"Calling eval\\(\\) is not supported yet.\"):\n+            graph_module.eval()\n+\n \n if __name__ == '__main__':\n     run_tests()",
      "torch/_export/__init__.py": "@@ -2,6 +2,7 @@\n import io\n import re\n import pathlib\n+import types\n import weakref\n import zipfile\n from collections import OrderedDict\n@@ -102,7 +103,6 @@ class ExportDynamoConfig:\n DECOMP_TABLE = core_aten_decompositions()\n \n \n-# FIXME: actually migrate it to pre_autograd tracing\n @compatibility(is_backward_compatible=False)\n def capture_pre_autograd_graph(\n     f: Callable,\n@@ -157,6 +157,15 @@ def capture_pre_autograd_graph(\n \n         for n in m.graph.nodes:\n             n.meta[\"is_torch_exported\"] = True\n+\n+        def _train(self, mode: bool = True):\n+            raise NotImplementedError(\"Calling train() is not supported yet.\")\n+\n+        def _eval(self, mode: bool = True):\n+            raise NotImplementedError(\"Calling eval() is not supported yet.\")\n+\n+        m.train = types.MethodType(_train, m)  # type: ignore[method-assign]\n+        m.eval = types.MethodType(_eval, m)  # type: ignore[method-assign]\n         return m\n \n "
    },
    "repo": "pytorch",
    "pr_number": 108258
  },
  {
    "issue": "Issue: [quant] Move dropout replacement to `move_model_to_eval` (#108184)\n\nSummary: This commit adds a public facing\r\n`torch.ao.quantization.move_model_to_eval` util function for QAT users. Instead of calling model.eval() on an exported model (which doesn't work, see https://github.com/pytorch/pytorch/issues/103681), the user would call this new util function instead. This ensures special ops such as dropout and batchnorm (not supported yet) will have the right behavior when the graph is later used for inference.\r\n\r\nNote: Support for an equivalent `move_model_to_train` will be added in the future. This is difficult to do for dropout currently because the eval pattern of dropout is simply a clone op, which we cannot just match and replace with a dropout op.\r\n\r\nTest Plan:\r\npython test/test_quantization.py TestQuantizePT2E.test_move_model_to_eval\r\n\r\nReviewers: jerryzh168, kimishpatel\r\n\r\nSubscribers: jerryzh168, kimishpatel, supriyar\r\n\r\nDifferential Revision: [D48814735](https://our.internmc.facebook.com/intern/diff/D48814735)\r\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108184\r\nApproved by: https://github.com/jerryzh168\n\n",
    "ctx": {
      "test/quantization/pt2e/test_quantize_pt2e.py": "# Owner(s): [\"oncall: quantization\"]\nimport copy\nimport operator\nfrom typing import Any, List, Optional, Tuple, Dict\n\nimport torch\nimport torch._dynamo as torchdynamo\nfrom torch._export import capture_pre_autograd_graph\nfrom torch import Tensor\nfrom torch.ao.ns.fx.utils import compute_sqnr\nfrom torch.ao.quantization import (\n    FusedMovingAvgObsFakeQuantize,\n    MovingAverageMinMaxObserver,\n    MovingAveragePerChannelMinMaxObserver,\n    observer,\n    ObserverOrFakeQuantize,\n    QConfigMapping,\n)\nfrom torch.ao.quantization.quantizer import (\n    DerivedQuantizationSpec,\n    FixedQParamsQuantizationSpec,\n    QuantizationAnnotation,\n    QuantizationSpec,\n    Quantizer,\n    SharedQuantizationSpec,\n)\nfrom torch.ao.quantization.quantizer.xnnpack_quantizer import (\n    XNNPACKQuantizer,\n    get_symmetric_quantization_config,\n)\nfrom torch.ao.quantization.quantizer.composable_quantizer import (  # noqa: F811\n    ComposableQuantizer,\n)\nfrom torch.ao.quantization.quantizer.embedding_quantizer import (  # noqa: F811\n    EmbeddingQuantizer,\n)\n\nfrom torch.ao.quantization.quantize_pt2e import (\n    _convert_to_reference_decomposed_fx,\n    convert_pt2e,\n    prepare_pt2e,\n    prepare_qat_pt2e,\n)\nfrom torch.ao.quantization.backend_config import (\n    get_executorch_backend_config,\n    get_qnnpack_backend_config,\n)\n\nfrom torch.ao.quantization.qconfig import (\n    default_per_channel_symmetric_qnnpack_qat_qconfig,\n    default_per_channel_symmetric_qnnpack_qconfig,\n    default_symmetric_qnnpack_qconfig,\n    default_symmetric_qnnpack_qat_qconfig,\n    float_qparams_weight_only_qconfig,\n    per_channel_weight_observer_range_neg_127_to_127,\n    QConfig,\n    weight_observer_range_neg_127_to_127,\n)\nfrom torch.ao.quantization.quantize_fx import (\n    convert_to_reference_fx,\n    prepare_fx,\n    prepare_qat_fx,\n)\nfrom torch.fx import Node\n\nfrom torch.testing._internal.common_quantization import (\n    NodeSpec as ns,\n    QuantizationTestCase,\n    skip_if_no_torchvision,\n    skipIfNoQNNPACK,\n)\nfrom torch.ao.quantization import (\n    default_dynamic_qconfig,\n)\nfrom torch.testing._internal.common_quantized import override_quantized_engine\nfrom torch._higher_order_ops.out_dtype import out_dtype  # noqa: F401\nfrom torch._export import dynamic_dim\n\nimport unittest\n\n# TODO: Move to common utils or use existing quant utils to fetch model instances\nclass TestHelperModules:\n    class Conv2dPropAnnotaton(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = torch.nn.Conv2d(3, 3, 3)\n            self.linear = torch.nn.Linear(3, 3)\n\n        def forward(self, x):\n            x = self.conv(x)\n            x = x.view(-1, 3)\n            x = torch.nn.functional.hardtanh(x, -0.5, 0.5)\n            x = self.linear(x)\n            return x\n\n    class Conv2dWithObsSharingOps(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = torch.nn.Conv2d(3, 3, 3)\n            self.hardtanh = torch.nn.Hardtanh()\n            self.adaptive_avg_pool2d = torch.nn.AdaptiveAvgPool2d((1, 1))\n\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.adaptive_avg_pool2d(x)\n            x = self.hardtanh(x)\n            x = torch.mean(x)\n            return x\n\n    class Conv2dWithTwoLinearPermute(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = torch.nn.Conv2d(3, 16, 3)\n            self.linear1 = torch.nn.Linear(16, 8, bias=False)\n            self.linear2 = torch.nn.Linear(8, 8)\n\n        def forward(self, x):\n            conv_out = self.conv(x)\n            permute_out = torch.permute(conv_out, (0, 2, 3, 1))\n            return self.linear2(self.linear1(permute_out))\n\n    class Conv2dWithTwoLinear(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = torch.nn.Conv2d(3, 16, 3)\n            self.linear1 = torch.nn.Linear(64, 8, bias=False)\n            self.linear2 = torch.nn.Linear(8, 8)\n\n        def forward(self, x):\n            conv_out = self.conv(x)\n            reshape_out = torch.reshape(conv_out, (2, 64))\n            return self.linear2(self.linear1(reshape_out))\n\n    class ConvLinearWPermute(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = torch.nn.Conv2d(3, 8, 3)\n            self.linear1 = torch.nn.Linear(8, 8)\n\n        def forward(self, x):\n            conv_out = self.conv(x)\n            permute_out = torch.permute(conv_out, (0, 2, 3, 1))\n            return self.linear1(permute_out)\n\n    class TwoLinearModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear1 = torch.nn.Linear(8, 16, bias=False)\n            self.linear2 = torch.nn.Linear(16, 8)\n\n        def forward(self, x):\n            return self.linear2(self.linear1(x))\n\n    class ConvMaxPool2d(torch.nn.Module):\n        def __init__(self):\n            super(TestHelperModules.ConvMaxPool2d, self).__init__()\n            self.conv = torch.nn.Conv2d(2, 2, 1)\n            self.pool = torch.nn.MaxPool2d(1, 1)\n\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.pool(x)\n            return x\n\n    class ConvWithAdaptiveAvgPool2d(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv = torch.nn.Conv2d(3, 3, 3)\n            self.adaptive_avg_pool2d = torch.nn.AdaptiveAvgPool2d((1, 1))\n\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.adaptive_avg_pool2d(x)\n            return x\n\n    class ConvWithBNRelu(torch.nn.Module):\n        def __init__(self, relu, bn=True, bias=True):\n            super().__init__()\n            self.conv = torch.nn.Conv2d(3, 3, 3, bias=bias)\n            if bn:\n                self.bn = torch.nn.BatchNorm2d(3)\n            else:\n                self.bn = torch.nn.Identity()\n            if relu:\n                self.relu = torch.nn.ReLU()\n            else:\n                self.relu = torch.nn.Identity()\n\n        def forward(self, x):\n            x = self.conv(x)\n            x = self.bn(x)\n            return self.relu(x)\n\n    class Conv2dWithCat(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = torch.nn.Conv2d(3, 3, 3)\n            self.conv2 = torch.nn.Conv2d(3, 3, 3)\n\n        def forward(self, x, y):\n            x = self.conv1(x)\n            y = self.conv2(y)\n            z = torch.cat([x, y], dim=1)\n            return z\n\n    class EmbeddingModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.emb = torch.nn.Embedding(num_embeddings=10, embedding_dim=12)\n\n        def forward(self, indices):\n            return self.emb(indices)\n\n    class EmbeddingConvLinearModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.emb = torch.nn.Embedding(num_embeddings=10, embedding_dim=8)\n            self.conv = torch.nn.Conv2d(8, 16, (1, 3))\n            self.linear = torch.nn.Linear(16, 8)\n\n        def forward(self, indices):\n            embeddings = self.emb(indices)\n            embeddings = torch.unsqueeze(embeddings, dim=0)\n            embeddings = torch.permute(embeddings, (0, 3, 1, 2))\n            conv_out = self.conv(embeddings)\n            conv_out = torch.permute(conv_out, (0, 2, 3, 1))\n            conv_out = torch.squeeze(conv_out, dim=0)\n            return self.linear(conv_out)\n\n    class AddInplaceAdd(torch.nn.Module):\n        def forward(self, x, y):\n            x = x + y\n            x += y\n            return x\n\n    class MulInplaceMul(torch.nn.Module):\n        def forward(self, x, y):\n            x = x * y\n            x *= y\n            return x\n\nclass PT2EQuantizationTestCase(QuantizationTestCase):\n    \"\"\"\n    Base QuantizationTestCase for PT2 with some helper methods.\n    \"\"\"\n    _MAP_TO_FX_TRACED_OPS = {\n        torch.ops.quantized_decomposed.quantize_per_tensor: torch.ops.quantized_decomposed.quantize_per_tensor.default,\n        torch.ops.quantized_decomposed.dequantize_per_tensor: torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n        torch.ops.quantized_decomposed.quantize_per_channel: torch.ops.quantized_decomposed.quantize_per_channel.default,\n        torch.ops.quantized_decomposed.dequantize_per_channel: torch.ops.quantized_decomposed.dequantize_per_channel.default,\n        torch.ops.quantized_decomposed.quantize_per_tensor.tensor: torch.ops.quantized_decomposed.quantize_per_tensor.tensor,\n        torch.ops.quantized_decomposed.dequantize_per_tensor.tensor: torch.ops.quantized_decomposed.dequantize_per_tensor.tensor,\n    }\n\n\n\n    def _test_quantizer(\n        self,\n        model,\n        example_inputs,\n        quantizer,\n        expected_node_occurrence,\n        expected_node_list=None,\n        check_against_fx_quant=False,\n        fx_qconfig_mapping=None,\n        export_with_dynamic_shape=False,\n    ):\n        m_eager = model.eval()\n\n        # program capture\n        m = copy.deepcopy(m_eager)\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n            constraints=[dynamic_dim(example_inputs[0], 0)] if export_with_dynamic_shape else [],\n        )\n\n        m = prepare_pt2e(m, quantizer)\n        # Calibrate\n        m(*example_inputs)\n        m = convert_pt2e(m)\n\n        pt2_quant_output = m(*example_inputs)\n        node_occurrence = {\n            ns.call_function(k): v for k, v in expected_node_occurrence.items()\n        }\n        if expected_node_list is None:\n            expected_node_list = []\n        node_list = [ns.call_function(n) for n in expected_node_list]\n        self.checkGraphModuleNodes(\n            m, expected_node_occurrence=node_occurrence, expected_node_list=node_list\n        )\n        if check_against_fx_quant:\n            qconfig_mapping = fx_qconfig_mapping\n            backend_config = get_executorch_backend_config()\n            m_copy = copy.deepcopy(m_eager)\n            m_fx = prepare_fx(\n                m_copy, qconfig_mapping, example_inputs, backend_config=backend_config\n            )\n            m_fx(*example_inputs)\n            m_fx = _convert_to_reference_decomposed_fx(\n                m_fx, backend_config=backend_config\n            )\n            m_fx = capture_pre_autograd_graph(\n                m_fx,\n                example_inputs,\n                constraints=[dynamic_dim(example_inputs[0], 0)] if export_with_dynamic_shape else [],\n            )\n            node_occurrence = {}\n            for k, v in PT2EQuantizationTestCase._MAP_TO_FX_TRACED_OPS.items():\n                if k in expected_node_occurrence:\n                    node_occurrence[ns.call_function(v)] = expected_node_occurrence[k]\n            self.checkGraphModuleNodes(m_fx, expected_node_occurrence=node_occurrence)\n            fx_quant_output = m_fx(*example_inputs)\n            self.assertEqual(fx_quant_output, pt2_quant_output)\n\n    def _verify_symmetric_qnnpack_qat_numerics(\n        self,\n        model: torch.nn.Module,\n        example_inputs: Tuple[Any, ...],\n        is_per_channel: bool,\n        verify_convert: bool = False,\n    ):\n        \"\"\"\n        Helper method to verify that the QAT numerics for PT2E quantization match those of\n        FX graph mode quantization for symmetric qnnpack.\n        \"\"\"\n        MANUAL_SEED = 100\n\n        # PT2 export\n\n        model_pt2e = copy.deepcopy(model)\n        quantizer = XNNPACKQuantizer()\n        quantizer.set_global(\n            get_symmetric_quantization_config(\n                is_per_channel=is_per_channel, is_qat=True\n            )\n        )\n        model_pt2e = capture_pre_autograd_graph(\n            model_pt2e,\n            example_inputs,\n        )\n        model_pt2e = prepare_qat_pt2e(model_pt2e, quantizer)\n        torch.manual_seed(MANUAL_SEED)\n        after_prepare_result_pt2e = model_pt2e(*example_inputs)\n\n        model_fx = copy.deepcopy(model)\n        if is_per_channel:\n            default_qconfig = default_per_channel_symmetric_qnnpack_qat_qconfig\n        else:\n            default_qconfig = default_symmetric_qnnpack_qat_qconfig\n        qconfig_mapping = QConfigMapping().set_global(default_qconfig)\n        backend_config = get_qnnpack_backend_config()\n        model_fx = prepare_qat_fx(\n            model_fx, qconfig_mapping, example_inputs, backend_config=backend_config\n        )\n        torch.manual_seed(MANUAL_SEED)\n        after_prepare_result_fx = model_fx(*example_inputs)\n\n        # Verify that numerics match\n        self.assertEqual(after_prepare_result_pt2e, after_prepare_result_fx)\n\n        if verify_convert:\n            model_pt2e.eval()\n            model_pt2e = convert_pt2e(model_pt2e)\n            quant_result_pt2e = model_pt2e(*example_inputs)\n            model_fx.eval()\n            model_fx = _convert_to_reference_decomposed_fx(\n                model_fx, backend_config=backend_config,\n            )\n            quant_result_fx = model_fx(*example_inputs)\n            self.assertEqual(quant_result_pt2e, quant_result_fx)\n\n    def _verify_symmetric_qnnpack_qat_graph(\n        self,\n        m: torch.fx.GraphModule,\n        example_inputs: Tuple[Any, ...],\n        is_per_channel: bool,\n        has_relu: bool,\n        has_bias: bool = True,\n        expected_conv_literal_args: Optional[Tuple[Any, ...]] = None,\n    ):\n        \"\"\"\n        Verify that the graph module matches the fused QAT [conv - bn (- relu)] pattern\n        with fake quantizes inserted into the correct places.\n        # TODO: also verify that metadata is copied over to the new nodes.\n        \"\"\"\n        quantizer = XNNPACKQuantizer()\n        quantizer.set_global(\n            get_symmetric_quantization_config(is_per_channel, is_qat=True)\n        )\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n        m = prepare_qat_pt2e(m, quantizer)\n        m(*example_inputs)\n\n        # Verify: getitem output activation fake quantize\n        output_node = list(m.graph.nodes)[-1]\n        output_fq_node = output_node.args[0][0]\n        self.assertTrue(output_fq_node.target.startswith(\"activation_post_process_\"))\n        output_fq_mod = getattr(m, output_fq_node.target)\n        self.assertEqual(type(output_fq_mod), FusedMovingAvgObsFakeQuantize)\n        self.assertEqual(\n            type(output_fq_mod.activation_post_process), MovingAverageMinMaxObserver\n        )\n        self.assertEqual(output_fq_mod.dtype, torch.qint8)\n        self.assertEqual(output_fq_mod.quant_min, -128)\n        self.assertEqual(output_fq_mod.quant_max, 127)\n\n        # Verify: getitem(bn, 0) or relu(getitem(bn, 0))\n        if has_relu:\n            relu_node = output_fq_node.args[0]\n            getitem_node = relu_node.args[0]\n            self.assertEqual(relu_node.target, torch.ops.aten.relu.default)\n        else:\n            relu_node = None\n            getitem_node = output_fq_node.args[0]\n        bn_node = getitem_node.args[0]\n        self.assertEqual(getitem_node.target, operator.getitem)\n        self.assertEqual(\n            bn_node.target, torch.ops.aten._native_batch_norm_legit.default\n        )\n\n        # Verify: conv / scale_factor.reshape [+ bias.reshape]\n        if has_bias:\n            add_bias_node = bn_node.args[0]\n            (div_scale_factor_node, bias_reshape_node) = add_bias_node.args\n            self.assertEqual(add_bias_node.target, torch.ops.aten.add.Tensor)\n            self.assertEqual(bias_reshape_node.target, torch.ops.aten.reshape.default)\n        else:\n            div_scale_factor_node = bn_node.args[0]\n        (conv_node, scale_factor_reshape_node) = div_scale_factor_node.args\n        self.assertEqual(div_scale_factor_node.target, torch.ops.aten.div.Tensor)\n        self.assertEqual(conv_node.target, torch.ops.aten.conv2d.default)\n        self.assertEqual(scale_factor_reshape_node.target, torch.ops.aten.reshape.default)\n\n        # Verify: conv literal args\n        if expected_conv_literal_args is not None:\n            assert (\n                len(expected_conv_literal_args) == 6\n            ), \"wrong num conv args, bad test setup\"\n            for i in range(6):\n                if i + 3 < len(conv_node.args):\n                    self.assertEqual(conv_node.args[i + 3], expected_conv_literal_args[i])\n\n        # Verify: conv input activation fake quantize\n        conv_input_fq_node = conv_node.args[0]\n        conv_input_node = conv_input_fq_node.args[0]\n        self.assertTrue(\n            conv_input_fq_node.target.startswith(\"activation_post_process_\")\n        )\n        conv_input_fq_mod = getattr(m, conv_input_fq_node.target)\n        self.assertEqual(type(conv_input_fq_mod), FusedMovingAvgObsFakeQuantize)\n        self.assertEqual(\n            type(conv_input_fq_mod.activation_post_process), MovingAverageMinMaxObserver\n        )\n        self.assertEqual(conv_input_fq_mod.dtype, torch.qint8)\n        self.assertEqual(conv_input_fq_mod.quant_min, -128)\n        self.assertEqual(conv_input_fq_mod.quant_max, 127)\n        self.assertTrue(conv_input_node.op, \"placeholder\")\n\n        # Verify: conv weight fake quantize\n        conv_weight_fq_node = conv_node.args[1]\n        self.assertTrue(\n            conv_weight_fq_node.target.startswith(\"activation_post_process_\")\n        )\n        conv_weight_fq_mod = getattr(m, conv_weight_fq_node.target)\n        if is_per_channel:\n            expected_weight_observer_type = MovingAveragePerChannelMinMaxObserver\n        else:\n            expected_weight_observer_type = MovingAverageMinMaxObserver\n        self.assertEqual(type(conv_weight_fq_mod), FusedMovingAvgObsFakeQuantize)\n        self.assertEqual(\n            type(conv_weight_fq_mod.activation_post_process),\n            expected_weight_observer_type,\n        )\n        self.assertEqual(conv_weight_fq_mod.dtype, torch.qint8)\n        self.assertEqual(conv_weight_fq_mod.quant_min, -127)\n        self.assertEqual(conv_weight_fq_mod.quant_max, 127)\n\n        # Verify: conv(fq(input), fq(weight * scale_factor.reshape), zero_bias)\n        zero_bias_node = conv_node.args[2] if len(conv_node.args) > 2 else None\n        mul_weight_scale_factor_node = conv_weight_fq_node.args[0]\n        (\n            conv_weight_fq_node,\n            scale_factor_reshape_node,\n        ) = mul_weight_scale_factor_node.args\n        if has_bias:\n            self.assertEqual(zero_bias_node.target, torch.ops.aten.zeros_like.default)\n        else:\n            self.assertTrue(zero_bias_node is None)\n        self.assertEqual(mul_weight_scale_factor_node.target, torch.ops.aten.mul.Tensor)\n        self.assertEqual(scale_factor_reshape_node.target, torch.ops.aten.reshape.default)\n\n        # Verify: scale_factor = bn_weight / sqrt(bn_running_var + eps)\n        scale_factor_node = scale_factor_reshape_node.args[0]\n        (bn_weight_node, sqrt_node) = scale_factor_node.args\n        bn_running_var_add_node = sqrt_node.args[0]\n        (bn_running_var_node, eps) = bn_running_var_add_node.args\n        self.assertEqual(scale_factor_node.target, torch.ops.aten.div.Tensor)\n        self.assertTrue(\"param_constant\" in bn_weight_node.target)\n        self.assertEqual(sqrt_node.target, torch.ops.aten.sqrt.default)\n        self.assertEqual(bn_running_var_add_node.target, torch.ops.aten.add.Tensor)\n        self.assertTrue(\"tensor_constant\" in bn_running_var_node.target)\n        self.assertEqual(eps, 1e-5)\n\n    def _test_representation(\n        self,\n        model: torch.nn.Module,\n        example_inputs: Tuple[Any, ...],\n        quantizer: Quantizer,\n        ref_node_occurrence: Dict[ns, int],\n        non_ref_node_occurrence: Dict[ns, int],\n        output_scale_idx: int = 3,\n    ) -> torch.nn.Module:\n        \"\"\" TODO: need to implement output checking based on output_scale once\n        torchdynamo issue is resolved\n        \"\"\"\n        # program capture\n        model = capture_pre_autograd_graph(\n            model,\n            example_inputs,\n        )\n        model_copy = copy.deepcopy(model)\n\n        model = prepare_pt2e(model, quantizer)\n        # Calibrate\n        model(*example_inputs)\n        model = convert_pt2e(model, use_reference_representation=True)\n        self.checkGraphModuleNodes(model, expected_node_occurrence=ref_node_occurrence)\n        # make sure it runs\n        pt2e_quant_output = model(*example_inputs)\n\n        # TODO: torchdynamo times out when we do this, we can enable numerical checking\n        # after that is fixed\n        model_copy = prepare_pt2e(model_copy, quantizer)\n        # Calibrate\n        model_copy(*example_inputs)\n        model_copy = convert_pt2e(model_copy, use_reference_representation=False)\n        self.checkGraphModuleNodes(model_copy, expected_node_occurrence=non_ref_node_occurrence)\n        pt2e_quant_output_copy = model_copy(*example_inputs)\n\n        idx = 0\n        for n in model_copy.graph.nodes:\n            if n.target == torch.ops.quantized_decomposed.quantize_per_tensor.default:\n                idx += 1\n                if idx == output_scale_idx:\n                    output_scale = n.args[1]\n        assert output_scale is not None\n\n        # make sure the result is off by one at most in the quantized integer representation\n        self.assertTrue(\n            torch.max(torch.abs(pt2e_quant_output_copy - pt2e_quant_output)) <= (2 * output_scale + 1e-5)\n        )\n\n@skipIfNoQNNPACK\nclass TestQuantizePT2E(PT2EQuantizationTestCase):\n    def test_simple_quantizer(self):\n        # TODO: use OP_TO_ANNOTATRO\n        class BackendAQuantizer(Quantizer):\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                for node in model.graph.nodes:\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.conv2d.default\n                    ):\n                        input_act = node.args[0]\n                        assert isinstance(input_act, Node)\n                        weight = node.args[1]\n                        assert isinstance(weight, Node)\n                        bias = node.args[2]\n                        assert isinstance(bias, Node)\n                        act_qspec = QuantizationSpec(\n                            dtype=torch.uint8,\n                            quant_min=0,\n                            quant_max=255,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_observer,\n                        )\n                        weight_qspec = QuantizationSpec(\n                            dtype=torch.int8,\n                            quant_min=-128,\n                            quant_max=127,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_weight_observer,\n                        )\n                        bias_qspec = QuantizationSpec(\n                            dtype=torch.float32,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.PlaceholderObserver,\n                        )\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                                weight: weight_qspec,\n                                bias: bias_qspec,\n                            },\n                            output_qspec=act_qspec,\n                            _annotated=True,\n                        )\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        node_occurrence = {\n            # two for input of the first conv, one for output for the first conv\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 3,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 3,\n        }\n        node_list = [\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.conv2d.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n        ]\n        self._test_quantizer(\n            TestHelperModules.ConvWithBNRelu(relu=False, bn=False),\n            example_inputs,\n            BackendAQuantizer(),\n            node_occurrence,\n            node_list,\n        )\n\n    def test_wo_annotate_conv_output_quantizer(self):\n        # TODO: use OP_TO_ANNOTATRO\n        class BackendAQuantizer(Quantizer):\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                act_qspec = QuantizationSpec(\n                    dtype=torch.uint8,\n                    quant_min=0,\n                    quant_max=255,\n                    qscheme=torch.per_tensor_affine,\n                    is_dynamic=False,\n                    observer_or_fake_quant_ctr=observer.default_observer,\n                )\n                weight_qspec = QuantizationSpec(\n                    dtype=torch.int8,\n                    quant_min=-128,\n                    quant_max=127,\n                    qscheme=torch.per_tensor_affine,\n                    is_dynamic=False,\n                    observer_or_fake_quant_ctr=observer.default_weight_observer,\n                )\n                bias_qspec = QuantizationSpec(\n                    dtype=torch.float32,\n                    is_dynamic=False,\n                    observer_or_fake_quant_ctr=observer.PlaceholderObserver,\n                )\n                for node in model.graph.nodes:\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.conv2d.default\n                    ):\n                        input_act = node.args[0]\n                        assert isinstance(input_act, Node)\n                        weight = node.args[1]\n                        assert isinstance(weight, Node)\n                        bias = node.args[2]\n                        assert isinstance(bias, Node)\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                                weight: weight_qspec,\n                                bias: bias_qspec,\n                            },\n                            _annotated=True,\n                        )\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n        m = torch.nn.Conv2d(2, 2, 1)\n        x = torch.rand(1, 2, 14, 14)\n        example_inputs = (x,)\n        # program capture\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n        m = prepare_pt2e(m, BackendAQuantizer())\n        m(*example_inputs)\n        m = convert_pt2e(m)\n        # Ensure the conv has no observer inserted at output\n        node_occurrence = {\n            # two for input of conv\n            ns.call_function(torch.ops.quantized_decomposed.quantize_per_tensor.default): 2,\n            ns.call_function(torch.ops.quantized_decomposed.dequantize_per_tensor.default): 2,\n        }\n        node_list = [\n            ns.call_function(torch.ops.quantized_decomposed.dequantize_per_tensor.default),\n            ns.call_function(torch.ops.quantized_decomposed.dequantize_per_tensor.default),\n            ns.call_function(torch.ops.aten.conv2d.default),\n        ]\n        self.checkGraphModuleNodes(\n            m, expected_node_list=node_list, expected_node_occurrence=node_occurrence\n        )\n\n    def test_max_pool2d_quantizer(self):\n        # TODO: use OP_TO_ANNOTATRO\n        class BackendAQuantizer(Quantizer):\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                act_qspec = QuantizationSpec(\n                    dtype=torch.uint8,\n                    quant_min=0,\n                    quant_max=255,\n                    qscheme=torch.per_tensor_affine,\n                    is_dynamic=False,\n                    observer_or_fake_quant_ctr=observer.default_observer,\n                )\n                weight_qspec = QuantizationSpec(\n                    dtype=torch.int8,\n                    quant_min=-128,\n                    quant_max=127,\n                    qscheme=torch.per_tensor_affine,\n                    is_dynamic=False,\n                    observer_or_fake_quant_ctr=observer.default_weight_observer,\n                )\n                bias_qspec = QuantizationSpec(\n                    dtype=torch.float32,\n                    is_dynamic=False,\n                    observer_or_fake_quant_ctr=observer.PlaceholderObserver,\n                )\n                for node in model.graph.nodes:\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.conv2d.default\n                    ):\n                        input_act = node.args[0]\n                        assert isinstance(input_act, Node)\n                        weight = node.args[1]\n                        assert isinstance(weight, Node)\n                        bias = node.args[2]\n                        assert isinstance(bias, Node)\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                                weight: weight_qspec,\n                                bias: bias_qspec,\n                            },\n                            _annotated=True,\n                        )\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.max_pool2d.default\n                    ):\n                        maxpool_node = node\n                        input_act = maxpool_node.args[0]\n                        assert isinstance(input_act, Node)\n                        maxpool_node.meta[\n                            \"quantization_annotation\"\n                        ] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                            },\n                            output_qspec=SharedQuantizationSpec(\n                                (input_act, maxpool_node)\n                            ),\n                            _annotated=True,\n                        )\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n        m = TestHelperModules.ConvMaxPool2d()\n        x = torch.rand(1, 2, 14, 14)\n        example_inputs = (x,)\n        # program capture\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n        m = prepare_pt2e(m, BackendAQuantizer())\n        m(*example_inputs)\n        m = convert_pt2e(m)\n        node_occurrence = {\n            # two for input of conv\n            # one for input of maxpool\n            # one for output of maxpool\n            ns.call_function(torch.ops.quantized_decomposed.quantize_per_tensor.default): 4,\n            ns.call_function(torch.ops.quantized_decomposed.dequantize_per_tensor.default): 4,\n        }\n        node_list = [\n            ns.call_function(torch.ops.quantized_decomposed.dequantize_per_tensor.default),\n            ns.call_function(torch.ops.quantized_decomposed.dequantize_per_tensor.default),\n            ns.call_function(torch.ops.aten.conv2d.default),\n            ns.call_function(torch.ops.quantized_decomposed.quantize_per_tensor.default),\n            ns.call_function(torch.ops.quantized_decomposed.dequantize_per_tensor.default),\n            ns.call_function(torch.ops.aten.max_pool2d.default),\n        ]\n        self.checkGraphModuleNodes(\n            m, expected_node_list=node_list, expected_node_occurrence=node_occurrence\n        )\n\n    def test_derived_qspec(self):\n        # TODO: use OP_TO_ANNOTATRO\n        class BackendAQuantizer(Quantizer):\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                for node in model.graph.nodes:\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.conv2d.default\n                    ):\n                        input_act = node.args[0]\n                        assert isinstance(input_act, Node)\n                        weight = node.args[1]\n                        assert isinstance(weight, Node)\n                        bias = node.args[2]\n                        assert isinstance(bias, Node)\n                        act_qspec = QuantizationSpec(\n                            dtype=torch.uint8,\n                            quant_min=0,\n                            quant_max=255,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_observer,\n                        )\n                        weight_qspec = QuantizationSpec(\n                            dtype=torch.int8,\n                            quant_min=-128,\n                            quant_max=127,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_weight_observer,\n                        )\n\n                        def derive_qparams_fn(\n                            obs_or_fqs: List[ObserverOrFakeQuantize],\n                        ) -> Tuple[Tensor, Tensor]:\n                            assert (\n                                len(obs_or_fqs) == 2\n                            ), f\"Expecting two obs/fqs, one for activation and one for weight, got: {len(obs_or_fq)}\"\n                            act_obs_or_fq = obs_or_fqs[0]\n                            weight_obs_or_fq = obs_or_fqs[1]\n                            act_scale, act_zp = act_obs_or_fq.calculate_qparams()\n                            (\n                                weight_scale,\n                                weight_zp,\n                            ) = weight_obs_or_fq.calculate_qparams()\n                            return torch.tensor([act_scale * weight_scale]).to(\n                                torch.float32\n                            ), torch.tensor([0]).to(torch.int32)\n\n                        bias_qspec = DerivedQuantizationSpec(\n                            derived_from=[(input_act, node), (weight, node)],\n                            derive_qparams_fn=derive_qparams_fn,\n                            dtype=torch.int32,\n                            quant_min=-(2**31),\n                            quant_max=2**31 - 1,\n                            qscheme=torch.per_tensor_symmetric,\n                        )\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                                weight: weight_qspec,\n                                bias: bias_qspec,\n                            },\n                            output_qspec=act_qspec,\n                            _annotated=True,\n                        )\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n        m = TestHelperModules.ConvWithBNRelu(relu=False, bn=False).eval()\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n\n        # program capture\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n        m = prepare_pt2e(m, BackendAQuantizer())\n        m(*example_inputs)\n        m = convert_pt2e(m)\n        node_occurrence = {\n            # input, weight, bias, output for the conv\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ): 4,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ): 4,\n        }\n        node_list = [\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ),\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ),\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ),\n            ns.call_function(torch.ops.aten.conv2d.default),\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ),\n        ]\n        self.checkGraphModuleNodes(\n            m, expected_node_list=node_list, expected_node_occurrence=node_occurrence\n        )\n\n    def test_derived_qspec_per_channel(self):\n        class BackendAQuantizer(Quantizer):\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                for node in model.graph.nodes:\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.conv2d.default\n                    ):\n                        input_act = node.args[0]\n                        assert isinstance(input_act, Node)\n                        weight = node.args[1]\n                        assert isinstance(weight, Node)\n                        bias = node.args[2]\n                        assert isinstance(bias, Node)\n                        act_qspec = QuantizationSpec(\n                            dtype=torch.uint8,\n                            quant_min=0,\n                            quant_max=255,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_observer,\n                        )\n                        weight_qspec = QuantizationSpec(\n                            dtype=torch.int8,\n                            quant_min=-128,\n                            quant_max=127,\n                            qscheme=torch.per_channel_affine,\n                            is_dynamic=False,\n                            ch_axis=0,\n                            observer_or_fake_quant_ctr=observer.default_per_channel_weight_observer,\n                        )\n\n                        def derive_qparams_fn(\n                            obs_or_fqs: List[ObserverOrFakeQuantize],\n                        ) -> Tuple[Tensor, Tensor]:\n                            assert (\n                                len(obs_or_fqs) == 1\n                            ), f\"Expecting one weight obs/fq, got: {len(obs_or_fq)}\"\n                            weight_obs_or_fq = obs_or_fqs[0]\n                            (\n                                weight_scale,\n                                weight_zp,\n                            ) = weight_obs_or_fq.calculate_qparams()\n                            return weight_scale, torch.zeros_like(weight_scale)\n\n                        bias_qspec = DerivedQuantizationSpec(\n                            derived_from=[(weight, node)],\n                            derive_qparams_fn=derive_qparams_fn,\n                            dtype=torch.int32,\n                            quant_min=-(2**31),\n                            quant_max=2**31 - 1,\n                            qscheme=torch.per_channel_symmetric,\n                            ch_axis=0,\n                        )\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                                weight: weight_qspec,\n                                bias: bias_qspec,\n                            },\n                            output_qspec=act_qspec,\n                            _annotated=True,\n                        )\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n        m = TestHelperModules.ConvWithBNRelu(relu=False, bn=False).eval()\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n\n        # program capture\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n        m = prepare_pt2e(m, BackendAQuantizer())\n        m(*example_inputs)\n        m = convert_pt2e(m)\n        m(*example_inputs)\n\n        node_occurrence = {\n            # input, output for the conv\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ): 2,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ): 2,\n            # weight and bias for conv\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_channel.default\n            ): 2,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_channel.default\n            ): 2,\n        }\n        node_list = [\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_channel.default\n            ),\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_channel.default\n            ),\n            ns.call_function(torch.ops.aten.conv2d.default),\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ),\n        ]\n        self.checkGraphModuleNodes(\n            m, expected_node_list=node_list, expected_node_occurrence=node_occurrence\n        )\n\n    def test_fixed_qparams_qspec(self):\n        class M(torch.nn.Module):\n            def forward(self, x):\n                return torch.sigmoid(x)\n\n        class BackendAQuantizer(Quantizer):\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                for node in model.graph.nodes:\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.sigmoid.default\n                    ):\n                        input_act = node.args[0]\n                        assert isinstance(input_act, Node)\n                        act_qspec = FixedQParamsQuantizationSpec(\n                            dtype=torch.uint8,\n                            quant_min=0,\n                            quant_max=255,\n                            qscheme=torch.per_tensor_affine,\n                            scale=1.0 / 256.0,\n                            zero_point=0,\n                        )\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                            },\n                            output_qspec=act_qspec,\n                            _annotated=True,\n                        )\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n        m = M().eval()\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n\n        # program capture\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n        m = prepare_pt2e(m, BackendAQuantizer())\n        m(*example_inputs)\n        m = convert_pt2e(m)\n        fixed_scale = 1.0 / 256.0\n        fixed_zero_point = 0\n        for n in m.graph.nodes:\n            if n.op == \"call_function\":\n                if (\n                    n.target\n                    == torch.ops.quantized_decomposed.quantize_per_tensor.default\n                ):\n                    scale_0 = n.args[1]\n                    zero_point_0 = n.args[2]\n                if (\n                    n.target\n                    == torch.ops.quantized_decomposed.dequantize_per_tensor.default\n                ):\n                    scale_1 = n.args[1]\n                    zero_point_1 = n.args[2]\n        self.assertEqual(scale_0, fixed_scale)\n        self.assertEqual(zero_point_0, fixed_zero_point)\n        self.assertEqual(scale_1, fixed_scale)\n        self.assertEqual(zero_point_1, fixed_zero_point)\n        node_occurrence = {\n            # two for input of the first conv, one for output for the first conv\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ): 2,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ): 2,\n        }\n        node_list = [\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ),\n            ns.call_function(torch.ops.aten.sigmoid.default),\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ),\n        ]\n        self.checkGraphModuleNodes(\n            m, expected_node_list=node_list, expected_node_occurrence=node_occurrence\n        )\n\n    def test_shared_qspec(self):\n        class BackendAQuantizer(Quantizer):\n            def annotate(self, model: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                for node in model.graph.nodes:\n                    if (\n                        node.op == \"call_function\"\n                        and node.target == torch.ops.aten.conv2d.default\n                    ):\n                        input_act = node.args[0]\n                        assert isinstance(input_act, Node)\n                        weight = node.args[1]\n                        assert isinstance(weight, Node)\n                        bias = node.args[2]\n                        assert isinstance(bias, Node)\n                        act_qspec = QuantizationSpec(\n                            dtype=torch.uint8,\n                            quant_min=0,\n                            quant_max=255,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_observer,\n                        )\n                        weight_qspec = QuantizationSpec(\n                            dtype=torch.int8,\n                            quant_min=-128,\n                            quant_max=127,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_weight_observer,\n                        )\n                        bias_qspec = QuantizationSpec(\n                            dtype=torch.float32,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.PlaceholderObserver,\n                        )\n                        node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n                            input_qspec_map={\n                                input_act: act_qspec,\n                                weight: weight_qspec,\n                                bias: bias_qspec,\n                            },\n                            output_qspec=act_qspec,\n                            _annotated=True,\n                        )\n                    elif node.target is torch.ops.aten.cat.default:\n                        cat_node = node\n                        input_nodes = cat_node.args[0]\n                        first_input_node = input_nodes[0]\n                        input_qspec_map = {}\n                        act_qspec = QuantizationSpec(\n                            dtype=torch.uint8,\n                            quant_min=0,\n                            quant_max=255,\n                            qscheme=torch.per_tensor_affine,\n                            is_dynamic=False,\n                            observer_or_fake_quant_ctr=observer.default_observer,\n                        )\n                        input_qspec_map[first_input_node] = act_qspec\n                        share_qparams_with_input_act0_qspec = SharedQuantizationSpec((first_input_node, cat_node))\n                        for input_node in input_nodes[1:]:\n                            input_qspec_map[input_node] = share_qparams_with_input_act0_qspec\n\n                        cat_node.meta[\n                            \"quantization_annotation\"\n                        ] = QuantizationAnnotation(\n                            input_qspec_map=input_qspec_map,\n                            output_qspec=share_qparams_with_input_act0_qspec,\n                            _annotated=True,\n                        )\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n\n        m = TestHelperModules.Conv2dWithCat().eval()\n        example_inputs = (torch.randn(1, 3, 5, 5), torch.randn(1, 3, 5, 5))\n\n        # program capture\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n        m = prepare_pt2e(m, BackendAQuantizer())\n        # make sure the two observers for input are shared\n        conv_output_obs = []\n        for n in m.graph.nodes:\n            if n.op == \"call_function\" and n.target == torch.ops.aten.conv2d.default:\n                conv_output_obs.append(getattr(m, list(n.users)[0].target))\n            if n.op == \"call_function\" and n.target == torch.ops.aten.cat.default:\n                inputs = n.args[0]\n                input0 = inputs[0]\n                input1 = inputs[1]\n                assert input0.op == \"call_module\"\n                assert input1.op == \"call_module\"\n                obs_ins0 = getattr(m, input0.target)\n                obs_ins1 = getattr(m, input1.target)\n                assert obs_ins0 == obs_ins1\n        assert len(conv_output_obs) == 2, \"expecting two observer that follows conv2d ops\"\n        # checking that the output observers for the two convs are shared as well\n        assert conv_output_obs[0] == conv_output_obs[1]\n\n        m(*example_inputs)\n        m = convert_pt2e(m)\n\n        node_occurrence = {\n            # two for input of the first conv, one for output for the first conv\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ): 7,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ): 7,\n        }\n        node_list = [\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ),\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ),\n            ns.call_function(torch.ops.aten.cat.default),\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ),\n        ]\n        self.checkGraphModuleNodes(\n            m, expected_node_list=node_list, expected_node_occurrence=node_occurrence\n        )\n\n    def test_add_and_inplace_add(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        example_inputs = (torch.randn(1, 3, 5, 5), torch.randn(1, 3, 5, 5),)\n        node_occurrence = {\n            # two input and one output for first add, and output for second add\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 4,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 4,\n        }\n        node_list = [\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.add.Tensor,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.add_.Tensor,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n        ]\n        self._test_quantizer(\n            TestHelperModules.AddInplaceAdd(),\n            example_inputs,\n            quantizer,\n            node_occurrence,\n            node_list,\n        )\n\n    def test_mul_and_inplace_mul(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        example_inputs = (torch.randn(1, 3, 5, 5), torch.randn(1, 3, 5, 5),)\n        node_occurrence = {\n            # two input and one output for first add, and output for second add\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 4,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 4,\n        }\n        node_list = [\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.mul.Tensor,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.mul_.Tensor,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n        ]\n        self._test_quantizer(\n            TestHelperModules.MulInplaceMul(),\n            example_inputs,\n            quantizer,\n            node_occurrence,\n            node_list,\n        )\n\n    def test_xnnpack_quantizer_conv(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 2,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 2,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 1,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 1,\n        }\n        node_list = [\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.conv2d.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n        ]\n        self._test_quantizer(\n            TestHelperModules.ConvWithBNRelu(relu=False, bn=False),\n            example_inputs,\n            quantizer,\n            node_occurrence,\n            node_list,\n        )\n\n    def test_xnnpack_quantizer_linear(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        m_eager = TestHelperModules.TwoLinearModule().eval()\n\n        # Test with 2d inputs\n        example_inputs_2d = (torch.randn(9, 8),)\n        example_inputs_3d = (torch.randn(9, 10, 8),)\n        example_inputs_4d = (torch.randn(9, 10, 11, 8),)\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 3,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 3,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 2,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 2,\n        }\n        qconfig = default_per_channel_symmetric_qnnpack_qconfig\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        for example_inputs in [example_inputs_2d, example_inputs_3d, example_inputs_4d]:\n            self._test_quantizer(\n                m_eager,\n                example_inputs,\n                quantizer,\n                node_occurrence,\n                [],\n                True,\n                qconfig_mapping,\n            )\n\n    def test_xnnpack_quantizer_conv_linear_no_permute(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 5,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 5,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 3,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 3,\n        }\n        qconfig = default_per_channel_symmetric_qnnpack_qconfig\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        # Test with 2d inputs\n        example_inputs = (torch.randn(2, 3, 4, 4),)\n        self._test_quantizer(\n            TestHelperModules.Conv2dWithTwoLinear(),\n            example_inputs,\n            quantizer,\n            node_occurrence,\n            [],\n            True,\n            qconfig_mapping,\n        )\n\n    def test_xnnpack_quantizer_conv_linear(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n\n        # Test with 2d inputs\n        example_inputs = (torch.randn(2, 3, 4, 4),)\n        node_occurrence = {\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 5,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 5,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 3,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 3,\n        }\n        qconfig = default_per_channel_symmetric_qnnpack_qconfig\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        self._test_quantizer(\n            TestHelperModules.Conv2dWithTwoLinearPermute(),\n            example_inputs,\n            quantizer,\n            node_occurrence,\n            [],\n            True,\n            qconfig_mapping,\n        )\n\n    def test_xnnpack_quantizer_linear_with_dynamic_shape(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        m_eager = TestHelperModules.TwoLinearModule().eval()\n\n        # Test with 2d inputs\n        example_inputs_3d = (torch.randn(9, 10, 8),)\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 3,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 3,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 2,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 2,\n        }\n        qconfig = default_per_channel_symmetric_qnnpack_qconfig\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        self._test_quantizer(\n            m_eager,\n            example_inputs_3d,\n            quantizer,\n            node_occurrence,\n            [],\n            True,\n            qconfig_mapping,\n            export_with_dynamic_shape=True,\n        )\n\n    def test_xnnpack_quantizer_obs_sharing_ops(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        m = TestHelperModules.Conv2dWithObsSharingOps().eval()\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 5,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 5,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 1,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 1,\n        }\n        node_list = [\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.conv2d.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.adaptive_avg_pool2d.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.hardtanh.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.mean.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n        ]\n        self._test_quantizer(m, example_inputs, quantizer, node_occurrence, node_list)\n\n    def test_xnnpack_quantizer_set_module_name(self):\n        class Sub(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n\n            def forward(self, x):\n                return self.linear(x)\n\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n                self.sub = Sub()\n\n            def forward(self, x):\n                x = self.linear(x)\n                x = self.sub(x)\n                return x\n\n        m = M().eval()\n        example_inputs = (torch.randn(3, 5),)\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_module_name(\"sub\", quantization_config)\n        node_occurrence = {\n            torch.ops.aten.linear.default: 2,\n            # input and output for the second linear\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 2,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 2,\n        }\n        node_list = [\n            # first linear is not quantized\n            torch.ops.aten.linear.default,\n            # second linear is quantized\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.linear.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n        ]\n        self._test_quantizer(m, example_inputs, quantizer, node_occurrence, node_list)\n\n    def test_xnnpack_quantizer_set_module_type(self):\n        class Sub(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n\n            def forward(self, x):\n                return self.linear(x)\n\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n                self.sub = Sub()\n\n            def forward(self, x):\n                x = self.linear(x)\n                x = self.sub(x)\n                return x\n\n        m = M().eval()\n        example_inputs = (torch.randn(3, 5),)\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_module_type(Sub, quantization_config)\n        node_occurrence = {\n            torch.ops.aten.linear.default: 2,\n            # input and output for the second linear\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 2,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 2,\n        }\n        node_list = [\n            # first linear is not quantized\n            torch.ops.aten.linear.default,\n            # second linear is quantized\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n            torch.ops.aten.linear.default,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default,\n        ]\n        self._test_quantizer(m, example_inputs, quantizer, node_occurrence, node_list)\n\n    def test_propagate_annotation(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        m = TestHelperModules.Conv2dPropAnnotaton().eval()\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n\n        # program capture\n        m = capture_pre_autograd_graph(\n            m,\n            example_inputs,\n        )\n\n        m = prepare_pt2e(m, quantizer)\n        m(*example_inputs)\n        self.assertEqual(\n            id(m.activation_post_process_2), id(m.activation_post_process_3)\n        )\n        self.assertEqual(\n            id(m.activation_post_process_3), id(m.activation_post_process_4)\n        )\n        m = convert_pt2e(m)\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ): 5,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ): 5,\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_channel.default\n            ): 2,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_channel.default\n            ): 2,\n        }\n        self.checkGraphModuleNodes(m, expected_node_occurrence=node_occurrence)\n\n    def test_xnnpack_quantizer_dynamic_linear(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(\n            is_per_channel=True, is_dynamic=True\n        )\n        quantizer.set_global(quantization_config)\n        m_eager = TestHelperModules.TwoLinearModule().eval()\n\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            torch.ops.quantized_decomposed.quantize_per_tensor.tensor: 2,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.tensor: 2,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 2,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 2,\n        }\n        act_affine_quant_obs = observer.PlaceholderObserver.with_args(\n            dtype=torch.qint8,\n            qscheme=torch.per_tensor_affine,\n            quant_min=-128,\n            quant_max=127,\n            eps=2**-12,\n            is_dynamic=True,\n        )\n        qconfig = QConfig(\n            activation=act_affine_quant_obs,\n            weight=per_channel_weight_observer_range_neg_127_to_127,\n        )\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        # Test with 2d inputs\n        example_inputs_2d = (torch.randn(9, 8),)\n        example_inputs_4d = (torch.randn(9, 10, 11, 8),)\n        for example_inputs in [example_inputs_2d, example_inputs_4d]:\n            # program capture\n            self._test_quantizer(\n                m_eager,\n                example_inputs,\n                quantizer,\n                node_occurrence,\n                [],\n                True,\n                qconfig_mapping,\n            )\n\n    def test_xnnpack_quantizer_dynamic_linear_with_conv(self):\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(\n            is_per_channel=False, is_dynamic=True\n        )\n        quantizer.set_global(quantization_config)\n        m_eager = TestHelperModules.ConvLinearWPermute().eval()\n\n        node_occurrence = {\n            # input and output are using quantize_per_tensor and weight is using quantize_per_channel\n            torch.ops.quantized_decomposed.quantize_per_tensor.tensor: 1,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.tensor: 1,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 1,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 1,\n        }\n        act_affine_quant_obs = observer.PlaceholderObserver.with_args(\n            dtype=torch.qint8,\n            qscheme=torch.per_tensor_affine,\n            quant_min=-128,\n            quant_max=127,\n            eps=2**-12,\n            is_dynamic=True,\n        )\n        qconfig = QConfig(\n            activation=act_affine_quant_obs,\n            weight=weight_observer_range_neg_127_to_127,\n        )\n        # Test with 2d inputs\n        example_inputs = (torch.randn(2, 3, 4, 4),)\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        self._test_quantizer(\n            m_eager,\n            example_inputs,\n            quantizer,\n            node_occurrence,\n            [],\n            True,\n            qconfig_mapping,\n        )\n\n    def test_composable_quantizer_linear_conv(self):\n        dynamic_quantizer = XNNPACKQuantizer()\n        quantization_config_dynamic = get_symmetric_quantization_config(\n            is_per_channel=False, is_dynamic=True\n        )\n        dynamic_quantizer.set_global(quantization_config_dynamic)\n        static_quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        static_quantizer.set_global(quantization_config)\n        # Note that dynamic quantization must be applied first here.\n        # this is because static quantizer also quantizes linear with static qspec\n        # and if we apply static_quantizer first then dynamic_quantizer cannot be applied\n        composable_quantizer = ComposableQuantizer(\n            [dynamic_quantizer, static_quantizer]\n        )\n        m_eager = TestHelperModules.ConvLinearWPermute().eval()\n\n        node_occurrence = {\n            torch.ops.quantized_decomposed.quantize_per_tensor.tensor: 1,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.tensor: 1,\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 4,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 4,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 1,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 1,\n        }\n        act_affine_quant_obs = observer.PlaceholderObserver.with_args(\n            dtype=torch.qint8,\n            qscheme=torch.per_tensor_affine,\n            quant_min=-128,\n            quant_max=127,\n            eps=2**-12,\n            is_dynamic=True,\n        )\n        dynamic_qconfig = QConfig(\n            activation=act_affine_quant_obs,\n            weight=weight_observer_range_neg_127_to_127,\n        )\n        # Test with 2d inputs\n        example_inputs = (torch.randn(2, 3, 4, 4),)\n        qconfig = default_per_channel_symmetric_qnnpack_qconfig\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        qconfig_mapping.set_object_type(torch.nn.Linear, dynamic_qconfig)\n        # Had to turn off check against fx because fx quant workflow does not seem\n        # to propagate observers for permute node for this model.\n        # Suprisingly it does propagate it for EmbeddingConvLinearModule\n        # TODO: Figure out the right behavior for propagation\n        self._test_quantizer(\n            m_eager,\n            example_inputs,\n            composable_quantizer,\n            node_occurrence,\n            [],\n            False,\n            qconfig_mapping,\n        )\n\n    def test_composable_quantizer_throw(self):\n        class BadQuantizer(Quantizer):\n            def annotate(self, gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n                for n in gm.graph.nodes:\n                    n.meta[\"quantization_annotation\"] = None\n\n            def validate(self, model: torch.fx.GraphModule) -> None:\n                pass\n\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        bad_quantizer = BadQuantizer()\n        composable_quantizer = ComposableQuantizer([quantizer, bad_quantizer])\n        m_eager = TestHelperModules.ConvLinearWPermute().eval()\n        example_inputs = (torch.randn(2, 3, 4, 4),)\n        self.assertRaises(\n            RuntimeError,\n            lambda: self._test_quantizer(\n                m_eager, example_inputs, composable_quantizer, {}\n            ),\n        )\n\n    def test_embedding_quantizer(self):\n        m_eager = TestHelperModules.EmbeddingModule().eval()\n        indices = torch.tensor(\n            [\n                9,\n                6,\n                5,\n                7,\n                8,\n                8,\n                9,\n                2,\n                8,\n                6,\n                6,\n                9,\n                1,\n                6,\n                8,\n                8,\n                3,\n                2,\n                3,\n                6,\n                3,\n                6,\n                5,\n                7,\n                0,\n                8,\n                4,\n                6,\n                5,\n                8,\n                2,\n                3,\n            ]\n        )\n        example_inputs = (indices,)\n\n        quantizer = EmbeddingQuantizer()\n        node_occurrence = {\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 1,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 1,\n        }\n        node_list = [\n            torch.ops.quantized_decomposed.quantize_per_channel.default,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default,\n            torch.ops.aten.embedding.default,\n        ]\n        # Compare against short term workflow\n        # cannot compare against fx quant because of the numerical differences coming\n        # from quantize and dequantize ops\n        qconfig = default_per_channel_symmetric_qnnpack_qconfig\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        qconfig_mapping = qconfig_mapping.set_object_type(\n            torch.nn.Embedding, float_qparams_weight_only_qconfig\n        )\n        self._test_quantizer(\n            m_eager,\n            example_inputs,\n            quantizer,\n            node_occurrence,\n            node_list,\n            True,\n            qconfig_mapping,\n        )\n\n    def test_embedding_conv_linear_quantization(self):\n        m_eager = TestHelperModules.EmbeddingConvLinearModule().eval()\n        indices = torch.tensor(\n            [\n                9,\n                6,\n                5,\n                7,\n                8,\n                8,\n                9,\n                2,\n                8,\n                6,\n                6,\n                9,\n                1,\n                6,\n                8,\n                8,\n                3,\n                2,\n                3,\n                6,\n                3,\n                6,\n                5,\n                7,\n                0,\n                8,\n                4,\n                6,\n                5,\n                8,\n                2,\n                3,\n            ]\n        )\n        indices = torch.unsqueeze(indices, 0)\n        example_inputs = (indices,)\n\n        embedding_quantizer = EmbeddingQuantizer()\n        dynamic_quantizer = XNNPACKQuantizer()\n        quantization_config_dynamic = get_symmetric_quantization_config(\n            is_per_channel=True, is_dynamic=True\n        )\n        dynamic_quantizer.set_global(quantization_config_dynamic)\n        static_quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        static_quantizer.set_global(quantization_config)\n        composed_quantizer = ComposableQuantizer(\n            [embedding_quantizer, dynamic_quantizer, static_quantizer]\n        )\n\n        act_affine_quant_obs = observer.PlaceholderObserver.with_args(\n            dtype=torch.qint8,\n            qscheme=torch.per_tensor_affine,\n            quant_min=-128,\n            quant_max=127,\n            eps=2**-12,\n            is_dynamic=True,\n        )\n        dynamic_qconfig = QConfig(\n            activation=act_affine_quant_obs,\n            weight=per_channel_weight_observer_range_neg_127_to_127,\n        )\n        qconfig = default_per_channel_symmetric_qnnpack_qconfig\n        qconfig_mapping = QConfigMapping().set_global(qconfig)\n        qconfig_mapping.set_object_type(torch.nn.Linear, dynamic_qconfig)\n        qconfig_mapping = qconfig_mapping.set_object_type(\n            torch.nn.Embedding, float_qparams_weight_only_qconfig\n        )\n\n        node_occurrence = {\n            torch.ops.quantized_decomposed.quantize_per_tensor.default: 4,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.default: 4,\n            torch.ops.quantized_decomposed.quantize_per_tensor.tensor: 1,\n            torch.ops.quantized_decomposed.dequantize_per_tensor.tensor: 1,\n            torch.ops.quantized_decomposed.quantize_per_channel.default: 3,\n            torch.ops.quantized_decomposed.dequantize_per_channel.default: 3,\n        }\n        self._test_quantizer(\n            m_eager,\n            example_inputs,\n            composed_quantizer,\n            node_occurrence,\n            [],\n            True,\n            qconfig_mapping,\n        )\n\n    def test_qat_conv_no_bias(self):\n        class M(torch.nn.Module):\n            def __init__(self, has_relu: bool):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(3, 3, 3, bias=False)\n                self.relu = torch.nn.ReLU() if has_relu else torch.nn.Identity()\n\n            def forward(self, x):\n                x = self.conv(x)\n                x = self.relu(x)\n                return x\n\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        # simple conv\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(has_relu=False), example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(has_relu=False), example_inputs, is_per_channel=True, verify_convert=True,\n        )\n        # conv + relu\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(has_relu=True), example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(has_relu=True), example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    def test_prepare_qat_conv_bn_fusion(self):\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        m = TestHelperModules.ConvWithBNRelu(relu=False)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m, example_inputs, is_per_channel=False, has_relu=False\n        )\n        m = TestHelperModules.ConvWithBNRelu(relu=False)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m, example_inputs, is_per_channel=True, has_relu=False\n        )\n\n    def test_qat_conv_bn_fusion_literal_args(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(3, 3, 3, stride=(2, 2), padding=(4, 4))\n                self.bn = torch.nn.BatchNorm2d(3)\n\n            def forward(self, x):\n                x = self.conv(x)\n                x = self.bn(x)\n                return x\n\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        # stride, padding, dilation, transposed, output_padding, groups\n        conv_args = ((2, 2), (4, 4), (1, 1), False, (0, 0), 1)\n        self._verify_symmetric_qnnpack_qat_graph(\n            M(),\n            example_inputs,\n            is_per_channel=False,\n            has_relu=False,\n            expected_conv_literal_args=conv_args,\n        )\n        self._verify_symmetric_qnnpack_qat_graph(\n            M(),\n            example_inputs,\n            is_per_channel=True,\n            has_relu=False,\n            expected_conv_literal_args=conv_args,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(), example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(), example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    def test_qat_conv_bn_fusion_no_conv_bias(self):\n        class M2(torch.nn.Module):\n            \"\"\"\n            Mixed conv + BN with and without conv bias.\n            \"\"\"\n            def __init__(self):\n                super().__init__()\n                self.conv1 = torch.nn.Conv2d(3, 3, 3, bias=False)\n                self.bn1 = torch.nn.BatchNorm2d(3)\n                self.conv2 = torch.nn.Conv2d(3, 3, 3, bias=True)\n                self.bn2 = torch.nn.BatchNorm2d(3)\n\n            def forward(self, x):\n                x = self.conv1(x)\n                x = self.bn1(x)\n                x = self.conv2(x)\n                x = self.bn2(x)\n                return x\n\n        m1 = TestHelperModules.ConvWithBNRelu(relu=False, bias=False)\n        example_inputs = (torch.randn(3, 3, 5, 5),)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m1, example_inputs, is_per_channel=False, has_relu=False, has_bias=False,\n        )\n        m1 = TestHelperModules.ConvWithBNRelu(relu=False, bias=False)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m1, example_inputs, is_per_channel=True, has_relu=False, has_bias=False,\n        )\n        m1 = TestHelperModules.ConvWithBNRelu(relu=False, bias=False)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m1, example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        m1 = TestHelperModules.ConvWithBNRelu(relu=False, bias=False)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m1, example_inputs, is_per_channel=True, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M2(), example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M2(), example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    def test_prepare_qat_conv_bn_relu_fusion(self):\n        m1 = TestHelperModules.ConvWithBNRelu(relu=True)\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m1, example_inputs, is_per_channel=False, has_relu=True\n        )\n        m1 = TestHelperModules.ConvWithBNRelu(relu=True)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m1, example_inputs, is_per_channel=True, has_relu=True\n        )\n\n    def test_qat_conv_bn_relu_fusion_no_conv_bias(self):\n        m1 = TestHelperModules.ConvWithBNRelu(relu=True, bias=False)\n        example_inputs = (torch.randn(3, 3, 5, 5),)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m1, example_inputs, is_per_channel=False, has_relu=True, has_bias=False,\n        )\n        m1 = TestHelperModules.ConvWithBNRelu(relu=True, bias=False)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m1, example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        m1 = TestHelperModules.ConvWithBNRelu(relu=True, bias=False)\n        self._verify_symmetric_qnnpack_qat_graph(\n            m1, example_inputs, is_per_channel=True, has_relu=True, has_bias=False,\n        )\n        m1 = TestHelperModules.ConvWithBNRelu(relu=True, bias=False)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m1, example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    def test_qat_inplace_add_relu(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(1, 1, 1)\n                self.relu = torch.nn.ReLU(inplace=True)\n\n            def forward(self, x):\n                x0 = x\n                x = self.conv(x)\n                x += x0\n                x = self.relu(x)\n                return x\n\n        example_inputs = (torch.randn(1, 1, 3, 3),)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(), example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(), example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    # @unittest.skip(\"not needed due to IR changes\")\n    def test_prepare_qat_conv_bn_fusion_getitem_placeholder(self):\n        \"\"\"\n        Test this special case seen in resnet18:\n\n          maxpool -> conv -> bn -> conv_bn_getitem\n\n        We want the metadata to be copied from the `conv_bn_getitem` node\n        \"\"\"\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.maxpool = torch.nn.MaxPool2d(kernel_size=1)\n                self.conv = torch.nn.Conv2d(3, 3, 3)\n                self.bn = torch.nn.BatchNorm2d(3)\n\n            def forward(self, x):\n                x = self.maxpool(x)\n                x = self.conv(x)\n                x = self.bn(x)\n                return x\n\n        def _get_getitem_nodes(m: torch.fx.GraphModule):\n            \"\"\"\n            Return a 2-tuple of (maxpool_getitem_node, conv_bn_getitem_node) from the graph.\n            \"\"\"\n            maxpool_getitem_node, conv_bn_getitem_node = None, None\n            for node in m.graph.nodes:\n                if node.target != operator.getitem:\n                    continue\n                if (\n                    node.args[0].target\n                    == torch.ops.aten._native_batch_norm_legit.default\n                ):\n                    conv_bn_getitem_node = node\n                else:\n                    raise ValueError(\"Unexpected getitem node \", node, node.args)\n            assert (\n                conv_bn_getitem_node is not None\n            ), \"did not find conv bn getitem node, bad test setup\"\n            return conv_bn_getitem_node\n\n        # Program capture\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        m = capture_pre_autograd_graph(\n            M(),\n            example_inputs,\n        )\n        m.graph.eliminate_dead_code()\n        m.recompile()\n        original_conv_bn_getitem_node = _get_getitem_nodes(m)\n\n        # Prepare QAT\n        quantizer = XNNPACKQuantizer()\n        quantizer.set_global(\n            get_symmetric_quantization_config(is_per_channel=False, is_qat=True)\n        )\n        m = prepare_qat_pt2e(m, quantizer)\n        conv_bn_getitem_node = _get_getitem_nodes(m)\n\n        # Verify that the metadata was copied from `conv_bn_getitem`, not `maxpool_getitem`\n        original_conv_bn_getitem_meta = original_conv_bn_getitem_node.meta[\n            \"quantization_annotation\"\n        ]\n        conv_bn_getitem_meta = conv_bn_getitem_node.meta[\"quantization_annotation\"]\n        self.assertEqual(conv_bn_getitem_meta, original_conv_bn_getitem_meta)\n\n    # TODO: merge these numerics tests with the graph tests above\n    def test_qat_conv_bn_numerics(self):\n        m = TestHelperModules.ConvWithBNRelu(relu=False)\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m, example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m, example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    def test_qat_conv_bn_relu_numerics(self):\n        m = TestHelperModules.ConvWithBNRelu(relu=True)\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m, example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            m, example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    def test_qat_update_shared_qspec(self):\n        \"\"\"\n        Test the case where nodes used in SharedQuantizationSpec were replaced\n        during QAT subgraph rewriting.\n        \"\"\"\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv = torch.nn.Conv2d(3, 3, 3)\n                self.bn = torch.nn.BatchNorm2d(3)\n                self.hardtanh = torch.nn.Hardtanh()\n\n            def forward(self, x):\n                x = self.conv(x)\n                x = self.bn(x)\n                x = self.hardtanh(x)\n                return x\n        m = M()\n        example_inputs = (torch.randn(1, 3, 5, 5),)\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(), example_inputs, is_per_channel=False, verify_convert=True,\n        )\n        self._verify_symmetric_qnnpack_qat_numerics(\n            M(), example_inputs, is_per_channel=True, verify_convert=True,\n        )\n\n    def test_representation_linear(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n\n            def forward(self, x):\n                return self.linear(x)\n\n        quantizer = XNNPACKQuantizer()\n        operator_config = get_symmetric_quantization_config(is_per_channel=False)\n        quantizer.set_global(operator_config)\n        example_inputs = (torch.randn(2, 5),)\n\n        self._test_representation(\n            M().eval(),\n            example_inputs,\n            quantizer,\n            ref_node_occurrence={},\n            non_ref_node_occurrence={}\n        )\n\n    def test_representation_conv2d(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.conv2d = torch.nn.Conv2d(3, 3, 3)\n\n            def forward(self, x):\n                return self.conv2d(x)\n\n        quantizer = XNNPACKQuantizer()\n        operator_config = get_symmetric_quantization_config(is_per_channel=False)\n        quantizer.set_global(operator_config)\n        example_inputs = (torch.randn(1, 3, 3, 3),)\n\n        self._test_representation(\n            M().eval(),\n            example_inputs,\n            quantizer,\n            ref_node_occurrence={},\n            non_ref_node_occurrence={}\n        )\n\n    def test_representation_add(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n\n            def forward(self, x, y):\n                return x + y\n\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        m_eager = M().eval()\n\n        example_inputs = (torch.randn(1, 3, 3, 3), torch.randn(1, 3, 3, 3),)\n\n        self._test_representation(\n            M().eval(),\n            example_inputs,\n            quantizer,\n            ref_node_occurrence={},\n            non_ref_node_occurrence={}\n        )\n\n    def test_representation_add_relu(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n\n            def forward(self, x, y):\n                out = x + y\n                out = torch.nn.functional.relu(out)\n                return out\n\n        quantizer = XNNPACKQuantizer()\n        operator_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(operator_config)\n\n        example_inputs = (torch.randn(1, 3, 3, 3), torch.randn(1, 3, 3, 3),)\n        ref_node_occurrence = {\n            ns.call_function(out_dtype): 2,\n        }\n\n        self._test_representation(\n            M().eval(),\n            example_inputs,\n            quantizer,\n            ref_node_occurrence=ref_node_occurrence,\n            non_ref_node_occurrence={}\n        )\n\n    def test_representation_maxpool2d(self):\n        quantizer = XNNPACKQuantizer()\n        operator_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(operator_config)\n        m_eager = TestHelperModules.ConvMaxPool2d().eval()\n\n        example_inputs = (torch.randn(1, 2, 2, 2),)\n\n        self._test_representation(\n            m_eager,\n            example_inputs,\n            quantizer,\n            ref_node_occurrence={},\n            non_ref_node_occurrence={}\n        )\n\n    @unittest.skip(\"will fix later\")\n    def test_representation_adaptive_avg_pool2d(self):\n        quantizer = XNNPACKQuantizer()\n        operator_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(operator_config)\n        m_eager = TestHelperModules.ConvWithAdaptiveAvgPool2d().eval()\n\n        example_inputs = (torch.randn(1, 3, 3, 3),)\n\n        self._test_representation(\n            m_eager,\n            example_inputs,\n            quantizer,\n            ref_node_occurrence={},\n            non_ref_node_occurrence={}\n        )\n\n    def test_representation_quantize_dequantize_per_channel(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.linear = torch.nn.Linear(5, 5)\n\n            def forward(self, x):\n                return self.linear(x)\n\n        quantizer = XNNPACKQuantizer()\n        # use per channel quantization for weight\n        operator_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(operator_config)\n        m_eager = M().eval()\n\n        inputs = [\n            (torch.randn(1, 5),),\n            (torch.randn(1, 3, 5),),\n            (torch.randn(1, 3, 3, 5),),\n            (torch.randn(1, 3, 3, 3, 5),),\n        ]\n        for example_inputs in inputs:\n            ref_node_occurrence = {\n                ns.call_function(\n                    torch.ops.quantized_decomposed.quantize_per_channel.default\n                ): 0,\n                ns.call_function(\n                    torch.ops.quantized_decomposed.dequantize_per_channel.default\n                ): 0,\n            }\n            non_ref_node_occurrence = {\n                ns.call_function(\n                    torch.ops.quantized_decomposed.quantize_per_channel.default\n                ): 1,\n                ns.call_function(\n                    torch.ops.quantized_decomposed.dequantize_per_channel.default\n                ): 1,\n            }\n\n            self._test_representation(\n                M().eval(),\n                example_inputs,\n                quantizer,\n                ref_node_occurrence,\n                non_ref_node_occurrence,\n                output_scale_idx=2,\n            )\n\n    def test_representation_quantize_dequantize(self):\n        class M(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n\n            def forward(self, x, y):\n                return x + y\n\n        quantizer = XNNPACKQuantizer()\n        quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n        quantizer.set_global(quantization_config)\n        m_eager = M().eval()\n\n        example_inputs = (torch.randn(1, 3, 3, 3), torch.randn(1, 3, 3, 3),)\n        ref_node_occurrence = {\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor\n            ): 0,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor\n            ): 0,\n        }\n        non_ref_node_occurrence = {\n            ns.call_function(\n                torch.ops.quantized_decomposed.quantize_per_tensor.default\n            ): 3,\n            ns.call_function(\n                torch.ops.quantized_decomposed.dequantize_per_tensor.default\n            ): 3,\n        }\n        self._test_representation(\n            M().eval(),\n            example_inputs,\n            quantizer,\n            ref_node_occurrence,\n            non_ref_node_occurrence\n        )\n\n@skipIfNoQNNPACK\nclass TestQuantizePT2EOps(QuantizationTestCase):\n    def test_gru(self):\n        \"\"\" this is a test for annotating fp32 GRU so that it produces\n        q -> dq -> fp32_gru -> q -> dq, this is currently enough for our use cases,\n        but we may change the annotation to be more precise in the future\n        \"\"\"\n        class RNNDynamicModel(torch.nn.Module):\n            def __init__(self, mod_type):\n                super().__init__()\n                self.qconfig = default_dynamic_qconfig\n                if mod_type == 'GRU':\n                    self.mod = torch.nn.GRU(2, 2).to(dtype=torch.float)\n                if mod_type == 'LSTM':\n                    self.mod = torch.nn.LSTM(2, 2).to(dtype=torch.float)\n\n            def forward(self, input_tensor, hidden_tensor):\n                input_tensor = 1 * input_tensor\n                hidden_tensor = 1 * hidden_tensor\n                output_tensor, hidden_out = self.mod(input_tensor, hidden_tensor)\n                return 1 * output_tensor, 1 * hidden_out\n\n        with override_quantized_engine(\"qnnpack\"):\n            model_fx = RNNDynamicModel(\"GRU\")\n            module_types = [torch.nn.GRU]\n            niter = 10\n            example_inputs = (\n                # input_tensor\n                torch.tensor([[100, -155],\n                              [-155, 100],\n                              [100, -155]], dtype=torch.float).unsqueeze(0).repeat(niter, 1, 1),\n                # hidden_tensor\n                # (D * num_layers, N, H_out)\n                torch.tensor([[[100, -155]]], dtype=torch.float).repeat(1, 3, 1),\n            )\n            model_graph = copy.deepcopy(model_fx)\n\n            qconfig_mapping = QConfigMapping().set_object_type(operator.mul, default_symmetric_qnnpack_qconfig)\n            model_fx = prepare_fx(model_fx, qconfig_mapping, example_inputs, backend_config=get_qnnpack_backend_config())\n            model_fx(*example_inputs)\n            model_fx = _convert_to_reference_decomposed_fx(model_fx)\n\n            torchdynamo.config.allow_rnn = True\n            model_graph = capture_pre_autograd_graph(\n                model_graph,\n                example_inputs,\n            )\n            quantizer = XNNPACKQuantizer()\n            quantization_config = get_symmetric_quantization_config(\n                is_per_channel=False, is_dynamic=False\n            )\n            quantizer.set_global(quantization_config)\n            model_graph = prepare_pt2e(model_graph, quantizer)\n            model_graph(*example_inputs)\n            model_graph = convert_pt2e(model_graph)\n            self.assertEqual(model_fx(*example_inputs), model_graph(*example_inputs))\n\n\n    def test_linear_gru(self):\n        \"\"\" this test is to make sure GRU annotation does not interfere with linear annotation\n        \"\"\"\n        class RNNDynamicModel(torch.nn.Module):\n            def __init__(self, mod_type):\n                super().__init__()\n                self.qconfig = default_dynamic_qconfig\n                self.linear = torch.nn.Linear(2, 2)\n                if mod_type == 'GRU':\n                    self.mod = torch.nn.GRU(2, 2).to(dtype=torch.float)\n                if mod_type == 'LSTM':\n                    self.mod = torch.nn.LSTM(2, 2).to(dtype=torch.float)\n\n            def forward(self, input_tensor, hidden_tensor):\n                input_tensor = self.linear(input_tensor)\n                input_tensor = 1 * input_tensor\n                hidden_tensor = 1 * hidden_tensor\n                output_tensor, hidden_out = self.mod(input_tensor, hidden_tensor)\n                return 1 * output_tensor, 1 * hidden_out\n\n        with override_quantized_engine(\"qnnpack\"):\n            model_fx = RNNDynamicModel(\"GRU\")\n            module_types = [torch.nn.GRU]\n            niter = 10\n            example_inputs = (\n                # input_tensor\n                torch.tensor([[100, -155],\n                              [-155, 100],\n                              [100, -155]], dtype=torch.float).unsqueeze(0).repeat(niter, 1, 1),\n                # hidden_tensor\n                # (D * num_layers, N, H_out)\n                torch.tensor([[[100, -155]]], dtype=torch.float).repeat(1, 3, 1),\n            )\n            model_graph = copy.deepcopy(model_fx)\n\n            qconfig_mapping = (\n                QConfigMapping().set_object_type(\n                    operator.mul, default_symmetric_qnnpack_qconfig\n                ).set_object_type(\n                    torch.nn.Linear, default_symmetric_qnnpack_qconfig\n                )\n            )\n            model_fx = prepare_fx(model_fx, qconfig_mapping, example_inputs, backend_config=get_qnnpack_backend_config())\n            model_fx(*example_inputs)\n            model_fx = _convert_to_reference_decomposed_fx(model_fx)\n\n            torchdynamo.config.allow_rnn = True\n            model_graph = capture_pre_autograd_graph(\n                model_graph,\n                example_inputs,\n            )\n            quantizer = XNNPACKQuantizer()\n            quantization_config = get_symmetric_quantization_config(\n                is_per_channel=False, is_dynamic=False\n            )\n            quantizer.set_global(quantization_config)\n            model_graph = prepare_pt2e(model_graph, quantizer)\n            model_graph(*example_inputs)\n            model_graph = convert_pt2e(model_graph)\n            self.assertEqual(model_fx(*example_inputs), model_graph(*example_inputs))\n\n# TODO: express this using self._test_quantizer\nclass TestQuantizePT2EModels(PT2EQuantizationTestCase):\n    @skip_if_no_torchvision\n    @skipIfNoQNNPACK\n    def test_resnet18(self):\n        import torchvision\n\n        with override_quantized_engine(\"qnnpack\"):\n            example_inputs = (torch.randn(1, 3, 224, 224),)\n            m = torchvision.models.resnet18().eval()\n            m_copy = copy.deepcopy(m)\n            # program capture\n            m = capture_pre_autograd_graph(\n                m,\n                example_inputs,\n            )\n\n            quantizer = XNNPACKQuantizer()\n            quantization_config = get_symmetric_quantization_config(is_per_channel=True)\n            quantizer.set_global(quantization_config)\n            m = prepare_pt2e(m, quantizer)\n            # checking that we inserted observers correctly for maxpool operator (input and\n            # output share observer instance)\n            self.assertEqual(\n                id(m.activation_post_process_3), id(m.activation_post_process_2)\n            )\n            after_prepare_result = m(*example_inputs)\n            m = convert_pt2e(m)\n\n            after_quant_result = m(*example_inputs)\n\n            # comparing with existing fx graph mode quantization reference flow\n            qconfig = default_per_channel_symmetric_qnnpack_qconfig\n            qconfig_mapping = QConfigMapping().set_global(qconfig)\n            backend_config = get_qnnpack_backend_config()\n            m_fx = prepare_fx(\n                m_copy, qconfig_mapping, example_inputs, backend_config=backend_config\n            )\n            after_prepare_result_fx = m_fx(*example_inputs)\n            m_fx = convert_to_reference_fx(m_fx, backend_config=backend_config)\n\n            after_quant_result_fx = m_fx(*example_inputs)\n\n            # the result matches exactly after prepare\n            # Note: this currently will always be true since we are inserting observers\n            # the check becomes useful when we add qat examples\n            # but we can still manully inspect the printed observers to make sure\n            # it matches\n            self.assertEqual(after_prepare_result, after_prepare_result_fx)\n            self.assertEqual(\n                compute_sqnr(after_prepare_result, after_prepare_result_fx),\n                torch.tensor(float(\"inf\")),\n            )\n            # there are slight differences after convert due to different implementations\n            # of quant/dequant\n            self.assertTrue(\n                torch.max(after_quant_result - after_quant_result_fx) < 1e-1\n            )\n            self.assertTrue(\n                compute_sqnr(after_quant_result, after_quant_result_fx) > 35\n            )\n\n    @skip_if_no_torchvision\n    @skipIfNoQNNPACK\n    def test_qat_resnet18(self):\n        import torchvision\n        with override_quantized_engine(\"qnnpack\"):\n            example_inputs = (torch.randn(1, 3, 224, 224),)\n            m = torchvision.models.resnet18()\n            self._verify_symmetric_qnnpack_qat_numerics(\n                m, example_inputs, is_per_channel=False, verify_convert=True,\n            )\n            self._verify_symmetric_qnnpack_qat_numerics(\n                m, example_inputs, is_per_channel=True, verify_convert=True,\n            )\n\n    @skip_if_no_torchvision\n    @skipIfNoQNNPACK\n    def test_qat_mobilenet_v2(self):\n        import torchvision\n        with override_quantized_engine(\"qnnpack\"):\n            example_inputs = (torch.randn(1, 3, 224, 224),)\n            m = torchvision.models.mobilenet_v2()\n            self._verify_symmetric_qnnpack_qat_numerics(\n                m, example_inputs, is_per_channel=False, verify_convert=True,\n            )\n            self._verify_symmetric_qnnpack_qat_numerics(\n                m, example_inputs, is_per_channel=True, verify_convert=True,\n            )\n",
      "torch/ao/quantization/__init__.py": "# flake8: noqa: F403\n\nfrom .fake_quantize import *  # noqa: F403\nfrom .fuse_modules import fuse_modules  # noqa: F403\nfrom .fuse_modules import fuse_modules_qat  # noqa: F403\nfrom .fuser_method_mappings import *  # noqa: F403\nfrom .observer import *  # noqa: F403\nfrom .qconfig import *  # noqa: F403\nfrom .qconfig_mapping import *  # noqa: F403\nfrom .quant_type import *  # noqa: F403\nfrom .quantization_mappings import *  # type: ignore[no-redef]\nfrom .quantize import *  # noqa: F403\nfrom .quantize_jit import *  # noqa: F403\nfrom .stubs import *  # noqa: F403\nfrom typing import Union, List, Callable, Tuple, Optional\nfrom torch import Tensor\nimport torch\n\nObserverOrFakeQuantize = Union[ObserverBase, FakeQuantizeBase]\nObserverOrFakeQuantize.__module__ = \"torch.ao.quantization\"\n\n__all__ = [\n    \"DeQuantStub\",\n    \"FakeQuantize\",\n    \"FakeQuantizeBase\",\n    \"FixedQParamsFakeQuantize\",\n    \"FixedQParamsObserver\",\n    \"FusedMovingAvgObsFakeQuantize\",\n    \"HistogramObserver\",\n    \"MatchAllNode\",\n    \"MinMaxObserver\",\n    \"MovingAverageMinMaxObserver\",\n    \"MovingAveragePerChannelMinMaxObserver\",\n    \"NoopObserver\",\n    \"ObserverBase\",\n    \"ObserverOrFakeQuantize\",\n    \"Pattern\",\n    \"PerChannelMinMaxObserver\",\n    \"PlaceholderObserver\",\n    \"QConfig\",\n    \"QConfigAny\",\n    \"QConfigDynamic\",\n    \"QConfigMapping\",\n    \"QuantStub\",\n    \"QuantType\",\n    \"QuantWrapper\",\n    \"RecordingObserver\",\n    \"ReuseInputObserver\",\n    \"UniformQuantizationObserverBase\",\n    \"add_quant_dequant\",\n    \"convert\",\n    \"convert_dynamic_jit\",\n    \"convert_jit\",\n    \"default_affine_fixed_qparams_fake_quant\",\n    \"default_affine_fixed_qparams_observer\",\n    \"default_debug_observer\",\n    \"default_dynamic_fake_quant\",\n    \"default_dynamic_quant_observer\",\n    \"default_embedding_fake_quant\",\n    \"default_embedding_fake_quant_4bit\",\n    \"default_eval_fn\",\n    \"default_fake_quant\",\n    \"default_fixed_qparams_range_0to1_fake_quant\",\n    \"default_fixed_qparams_range_0to1_observer\",\n    \"default_fixed_qparams_range_neg1to1_fake_quant\",\n    \"default_fixed_qparams_range_neg1to1_observer\",\n    \"default_float_qparams_observer\",\n    \"default_float_qparams_observer_4bit\",\n    \"default_fused_act_fake_quant\",\n    \"default_fused_per_channel_wt_fake_quant\",\n    \"default_fused_wt_fake_quant\",\n    \"default_histogram_fake_quant\",\n    \"default_histogram_observer\",\n    \"default_observer\",\n    \"default_per_channel_weight_fake_quant\",\n    \"default_per_channel_weight_observer\",\n    \"default_placeholder_observer\",\n    \"default_reuse_input_observer\",\n    \"default_symmetric_fixed_qparams_fake_quant\",\n    \"default_symmetric_fixed_qparams_observer\",\n    \"default_weight_fake_quant\",\n    \"default_weight_observer\",\n    \"disable_fake_quant\",\n    \"disable_observer\",\n    \"enable_fake_quant\",\n    \"enable_observer\",\n    \"fuse_conv_bn\",\n    \"fuse_conv_bn_jit\",\n    \"fuse_conv_bn_relu\",\n    \"fuse_convtranspose_bn\",\n    \"fuse_linear_bn\",\n    \"fuse_modules\",\n    \"fuse_modules_qat\",\n    \"fused_per_channel_wt_fake_quant_range_neg_127_to_127\",\n    \"fused_wt_fake_quant_range_neg_127_to_127\",\n    \"get_combined_dict\",\n    \"get_default_compare_output_module_list\",\n    \"get_default_custom_config_dict\",\n    \"get_default_dynamic_quant_module_mappings\",\n    \"get_default_dynamic_sparse_quant_module_mappings\",\n    \"get_default_float_to_quantized_operator_mappings\",\n    \"get_default_qat_module_mappings\",\n    \"get_default_qat_qconfig\",\n    \"get_default_qat_qconfig_dict\",\n    \"get_default_qat_qconfig_mapping\",\n    \"get_default_qconfig\",\n    \"get_default_qconfig_dict\",\n    \"get_default_qconfig_mapping\",\n    \"get_default_qconfig_propagation_list\",\n    \"get_default_static_quant_module_mappings\",\n    \"get_default_static_quant_reference_module_mappings\",\n    \"get_default_static_sparse_quant_module_mappings\",\n    \"get_dynamic_quant_module_class\",\n    \"get_embedding_qat_module_mappings\",\n    \"get_embedding_static_quant_module_mappings\",\n    \"get_fuser_method\",\n    \"get_fuser_method_new\",\n    \"get_observer_state_dict\",\n    \"get_quantized_operator\",\n    \"get_static_quant_module_class\",\n    \"load_observer_state_dict\",\n    \"no_observer_set\",\n    \"per_channel_weight_observer_range_neg_127_to_127\",\n    \"prepare\",\n    \"prepare_dynamic_jit\",\n    \"prepare_jit\",\n    \"prepare_qat\",\n    \"propagate_qconfig_\",\n    \"qconfig_equals\",\n    \"quantize\",\n    \"quantize_dynamic\",\n    \"quantize_dynamic_jit\",\n    \"quantize_jit\",\n    \"quantize_qat\",\n    \"script_qconfig\",\n    \"script_qconfig_dict\",\n    \"swap_module\",\n    \"weight_observer_range_neg_127_to_127\",\n]\n\ndef default_eval_fn(model, calib_data):\n    r\"\"\"\n    Default evaluation function takes a torch.utils.data.Dataset or a list of\n    input Tensors and run the model on the dataset\n    \"\"\"\n    for data, target in calib_data:\n        model(data)\n\nclass _DerivedObserverOrFakeQuantize(ObserverBase):\n    r\"\"\" This observer is used to describe an observer whose quantization parameters\n    are derived from other observers\n    \"\"\"\n    def __init__(\n        self,\n        dtype: torch.dtype,\n        obs_or_fqs: List[ObserverOrFakeQuantize],\n        derive_qparams_fn: Callable[[List[ObserverOrFakeQuantize]], Tuple[Tensor, Tensor]],\n        quant_min: Optional[int]=None,\n        quant_max: Optional[int]=None,\n        qscheme: Optional[torch.qscheme]=None,\n        ch_axis: Optional[int] = None\n    ):\n        super().__init__(dtype)\n        self.obs_or_fqs = obs_or_fqs\n        self.derive_qparams_fn = derive_qparams_fn\n        self.quant_min = quant_min\n        self.quant_max = quant_max\n        self.qscheme = qscheme\n        self.ch_axis = ch_axis\n\n        from .utils import is_per_channel\n        if is_per_channel(self.qscheme):\n            assert self.ch_axis is not None, \"Must provide a valid ch_axis if qscheme is per channel\"\n\n    def forward(self, x: Tensor) -> Tensor:\n        return x\n\n    def calculate_qparams(self):\n        return self.derive_qparams_fn(self.obs_or_fqs)\n",
      "torch/ao/quantization/pt2e/utils.py": "import torch\nfrom torch.fx import (\n    GraphModule,\n    Node,\n)\nfrom torch.fx.subgraph_rewriter import replace_pattern_with_filters\nimport torch.nn.functional as F\nfrom torch.nn.utils.fusion import fuse_conv_bn_weights\nimport operator\nfrom typing import Any, Callable, Dict, Optional, Tuple, List, Union\nfrom torch.utils._pytree import LeafSpec\nfrom torch._export import capture_pre_autograd_graph\n\n__all__ = [\n    \"fold_bn_weights_into_conv_node\",\n    \"get_aten_graph_module\",\n    \"remove_tensor_overload_for_qdq_ops\",\n]\n\ndef _get_tensor_constant_from_node(node, m):\n    if node is None:\n        return None\n    assert node.op == \"get_attr\"\n    return getattr(m, node.target)\n\ndef _get_all_arguments(orig_args, orig_kwargs, args_schema):\n    all_args = []\n    for i, schema in enumerate(args_schema):\n        if schema.name in orig_kwargs:\n            all_args.append(orig_kwargs[schema.name])\n        elif not schema.kwarg_only and i < len(orig_args):\n            all_args.append(orig_args[i])\n        else:\n            all_args.append(schema.default_value)\n    return all_args\n\ndef fold_bn_weights_into_conv_node(\n    conv_node: Node,\n    conv_weight_node: Node,\n    conv_bias_node: Optional[Node],\n    bn_node: Node,\n    m: GraphModule\n) -> None:\n    # conv2d args: input, weight, bias, stride, padding, dilation, ...\n    # Note: this should also work for conv1d, conv3d and transposed conv1-3d as well with\n    # easy tweaks\n    conv_w = _get_tensor_constant_from_node(conv_weight_node, m)\n    conv_b = _get_tensor_constant_from_node(conv_bias_node, m)\n    transpose = not (conv_node.target == torch.ops.aten.conv2d.default)\n    # TODO(Leslie): WA to support both graph capture of `torch._export.capture_pre_autograd_graph`\n    # and `torch._dynamo_export` for 2.1 release, remove it after formal support of new graph capture\n    # API in Inductor for X86.\n    if conv_node.target == torch.ops.aten.convolution.default:\n        assert type(conv_node.args[6]) is bool\n        transpose = conv_node.args[6]\n\n    # eval bn args: input, weight, bias, running mean, running var, momentum, eps\n    # train bn args: input, weight, bias, running mean, running var, training, momentum, eps\n    bn_args_schema = bn_node.target._schema.arguments  # type: ignore[union-attr]\n    bn_args = _get_all_arguments(bn_node.args, bn_node.kwargs, bn_args_schema)\n    bn_w = _get_tensor_constant_from_node(bn_args[1], m)\n    bn_b = _get_tensor_constant_from_node(bn_args[2], m)\n    bn_rm = _get_tensor_constant_from_node(bn_args[3], m)\n    bn_rv = _get_tensor_constant_from_node(bn_args[4], m)\n    if bn_node.target == torch.ops.aten._native_batch_norm_legit_no_training.default:\n        eps_arg_index = 6\n    elif bn_node.target == torch.ops.aten._native_batch_norm_legit.default:\n        eps_arg_index = 7\n    else:\n        raise ValueError(\"BN node target is unexpected \", bn_node.target)\n    bn_eps = bn_args[eps_arg_index]\n\n    fused_weight, fused_bias = fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b, transpose=transpose)\n\n    # update the weight and bias for conv\n    conv_args = list(conv_node.args)\n    # TODO(Leslie): Remove the check of node target after formal support of new graph capture\n    # API `torch._export.capture_pre_autograd_graph` in Inductor for X86.\n    # filling in the default bias argument\n    if len(conv_args) == 2 and (conv_node.target == torch.ops.aten.conv2d.default):\n        conv_args.append(None)\n\n    # calling data since the fused_weight and fused_bias are nn.Parameter\n    weight_attr_name = conv_weight_node.target\n    assert isinstance(weight_attr_name, str)\n    setattr(m, weight_attr_name, fused_weight)\n    if conv_bias_node is not None:\n        bias_attr_name = conv_bias_node.target\n    else:\n        bias_attr_name = weight_attr_name + \"_bias\"\n        with m.graph.inserting_before(conv_node):\n            get_bias_node = m.graph.get_attr(bias_attr_name)\n        # NOTE: here we assume the bias of conv is not quantized!\n        conv_args[2] = get_bias_node\n    setattr(m, bias_attr_name, fused_bias)  # type: ignore[arg-type]\n    conv_node.args = tuple(conv_args)\n\n    # native_batch_norm has 3 outputs, we expect getitem calls on the output\n    # and we want to replace the uses of getitem 0 with the output of conv\n    #\n    # Before:\n    # conv -> bn - (first output) -> users1\n    #          \\ - (second output) -> users2\n    #          \\ - (third output) -> users3\n    # After:\n    # conv -> (first output) -> users1\n    #       bn -\n    #          \\ - (second output) -> users2\n    #          \\ - (third output) -> users3\n    # if users2 and users3 are empty then bn will be removed through dead code elimination\n\n    for user in bn_node.users:\n        if user.op != \"call_function\" or user.target != operator.getitem or user.args[1] != 0:\n            continue\n        user.replace_all_uses_with(conv_node)\n\n# fuse conv bn weights, inplace modification of the graph_module and graph\ndef _fuse_conv_bn_(m: GraphModule) -> None:\n    for n in m.graph.nodes:\n        if n.op != \"call_function\" or n.target != torch.ops.aten._native_batch_norm_legit_no_training.default:\n            continue\n        bn_node = n\n        n = bn_node.args[0]\n        # TODO(Leslie): Remove the check of node target torch.ops.aten.convolution.default after formal\n        # support of new graph capture API `torch._export.capture_pre_autograd_graph` in Inductor for X86.\n        if n.op != \"call_function\" or (\n            n.target != torch.ops.aten.conv2d.default\n            and n.target != torch.ops.aten.convolution.default\n        ):\n            continue\n        conv_node = n\n        conv_weight_node = conv_node.args[1]\n        conv_bias_node = conv_node.args[2] if len(conv_node.args) > 2 else None\n        fold_bn_weights_into_conv_node(conv_node, conv_weight_node, conv_bias_node, bn_node, m)\n\n    m.graph.eliminate_dead_code()\n    m.recompile()\n\ndef _get_node_name_to_scope(model: GraphModule) -> Dict[str, Tuple[str, type]]:\n    # TODO: move this information to fx node itself\n    node_name_to_scope: Dict[str, Tuple[str, type]] = {}\n    for n in model.graph.nodes:\n        nn_module_stack = n.meta.get(\"nn_module_stack\", None)\n        current_scope = (\"\", type(None))\n        if nn_module_stack:\n            bt = list(nn_module_stack.values())[-1]\n            current_scope = (bt[0].split(\".\")[-1], bt[1])\n        node_name_to_scope[n.name] = current_scope\n    return node_name_to_scope\n\ndef get_aten_graph_module(\n    pattern: Callable,\n    example_inputs: Tuple[Any, ...],\n    **kwargs,\n) -> GraphModule:\n    \"\"\"\n    Convert the pattern to an FX graph with decomposed aten ops.\n    \"\"\"\n    aten_pattern = capture_pre_autograd_graph(\n        pattern,\n        example_inputs,\n        kwargs,\n    )\n    aten_pattern.graph.eliminate_dead_code()\n    aten_pattern.recompile()\n    return aten_pattern\n\ndef remove_tensor_overload_for_qdq_ops(match_pattern: GraphModule) -> None:\n    \"\"\" Remove .tensor overload for quantize/dequantize ops so that we can\n    use the match_pattern that we get from torchdynamo export to match the output of convert_pt2e\n    \"\"\"\n    _MAP = {\n        torch.ops.quantized_decomposed.quantize_per_tensor.default: torch.ops.quantized_decomposed.quantize_per_tensor,\n        torch.ops.quantized_decomposed.dequantize_per_tensor.default: torch.ops.quantized_decomposed.dequantize_per_tensor,\n        torch.ops.quantized_decomposed.quantize_per_tensor.tensor: torch.ops.quantized_decomposed.quantize_per_tensor,\n        torch.ops.quantized_decomposed.dequantize_per_tensor.tensor: torch.ops.quantized_decomposed.dequantize_per_tensor,\n        torch.ops.quantized_decomposed.quantize_per_tensor.tensor2: torch.ops.quantized_decomposed.quantize_per_tensor,\n        torch.ops.quantized_decomposed.dequantize_per_tensor.tensor2: torch.ops.quantized_decomposed.dequantize_per_tensor,\n        torch.ops.quantized_decomposed.quantize_per_channel.default: torch.ops.quantized_decomposed.quantize_per_channel,\n        torch.ops.quantized_decomposed.dequantize_per_channel.default: torch.ops.quantized_decomposed.dequantize_per_channel,\n        torch.ops.aten.clamp.Tensor: torch.ops.aten.clamp,\n    }\n    for n in match_pattern.graph.nodes:\n        if n.op != \"call_function\":\n            continue\n        if n.target in _MAP:\n            n.target = _MAP[n.target]\n\ndef _replace_dropout_for_eval(m: GraphModule):\n    \"\"\"\n    Replace the aten training dropout pattern with a noop, intended for eval.\n\n    For models with dropout torch ops (nn.Dropout, F.dropout), calling model.eval()\n    effectively turns these dropout ops into noops. For exported models, however,\n    this is not done automatically, since the aten dropout patterns previously generated\n    for training remain in the graph. Here we rewrite these dropout patterns with noops\n    to avoid incorrectly applying further dropout during eval.\n\n    See https://github.com/pytorch/pytorch/issues/103681.\n    \"\"\"\n    def dropout_train(x):\n        return F.dropout(x, p=0.5, training=True)\n\n    def dropout_eval(x):\n        return F.dropout(x, p=0.5, training=False)\n\n    example_inputs = (torch.randn(1),)\n    match_pattern = get_aten_graph_module(dropout_train, example_inputs)\n    replacement_pattern = get_aten_graph_module(dropout_eval, example_inputs)\n\n    replace_pattern_with_filters(\n        m,\n        match_pattern,\n        replacement_pattern,\n        match_filters=[],\n        ignore_literals=True,\n    )\n    m.recompile()\n\ndef _is_literal(arg):\n    if isinstance(arg, (int, float)):\n        return True\n    if isinstance(arg, (tuple, list)):\n        return all(map(_is_literal, arg))\n    return False\n\ndef _replace_literals_with_new_placeholders(\n    gm: torch.fx.GraphModule,\n    merge_dup: bool = False,\n    exclude_literals: Optional[List[Any]] = None\n):\n    \"\"\"Replace the literals in the graph with placeholder nodes that's created on the fly while we\n    traverse the graph, so that the literal arguments in the graph can be matched and replaced\n\n    To use this, the pattern and replacement graph should have the exact same number of literal args\n    and they should be used in the exact same order in the pattern and replacement graph.\n\n    If the literal arguments are not used in the same order in pattern and replacement graph, please\n    use `_replace_literals_with_existing_placeholders` instead\n\n    Args:\n        `gm`: input GraphModule that we'll transform\n        `merge_dup`: boolean flag to indicate that if the same literal appears multiple times in\n         the graph, whether they should correspond to the same placeholder or not\n        `exclude_literals`: a list of literals that will not be replaced with placeholders\n\n    Example:\n\n    # 1. Original Graph\n    def pattern(self, x):\n        return x + 3\n\n    def replacement(self, x):\n        return x - 3\n\n    example_inputs = (torch.randn(1, 3, 3, 3),)\n    pattern_gm = get_aten_graph_module(pattern, example_inputs)\n    replacement_gm = get_aten_graph_module(pattern, example_inptus)\n\n    # 2. Before calling replace literals we'll see the following graph:\n    def pattern(self, x):\n        return x + 3\n\n    def replacement(self, x):\n        return x - 3\n\n    pattern_gm = _replace_literals_with_new_placeholders(pattern_gm)\n    replacement_gm = _replace_literals_with_new_placeholders(replacement_gm)\n\n    # 3. After replacing literals with new placeholder nodes\n\n    def pattern(self, x, new_ph):\n        return x + new_ph\n\n    def pattern(self, x, new_ph):\n        return x - new_ph\n\n    \"\"\"\n    last_ph = None\n    cnt = 0\n    literal_to_ph: Dict[Union[float, bool, int, torch.dtype], Node] = {}\n    if exclude_literals is None:\n        exclude_literals = []\n\n    for node in gm.graph.nodes:\n        if node.op == \"placeholder\":\n            last_ph = node\n            cnt += 1\n            continue\n        with gm.graph.inserting_after(last_ph):\n            new_args = []\n            for arg in node.args:\n                if _is_literal(arg) and arg not in exclude_literals:\n                    if merge_dup and arg in literal_to_ph:\n                        new_args.append(literal_to_ph[arg])\n                    else:\n                        ph_node = gm.graph.placeholder(\"arg\" + str(cnt))\n                        new_args.append(ph_node)\n                        gm._in_spec.children_specs[0].children_specs.append(LeafSpec())\n                        cnt += 1\n                        if merge_dup:\n                            literal_to_ph[arg] = ph_node\n                else:\n                    new_args.append(arg)\n            new_args = tuple(new_args)\n\n        node.args = new_args\n    return gm\n\n\ndef _replace_literals_with_existing_placeholders(\n    gm: torch.fx.GraphModule,\n    exclude_literals: Optional[List[Any]] = None,\n    literal_to_ph_idx: Optional[Dict[Union[float, int, bool, torch.dtype], int]] = None\n):\n    \"\"\"Replace the literals in the graph with **existing** placeholder nodes, so that the literal arguments\n    in the graph can be matched and replaced\n\n    To use this, all literal args in the graph should be unique and each of them should correspond\n    to exactly one placeholder node\n\n    # 1. Original Graph\n    def pattern(self, x_i8, scale, zero_point, quant_min, quant_max):\n        return torch.dequantize_per_tensor(x_i8, scale, zero_point, quant_min, quant_max)\n\n    def replacement(x_i8, scale, zero_point, quant_min, quant_max):\n        x_i8 = torch.clamp(x_i8, quant_min, quant_max)\n        return ((x_i8.to(torch.float32) - zero_point) * scale).to(dtype=torch.float32)\n\n    example_inputs = (\n        torch.randn(1, 3, 3, 3),\n        1.0,\n        0,\n        -128,\n        127,\n    )\n    pattern_gm = get_aten_graph_module(pattern, example_inputs)\n    replacement_gm = get_aten_graph_module(pattern, example_inptus)\n\n    # 2. Before calling replace literals we'll see the following graph:\n    def pattern(self, x_i8, scale, zero_point, quant_min, quant_max):\n        # scale/zero_point/quant_min/quant_max are burnt in since they are scalar values\n        return torch.dequantize_per_tensor(x_i8, 1.0, 0, -128, 127)\n\n    def replacement(x_i8, scale, zero_point, quant_min, quant_max):\n        # scale/zero_point/quant_min/quant_max are burnt in since they are scalar values\n        x_i8 = torch.clamp(x_i8, -128, 127)\n        return ((x_i8.to(torch.float32) - 0) * 1.0).to(dtype=torch.float32)\n\n    # Note that literal args appear in different order in pattern and replacement graph, so\n    # we can't use _replace_literals_with_new_placeholders\n\n    literal_to_ph_idx = {1.0: 1, 0: 2, -128: 3, 127: 4}\n    pattern_gm = _replace_literals_with_existing_placeholders(pattern_gm, literal_to_ph_idx)\n    replacement_gm = _replace_literals_with_existing_placeholders(replacement_gm, literal_to_ph_idx)\n\n    # 3. After replacing literals with existing placeholder nodes\n\n    def pattern(self, x_i8, scale, zero_point, quant_min, quant_max):\n        # scale/zero_point/quant_min/quant_max are burnt in since they are scalar values\n        return torch.dequantize_per_tensor(x_i8, scale, zero_point, quant_min, quant_max)\n\n    def replacement(x_i8, scale, zero_point, quant_min, quant_max):\n        # scale/zero_point/quant_min/quant_max are burnt in since they are scalar values\n        x_i8 = torch.clamp(x_i8, quant_min, quant_max)\n        return ((x_i8.to(torch.float32) - zero_point) * scale).to(dtype=torch.float32)\n    \"\"\"\n    if exclude_literals is None:\n        exclude_literals = []\n\n    if literal_to_ph_idx is None:\n        literal_to_ph_idx = {}\n\n    phs = [node for node in gm.graph.nodes if node.op == \"placeholder\"]\n\n    for node in gm.graph.nodes:\n        if node.op != \"call_function\":\n            continue\n        new_args = []\n        for arg in node.args:\n            if _is_literal(arg) and arg not in exclude_literals and arg in literal_to_ph_idx:\n                ph_idx = literal_to_ph_idx[arg]\n                ph_node = phs[ph_idx]\n                new_args.append(ph_node)\n            else:\n                new_args.append(arg)\n        new_args = tuple(new_args)\n        node.args = new_args\n    return gm\n",
      "torch/ao/quantization/quantize_pt2e.py": "from torch.fx import GraphModule\n\nfrom .pt2e.prepare import prepare\nfrom .pt2e._propagate_annotation import propagate_annotation\nfrom .pt2e.qat_utils import (\n    _fuse_conv_bn_qat,\n    _fold_conv_bn_qat,\n)\nfrom .pt2e.utils import (\n    _get_node_name_to_scope,\n    _fuse_conv_bn_,\n    _replace_dropout_for_eval,\n)\nfrom .pt2e.representation import reference_representation_rewrite\nfrom .fx.prepare import prepare as fx_prepare\nfrom .quantize_fx import _convert_to_reference_decomposed_fx\nfrom torch.ao.quantization import QConfigMapping\nfrom torch.ao.quantization.quantizer import (  # noqa: F401\n    Quantizer,\n    QuantizationSpecBase,\n    QuantizationSpec,\n    FixedQParamsQuantizationSpec,\n    SharedQuantizationSpec,\n    DerivedQuantizationSpec,\n    QuantizationAnnotation,\n)\nfrom torch.ao.quantization.backend_config import BackendConfig\n\nfrom typing import Any, Tuple\n\n__all__ = [\n    \"prepare_pt2e\",\n    \"prepare_qat_pt2e\",\n    \"convert_pt2e\",\n]\n\ndef _prepare_pt2e_deprecated(\n    model: GraphModule,\n    qconfig_mapping: QConfigMapping,\n    example_inputs: Tuple[Any, ...],\n    backend_config: BackendConfig,\n) -> GraphModule:\n    node_name_to_scope = _get_node_name_to_scope(model)\n\n    # TODO: check qconfig_mapping to make sure conv and bn are both configured\n    # to be quantized before fusion\n    # TODO: (maybe) rewrite this with subgraph_rewriter\n    _fuse_conv_bn_(model)\n    model = fx_prepare(\n        model,\n        qconfig_mapping,\n        False,  # is_qat\n        node_name_to_scope,\n        example_inputs,\n        backend_config=backend_config\n    )\n    return model\n\ndef prepare_pt2e(\n    model: GraphModule,\n    quantizer: Quantizer,\n) -> GraphModule:\n    original_graph_meta = model.meta\n    node_name_to_scope = _get_node_name_to_scope(model)\n    # TODO: check qconfig_mapping to make sure conv and bn are both configured\n    # to be quantized before fusion\n    # TODO: (maybe) rewrite this with subgraph_rewriter\n    _fuse_conv_bn_(model)\n    quantizer.annotate(model)\n    quantizer.validate(model)\n    propagate_annotation(model)\n    model = prepare(model, node_name_to_scope, is_qat=False)\n    model.meta.update(original_graph_meta)\n    return model\n\ndef prepare_qat_pt2e(\n    model: GraphModule,\n    quantizer: Quantizer,\n) -> GraphModule:\n    original_graph_meta = model.meta\n    node_name_to_scope = _get_node_name_to_scope(model)\n    quantizer.annotate(model)\n    quantizer.validate(model)\n    propagate_annotation(model)\n    # Perform fusion after annotate to avoid quantizing ops in the new\n    # subgraph that don't need to be quantized\n    # TODO: only fuse if conv and bn are both configured to be quantized\n    _fuse_conv_bn_qat(model)\n    model = prepare(model, node_name_to_scope, is_qat=True)\n    model.meta.update(original_graph_meta)\n    return model\n\ndef convert_pt2e(\n    model: GraphModule,\n    use_reference_representation: bool = False,\n) -> GraphModule:\n    original_graph_meta = model.meta\n    # TODO: Handle this in export itself, outside of quantization\n    # See https://github.com/pytorch/pytorch/issues/103681.\n    _replace_dropout_for_eval(model)\n    model = _convert_to_reference_decomposed_fx(model)\n    model = _fold_conv_bn_qat(model)\n    if use_reference_representation:\n        model = reference_representation_rewrite(model)\n\n    model.meta.update(original_graph_meta)\n    return model\n",
      "torch/_VF.py": "\"\"\"\nThis makes the functions in torch._C._VariableFunctions available as\n    torch._VF.<funcname>\nwithout mypy being able to find them.\n\nA subset of those functions are mapped to ATen functions in\ntorch/jit/_builtins.py\n\nSee https://github.com/pytorch/pytorch/issues/21478 for the reason for\nintroducing torch._VF\n\n\"\"\"\nimport sys\nimport types\n\nimport torch\n\n\nclass VFModule(types.ModuleType):\n    vf: types.ModuleType\n\n    def __init__(self, name):\n        super().__init__(name)\n        self.vf = torch._C._VariableFunctions\n\n    def __getattr__(self, attr):\n        return getattr(self.vf, attr)\n\n\nsys.modules[__name__] = VFModule(__name__)\n",
      "torch/__config__.py": "import torch\n\n\ndef show():\n    \"\"\"\n    Return a human-readable string with descriptions of the\n    configuration of PyTorch.\n    \"\"\"\n    return torch._C._show_config()\n\n\n# TODO: In principle, we could provide more structured version/config\n# information here. For now only CXX_FLAGS is exposed, as Timer\n# uses them.\ndef _cxx_flags():\n    \"\"\"Returns the CXX_FLAGS used when building PyTorch.\"\"\"\n    return torch._C._cxx_flags()\n\n\ndef parallel_info():\n    r\"\"\"Returns detailed string with parallelization settings\"\"\"\n    return torch._C._parallel_info()\n",
      "torch/__future__.py": "\"\"\"\nThis global flag controls whether to assign new tensors to the parameters\ninstead of changing the existing parameters in-place when converting an `nn.Module`\nusing the following methods:\n1. `module.cuda()` / `.cpu()` (for moving `module` between devices)\n2. `module.float()` / `.double()` / `.half()` (for converting `module` to a different dtype)\n3. `module.to()` / `.type()` (for changing `module`'s device or dtype)\n4. `module._apply(fn)` (for generic functions applied to `module`)\n\nDefault: False\n\"\"\"\n_overwrite_module_params_on_conversion = False\n\n\ndef set_overwrite_module_params_on_conversion(value):\n    global _overwrite_module_params_on_conversion\n    _overwrite_module_params_on_conversion = value\n\n\ndef get_overwrite_module_params_on_conversion():\n    return _overwrite_module_params_on_conversion\n",
      "torch/__init__.py": "\nr\"\"\"\nThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\n\nIt has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0.\n\"\"\"\n\nimport math\nimport os\nimport sys\nimport platform\nimport textwrap\nimport ctypes\nimport inspect\n\n# multipy/deploy is setting this import before importing torch, this is the most\n# reliable way we have to detect if we're running within deploy.\n# https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137\ndef _running_with_deploy():\n    return sys.modules.get(\"torch._meta_registrations\", None) is object\n\nfrom ._utils import _import_dotted_name, classproperty\nfrom ._utils_internal import get_file_path, prepare_multiprocessing_environment, \\\n    USE_RTLD_GLOBAL_WITH_LIBTORCH, USE_GLOBAL_DEPS\n\n# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\nif _running_with_deploy():\n    __version__ = \"torch-deploy-1.8\"\nelse:\n    from .torch_version import __version__ as __version__\n\nfrom typing import Any, Callable, Dict, Optional, Set, Tuple, Type, TYPE_CHECKING, Union, List\nimport builtins\n\n__all__ = [\n    'typename', 'is_tensor', 'is_storage',\n    'set_default_tensor_type', 'set_default_device',\n    'set_rng_state', 'get_rng_state', 'manual_seed', 'initial_seed', 'seed',\n    'save', 'load', 'set_printoptions', 'chunk', 'split', 'stack', 'matmul',\n    'no_grad', 'enable_grad', 'rand', 'randn', 'inference_mode',\n    'DoubleStorage', 'FloatStorage', 'LongStorage', 'IntStorage',\n    'ShortStorage', 'CharStorage', 'ByteStorage', 'BoolStorage',\n    'TypedStorage', 'UntypedStorage',\n    'DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',\n    'ShortTensor', 'CharTensor', 'ByteTensor', 'BoolTensor', 'Tensor',\n    'lobpcg', 'use_deterministic_algorithms',\n    'are_deterministic_algorithms_enabled',\n    'is_deterministic_algorithms_warn_only_enabled',\n    'set_deterministic_debug_mode', 'get_deterministic_debug_mode',\n    'set_float32_matmul_precision', 'get_float32_matmul_precision',\n    'set_warn_always', 'is_warn_always_enabled', 'SymInt', 'SymFloat',\n    'SymBool', 'sym_not',\n    'sym_int', 'sym_float', 'sym_max', 'sym_min', 'compile', 'vmap',\n    'export',\n]\n\n################################################################################\n# Load the extension module\n################################################################################\n\nif sys.platform == 'win32':\n    pfiles_path = os.getenv('ProgramFiles', 'C:\\\\Program Files')\n    py_dll_path = os.path.join(sys.exec_prefix, 'Library', 'bin')\n    th_dll_path = os.path.join(os.path.dirname(__file__), 'lib')\n\n    # When users create a virtualenv that inherits the base environment,\n    # we will need to add the corresponding library directory into\n    # DLL search directories. Otherwise, it will rely on `PATH` which\n    # is dependent on user settings.\n    if sys.exec_prefix != sys.base_exec_prefix:\n        base_py_dll_path = os.path.join(sys.base_exec_prefix, 'Library', 'bin')\n    else:\n        base_py_dll_path = ''\n\n    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path]))\n\n    if all(not os.path.exists(os.path.join(p, 'nvToolsExt64_1.dll')) for p in dll_paths):\n        nvtoolsext_dll_path = os.path.join(\n            os.getenv('NVTOOLSEXT_PATH', os.path.join(pfiles_path, 'NVIDIA Corporation', 'NvToolsExt')), 'bin', 'x64')\n    else:\n        nvtoolsext_dll_path = ''\n\n    from .version import cuda as cuda_version\n    import glob\n    if cuda_version and all(not glob.glob(os.path.join(p, 'cudart64*.dll')) for p in dll_paths):\n        cuda_version_1 = cuda_version.replace('.', '_')\n        cuda_path_var = 'CUDA_PATH_V' + cuda_version_1\n        default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)\n        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')\n    else:\n        cuda_path = ''\n\n    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))\n\n    kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)\n    with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')\n    prev_error_mode = kernel32.SetErrorMode(0x0001)\n\n    kernel32.LoadLibraryW.restype = ctypes.c_void_p\n    if with_load_library_flags:\n        kernel32.LoadLibraryExW.restype = ctypes.c_void_p\n\n    for dll_path in dll_paths:\n        os.add_dll_directory(dll_path)\n\n    try:\n        ctypes.CDLL('vcruntime140.dll')\n        ctypes.CDLL('msvcp140.dll')\n        ctypes.CDLL('vcruntime140_1.dll')\n    except OSError:\n        print('''Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe''')\n\n    dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))\n    path_patched = False\n    for dll in dlls:\n        is_loaded = False\n        if with_load_library_flags:\n            res = kernel32.LoadLibraryExW(dll, None, 0x00001100)\n            last_error = ctypes.get_last_error()\n            if res is None and last_error != 126:\n                err = ctypes.WinError(last_error)\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n            elif res is not None:\n                is_loaded = True\n        if not is_loaded:\n            if not path_patched:\n                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])\n                path_patched = True\n            res = kernel32.LoadLibraryW(dll)\n            if res is None:\n                err = ctypes.WinError(ctypes.get_last_error())\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n\n    kernel32.SetErrorMode(prev_error_mode)\n\n\ndef _preload_cuda_deps(lib_folder, lib_name):\n    \"\"\"Preloads cuda deps if they could not be found otherwise.\"\"\"\n    # Should only be called on Linux if default path resolution have failed\n    assert platform.system() == 'Linux', 'Should only be called on Linux'\n    import glob\n    lib_path = None\n    for path in sys.path:\n        nvidia_path = os.path.join(path, 'nvidia')\n        if not os.path.exists(nvidia_path):\n            continue\n        candidate_lib_paths = glob.glob(os.path.join(nvidia_path, lib_folder, 'lib', lib_name))\n        if candidate_lib_paths and not lib_path:\n            lib_path = candidate_lib_paths[0]\n        if lib_path:\n            break\n    if not lib_path:\n        raise ValueError(f\"{lib_name} not found in the system path {sys.path}\")\n    ctypes.CDLL(lib_path)\n\n\n# See Note [Global dependencies]\ndef _load_global_deps() -> None:\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return\n\n    lib_name = 'libtorch_global_deps' + ('.dylib' if platform.system() == 'Darwin' else '.so')\n    here = os.path.abspath(__file__)\n    lib_path = os.path.join(os.path.dirname(here), 'lib', lib_name)\n\n    try:\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n    except OSError as err:\n        # Can only happen for wheel with cuda libs as PYPI deps\n        # As PyTorch is not purelib, but nvidia-*-cu11 is\n        cuda_libs: Dict[str, str] = {\n            'cublas': 'libcublas.so.*[0-9]',\n            'cudnn': 'libcudnn.so.*[0-9]',\n            'cuda_nvrtc': 'libnvrtc.so.*[0-9].*[0-9]',\n            'cuda_runtime': 'libcudart.so.*[0-9].*[0-9]',\n            'cuda_cupti': 'libcupti.so.*[0-9].*[0-9]',\n            'cufft': 'libcufft.so.*[0-9]',\n            'curand': 'libcurand.so.*[0-9]',\n            'cusolver': 'libcusolver.so.*[0-9]',\n            'cusparse': 'libcusparse.so.*[0-9]',\n            'nccl': 'libnccl.so.*[0-9]',\n            'nvtx': 'libnvToolsExt.so.*[0-9]',\n        }\n        is_cuda_lib_err = [lib for lib in cuda_libs.values() if(lib.split('.')[0] in err.args[0])]\n        if not is_cuda_lib_err:\n            raise err\n        for lib_folder, lib_name in cuda_libs.items():\n            _preload_cuda_deps(lib_folder, lib_name)\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n\n\nif (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv('TORCH_USE_RTLD_GLOBAL')) and \\\n        (_running_with_deploy() or platform.system() != 'Windows'):\n    # Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a\n    # few circumstances:\n    #\n    #   1. You're in a build environment (e.g., fbcode) where\n    #      libtorch_global_deps is not available, but you still need\n    #      to get mkl to link in with RTLD_GLOBAL or it will just\n    #      not work.\n    #\n    #   2. You're trying to run PyTorch under UBSAN and you need\n    #      to ensure that only one copy of libtorch is loaded, so\n    #      vptr checks work properly\n    #\n    # If you're using this setting, you must verify that all the libraries\n    # you load consistently use the same libstdc++, or you may have\n    # mysterious segfaults.\n    #\n    old_flags = sys.getdlopenflags()\n    sys.setdlopenflags(os.RTLD_GLOBAL | os.RTLD_LAZY)\n    from torch._C import *  # noqa: F403\n    sys.setdlopenflags(old_flags)\n    del old_flags\n\nelse:\n    # Easy way.  You want this most of the time, because it will prevent\n    # C++ symbols from libtorch clobbering C++ symbols from other\n    # libraries, leading to mysterious segfaults.\n    #\n    # If building in an environment where libtorch_global_deps isn't available\n    # like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will\n    # want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False\n    #\n    # See Note [Global dependencies]\n    if USE_GLOBAL_DEPS:\n        _load_global_deps()\n    from torch._C import *  # noqa: F403\n\n# Appease the type checker; ordinarily this binding is inserted by the\n# torch._C module initialization code in C\nif TYPE_CHECKING:\n    import torch._C as _C\n\nclass SymInt:\n    \"\"\"\n    Like an int (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return builtins.bool(self != 0)\n\n    def __int__(self):\n        return self.node.int_()\n\n    def __index__(self):\n        return self.node.int_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_float__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return str(self.node)\n\nclass SymFloat:\n    \"\"\"\n    Like an float (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_int__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\nclass SymBool:\n    \"\"\"\n    Like an bool (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n\n    Unlike regular bools, regular boolean operators will force extra guards instead\n    of symbolically evaluate.  Use the bitwise operators instead to handle this.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    def __int__(self):\n        return builtins.int(self.node.bool_())\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n    def __and__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __or__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    # We very carefully define __sym_not__, and not a number of other\n    # plausible alternatives:\n    #\n    #   - We do not override __not__ because this is not a real magic\n    #     method; you cannot override the meaning of the not builtin in\n    #     Python.  We use the name 'sym_not' to clarify that in user code you\n    #     cannot use the builtin not or operator.not_ or operator.__not__ and\n    #     hit this magic method; you must use our custom sym_not operator.\n    #\n    #   - We do not override the __invert__ method because SymBool is\n    #     meant to be usable in situations where bool is expected.  However,\n    #     bitwise negation ~a does the wrong thing with booleans (because\n    #     bool is a subclass of int, so ~1 = -2 which is not falseish.)\n    #     This would be a giant footgun, so we get around it by defining\n    #     our own operator.  Note that bitwise and/or do the right thing,\n    #     so we reuse the conventional operators there for readability.\n    #\n    def __sym_not__(self) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\ndef sym_not(a):\n    r\"\"\" SymInt-aware utility for logical negation.\n\n    Args:\n        a (SymBool or bool): Object to negate\n    \"\"\"\n    if hasattr(a, '__sym_not__'):\n        return a.__sym_not__()\n    return not a\n\ndef sym_float(a):\n    r\"\"\" SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymFloat):\n        return a\n    elif hasattr(a, '__sym_float__'):\n        return a.__sym_float__()\n    return py_float(a)  # type: ignore[operator]\n\n\ndef sym_int(a):\n    r\"\"\" SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymInt):\n        return a\n    elif isinstance(a, SymFloat):\n        return math.floor(a) if a >= 0 else math.ceil(a)  # type: ignore[arg-type, call-overload]\n    return py_int(a)  # type: ignore[operator]\n\ndef sym_max(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_max__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        # NB: If you actually care about preserving output type exactly\n        # if you do something like max(0, 0.0), it is NOT sound to treat\n        # min/max as commutative\n        return b.__sym_max__(a)\n    return builtins.max(a, b)  # type: ignore[operator]\n\ndef sym_min(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_min__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        return b.__sym_min__(a)\n    return builtins.min(a, b)  # type: ignore[operator]\n\n# Check to see if we can load C extensions, and if not provide some guidance\n# on what the problem might be.\ntry:\n    # _initExtension is chosen (arbitrarily) as a sentinel.\n    from torch._C import _initExtension\nexcept ImportError:\n    import torch._C as _C_for_compiled_check\n\n    # The __file__ check only works for Python 3.7 and above.\n    if _C_for_compiled_check.__file__ is None:\n        raise ImportError(textwrap.dedent('''\n            Failed to load PyTorch C extensions:\n                It appears that PyTorch has loaded the `torch/_C` folder\n                of the PyTorch repository rather than the C extensions which\n                are expected in the `torch._C` namespace. This can occur when\n                using the `install` workflow. e.g.\n                    $ python setup.py install && python -c \"import torch\"\n\n                This error can generally be solved using the `develop` workflow\n                    $ python setup.py develop && python -c \"import torch\"  # This should succeed\n                or by running Python from a different directory.\n            ''').strip()) from None\n    raise  # If __file__ is not None the cause is unknown, so just re-raise.\n\nfor name in dir(_C):\n    if name[0] != '_' and not name.endswith('Base'):\n        __all__.append(name)\n        obj = getattr(_C, name)\n        if (isinstance(obj, Callable) or inspect.isclass(obj)):  # type: ignore[arg-type]\n            if (obj.__module__ != 'torch'):\n                # TODO: fix their module from C++ side\n                if name not in ['DisableTorchFunctionSubclass', 'DisableTorchFunction', 'Generator']:\n                    obj.__module__ = 'torch'\n\nif not TYPE_CHECKING:\n    # issue 38137 and python issue 43367. Submodules of a C extension are\n    # non-standard, and attributes of those submodules cannot be pickled since\n    # pickle expect to be able to import them as \"from _C.sub import attr\"\n    # which fails with \"_C is not a package\n    for attr in dir(_C):\n        candidate = getattr(_C, attr)\n        if type(candidate) is type(_C):\n            # submodule\n            if f'torch._C.{attr}' not in sys.modules:\n                sys.modules[f'torch._C.{attr}'] = candidate\n\n\n################################################################################\n# Define basic utilities\n################################################################################\n\n\ndef typename(o):\n    if isinstance(o, torch.Tensor):\n        return o.type()\n\n    module = ''\n    class_name = ''\n    if hasattr(o, '__module__') and o.__module__ != 'builtins' \\\n            and o.__module__ != '__builtin__' and o.__module__ is not None:\n        module = o.__module__ + '.'\n\n    if hasattr(o, '__qualname__'):\n        class_name = o.__qualname__\n    elif hasattr(o, '__name__'):\n        class_name = o.__name__\n    else:\n        class_name = o.__class__.__name__\n\n    return module + class_name\n\n\ndef is_tensor(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch tensor.\n\n    Note that this function is simply doing ``isinstance(obj, Tensor)``.\n    Using that ``isinstance`` check is better for typechecking with mypy,\n    and more explicit - so it's recommended to use that instead of\n    ``is_tensor``.\n\n    Args:\n        obj (Object): Object to test\n    Example::\n\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.is_tensor(x)\n        True\n\n    \"\"\"\n    return isinstance(obj, torch.Tensor)\n\n\ndef is_storage(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch storage object.\n\n    Args:\n        obj (Object): Object to test\n    \"\"\"\n    return type(obj) in _storage_classes\n\n\n_GLOBAL_DEVICE_CONTEXT = None\n\ndef set_default_device(device):\n    \"\"\"Sets the default ``torch.Tensor`` to be allocated on ``device``.  This\n    does not affect factory function calls which are called with an explicit\n    ``device`` argument.  Factory calls will be performed as if they\n    were passed ``device`` as an argument.\n\n    To only temporarily change the default device instead of setting it\n    globally, use ``with torch.device(device):`` instead.\n\n    The default device is initially ``cpu``.  If you set the default tensor\n    device to another device (e.g., ``cuda``) without a device index, tensors\n    will be allocated on whatever the current device for the device type,\n    even after :func:`torch.cuda.set_device` is called.\n\n    .. warning::\n\n        This function imposes a slight performance cost on every Python\n        call to the torch API (not just factory functions).  If this\n        is causing problems for you, please comment on\n        https://github.com/pytorch/pytorch/issues/92701\n\n    Args:\n        device (device or string): the device to set as default\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"requires cuda, changes global state\")\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cpu')\n        >>> torch.set_default_device('cuda')  # current device is 0\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=0)\n        >>> torch.set_default_device('cuda:1')\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=1)\n\n    \"\"\"\n    global _GLOBAL_DEVICE_CONTEXT\n    if _GLOBAL_DEVICE_CONTEXT is not None:\n        _GLOBAL_DEVICE_CONTEXT.__exit__(None, None, None)\n    if device is None:\n        _GLOBAL_DEVICE_CONTEXT = None\n        return\n    from torch.utils._device import DeviceContext\n    _GLOBAL_DEVICE_CONTEXT = DeviceContext(device)\n    _GLOBAL_DEVICE_CONTEXT.__enter__()\n\n\ndef set_default_tensor_type(t):\n    r\"\"\"Sets the default ``torch.Tensor`` type to floating point tensor type\n    ``t``. This type will also be used as default floating point type for\n    type inference in :func:`torch.tensor`.\n\n    The default floating point tensor type is initially ``torch.FloatTensor``.\n\n    Args:\n        t (type or string): the floating point tensor type or its name\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\n        torch.float32\n        >>> torch.set_default_tensor_type(torch.DoubleTensor)\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n\n    \"\"\"\n    if isinstance(t, str):\n        t = _import_dotted_name(t)\n    _C._set_default_tensor_type(t)\n\n\ndef set_default_dtype(d):\n    r\"\"\"\n\n    Sets the default floating point dtype to :attr:`d`. Supports torch.float32\n    and torch.float64 as inputs. Other dtypes may be accepted without complaint\n    but are not supported and are unlikely to work as expected.\n\n    When PyTorch is initialized its default floating point dtype is torch.float32,\n    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like\n    type inference. The default floating point dtype is used to:\n\n    1. Implicitly determine the default complex dtype. When the default floating point\n       type is float32 the default complex dtype is complex64, and when the default\n       floating point type is float64 the default complex type is complex128.\n    2. Infer the dtype for tensors constructed using Python floats or complex Python\n       numbers. See examples below.\n    3. Determine the result of type promotion between bool and integer tensors and\n       Python floats and complex Python numbers.\n\n    Args:\n        d (:class:`torch.dtype`): the floating point dtype to make the default.\n                                  Either torch.float32 or torch.float64.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> # initial default for floating point is torch.float32\n        >>> # Python floats are interpreted as float32\n        >>> torch.tensor([1.2, 3]).dtype\n        torch.float32\n        >>> # initial default for floating point is torch.complex64\n        >>> # Complex Python numbers are interpreted as complex64\n        >>> torch.tensor([1.2, 3j]).dtype\n        torch.complex64\n\n        >>> torch.set_default_dtype(torch.float64)\n\n        >>> # Python floats are now interpreted as float64\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\n        torch.complex128\n\n    \"\"\"\n    _C._set_default_dtype(d)\n\ndef use_deterministic_algorithms(mode: builtins.bool, *, warn_only: builtins.bool = False) -> None:\n    r\"\"\" Sets whether PyTorch operations must use \"deterministic\"\n    algorithms. That is, algorithms which, given the same input, and when\n    run on the same software and hardware, always produce the same output.\n    When enabled, operations will use deterministic algorithms when available,\n    and if only nondeterministic algorithms are available they will throw a\n    :class:`RuntimeError` when called.\n\n    .. note:: This setting alone is not always enough to make an application\n        reproducible. Refer to :ref:`reproducibility` for more information.\n\n    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative\n        interface for this feature.\n\n    The following normally-nondeterministic operations will act\n    deterministically when ``mode=True``:\n\n        * :class:`torch.nn.Conv1d` when called on CUDA tensor\n        * :class:`torch.nn.Conv2d` when called on CUDA tensor\n        * :class:`torch.nn.Conv3d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor\n        * :func:`torch.bmm` when called on sparse-dense CUDA tensors\n        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor\n          and the index is a list of tensors\n        * :func:`torch.Tensor.index_put` with ``accumulate=False``\n        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor\n        * :func:`torch.gather` when called on a CUDA tensor that requires grad\n        * :func:`torch.index_add` when called on CUDA tensor\n        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor\n        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor\n        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor\n        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='sum'`` or ``reduce='mean'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_`, when called with a tensor that is not\n          quantized, sets new elements to a known value.  Floating point or\n          complex values are set to NaN. Integer values are set to the maximum\n          value.\n        * :func:`torch.empty`, :func:`torch.empty_like`, :func:`torch.empty_strided`,\n          and :func:`torch.empty_permuted` will fill the output tensor with a known\n          value. Floating point or complex dtype tensors are filled with NaN. Integer\n          dtype tensors are filled with the maximum value.\n\n    The following normally-nondeterministic operations will throw a\n    :class:`RuntimeError` when ``mode=True``:\n\n        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxUnpool1d`\n        * :class:`torch.nn.MaxUnpool2d`\n        * :class:`torch.nn.MaxUnpool3d`\n        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor\n          and one of the following modes is used:\n\n          - ``linear``\n          - ``bilinear``\n          - ``bicubic``\n          - ``trilinear``\n\n        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor\n        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when\n          ``mode='max'``\n        * :func:`torch.Tensor.put_` when ``accumulate=False``\n        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor\n        * :func:`torch.histc` when called on a CUDA tensor\n        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``\n          tensor is given\n        * :func:`torch.kthvalue` with called on a CUDA tensor\n        * :func:`torch.median` with indices output when called on a CUDA tensor\n        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor\n        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='prod'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_` when called with a quantized tensor\n\n    A handful of CUDA operations are nondeterministic if the CUDA version is\n    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``\n    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more\n    details: `<https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility>`_\n    If one of these environment variable configurations is not set, a :class:`RuntimeError`\n    will be raised from these operations when called with CUDA tensors:\n\n        * :func:`torch.mm`\n        * :func:`torch.mv`\n        * :func:`torch.bmm`\n\n    Note that deterministic operations tend to have worse performance than\n    nondeterministic operations.\n\n    .. note::\n\n        This flag does not detect or prevent nondeterministic behavior caused\n        by calling an inplace operation on a tensor with an internal memory\n        overlap or by giving such a tensor as the :attr:`out` argument for an\n        operation. In these cases, multiple writes of different data may target\n        a single memory location, and the order of writes is not guaranteed.\n\n    Args:\n        mode (:class:`bool`): If True, makes potentially nondeterministic\n            operations switch to a deterministic algorithm or throw a runtime\n            error. If False, allows nondeterministic operations.\n\n    Keyword args:\n        warn_only (:class:`bool`, optional): If True, operations that do not\n            have a deterministic implementation will throw a warning instead of\n            an error. Default: ``False``\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> torch.use_deterministic_algorithms(True)\n\n        # Forward mode nondeterministic error\n        >>> torch.randn(10, device='cuda').kthvalue(0)\n        ...\n        RuntimeError: kthvalue CUDA does not have a deterministic implementation...\n\n        # Backward mode nondeterministic error\n        >>> torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()\n        ...\n        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...\n    \"\"\"\n    _C._set_deterministic_algorithms(mode, warn_only=warn_only)\n\ndef are_deterministic_algorithms_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is turned on. Refer to\n    :func:`torch.use_deterministic_algorithms` documentation for more details.\n    \"\"\"\n    return _C._get_deterministic_algorithms()\n\ndef is_deterministic_algorithms_warn_only_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is set to warn only.\n    Refer to :func:`torch.use_deterministic_algorithms` documentation for more\n    details.\n    \"\"\"\n    return _C._get_deterministic_algorithms_warn_only()\n\ndef set_deterministic_debug_mode(debug_mode: Union[builtins.int, str]) -> None:\n    r\"\"\"Sets the debug mode for deterministic operations.\n\n    .. note:: This is an alternative interface for\n        :func:`torch.use_deterministic_algorithms`. Refer to that function's\n        documentation for details about affected operations.\n\n    Args:\n        debug_mode(str or int): If \"default\" or 0, don't error or warn on\n            nondeterministic operations. If \"warn\" or 1, warn on\n            nondeterministic operations. If \"error\" or 2, error on\n            nondeterministic operations.\n    \"\"\"\n\n    # NOTE: builtins.int is used here because int in this scope resolves\n    # to torch.int\n    if not isinstance(debug_mode, (builtins.int, str)):\n        raise TypeError(f'debug_mode must be str or int, but got {type(debug_mode)}')\n\n    if isinstance(debug_mode, str):\n        if debug_mode == 'default':\n            debug_mode = 0\n        elif debug_mode == 'warn':\n            debug_mode = 1\n        elif debug_mode == 'error':\n            debug_mode = 2\n        else:\n            raise RuntimeError(\n                'invalid value of debug_mode, expected one of `default`, '\n                f'`warn`, `error`, but got {debug_mode}')\n\n    if debug_mode == 0:\n        _C._set_deterministic_algorithms(False)\n    elif debug_mode == 1:\n        _C._set_deterministic_algorithms(True, warn_only=True)\n    elif debug_mode == 2:\n        _C._set_deterministic_algorithms(True)\n    else:\n        raise RuntimeError(\n            'invalid value of debug_mode, expected 0, 1, or 2, '\n            f'but got {debug_mode}')\n\ndef get_deterministic_debug_mode() -> builtins.int:\n    r\"\"\"Returns the current value of the debug mode for deterministic\n    operations. Refer to :func:`torch.set_deterministic_debug_mode`\n    documentation for more details.\n    \"\"\"\n\n    if _C._get_deterministic_algorithms():\n        if _C._get_deterministic_algorithms_warn_only():\n            return 1\n        else:\n            return 2\n    else:\n        return 0\n\ndef get_float32_matmul_precision() -> builtins.str:\n    r\"\"\"Returns the current value of float32 matrix multiplication precision. Refer to\n    :func:`torch.set_float32_matmul_precision` documentation for more details.\n    \"\"\"\n    return _C._get_float32_matmul_precision()\n\ndef set_float32_matmul_precision(precision: str) -> None:\n    r\"\"\"Sets the internal precision of float32 matrix multiplications.\n\n    Running float32 matrix multiplications in lower precision may significantly increase\n    performance, and in some programs the loss of precision has a negligible impact.\n\n    Supports three settings:\n\n        * \"highest\", float32 matrix multiplications use the float32 datatype (24 mantissa\n          bits) for internal computations.\n        * \"high\", float32 matrix multiplications either use the TensorFloat32 datatype (10\n          mantissa bits) or treat each float32 number as the sum of two bfloat16 numbers\n          (approximately 16 mantissa bits), if the appropriate fast matrix multiplication\n          algorithms are available.  Otherwise float32 matrix multiplications are computed\n          as if the precision is \"highest\".  See below for more information on the bfloat16\n          approach.\n        * \"medium\", float32 matrix multiplications use the bfloat16 datatype (8 mantissa\n          bits) for internal computations, if a fast matrix multiplication algorithm\n          using that datatype internally is available. Otherwise float32\n          matrix multiplications are computed as if the precision is \"high\".\n\n    When using \"high\" precision, float32 multiplications may use a bfloat16-based algorithm\n    that is more complicated than simply truncating to some smaller number mantissa bits\n    (e.g. 10 for TensorFloat32, 8 for bfloat16).  Refer to [Henry2019]_ for a complete\n    description of this algorithm.  To briefly explain here, the first step is to realize\n    that we can perfectly encode a single float32 number as the sum of three bfloat16\n    numbers (because float32 has 24 mantissa bits while bfloat16 has 8, and both have the\n    same number of exponent bits).  This means that the product of two float32 numbers can\n    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade\n    accuracy for speed by dropping some of these products.  The \"high\" precision algorithm\n    specifically keeps only the three most significant products, which conveniently excludes\n    all of the products involving the last 8 mantissa bits of either input.  This means that\n    we can represent our inputs as the sum of two bfloat16 numbers rather than three.\n    Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than\n    float32 ones, it's faster to do three multiplications and 2 additions with bfloat16\n    precision than it is to do a single multiplication with float32 precision.\n\n    .. [Henry2019] http://arxiv.org/abs/1904.06376\n\n    .. note::\n\n        This does not change the output dtype of float32 matrix multiplications,\n        it controls how the internal computation of the matrix multiplication is performed.\n\n    .. note::\n\n        This does not change the precision of convolution operations. Other flags,\n        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution\n        operations.\n\n    .. note::\n\n        This flag currently only affects one native device type: CUDA.\n        If \"high\" or \"medium\" are set then the TensorFloat32 datatype will be used\n        when computing float32 matrix multiplications, equivalent to setting\n        `torch.backends.cuda.matmul.allow_tf32 = True`. When \"highest\" (the default)\n        is set then the float32 datatype is used for internal computations, equivalent\n        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.\n\n    Args:\n        precision(str): can be set to \"highest\" (default), \"high\", or \"medium\" (see above).\n\n    \"\"\"\n    _C._set_float32_matmul_precision(precision)\n\ndef set_warn_always(b: builtins.bool) -> None:\n    r\"\"\"When this flag is False (default) then some PyTorch warnings may only\n    appear once per process. This helps avoid excessive warning information.\n    Setting it to True causes these warnings to always appear, which may be\n    helpful when debugging.\n\n    Args:\n        b (:class:`bool`): If True, force warnings to always be emitted\n                           If False, set to the default behaviour\n    \"\"\"\n    _C._set_warnAlways(b)\n\ndef is_warn_always_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global warn_always flag is turned on. Refer to\n    :func:`torch.set_warn_always` documentation for more details.\n    \"\"\"\n    return _C._get_warnAlways()\n\n################################################################################\n# Define error checking functions\n################################################################################\n\n# These error checking functions must be kept consistent with their C++\n# equivalents. Their C++ equivalents are mentioned where applicable.\n\ndef _check_with(error_type, cond: Union[builtins.bool, SymBool], message: Callable[[], str]):\n    if not isinstance(cond, (builtins.bool, torch.SymBool)):\n        raise TypeError(f'cond must be a bool, but got {type(cond)}')\n\n    if torch.fx.experimental.symbolic_shapes.expect_true(cond):\n        return\n\n    # error_type must be a subclass of Exception and not subclass of Warning\n    assert issubclass(error_type, Exception) and not issubclass(error_type, Warning)\n\n    if message is None:\n        message_evaluated = (\n            'Expected cond to be True, but got False. (Could this error '\n            'message be improved? If so, please report an enhancement request '\n            'to PyTorch.)')\n\n    else:\n        if not callable(message):\n            raise TypeError('message must be a callable')\n\n        message_evaluated = str(message())\n\n    raise error_type(message_evaluated)\n\ndef _check(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(RuntimeError, cond, message)\n\ndef _check_index(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``IndexError``\n\n    C++ equivalent: ``TORCH_CHECK_INDEX``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(IndexError, cond, message)\n\ndef _check_value(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``ValueError``\n\n    C++ equivalent: ``TORCH_CHECK_VALUE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(ValueError, cond, message)\n\ndef _check_type(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``TypeError``\n\n    C++ equivalent: ``TORCH_CHECK_TYPE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(TypeError, cond, message)\n\ndef _check_not_implemented(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``NotImplementedError``\n\n    C++ equivalent: ``TORCH_CHECK_NOT_IMPLEMENTED``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(NotImplementedError, cond, message)\n\ndef _check_tensor_all_with(error_type, cond, message=None):\n    if not torch.is_tensor(cond):\n        raise TypeError(f'cond must be a tensor, but got {type(cond)}')\n\n    if not cond.dtype == torch.bool:\n        raise TypeError(\n            f'cond tensor must have dtype torch.bool, but got {cond.dtype}')\n\n    _check_with(error_type, cond._is_all_true().item(), message)\n\n# C++ equivalent: `TORCH_CHECK_TENSOR_ALL`\ndef _check_tensor_all(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK_TENSOR_ALL``\n\n    Args:\n        cond (:class:`torch.Tensor`): Tensor of dtype ``torch.bool``. If any\n            element is ``False``, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_tensor_all_with(RuntimeError, cond, message)\n\n################################################################################\n# Define numeric constants\n################################################################################\n\n# For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and\n# NumPy consistency (https://numpy.org/devdocs/reference/constants.html)\nfrom math import e , nan , inf , pi\n__all__.extend(['e', 'pi', 'nan', 'inf'])\n\n################################################################################\n# Define Storage and Tensor classes\n################################################################################\n\nfrom ._tensor import Tensor\nfrom .storage import _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n\n# NOTE: New <type>Storage classes should never be added. When adding a new\n# dtype, use torch.storage.TypedStorage directly.\n\nclass ByteStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.uint8\n\nclass DoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.double\n\nclass FloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.float\n\nclass HalfStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.half\n\nclass LongStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.long\n\nclass IntStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int\n\nclass ShortStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.short\n\nclass CharStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int8\n\nclass BoolStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bool\n\nclass BFloat16Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bfloat16\n\nclass ComplexDoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cdouble\n\nclass ComplexFloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cfloat\n\nclass QUInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint8\n\nclass QInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint8\n\nclass QInt32Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint32\n\nclass QUInt4x2Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint4x2\n\nclass QUInt2x4Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint2x4\n\n_storage_classes = {\n    UntypedStorage, DoubleStorage, FloatStorage, LongStorage, IntStorage,\n    ShortStorage, CharStorage, ByteStorage, HalfStorage, BoolStorage,\n    QUInt8Storage, QInt8Storage, QInt32Storage, BFloat16Storage,\n    ComplexFloatStorage, ComplexDoubleStorage, QUInt4x2Storage, QUInt2x4Storage,\n    TypedStorage\n}\n\n# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()\n_tensor_classes: Set[Type] = set()\n\n# If you edit these imports, please update torch/__init__.py.in as well\nfrom .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed\nfrom .serialization import save, load\nfrom ._tensor_str import set_printoptions\n\n################################################################################\n# Initialize extension\n################################################################################\n\ndef manager_path():\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return b\"\"\n    path = get_file_path('torch', 'bin', 'torch_shm_manager')\n    prepare_multiprocessing_environment(get_file_path('torch'))\n    if not os.path.exists(path):\n        raise RuntimeError(\"Unable to find torch_shm_manager at \" + path)\n    return path.encode('utf-8')\n\nfrom torch.amp import autocast\n\n# Initializing the extension shadows the built-in python float / int classes;\n# store them for later use by SymInt / SymFloat.\npy_float = float\npy_int = int\n\n# Shared memory manager needs to know the exact location of manager executable\n_C._initExtension(manager_path())\ndel manager_path\n\n# Appease the type checker: it can't deal with direct setting of globals().\n# Note that we will see \"too many\" functions when reexporting this way; there\n# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\n# so that this import is good enough\nif TYPE_CHECKING:\n    # Some type signatures pulled in from _VariableFunctions here clash with\n    # signatures already imported. For now these clashes are ignored; see\n    # PR #43339 for details.\n    from torch._C._VariableFunctions import *  # type: ignore[assignment, misc] # noqa: F403\n    # Fixup segment_reduce visibility\n    _segment_reduce = segment_reduce\n    del segment_reduce\n\n# Ops not to be exposed in `torch` namespace,\n# mostly helper ops.\nPRIVATE_OPS = (\n    'unique_dim',\n)\n\nfor name in dir(_C._VariableFunctions):\n    if name.startswith('__') or name in PRIVATE_OPS:\n        continue\n    obj = getattr(_C._VariableFunctions, name)\n    obj.__module__ = 'torch'\n    # Hide some APIs that should not be public\n    if name == \"segment_reduce\":\n        # TODO: Once the undocumented FC window is passed, remove the line bellow\n        globals()[name] = obj\n        name = \"_\" + name\n    globals()[name] = obj\n    if not name.startswith(\"_\"):\n        __all__.append(name)\n\n\n\n################################################################################\n# Import TorchDynamo's lazy APIs to avoid circular dependenices\n################################################################################\n\n# needs to be before from .functional import * to avoid circular dependencies\nfrom ._compile import _disable_dynamo\n\n################################################################################\n# Import interface functions defined in Python\n################################################################################\n\n# needs to be after the above ATen bindings so we can overwrite from Python side\nfrom .functional import *  # noqa: F403\n\n\n################################################################################\n# Remove unnecessary members\n################################################################################\n\ndel _StorageBase\ndel _LegacyStorage\n\n################################################################################\n# Define _assert\n################################################################################\n\n# needs to be before the submodule imports to avoid circular dependencies\ndef _assert(condition, message):\n    r\"\"\"A wrapper around Python's assert which is symbolically traceable.\n    \"\"\"\n    from .overrides import has_torch_function, handle_torch_function\n\n    if type(condition) is not torch.Tensor and has_torch_function((condition,)):\n        return handle_torch_function(_assert, (condition,), condition, message)\n    assert condition, message\n\n################################################################################\n# Import most common subpackages\n################################################################################\n\n# Use the redundant form so that type checkers know that these are a part of\n# the public API. The \"regular\" import lines are there solely for the runtime\n# side effect of adding to the imported module's members for other users.\nfrom torch import cuda as cuda\nfrom torch import cpu as cpu\nfrom torch import mps as mps\nfrom torch import autograd as autograd\nfrom torch.autograd import (\n    no_grad as no_grad,\n    enable_grad as enable_grad,\n    set_grad_enabled as set_grad_enabled,\n    inference_mode as inference_mode,\n)\nfrom torch import fft as fft\nfrom torch import futures as futures\nfrom torch import _awaits as _awaits\nfrom torch import nested as nested\nfrom torch import nn as nn\nfrom torch.signal import windows as windows\nfrom torch import optim as optim\nimport torch.optim._multi_tensor\nfrom torch import multiprocessing as multiprocessing\nfrom torch import sparse as sparse\nfrom torch import special as special\nimport torch.utils.backcompat\nfrom torch import jit as jit\nfrom torch import linalg as linalg\nfrom torch import hub as hub\nfrom torch import random as random\nfrom torch import distributions as distributions\nfrom torch import testing as testing\nfrom torch import backends as backends\nimport torch.utils.data\nfrom torch import __config__ as __config__\nfrom torch import __future__ as __future__\nfrom torch import profiler as profiler\n\n# Quantized, sparse, AO, etc. should be last to get imported, as nothing\n# is expected to depend on them.\nfrom torch import ao as ao\n# nn.quant* depends on ao -- so should be after those.\nimport torch.nn.quantizable\nimport torch.nn.quantized\nimport torch.nn.qat\nimport torch.nn.intrinsic\n\n_C._init_names(list(torch._storage_classes))\n\n# attach docstrings to torch and tensor functions\nfrom . import _torch_docs, _tensor_docs, _storage_docs\ndel _torch_docs, _tensor_docs, _storage_docs\n\n\ndef compiled_with_cxx11_abi() -> builtins.bool:\n    r\"\"\"Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\"\"\"\n    return _C._GLIBCXX_USE_CXX11_ABI\n\n\n# Import the ops \"namespace\"\nfrom torch._ops import ops\nfrom torch._classes import classes\n\n# quantization depends on torch.fx\n# Import quantization\nfrom torch import quantization as quantization\n\n# Import the quasi random sampler\nfrom torch import quasirandom as quasirandom\n\n# If you are seeing this, it means that this call site was not checked if\n# the memory format could be preserved, and it was switched to old default\n# behaviour of contiguous\nlegacy_contiguous_format = contiguous_format\n\n# Register fork handler to initialize OpenMP in child processes (see gh-28389)\nfrom torch.multiprocessing._atfork import register_after_fork\nregister_after_fork(torch.get_num_threads)\ndel register_after_fork\n\n# Import tools that require fully imported torch (for applying\n# torch.jit.script as a decorator, for instance):\nfrom ._lobpcg import lobpcg as lobpcg\n\n# These were previously defined in native_functions.yaml and appeared on the\n# `torch` namespace, but we moved them to c10 dispatch to facilitate custom\n# class usage. We add these lines here to preserve backward compatibility.\nquantized_lstm = torch.ops.aten.quantized_lstm\nquantized_gru = torch.ops.aten.quantized_gru\n\nfrom torch.utils.dlpack import from_dlpack, to_dlpack\n\n# Import experimental masked operations support. See\n# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\n# information.\nfrom . import masked\n\n# Import removed ops with error message about removal\nfrom ._linalg_utils import (  # type: ignore[misc]\n    matrix_rank,\n    eig,\n    solve,\n    lstsq,\n)\nfrom ._linalg_utils import _symeig as symeig  # type: ignore[misc]\n\nclass _TorchCompileInductorWrapper:\n    compiler_name = \"inductor\"\n\n    def __init__(self, mode, options, dynamic):\n        self.config: Dict[str, Any] = dict()\n        self.dynamic = dynamic\n        self.apply_mode(mode)\n        self.apply_options(options)\n\n        # FIXME: CUPTI Lazy Re-init and CUDA Graph crashes with CUDA 11.\n        if self.config.get(\"triton.cudagraphs\", False):\n            os.environ[\"DISABLE_CUPTI_LAZY_REINIT\"] = \"1\"\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileInductorWrapper) and\n                self.config == other.config and\n                self.dynamic == other.dynamic)\n\n    def apply_mode(self, mode: Optional[str]):\n        if mode is None or mode == \"default\":\n            pass\n        elif mode in (\"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"):\n            from torch._inductor import list_mode_options\n            self.apply_options(list_mode_options(mode, self.dynamic))\n        else:\n            raise RuntimeError(\n                f\"Unrecognized mode={mode}, should be one of: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs\"\n            )\n\n    def apply_options(self, options: Optional[Dict[str, Any]]):\n        if not options:\n            return\n\n        from torch._inductor import config\n        current_config: Dict[str, Any] = config.to_dict()  # type: ignore[attr-defined]\n\n        for key, val in options.items():\n            attr_name = key.replace(\"-\", \"_\")\n            if attr_name not in current_config:\n                raise RuntimeError(\n                    f\"Unexpected optimization option {key}, known options are {list(current_config.keys())}\"\n                )\n            if type(val) is not type(current_config[attr_name]):\n                val_type_str = type(val).__name__\n                expected_type_str = type(current_config[attr_name]).__name__\n                raise RuntimeError(\n                    f\"Unexpected type of attr {key}, got {val_type_str} should be {expected_type_str}\"\n                )\n            self.config[attr_name] = val\n\n    def __call__(self, model_, inputs_):\n        from torch._inductor.compile_fx import compile_fx\n\n        return compile_fx(model_, inputs_, config_patches=self.config)\n\n    def get_compiler_config(self):\n        from torch._inductor.compile_fx import get_patched_config_dict\n        return get_patched_config_dict(config_patches=self.config)\n\n    def reset(self):\n        from torch._inductor import config\n        if \"triton.cudagraphs\" in self.config or config.triton.cudagraphs:\n            if self.config.get(\"triton.cudagraphs\", True):\n                from torch._inductor.cudagraph_trees import reset_cudagraph_trees\n                reset_cudagraph_trees()\n\nclass _TorchCompileWrapper:\n    def __init__(self, backend, mode, options, dynamic):\n        from torch._dynamo.backends.registry import lookup_backend\n\n        if isinstance(backend, str):\n            self.compiler_name = backend\n        elif hasattr(backend, \"__name__\"):\n            self.compiler_name = backend.__name__\n        else:\n            self.compiler_name = str(backend)\n        self.dynamic = dynamic\n        self.compiler_fn = lookup_backend(backend)\n        self.kwargs = {}\n        # only pass the args if they non-empty\n        if mode and mode != \"default\":\n            self.kwargs[\"mode\"] = mode\n        if options:\n            self.kwargs[\"options\"] = options\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileWrapper) and\n                self.compiler_fn == other.compiler_fn and\n                self.kwargs == other.kwargs and\n                self.dynamic == other.dynamic)\n\n    def __call__(self, model_, inputs_):\n        return self.compiler_fn(model_, inputs_, **self.kwargs)\n\n\ndef compile(model: Optional[Callable] = None, *,\n            fullgraph: builtins.bool = False,\n            dynamic: Optional[builtins.bool] = None,\n            backend: Union[str, Callable] = \"inductor\",\n            mode: Union[str, None] = None,\n            options: Optional[Dict[str, Union[str, builtins.int, builtins.bool]]] = None,\n            disable: builtins.bool = False) -> Callable:\n    \"\"\"\n    Optimizes given model/function using TorchDynamo and specified backend.\n\n    Concretely, for every frame executed within the compiled region, we will attempt\n    to compile it and cache the compiled result on the code object for future\n    use.  A single frame may be compiled multiple times if previous compiled\n    results are not applicable for subsequent calls (this is called a \"guard\n    failure), you can use TORCH_LOGS=guards to debug these situations.\n    Multiple compiled results can be associated with a frame up to\n    ``torch._dynamo.config.cache_size_limit``, which defaults to 64; at which\n    point we will fall back to eager.  Note that compile caches are per\n    *code object*, not frame; if you dynamically create multiple copies of a\n    function, they will all share the same code cache.\n\n    Args:\n       model (Callable): Module/function to optimize\n       fullgraph (bool): Whether it is ok to break model into several subgraphs\n       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\n        to generate a kernel that is as dynamic as possible to avoid recompilations when\n        sizes change.  This may not always work as some operations/optimizations will\n        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\n        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\n        By default (None), we automatically detect if dynamism has occurred and compile a more\n        dynamic kernel upon recompile.\n       backend (str or Callable): backend to be used\n\n        - \"inductor\" is the default backend, which is a good balance between performance and overhead\n\n        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\n\n        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\n\n        - To register an out-of-tree custom backend: https://pytorch.org/docs/main/compile/custom-backends.html\n       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\n\n        - \"default\" is the default mode, which is a good balance between performance and overhead\n\n        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\n          useful for small batches.  Reduction of overhead can come at the cost of more memory\n          usage, as we will cache the workspace memory required for the invocation so that we\n          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\n          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\n          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\n          to debug.\n\n        - \"max-autotune\" is a mode that leverages Triton based matrix multiplications and convolutions\n          It enables CUDA graphs by default.\n\n        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\n\n        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\n\n       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\n\n        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\n\n        - `max_autotune` which will profile to pick the best matmul configuration\n\n        - `fallback_random` which is useful when debugging accuracy issues\n\n        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\n\n        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\n\n        - `trace.enabled` which is the most useful debugging flag to turn on\n\n        - `trace.graph_diagram` which will show you a picture of your graph after fusion\n\n        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\n       disable (bool): Turn torch.compile() into a no-op for testing\n\n    Example::\n\n        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n        def foo(x):\n            return torch.sin(x) + torch.cos(x)\n\n    \"\"\"\n    _C._log_api_usage_once(\"torch.compile\")\n    # Temporary until we get proper support for python 3.12\n    if sys.version_info >= (3, 12):\n        raise RuntimeError(\"Dynamo is not supported on Python 3.12+\")\n\n    # Decorator mode\n    if model is None:\n        def fn(model: Callable):\n            if model is None:\n                raise RuntimeError(\"Model can't be None\")\n            return compile(model,\n                           fullgraph=fullgraph,\n                           dynamic=dynamic,\n                           backend=backend,\n                           mode=mode,\n                           options=options,\n                           disable=disable)\n        return fn\n\n    if mode is not None and options is not None:\n        raise RuntimeError(\"Either mode or options can be specified, but both can't be specified at the same time.\")\n    if mode is None and options is None:\n        mode = \"default\"\n    if backend == \"inductor\":\n        backend = _TorchCompileInductorWrapper(mode, options, dynamic)\n    else:\n        backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n\n    return torch._dynamo.optimize(backend=backend, nopython=fullgraph, dynamic=dynamic, disable=disable)(model)\n\n\nfrom torch import export as export\n\n\ndef _register_device_module(device_type, module):\n    r\"\"\"Register an external runtime module of the specific :attr:`device_type`\n    supported by torch.\n\n    After the :attr:`module` is registered correctly, the user can refer\n    the external runtime module as part of torch with attribute torch.xxx.\n    \"\"\"\n    # Make sure the device_type represent a supported device type for torch.\n    device_type = torch.device(device_type).type\n    m = sys.modules[__name__]\n    if hasattr(m, device_type):\n        raise RuntimeError(f\"The runtime module of '{device_type}' has already \"\n                           f\"been registered with '{getattr(m, device_type)}'\")\n    setattr(m, device_type, module)\n    torch_module_name = '.'.join([__name__, device_type])\n    sys.modules[torch_module_name] = module\n\n# expose return_types\nfrom . import return_types\nfrom . import library\nif not TYPE_CHECKING:\n    from . import _meta_registrations\n\n# Enable CUDA Sanitizer\nif 'TORCH_CUDA_SANITIZER' in os.environ:\n    import torch.cuda._sanitizer as csan\n\n    csan.enable_cuda_sanitizer()\n\n# Populate magic methods on SymInt and SymFloat\nimport torch.fx.experimental.symbolic_shapes\n\nfrom torch import func as func\nfrom torch.func import vmap\n\n\n# The function _sparse_coo_tensor_unsafe is removed from PyTorch\n# Python API (v. 1.13), here we temporarily provide its replacement\n# with a deprecation warning.\n# TODO: remove the function for PyTorch v 1.15.\ndef _sparse_coo_tensor_unsafe(*args, **kwargs):\n    import warnings\n    warnings.warn('torch._sparse_coo_tensor_unsafe is deprecated, '\n                  'use torch.sparse_coo_tensor(..., check_invariants=False) instead.')\n    kwargs['check_invariants'] = False\n    return torch.sparse_coo_tensor(*args, **kwargs)\n\n# Register MPS specific decomps\ntorch.backends.mps._init()\n\nif not _running_with_deploy():\n    from torch import compiler as compiler\n\n    class _TritonLibrary:\n        lib = torch.library.Library(\"triton\", \"DEF\")\n        ops_table: Dict[Tuple[str, str], Callable] = {}\n\n        @classmethod\n        def registerOp(cls, op_key, full_schema, op_impl, dispatch_key):\n            if (op_key, dispatch_key) not in cls.ops_table:\n                cls.lib.define(full_schema)\n                cls.lib.impl(\"triton::\" + op_key, op_impl, dispatch_key)\n                cls.ops_table[(op_key, dispatch_key)] = op_impl\n\n            return cls.ops_table[(op_key, dispatch_key)]\n\n\n# Deprecated attributes\n_deprecated_attrs = {\n    \"has_mps\": torch.backends.mps.is_built,\n    \"has_cuda\": torch.backends.cuda.is_built,\n    \"has_cudnn\": torch.backends.cudnn.is_available,\n    \"has_mkldnn\": torch.backends.mkldnn.is_available,\n}\n\nif TYPE_CHECKING:\n    # Import the following modules during type checking to enable code intelligence features,\n    # such as auto-completion in tools like pylance, even when these modules are not explicitly\n    # imported in user code.\n    from torch import _dynamo as _dynamo\n    from torch import _inductor as _inductor\n    from torch import onnx as onnx\n\n_lazy_modules = {\n    \"_dynamo\",\n    \"_inductor\",\n    \"_export\",\n    # ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit\n    \"onnx\",\n}\n\ndef __getattr__(name):\n    # Deprecated attrs\n    replacement = _deprecated_attrs.get(name)\n    if replacement is not None:\n        import warnings\n        warnings.warn(f\"'{name}' is deprecated, please use '{replacement.__module__}.{replacement.__name__}()'\", stacklevel=2)\n        return replacement()\n\n    # Lazy modules\n    if name in _lazy_modules:\n        import importlib\n        return importlib.import_module(f\".{name}\", __name__)\n\n    raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")\n\nfrom . import _logging\n_logging._init_logs()\n",
      "torch/_appdirs.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (c) 2005-2010 ActiveState Software Inc.\n# Copyright (c) 2013 Eddy Petri\u0219or\n\n# flake8: noqa\n\n\"\"\"\nThis file is directly from\nhttps://github.com/ActiveState/appdirs/blob/3fe6a83776843a46f20c2e5587afcffe05e03b39/appdirs.py\n\nThe license of https://github.com/ActiveState/appdirs copied below:\n\n\n# This is the MIT license\n\nCopyright (c) 2010 ActiveState Software Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\n\"\"\"Utilities for determining application-specific dirs.\n\nSee <https://github.com/ActiveState/appdirs> for details and usage.\n\"\"\"\n# Dev Notes:\n# - MSDN on where to store app data files:\n#   http://support.microsoft.com/default.aspx?scid=kb;en-us;310294#XSLTH3194121123120121120120\n# - Mac OS X: http://developer.apple.com/documentation/MacOSX/Conceptual/BPFileSystem/index.html\n# - XDG spec for Un*x: https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html\n\n__version__ = \"1.4.4\"\n__version_info__ = tuple(int(segment) for segment in __version__.split(\".\"))\n\n\nimport os\nimport sys\n\nunicode = str\n\nif sys.platform.startswith(\"java\"):\n    import platform\n\n    os_name = platform.java_ver()[3][0]\n    if os_name.startswith(\"Windows\"):  # \"Windows XP\", \"Windows 7\", etc.\n        system = \"win32\"\n    elif os_name.startswith(\"Mac\"):  # \"Mac OS X\", etc.\n        system = \"darwin\"\n    else:  # \"Linux\", \"SunOS\", \"FreeBSD\", etc.\n        # Setting this to \"linux2\" is not ideal, but only Windows or Mac\n        # are actually checked for and the rest of the module expects\n        # *sys.platform* style strings.\n        system = \"linux2\"\nelse:\n    system = sys.platform\n\n\ndef user_data_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user data directories are:\n        Mac OS X:               ~/Library/Application Support/<AppName>\n        Unix:                   ~/.local/share/<AppName>    # or in $XDG_DATA_HOME, if defined\n        Win XP (not roaming):   C:\\Documents and Settings\\<username>\\Application Data\\<AppAuthor>\\<AppName>\n        Win XP (roaming):       C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\n        Win 7  (not roaming):   C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\n        Win 7  (roaming):       C:\\Users\\<username>\\AppData\\Roaming\\<AppAuthor>\\<AppName>\n\n    For Unix, we follow the XDG spec and support $XDG_DATA_HOME.\n    That means, by default \"~/.local/share/<AppName>\".\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        const = roaming and \"CSIDL_APPDATA\" or \"CSIDL_LOCAL_APPDATA\"\n        path = os.path.normpath(_get_win_folder(const))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Application Support/\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_DATA_HOME\", os.path.expanduser(\"~/.local/share\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef site_data_dir(appname=None, appauthor=None, version=None, multipath=False):\n    r\"\"\"Return full path to the user-shared data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"multipath\" is an optional parameter only applicable to *nix\n            which indicates that the entire list of data dirs should be\n            returned. By default, the first item from XDG_DATA_DIRS is\n            returned, or '/usr/local/share/<AppName>',\n            if XDG_DATA_DIRS is not set\n\n    Typical site data directories are:\n        Mac OS X:   /Library/Application Support/<AppName>\n        Unix:       /usr/local/share/<AppName> or /usr/share/<AppName>\n        Win XP:     C:\\Documents and Settings\\All Users\\Application Data\\<AppAuthor>\\<AppName>\n        Vista:      (Fail! \"C:\\ProgramData\" is a hidden *system* directory on Vista.)\n        Win 7:      C:\\ProgramData\\<AppAuthor>\\<AppName>   # Hidden, but writeable on Win 7.\n\n    For Unix, this is using the $XDG_DATA_DIRS[0] default.\n\n    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        path = os.path.normpath(_get_win_folder(\"CSIDL_COMMON_APPDATA\"))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"/Library/Application Support\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        # XDG default for $XDG_DATA_DIRS\n        # only first, if multipath is False\n        path = os.getenv(\n            \"XDG_DATA_DIRS\", os.pathsep.join([\"/usr/local/share\", \"/usr/share\"])\n        )\n        pathlist = [\n            os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)\n        ]\n        if appname:\n            if version:\n                appname = os.path.join(appname, version)\n            pathlist = [os.sep.join([x, appname]) for x in pathlist]\n\n        if multipath:\n            path = os.pathsep.join(pathlist)\n        else:\n            path = pathlist[0]\n        return path\n\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_config_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific config dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user config directories are:\n        Mac OS X:               ~/Library/Preferences/<AppName>\n        Unix:                   ~/.config/<AppName>     # or in $XDG_CONFIG_HOME, if defined\n        Win *:                  same as user_data_dir\n\n    For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME.\n    That means, by default \"~/.config/<AppName>\".\n    \"\"\"\n    if system == \"win32\":\n        path = user_data_dir(appname, appauthor, None, roaming)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Preferences/\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_CONFIG_HOME\", os.path.expanduser(\"~/.config\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef site_config_dir(appname=None, appauthor=None, version=None, multipath=False):\n    r\"\"\"Return full path to the user-shared data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"multipath\" is an optional parameter only applicable to *nix\n            which indicates that the entire list of config dirs should be\n            returned. By default, the first item from XDG_CONFIG_DIRS is\n            returned, or '/etc/xdg/<AppName>', if XDG_CONFIG_DIRS is not set\n\n    Typical site config directories are:\n        Mac OS X:   same as site_data_dir\n        Unix:       /etc/xdg/<AppName> or $XDG_CONFIG_DIRS[i]/<AppName> for each value in\n                    $XDG_CONFIG_DIRS\n        Win *:      same as site_data_dir\n        Vista:      (Fail! \"C:\\ProgramData\" is a hidden *system* directory on Vista.)\n\n    For Unix, this is using the $XDG_CONFIG_DIRS[0] default, if multipath=False\n\n    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.\n    \"\"\"\n    if system == \"win32\":\n        path = site_data_dir(appname, appauthor)\n        if appname and version:\n            path = os.path.join(path, version)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"/Library/Preferences\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        # XDG default for $XDG_CONFIG_DIRS\n        # only first, if multipath is False\n        path = os.getenv(\"XDG_CONFIG_DIRS\", \"/etc/xdg\")\n        pathlist = [\n            os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)\n        ]\n        if appname:\n            if version:\n                appname = os.path.join(appname, version)\n            pathlist = [os.sep.join([x, appname]) for x in pathlist]\n\n        if multipath:\n            path = os.pathsep.join(pathlist)\n        else:\n            path = pathlist[0]\n    return path\n\n\ndef user_cache_dir(appname=None, appauthor=None, version=None, opinion=True):\n    r\"\"\"Return full path to the user-specific cache dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"opinion\" (boolean) can be False to disable the appending of\n            \"Cache\" to the base app data dir for Windows. See\n            discussion below.\n\n    Typical user cache directories are:\n        Mac OS X:   ~/Library/Caches/<AppName>\n        Unix:       ~/.cache/<AppName> (XDG default)\n        Win XP:     C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\\Cache\n        Vista:      C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\\Cache\n\n    On Windows the only suggestion in the MSDN docs is that local settings go in\n    the `CSIDL_LOCAL_APPDATA` directory. This is identical to the non-roaming\n    app data dir (the default returned by `user_data_dir` above). Apps typically\n    put cache data somewhere *under* the given dir here. Some examples:\n        ...\\Mozilla\\Firefox\\Profiles\\<ProfileName>\\Cache\n        ...\\Acme\\SuperApp\\Cache\\1.0\n    OPINION: This function appends \"Cache\" to the `CSIDL_LOCAL_APPDATA` value.\n    This can be disabled with the `opinion=False` option.\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        path = os.path.normpath(_get_win_folder(\"CSIDL_LOCAL_APPDATA\"))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n            if opinion:\n                path = os.path.join(path, \"Cache\")\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Caches\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_state_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific state dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user state directories are:\n        Mac OS X:  same as user_data_dir\n        Unix:      ~/.local/state/<AppName>   # or in $XDG_STATE_HOME, if defined\n        Win *:     same as user_data_dir\n\n    For Unix, we follow this Debian proposal <https://wiki.debian.org/XDGBaseDirectorySpecification#state>\n    to extend the XDG spec and support $XDG_STATE_HOME.\n\n    That means, by default \"~/.local/state/<AppName>\".\n    \"\"\"\n    if system in [\"win32\", \"darwin\"]:\n        path = user_data_dir(appname, appauthor, None, roaming)\n    else:\n        path = os.getenv(\"XDG_STATE_HOME\", os.path.expanduser(\"~/.local/state\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_log_dir(appname=None, appauthor=None, version=None, opinion=True):\n    r\"\"\"Return full path to the user-specific log dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"opinion\" (boolean) can be False to disable the appending of\n            \"Logs\" to the base app data dir for Windows, and \"log\" to the\n            base cache dir for Unix. See discussion below.\n\n    Typical user log directories are:\n        Mac OS X:   ~/Library/Logs/<AppName>\n        Unix:       ~/.cache/<AppName>/log  # or under $XDG_CACHE_HOME if defined\n        Win XP:     C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\\Logs\n        Vista:      C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\\Logs\n\n    On Windows the only suggestion in the MSDN docs is that local settings\n    go in the `CSIDL_LOCAL_APPDATA` directory. (Note: I'm interested in\n    examples of what some windows apps use for a logs dir.)\n\n    OPINION: This function appends \"Logs\" to the `CSIDL_LOCAL_APPDATA`\n    value for Windows and appends \"log\" to the user cache dir for Unix.\n    This can be disabled with the `opinion=False` option.\n    \"\"\"\n    if system == \"darwin\":\n        path = os.path.join(os.path.expanduser(\"~/Library/Logs\"), appname)\n    elif system == \"win32\":\n        path = user_data_dir(appname, appauthor, version)\n        version = False\n        if opinion:\n            path = os.path.join(path, \"Logs\")\n    else:\n        path = user_cache_dir(appname, appauthor, version)\n        version = False\n        if opinion:\n            path = os.path.join(path, \"log\")\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\nclass AppDirs(object):\n    \"\"\"Convenience wrapper for getting application dirs.\"\"\"\n\n    def __init__(\n        self, appname=None, appauthor=None, version=None, roaming=False, multipath=False\n    ):\n        self.appname = appname\n        self.appauthor = appauthor\n        self.version = version\n        self.roaming = roaming\n        self.multipath = multipath\n\n    @property\n    def user_data_dir(self):\n        return user_data_dir(\n            self.appname, self.appauthor, version=self.version, roaming=self.roaming\n        )\n\n    @property\n    def site_data_dir(self):\n        return site_data_dir(\n            self.appname, self.appauthor, version=self.version, multipath=self.multipath\n        )\n\n    @property\n    def user_config_dir(self):\n        return user_config_dir(\n            self.appname, self.appauthor, version=self.version, roaming=self.roaming\n        )\n\n    @property\n    def site_config_dir(self):\n        return site_config_dir(\n            self.appname, self.appauthor, version=self.version, multipath=self.multipath\n        )\n\n    @property\n    def user_cache_dir(self):\n        return user_cache_dir(self.appname, self.appauthor, version=self.version)\n\n    @property\n    def user_state_dir(self):\n        return user_state_dir(self.appname, self.appauthor, version=self.version)\n\n    @property\n    def user_log_dir(self):\n        return user_log_dir(self.appname, self.appauthor, version=self.version)\n\n\n# ---- internal support stuff\n\n\ndef _get_win_folder_from_registry(csidl_name):\n    \"\"\"This is a fallback technique at best. I'm not sure if using the\n    registry for this guarantees us the correct answer for all CSIDL_*\n    names.\n    \"\"\"\n    import winreg as _winreg\n\n    shell_folder_name = {\n        \"CSIDL_APPDATA\": \"AppData\",\n        \"CSIDL_COMMON_APPDATA\": \"Common AppData\",\n        \"CSIDL_LOCAL_APPDATA\": \"Local AppData\",\n    }[csidl_name]\n\n    key = _winreg.OpenKey(\n        _winreg.HKEY_CURRENT_USER,\n        r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\",\n    )\n    dir, type = _winreg.QueryValueEx(key, shell_folder_name)\n    return dir\n\n\ndef _get_win_folder_with_pywin32(csidl_name):\n    from win32com.shell import shell, shellcon\n\n    dir = shell.SHGetFolderPath(0, getattr(shellcon, csidl_name), 0, 0)\n    # Try to make this a unicode path because SHGetFolderPath does\n    # not return unicode strings when there is unicode data in the\n    # path.\n    try:\n        dir = unicode(dir)\n\n        # Downgrade to short path name if have highbit chars. See\n        # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n        has_high_char = False\n        for c in dir:\n            if ord(c) > 255:\n                has_high_char = True\n                break\n        if has_high_char:\n            try:\n                import win32api\n\n                dir = win32api.GetShortPathName(dir)\n            except ImportError:\n                pass\n    except UnicodeError:\n        pass\n    return dir\n\n\ndef _get_win_folder_with_ctypes(csidl_name):\n    import ctypes\n\n    csidl_const = {\n        \"CSIDL_APPDATA\": 26,\n        \"CSIDL_COMMON_APPDATA\": 35,\n        \"CSIDL_LOCAL_APPDATA\": 28,\n    }[csidl_name]\n\n    buf = ctypes.create_unicode_buffer(1024)\n    ctypes.windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)\n\n    # Downgrade to short path name if have highbit chars. See\n    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n    has_high_char = False\n    for c in buf:\n        if ord(c) > 255:\n            has_high_char = True\n            break\n    if has_high_char:\n        buf2 = ctypes.create_unicode_buffer(1024)\n        if ctypes.windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):\n            buf = buf2\n\n    return buf.value\n\n\ndef _get_win_folder_with_jna(csidl_name):\n    import array\n\n    from com.sun import jna\n    from com.sun.jna.platform import win32\n\n    buf_size = win32.WinDef.MAX_PATH * 2\n    buf = array.zeros(\"c\", buf_size)\n    shell = win32.Shell32.INSTANCE\n    shell.SHGetFolderPath(\n        None,\n        getattr(win32.ShlObj, csidl_name),\n        None,\n        win32.ShlObj.SHGFP_TYPE_CURRENT,\n        buf,\n    )\n    dir = jna.Native.toString(buf.tostring()).rstrip(\"\\0\")\n\n    # Downgrade to short path name if have highbit chars. See\n    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n    has_high_char = False\n    for c in dir:\n        if ord(c) > 255:\n            has_high_char = True\n            break\n    if has_high_char:\n        buf = array.zeros(\"c\", buf_size)\n        kernel = win32.Kernel32.INSTANCE\n        if kernel.GetShortPathName(dir, buf, buf_size):\n            dir = jna.Native.toString(buf.tostring()).rstrip(\"\\0\")\n\n    return dir\n\n\nif system == \"win32\":\n    try:\n        import win32com.shell\n\n        _get_win_folder = _get_win_folder_with_pywin32\n    except ImportError:\n        try:\n            from ctypes import windll\n\n            _get_win_folder = _get_win_folder_with_ctypes\n        except ImportError:\n            try:\n                import com.sun.jna\n\n                _get_win_folder = _get_win_folder_with_jna\n            except ImportError:\n                _get_win_folder = _get_win_folder_from_registry\n\n\n# ---- self test code\n\nif __name__ == \"__main__\":\n    appname = \"MyApp\"\n    appauthor = \"MyCompany\"\n\n    props = (\n        \"user_data_dir\",\n        \"user_config_dir\",\n        \"user_cache_dir\",\n        \"user_state_dir\",\n        \"user_log_dir\",\n        \"site_data_dir\",\n        \"site_config_dir\",\n    )\n\n    print(f\"-- app dirs {__version__} --\")\n\n    print(\"-- app dirs (with optional 'version')\")\n    dirs = AppDirs(appname, appauthor, version=\"1.0\")\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (without optional 'version')\")\n    dirs = AppDirs(appname, appauthor)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (without optional 'appauthor')\")\n    dirs = AppDirs(appname)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (with disabled 'appauthor')\")\n    dirs = AppDirs(appname, appauthor=False)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n"
    },
    "patchgt": {
      "test/quantization/pt2e/test_quantize_pt2e.py": "@@ -361,7 +361,7 @@ def _verify_symmetric_qnnpack_qat_numerics(\n         self.assertEqual(after_prepare_result_pt2e, after_prepare_result_fx)\n \n         if verify_convert:\n-            model_pt2e.eval()\n+            torch.ao.quantization.move_model_to_eval(model_pt2e)\n             model_pt2e = convert_pt2e(model_pt2e)\n             quant_result_pt2e = model_pt2e(*example_inputs)\n             model_fx.eval()\n@@ -2392,6 +2392,39 @@ def forward(self, x, y):\n             non_ref_node_occurrence\n         )\n \n+    def test_move_model_to_eval(self):\n+        class M(torch.nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.dropout = torch.nn.Dropout(0.5)\n+\n+            def forward(self, x):\n+                return self.dropout(x)\n+\n+        example_inputs = (torch.randn(1),)\n+        m = M().train()\n+        m = capture_pre_autograd_graph(m, example_inputs)\n+        m.graph.eliminate_dead_code()\n+        m.recompile()\n+\n+        # Assert that dropout op exists and is in train mode\n+        dropout_node = None\n+        for n in m.graph.nodes:\n+            if n.target == torch.ops.aten.native_dropout.default:\n+                dropout_node = n\n+                break\n+        self.assertTrue(dropout_node is not None)\n+        self.assertTrue(dropout_node.args[2])\n+\n+        # Do the subgraph rewriting\n+        torch.ao.quantization.move_model_to_eval(m)\n+\n+        # Assert that dropout op is now replaced with a clone op\n+        targets = [n.target for n in m.graph.nodes]\n+        self.assertTrue(torch.ops.aten.clone.default in targets)\n+        self.assertTrue(torch.ops.aten.native_dropout.default not in targets)\n+\n+\n @skipIfNoQNNPACK\n class TestQuantizePT2EOps(QuantizationTestCase):\n     def test_gru(self):",
      "torch/ao/quantization/__init__.py": "@@ -12,6 +12,7 @@\n from .quantize import *  # noqa: F403\n from .quantize_jit import *  # noqa: F403\n from .stubs import *  # noqa: F403\n+from .pt2e.utils import move_model_to_eval\n from typing import Union, List, Callable, Tuple, Optional\n from torch import Tensor\n import torch\n@@ -119,6 +120,7 @@\n     \"get_quantized_operator\",\n     \"get_static_quant_module_class\",\n     \"load_observer_state_dict\",\n+    \"move_model_to_eval\",\n     \"no_observer_set\",\n     \"per_channel_weight_observer_range_neg_127_to_127\",\n     \"prepare\",",
      "torch/ao/quantization/pt2e/utils.py": "@@ -9,11 +9,11 @@\n import operator\n from typing import Any, Callable, Dict, Optional, Tuple, List, Union\n from torch.utils._pytree import LeafSpec\n-from torch._export import capture_pre_autograd_graph\n \n __all__ = [\n     \"fold_bn_weights_into_conv_node\",\n     \"get_aten_graph_module\",\n+    \"move_model_to_eval\",\n     \"remove_tensor_overload_for_qdq_ops\",\n ]\n \n@@ -156,6 +156,8 @@ def get_aten_graph_module(\n     \"\"\"\n     Convert the pattern to an FX graph with decomposed aten ops.\n     \"\"\"\n+    # Avoid circular dependencies\n+    from torch._export import capture_pre_autograd_graph\n     aten_pattern = capture_pre_autograd_graph(\n         pattern,\n         example_inputs,\n@@ -387,3 +389,15 @@ def replacement(x_i8, scale, zero_point, quant_min, quant_max):\n         new_args = tuple(new_args)\n         node.args = new_args\n     return gm\n+\n+# TODO: also support move_model_to_train\n+# TODO: also support standalone batchnorm\n+def move_model_to_eval(m: GraphModule):\n+    \"\"\"\n+    Move an exported GraphModule to eval mode.\n+\n+    This is equivalent to model.eval() but only for certain special ops like dropout.\n+    QAT users should call this before performing inference on the model.\n+    \"\"\"\n+    _replace_dropout_for_eval(m)\n+    return m",
      "torch/ao/quantization/quantize_pt2e.py": "@@ -9,7 +9,6 @@\n from .pt2e.utils import (\n     _get_node_name_to_scope,\n     _fuse_conv_bn_,\n-    _replace_dropout_for_eval,\n )\n from .pt2e.representation import reference_representation_rewrite\n from .fx.prepare import prepare as fx_prepare\n@@ -95,9 +94,6 @@ def convert_pt2e(\n     use_reference_representation: bool = False,\n ) -> GraphModule:\n     original_graph_meta = model.meta\n-    # TODO: Handle this in export itself, outside of quantization\n-    # See https://github.com/pytorch/pytorch/issues/103681.\n-    _replace_dropout_for_eval(model)\n     model = _convert_to_reference_decomposed_fx(model)\n     model = _fold_conv_bn_qat(model)\n     if use_reference_representation:"
    },
    "repo": "pytorch",
    "pr_number": 108255
  },
  {
    "issue": "Issue: Fix LayerNorm(bias=False) error (#108060)\n\ncherry pick #108060 into release branch\n\n",
    "ctx": {
      "torch/nn/modules/normalization.py": "import torch\nimport numbers\nfrom torch.nn.parameter import Parameter\nfrom .module import Module\nfrom ._functions import CrossMapLRN2d as _cross_map_lrn2d\nfrom .. import functional as F\nfrom .. import init\n\nfrom torch import Tensor, Size\nfrom typing import Union, List, Tuple\n\n__all__ = ['LocalResponseNorm', 'CrossMapLRN2d', 'LayerNorm', 'GroupNorm']\n\nclass LocalResponseNorm(Module):\n    r\"\"\"Applies local response normalization over an input signal composed\n    of several input planes, where channels occupy the second dimension.\n    Applies normalization across channels.\n\n    .. math::\n        b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n}\n        \\sum_{c'=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c'}^2\\right)^{-\\beta}\n\n    Args:\n        size: amount of neighbouring channels used for normalization\n        alpha: multiplicative factor. Default: 0.0001\n        beta: exponent. Default: 0.75\n        k: additive factor. Default: 1\n\n    Shape:\n        - Input: :math:`(N, C, *)`\n        - Output: :math:`(N, C, *)` (same shape as input)\n\n    Examples::\n\n        >>> lrn = nn.LocalResponseNorm(2)\n        >>> signal_2d = torch.randn(32, 5, 24, 24)\n        >>> signal_4d = torch.randn(16, 5, 7, 7, 7, 7)\n        >>> output_2d = lrn(signal_2d)\n        >>> output_4d = lrn(signal_4d)\n\n    \"\"\"\n    __constants__ = ['size', 'alpha', 'beta', 'k']\n    size: int\n    alpha: float\n    beta: float\n    k: float\n\n    def __init__(self, size: int, alpha: float = 1e-4, beta: float = 0.75, k: float = 1.) -> None:\n        super().__init__()\n        self.size = size\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.local_response_norm(input, self.size, self.alpha, self.beta,\n                                     self.k)\n\n    def extra_repr(self):\n        return '{size}, alpha={alpha}, beta={beta}, k={k}'.format(**self.__dict__)\n\n\nclass CrossMapLRN2d(Module):\n    size: int\n    alpha: float\n    beta: float\n    k: float\n\n    def __init__(self, size: int, alpha: float = 1e-4, beta: float = 0.75, k: float = 1) -> None:\n        super().__init__()\n        self.size = size\n        self.alpha = alpha\n        self.beta = beta\n        self.k = k\n\n    def forward(self, input: Tensor) -> Tensor:\n        return _cross_map_lrn2d.apply(input, self.size, self.alpha, self.beta,\n                                      self.k)\n\n    def extra_repr(self) -> str:\n        return '{size}, alpha={alpha}, beta={beta}, k={k}'.format(**self.__dict__)\n\n\n_shape_t = Union[int, List[int], Size]\n\n\nclass LayerNorm(Module):\n    r\"\"\"Applies Layer Normalization over a mini-batch of inputs as described in\n    the paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__\n\n    .. math::\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The mean and standard-deviation are calculated over the last `D` dimensions, where `D`\n    is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`\n    is ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over\n    the last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).\n    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\n    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\n    The standard-deviation is calculated via the biased estimator, equivalent to\n    `torch.var(input, unbiased=False)`.\n\n    .. note::\n        Unlike Batch Normalization and Instance Normalization, which applies\n        scalar scale and bias for each entire channel/plane with the\n        :attr:`affine` option, Layer Normalization applies per-element scale and\n        bias with :attr:`elementwise_affine`.\n\n    This layer uses statistics computed from input data in both training and\n    evaluation modes.\n\n    Args:\n        normalized_shape (int or list or torch.Size): input shape from an expected input\n            of size\n\n            .. math::\n                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n\n            If a single integer is used, it is treated as a singleton list, and this module will\n            normalize over the last dimension which is expected to be of that specific size.\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        elementwise_affine: a boolean value that when set to ``True``, this module\n            has learnable per-element affine parameters initialized to ones (for weights)\n            and zeros (for biases). Default: ``True``.\n        bias: If set to ``False``, the layer will not learn an additive bias (only relevant if\n            :attr:`elementwise_affine` is ``True``). Default: ``True``.\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`\\text{normalized\\_shape}` when :attr:`elementwise_affine` is set to ``True``.\n            The values are initialized to 1.\n        bias:   the learnable bias of the module of shape\n                :math:`\\text{normalized\\_shape}` when :attr:`elementwise_affine` is set to ``True``.\n                The values are initialized to 0.\n\n    Shape:\n        - Input: :math:`(N, *)`\n        - Output: :math:`(N, *)` (same shape as input)\n\n    Examples::\n\n        >>> # NLP Example\n        >>> batch, sentence_length, embedding_dim = 20, 5, 10\n        >>> embedding = torch.randn(batch, sentence_length, embedding_dim)\n        >>> layer_norm = nn.LayerNorm(embedding_dim)\n        >>> # Activate module\n        >>> layer_norm(embedding)\n        >>>\n        >>> # Image Example\n        >>> N, C, H, W = 20, 5, 10, 10\n        >>> input = torch.randn(N, C, H, W)\n        >>> # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n        >>> # as shown in the image below\n        >>> layer_norm = nn.LayerNorm([C, H, W])\n        >>> output = layer_norm(input)\n\n    .. image:: ../_static/img/nn/layer_norm.jpg\n        :scale: 50 %\n\n    \"\"\"\n    __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']\n    normalized_shape: Tuple[int, ...]\n    eps: float\n    elementwise_affine: bool\n\n    def __init__(self, normalized_shape: _shape_t, eps: float = 1e-5, elementwise_affine: bool = True,\n                 bias: bool = True, device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            # mypy error: incompatible types in assignment\n            normalized_shape = (normalized_shape,)  # type: ignore[assignment]\n        self.normalized_shape = tuple(normalized_shape)  # type: ignore[arg-type]\n        self.eps = eps\n        self.elementwise_affine = elementwise_affine\n        if self.elementwise_affine:\n            self.weight = Parameter(torch.empty(self.normalized_shape, **factory_kwargs))\n            if bias:\n                self.bias = Parameter(torch.empty(self.normalized_shape, **factory_kwargs))\n            else:\n                self.register_parameter('bias', None)\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        if self.elementwise_affine:\n            init.ones_(self.weight)\n            init.zeros_(self.bias)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.layer_norm(\n            input, self.normalized_shape, self.weight, self.bias, self.eps)\n\n    def extra_repr(self) -> str:\n        return '{normalized_shape}, eps={eps}, ' \\\n            'elementwise_affine={elementwise_affine}'.format(**self.__dict__)\n\n\nclass GroupNorm(Module):\n    r\"\"\"Applies Group Normalization over a mini-batch of inputs as described in\n    the paper `Group Normalization <https://arxiv.org/abs/1803.08494>`__\n\n    .. math::\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\n    The input channels are separated into :attr:`num_groups` groups, each containing\n    ``num_channels / num_groups`` channels. :attr:`num_channels` must be divisible by\n    :attr:`num_groups`. The mean and standard-deviation are calculated\n    separately over the each group. :math:`\\gamma` and :math:`\\beta` are learnable\n    per-channel affine transform parameter vectors of size :attr:`num_channels` if\n    :attr:`affine` is ``True``.\n    The standard-deviation is calculated via the biased estimator, equivalent to\n    `torch.var(input, unbiased=False)`.\n\n    This layer uses statistics computed from input data in both training and\n    evaluation modes.\n\n    Args:\n        num_groups (int): number of groups to separate the channels into\n        num_channels (int): number of channels expected in input\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        affine: a boolean value that when set to ``True``, this module\n            has learnable per-channel affine parameters initialized to ones (for weights)\n            and zeros (for biases). Default: ``True``.\n\n    Shape:\n        - Input: :math:`(N, C, *)` where :math:`C=\\text{num\\_channels}`\n        - Output: :math:`(N, C, *)` (same shape as input)\n\n    Examples::\n\n        >>> input = torch.randn(20, 6, 10, 10)\n        >>> # Separate 6 channels into 3 groups\n        >>> m = nn.GroupNorm(3, 6)\n        >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n        >>> m = nn.GroupNorm(6, 6)\n        >>> # Put all 6 channels into a single group (equivalent with LayerNorm)\n        >>> m = nn.GroupNorm(1, 6)\n        >>> # Activating the module\n        >>> output = m(input)\n    \"\"\"\n    __constants__ = ['num_groups', 'num_channels', 'eps', 'affine']\n    num_groups: int\n    num_channels: int\n    eps: float\n    affine: bool\n\n    def __init__(self, num_groups: int, num_channels: int, eps: float = 1e-5, affine: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        if num_channels % num_groups != 0:\n            raise ValueError('num_channels must be divisible by num_groups')\n\n        self.num_groups = num_groups\n        self.num_channels = num_channels\n        self.eps = eps\n        self.affine = affine\n        if self.affine:\n            self.weight = Parameter(torch.empty(num_channels, **factory_kwargs))\n            self.bias = Parameter(torch.empty(num_channels, **factory_kwargs))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        if self.affine:\n            init.ones_(self.weight)\n            init.zeros_(self.bias)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.group_norm(\n            input, self.num_groups, self.weight, self.bias, self.eps)\n\n    def extra_repr(self) -> str:\n        return '{num_groups}, {num_channels}, eps={eps}, ' \\\n            'affine={affine}'.format(**self.__dict__)\n\n\n# TODO: ContrastiveNorm2d\n# TODO: DivisiveNorm2d\n# TODO: SubtractiveNorm2d\n",
      "torch/testing/_internal/common_modules.py": "import torch\nimport unittest\nfrom copy import deepcopy\nfrom enum import Enum\nfrom functools import wraps, partial\nfrom itertools import chain, product\nimport itertools\nimport math\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.testing import make_tensor\nfrom torch.testing._internal.common_cuda import TEST_CUDNN\nfrom torch.testing._internal.common_dtype import (\n    floating_types, floating_and_complex_types_and, get_all_fp_dtypes, complex_types_and)\nfrom torch.testing._internal.common_device_type import (\n    _TestParametrizer, _update_param_kwargs, toleranceOverride, tol,\n    skipCUDAIfCudnnVersionLessThan, skipCUDAIfRocm, precisionOverride, skipMeta, skipMPS, skipCUDAVersionIn)\nfrom torch.testing._internal.common_methods_invocations import DecorateInfo\nfrom torch.testing._internal.common_nn import nllloss_reference, get_reduction\nfrom torch.testing._internal.common_utils import (\n    freeze_rng_state, set_single_threaded_if_parallel_tbb, skipIfMps, GRADCHECK_NONDET_TOL, TEST_WITH_ROCM, IS_WINDOWS)\nfrom types import ModuleType\nfrom typing import List, Tuple, Type, Set, Dict\n\n# List of all namespaces containing modules to test.\nMODULE_NAMESPACES: List[ModuleType] = [\n    torch.nn.modules,\n    torch.ao.nn.qat.modules,\n    torch.ao.nn.quantizable.modules,\n    torch.ao.nn.quantized.modules,\n    torch.ao.nn.quantized.modules,\n]\n\n# Modules that shouldn't be tested for one reason or another.\nMODULES_TO_SKIP: Set[Type] = {\n    torch.nn.Module,  # abstract base class\n    torch.nn.Container,  # deprecated\n    torch.nn.NLLLoss2d,  # deprecated\n    torch.ao.nn.quantized.MaxPool2d,  # aliases to nn.MaxPool2d\n    torch.ao.nn.quantized.MaxPool2d,  # aliases to nn.MaxPool2d\n}\n\n# List of all module classes to test.\nMODULE_CLASSES: List[Type] = list(chain(*[\n    [getattr(namespace, module_name) for module_name in namespace.__all__]  # type: ignore[attr-defined]\n    for namespace in MODULE_NAMESPACES]))\nMODULE_CLASSES = [cls for cls in MODULE_CLASSES if cls not in MODULES_TO_SKIP]\n\n# Dict of module class -> common name. Useful for making test names more intuitive.\n# Example: torch.nn.modules.linear.Linear -> \"nn.Linear\"\nMODULE_CLASS_NAMES: Dict[Type, str] = {}\nfor namespace in MODULE_NAMESPACES:\n    for module_name in namespace.__all__:  # type: ignore[attr-defined]\n        module_cls = getattr(namespace, module_name)\n        namespace_name = namespace.__name__.replace('torch.', '').replace('.modules', '')\n\n        # Deal with any aliases by preferring earlier names.\n        if module_cls not in MODULE_CLASS_NAMES:\n            MODULE_CLASS_NAMES[module_cls] = f'{namespace_name}.{module_name}'\n\n\n# Specifies the modes (i.e. train, eval) to test over.\nTrainEvalMode = Enum('TrainEvalMode', ('train_only', 'eval_only', 'train_and_eval'))\n\n\nclass modules(_TestParametrizer):\n    \"\"\" PROTOTYPE: Decorator for specifying a list of modules over which to run a test. \"\"\"\n\n    def __init__(self, module_info_iterable, allowed_dtypes=None, train_eval_mode=TrainEvalMode.train_and_eval):\n        self.module_info_list = list(module_info_iterable)\n        self.allowed_dtypes = set(allowed_dtypes) if allowed_dtypes is not None else None\n        self.train_eval_mode = train_eval_mode\n\n    def _get_training_flags(self, module_info):\n        training_flags = []\n        if (self.train_eval_mode == TrainEvalMode.train_only or\n                self.train_eval_mode == TrainEvalMode.train_and_eval):\n            training_flags.append(True)\n\n        if (self.train_eval_mode == TrainEvalMode.eval_only or\n                self.train_eval_mode == TrainEvalMode.train_and_eval):\n            training_flags.append(False)\n\n        # If train and eval modes don't differ for the module, don't bother using more than one.\n        if not module_info.train_and_eval_differ:\n            training_flags = training_flags[:1]\n\n        return training_flags\n\n    def _parametrize_test(self, test, generic_cls, device_cls):\n        if device_cls is None:\n            raise RuntimeError('The @modules decorator is only intended to be used in a device-specific '\n                               'context; use it with instantiate_device_type_tests() instead of '\n                               'instantiate_parametrized_tests()')\n\n        for module_info in self.module_info_list:\n            dtypes = set(module_info.dtypes)\n            if self.allowed_dtypes is not None:\n                dtypes = dtypes.intersection(self.allowed_dtypes)\n\n            training_flags = self._get_training_flags(module_info)\n            for (training, dtype) in product(training_flags, dtypes):\n                # Construct the test name; device / dtype parts are handled outside.\n                # See [Note: device and dtype suffix placement]\n                test_name = module_info.formatted_name\n                if len(training_flags) > 1:\n                    test_name += f\"_{'train_mode' if training else 'eval_mode'}\"\n\n                # Construct parameter kwargs to pass to the test.\n                param_kwargs = {'module_info': module_info}\n                _update_param_kwargs(param_kwargs, 'dtype', dtype)\n                _update_param_kwargs(param_kwargs, 'training', training)\n\n                try:\n\n                    @wraps(test)\n                    def test_wrapper(*args, **kwargs):\n                        return test(*args, **kwargs)\n\n                    decorator_fn = partial(module_info.get_decorators, generic_cls.__name__,\n                                           test.__name__, device_cls.device_type, dtype)\n\n                    yield (test_wrapper, test_name, param_kwargs, decorator_fn)\n                except Exception as ex:\n                    # Provides an error message for debugging before rethrowing the exception\n                    print(f\"Failed to instantiate {test_name} for module {module_info.name}!\")\n                    raise ex\n\n\ndef get_module_common_name(module_cls):\n    if module_cls in MODULE_CLASS_NAMES:\n        # Example: \"nn.Linear\"\n        return MODULE_CLASS_NAMES[module_cls]\n    else:\n        return module_cls.__name__\n\n\nclass FunctionInput:\n    \"\"\" Contains args and kwargs to pass as input to a function. \"\"\"\n    __slots__ = ['args', 'kwargs']\n\n    def __init__(self, *args, **kwargs):\n        self.args = args\n        self.kwargs = kwargs\n\n\nclass ModuleInput:\n    \"\"\" Contains args / kwargs for module instantiation + forward pass. \"\"\"\n    __slots__ = ['constructor_input', 'forward_input', 'desc', 'reference_fn']\n\n    def __init__(self, constructor_input, forward_input=None, desc='', reference_fn=None):\n        self.constructor_input = constructor_input  # Inputs to pass during construction\n        self.forward_input = forward_input  # Inputs to pass to forward()\n        self.desc = desc  # Description for this set of inputs\n        self.reference_fn = reference_fn  # Reference with signature: reference_fn(module, parameters, *args, **kwargs)\n\n        if reference_fn is not None:\n\n            @wraps(reference_fn)\n            def copy_reference_fn(m, *args, **kwargs):\n                # Copy inputs to avoid undesired side effects from calling the reference.\n                args, kwargs = deepcopy(args), deepcopy(kwargs)\n\n                # Note that module parameters are passed in for convenience.\n                return reference_fn(m, list(m.parameters()), *args, **kwargs)\n\n            self.reference_fn = copy_reference_fn\n\nclass ModuleErrorEnum(Enum):\n    \"\"\" Enumerates when error is raised when testing modules. \"\"\"\n    CONSTRUCTION_ERROR = 0\n    FORWARD_ERROR = 1\n\nclass ErrorModuleInput:\n    \"\"\"\n    A ModuleInput that will cause the operation to throw an error plus information\n    about the resulting error.\n    \"\"\"\n\n    __slots__ = [\"module_error_input\", \"error_on\", \"error_type\", \"error_regex\"]\n\n    def __init__(self,\n                 module_error_input,\n                 *,\n                 error_on=ModuleErrorEnum.CONSTRUCTION_ERROR,\n                 error_type=RuntimeError,\n                 error_regex):\n        self.module_error_input = module_error_input\n        self.error_on = error_on\n        self.error_type = error_type\n        self.error_regex = error_regex\n\n\nclass ModuleInfo:\n    \"\"\" Module information to be used in testing. \"\"\"\n\n    def __init__(self,\n                 module_cls,  # Class object for the module under test\n                 *,\n                 module_inputs_func,  # Function to generate module inputs\n                 skips=(),  # Indicates which tests to skip\n                 decorators=None,  # Additional decorators to apply to generated tests\n                 dtypes=floating_types(),  # dtypes this function is expected to work with\n                 supports_gradgrad=True,  # whether the op supports second order gradients\n                 gradcheck_nondet_tol=0.0,  # tolerance for nondeterminism while performing gradcheck\n                 module_memformat_affects_out=False,  # whether converting module to channels last will generate\n                                                      # channels last output\n                 train_and_eval_differ=False,  # whether the module has differing behavior between train and eval\n                 module_error_inputs_func=None,  # Function to generate module inputs that error\n                 ):\n        self.module_cls = module_cls\n        self.module_inputs_func = module_inputs_func\n        self.decorators = (*(decorators if decorators else []), *(skips if skips else []))\n        self.dtypes = dtypes\n        self.supports_gradgrad = supports_gradgrad\n        self.gradcheck_nondet_tol = gradcheck_nondet_tol\n        self.module_memformat_affects_out = module_memformat_affects_out\n        self.train_and_eval_differ = train_and_eval_differ\n        self.module_error_inputs_func = module_error_inputs_func\n\n    def get_decorators(self, test_class, test_name, device, dtype, param_kwargs):\n        result = [set_single_threaded_if_parallel_tbb]\n        for decorator in self.decorators:\n            if isinstance(decorator, DecorateInfo):\n                if decorator.is_active(test_class, test_name, device, dtype, param_kwargs):\n                    result.extend(decorator.decorators)\n            else:\n                result.append(decorator)\n        return result\n\n    @property\n    def name(self):\n        return get_module_common_name(self.module_cls)\n\n    @property\n    def formatted_name(self):\n        return self.name.replace('.', '_')\n\n# Start of module inputs functions.\n\ndef module_inputs_torch_nn_Linear(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    module_inputs = [\n        ModuleInput(constructor_input=FunctionInput(10, 8),\n                    forward_input=FunctionInput(input=make_input((4, 10))),\n                    reference_fn=lambda m, p, input: torch.mm(input, p[0].t()) + p[1].view(1, -1).expand(4, 8)),\n        ModuleInput(constructor_input=FunctionInput(10, 8, bias=False),\n                    forward_input=FunctionInput(make_input((4, 10))),\n                    desc='no_bias',\n                    reference_fn=lambda m, p, i: torch.mm(i, p[0].t())),\n        ModuleInput(constructor_input=FunctionInput(3, 5),\n                    forward_input=FunctionInput(make_input(3)),\n                    desc='no_batch_dim',\n                    reference_fn=lambda m, p, i: torch.mm(i.view(1, -1), p[0].t()).view(-1) + p[1])\n    ]\n\n    return module_inputs\n\n\ndef module_inputs_torch_nn_Bilinear(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    def bilinear_reference_fn(m, p, x1, x2, bias=True):\n        result = torch.einsum('bn,anm,bm->ba', x1, p[0], x2)\n        if bias:\n            if x1.shape[0] == 1:\n                result = result.view(-1) + p[1]\n            else:\n                result = result + p[1].view(1, -1).expand(x1.shape[0], p[0].shape[0])\n        return result\n\n    module_inputs = [\n        ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n                    forward_input=FunctionInput(make_input((8, 2)), make_input((8, 3))),\n                    reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2)),\n        ModuleInput(constructor_input=FunctionInput(2, 3, 4, bias=False),\n                    forward_input=FunctionInput(make_input((8, 2)), make_input((8, 3))),\n                    desc='no_bias',\n                    reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1, x2, bias=False)),\n        ModuleInput(constructor_input=FunctionInput(2, 3, 4),\n                    forward_input=FunctionInput(make_input(2), make_input(3)),\n                    desc='no_batch_dim',\n                    reference_fn=lambda m, p, x1, x2: bilinear_reference_fn(m, p, x1.view(1, -1), x2.view(1, -1))),\n    ]\n\n    return module_inputs\n\n\ndef module_inputs_torch_nn_NLLLoss(module_info, device, dtype, requires_grad, training, **kwargs):\n    def make_input(shape, device=device, dtype=dtype, requires_grad=requires_grad):\n        return make_tensor(shape, device=device, dtype=dtype,\n                           requires_grad=False).log_softmax(dim=1).requires_grad_(requires_grad)\n    make_weight = partial(make_tensor, device=device, dtype=dtype, requires_grad=False)\n\n    cases: List[Tuple[str, dict]] = [\n        ('', {}),\n        ('reduction_sum', {'reduction': 'sum'}),\n        ('reduction_none', {'reduction': 'none'}),\n        ('ignore_index', {'ignore_index': 2}),\n        ('weights', {'weight': make_weight(10).abs()}),\n        ('weights_ignore_index', {'weight': make_weight(10).abs(), 'ignore_index': 2}),\n        ('weights_ignore_index_neg', {'weight': make_weight(10).abs(), 'ignore_index': -1})\n    ]\n\n    # TODO: Uncomment when negative weights is supported.\n    # negative_weight = make_weight(10)\n    # negative_weight[0] = -1\n    # cases.append(('weights_negative', {'weight': negative_weight}))\n    module_inputs = []\n    for desc, constructor_kwargs in cases:\n\n        def reference_fn(m, p, i, t, constructor_kwargs=constructor_kwargs):\n            return nllloss_reference(i, t, **constructor_kwargs)\n\n        module_inputs.append(\n            ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n                        forward_input=FunctionInput(make_input((15, 10)),\n                                                    torch.empty(15, device=device).uniform_().mul(10).floor().long()),\n                        desc=desc,\n                        reference_fn=reference_fn)\n        )\n\n    return module_inputs\n\n\ndef module_inputs_torch_nn_GaussianNLLLoss(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    make_target = partial(make_tensor, device=device, dtype=dtype, requires_grad=False)\n\n    cases: List[Tuple[str, dict]] = [\n        ('', {}),\n        ('reduction_sum', {'reduction': 'sum'}),\n        ('reduction_mean', {'reduction': 'mean'}),\n        ('reduction_none', {'reduction': 'none'}),\n    ]\n\n    module_inputs = []\n    for desc, constructor_kwargs in cases:\n        module_inputs.append(\n            ModuleInput(constructor_input=FunctionInput(**constructor_kwargs),\n                        forward_input=FunctionInput(make_input(3),\n                                                    make_target(3),\n                                                    make_input(1).abs()),\n                        desc=desc,\n                        reference_fn=no_batch_dim_reference_fn)\n        )\n\n    return module_inputs\n\n\ndef no_batch_dim_reference_fn(m, p, *args, **kwargs):\n    \"\"\"Reference function for modules supporting no batch dimensions.\n\n    Unbatched inputs are unsqueezed to form a\n    single batch input before passing them to the module.\n    The output is squeezed to compare with the\n    output of unbatched input to the module.\n\n    Currently it only supports modules which return a single Tensor as output.\n    You can bind the following kwargs.\n    Kwargs:\n        batch_first[bool] : If True, all the Tensors in `args` while be unsqueezed at dim `0` .\n                        and output will be squeezed at dim `0` else dim `1` for both.\n        kwargs_to_batchify[dict] : Dictionary specifying the name of the argument and dimension to unsqueeze.\n                               Useful if there are few arguments whose batch dimension are different\n                               from the ones selected by `batch_first`.\n        is_criterion[bool] : Specify if the module is a criterion and handle the reduction for output accordingly.\n    \"\"\"\n    def get_and_pop(key, default):\n        v = kwargs.get(key, default)\n        if key in kwargs:\n            kwargs.pop(key)\n        return v\n\n    batch_dim = 0 if get_and_pop('batch_first', True) else 1\n    kwargs_to_batchify = get_and_pop('kwargs_to_batchify', None)\n    is_criterion = get_and_pop('is_criterion', False)\n\n    if kwargs_to_batchify is not None:\n        assert isinstance(kwargs_to_batchify, dict)\n        for k, v in kwargs.items():\n            if k in kwargs_to_batchify and v is not None:\n                bdim = kwargs_to_batchify[k]\n                kwargs[k] = v.unsqueeze(bdim)\n\n    single_batch_input_args = [input.unsqueeze(batch_dim) for input in args]\n    with freeze_rng_state():\n        output = m(*single_batch_input_args, **kwargs).squeeze(batch_dim)\n\n    if is_criterion:\n        reduction = get_reduction(m)\n        if reduction == 'none':\n            return output.squeeze(0)\n    return output\n\n\ndef no_batch_dim_reference_mha(m, p, *args, **kwargs):\n    \"\"\"Reference function for MultiheadAttention supporting no batch dimensions.\n\n    Unbatched inputs are unsqueezed to form a\n    single batch input before passing them to the module.\n    The output is squeezed to compare with the\n    output of unbatched input to the module.\n    \"\"\"\n    batch_dim = 0 if kwargs.get('batch_first', True) else 1\n    if 'batch_first' in kwargs:\n        kwargs.pop('batch_first')\n    if 'key_padding_mask' in kwargs and kwargs['key_padding_mask'] is not None:\n        kwargs['key_padding_mask'] = kwargs['key_padding_mask'].unsqueeze(0)\n    single_batch_input_args = [input.unsqueeze(batch_dim) for input in args]\n    with freeze_rng_state():\n        output = m(*single_batch_input_args, **kwargs)\n        return (output[0].squeeze(batch_dim), output[1].squeeze(0))\n\n\ndef no_batch_dim_reference_rnn_gru(m, p, *args, **kwargs):\n    \"\"\"Reference function for RNN and GRU supporting no batch dimensions.\n\n    Unbatched inputs are unsqueezed to form a\n    single batch input before passing them to the module.\n    The output is squeezed to compare with the\n    output of unbatched input to the module.\n    \"\"\"\n    if len(args) == 1:\n        inp, = args\n        h = None\n    elif len(args) == 2:\n        inp, h = args\n        h = h.unsqueeze(1)\n\n    batch_dim = 0 if kwargs['batch_first'] else 1\n    kwargs.pop('batch_first')\n    inp = inp.unsqueeze(batch_dim)\n    single_batch_input_args = (inp, h)\n    with freeze_rng_state():\n        output = m(*single_batch_input_args, **kwargs)\n        return (output[0].squeeze(batch_dim), output[1].squeeze(1))\n\n\ndef no_batch_dim_reference_lstm(m, p, *args, **kwargs):\n    \"\"\"Reference function for LSTM supporting no batch dimensions.\n\n    Unbatched inputs are unsqueezed to form a\n    single batch input before passing them to the module.\n    The output is squeezed to compare with the\n    output of unbatched input to the module.\n    \"\"\"\n    if len(args) == 1:\n        inp, = args\n        h = None\n    elif len(args) == 2:\n        inp, h = args\n        h = (h[0].unsqueeze(1), h[1].unsqueeze(1))\n\n    batch_dim = 0 if kwargs['batch_first'] else 1\n    kwargs.pop('batch_first')\n    inp = inp.unsqueeze(batch_dim)\n    single_batch_input_args = (inp, h)\n    with freeze_rng_state():\n        output = m(*single_batch_input_args, **kwargs)\n        return (output[0].squeeze(batch_dim), (output[1][0].squeeze(1), output[1][1].squeeze(1)))\n\n\ndef no_batch_dim_reference_lstmcell(m, p, *args, **kwargs):\n    \"\"\"Reference function for LSTMCell supporting no batch dimensions.\n\n    The module is passed the input and target in batched form with a single item.\n    The output is squeezed to compare with the no-batch input.\n    \"\"\"\n    inp, (h, c) = args\n    single_batch_input_args = (inp.unsqueeze(0), (h.unsqueeze(0), c.unsqueeze(0)))\n    with freeze_rng_state():\n        output = m(*single_batch_input_args, **kwargs)\n        return (output[0].squeeze(0), output[1].squeeze(0))\n\n\ndef generate_regression_criterion_inputs(make_input):\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(reduction=reduction),\n            forward_input=FunctionInput(make_input((4, )), make_input(4,)),\n            reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True),\n            desc=f'no_batch_dim_{reduction}'\n        ) for reduction in ['none', 'mean', 'sum']]\n\n\ndef module_inputs_torch_nn_AvgPool1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(kernel_size=2),\n                    forward_input=FunctionInput(make_input((3, 6))),\n                    desc='no_batch_dim',\n                    reference_fn=no_batch_dim_reference_fn),\n        ModuleInput(constructor_input=FunctionInput(2),\n                    forward_input=FunctionInput(make_input((2, 3, 6)))),\n        ModuleInput(constructor_input=FunctionInput((2,), (2,)),\n                    forward_input=FunctionInput(make_input((2, 3, 6))),\n                    desc='stride'),\n        ModuleInput(constructor_input=FunctionInput(2, 2, 1),\n                    forward_input=FunctionInput(make_input((2, 3, 6))),\n                    desc='stride_pad')]\n\n\ndef module_inputs_torch_nn_AvgPool2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput((2, 2)),\n                    forward_input=FunctionInput(make_input((3, 6, 6))),\n                    desc='no_batch_dim',\n                    reference_fn=no_batch_dim_reference_fn),\n        ModuleInput(constructor_input=FunctionInput((2, 2)),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6)))),\n        ModuleInput(constructor_input=FunctionInput((2, 2), (2, 2)),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='stride'),\n        ModuleInput(constructor_input=FunctionInput((2, 2), (2, 2), (1, 1)),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='stride_pad'),\n        ModuleInput(constructor_input=FunctionInput((2, 2), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='divisor'),\n        ModuleInput(constructor_input=FunctionInput((2, 2), (2, 2), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='divisor_stride'),\n        ModuleInput(constructor_input=FunctionInput((2, 2), (2, 2), (1, 1), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='divisor_stride_pad')]\n\n\n\ndef module_inputs_torch_nn_AvgPool3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput((2, 2, 2)),\n                    forward_input=FunctionInput(make_input((3, 4, 4, 4))),\n                    desc='no_batch_dim',\n                    reference_fn=no_batch_dim_reference_fn),\n        ModuleInput(constructor_input=FunctionInput((2, 2, 2)),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4)))),\n        ModuleInput(constructor_input=FunctionInput(2, (2, 2, 2)),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n                    desc='stride'),\n        ModuleInput(constructor_input=FunctionInput(2, 2, (1, 1, 1)),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n                    desc='stride_pad'),\n        ModuleInput(constructor_input=FunctionInput(4, 2, (1, 2, 1)),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n                    desc='stride_pad_gpu_fixedkw_output'),\n        ModuleInput(constructor_input=FunctionInput((2, 4, 8), 1, (1, 1, 2)),\n                    forward_input=FunctionInput(make_input((2, 3, 2, 4, 8))),\n                    desc='stride_pad_gpu_general_output'),\n        ModuleInput(constructor_input=FunctionInput(3, 1, 0),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='stride1_pad0_gpu_input'),\n        ModuleInput(constructor_input=FunctionInput(2, 2, (1, 1, 1)),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='stride_pad_gpu_input_nooverlap'),\n        ModuleInput(constructor_input=FunctionInput((2, 2, 2), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='divisor'),\n        ModuleInput(constructor_input=FunctionInput(2, (2, 2, 2), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n                    desc='divisor_stride'),\n        ModuleInput(constructor_input=FunctionInput(2, 2, (1, 1, 1), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n                    desc='divisor_stride_pad'),\n        ModuleInput(constructor_input=FunctionInput(4, 2, (1, 2, 1), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n                    desc='divisor_stride_pad_gpu_fixedkw_output'),\n        ModuleInput(constructor_input=FunctionInput((2, 4, 8), 1, (1, 1, 2), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 2, 4, 8))),\n                    desc='divisor_stride_pad_gpu_general_output'),\n        ModuleInput(constructor_input=FunctionInput(3, 1, 0, divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='divisor_stride1_pad0_gpu_input'),\n        ModuleInput(constructor_input=FunctionInput(2, 2, (1, 1, 1), divisor_override=1),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='divisor_stride_pad_gpu_input_nooverlap')]\n\n\n\ndef module_inputs_torch_nn_AdaptiveAvgPool1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((1, 3, 5))),\n                    desc='single'),\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((3, 5))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(1,),\n                    forward_input=FunctionInput(make_input((1, 3, 5))),\n                    desc='one_output')]\n\n\ndef module_inputs_torch_nn_AdaptiveAvgPool2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((1, 3, 5, 6))),\n                    desc='single'),\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((3, 5, 6))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(1,),\n                    forward_input=FunctionInput(make_input((1, 3, 5, 6))),\n                    desc='single_1x1output'),\n        ModuleInput(constructor_input=FunctionInput((3, 4)),\n                    forward_input=FunctionInput(make_input((1, 3, 5, 6))),\n                    desc='tuple'),\n        ModuleInput(constructor_input=FunctionInput((3, None)),\n                    forward_input=FunctionInput(make_input((1, 3, 5, 6))),\n                    desc='tuple_none')]\n\ndef module_inputs_torch_nn_AdaptiveAvgPool3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 2, 7))),\n                    desc='single'),\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((3, 5, 2, 7))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput((3, 4, 5)),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 3, 7))),\n                    desc='tuple'),\n        ModuleInput(constructor_input=FunctionInput((None, 4, 5)),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 3, 7))),\n                    desc='tuple_none'),\n        ModuleInput(constructor_input=FunctionInput((3, 2, 2)),\n                    forward_input=FunctionInput(make_input((1, 1, 3, 2, 6))),\n                    desc='last_dim')]\n\n\ndef module_inputs_torch_nn_AdaptiveMaxPool1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((1, 3, 5))),\n                    desc='single'),\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((3, 5))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_AdaptiveMaxPool2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((1, 3, 5, 6))),\n                    desc='single'),\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((3, 5, 6))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput((3, 4)),\n                    forward_input=FunctionInput(make_input((1, 3, 5, 6))),\n                    desc='tuple'),\n        ModuleInput(constructor_input=FunctionInput((3, None)),\n                    forward_input=FunctionInput(make_input((1, 3, 5, 6))),\n                    desc='tuple_none')]\n\n\ndef module_inputs_torch_nn_AdaptiveMaxPool3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 6, 7))),\n                    desc='single'),\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((3, 5, 6, 7))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput((3, 4, 5)),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 6, 7))),\n                    desc='tuple'),\n        ModuleInput(constructor_input=FunctionInput((3, None, 5)),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 6, 7))),\n                    desc='tuple_none'),\n        ModuleInput(constructor_input=FunctionInput(3),\n                    forward_input=FunctionInput(make_input((2, 3, 12, 9, 3))),\n                    desc='single_nonatomic'),\n        ModuleInput(constructor_input=FunctionInput((3, 4, 5)),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 4, 10))),\n                    desc='tuple_nonatomic')]\n\n\ndef module_inputs_torch_nn_BatchNorm1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(10,),\n                    forward_input=FunctionInput(make_input((4, 10))),\n                    desc='affine'),\n        ModuleInput(constructor_input=FunctionInput(5,),\n                    forward_input=FunctionInput(make_input((4, 5, 3))),\n                    desc='3d_input'),\n        ModuleInput(constructor_input=FunctionInput(10, 1e-3, None),\n                    forward_input=FunctionInput(make_input((4, 10))),\n                    desc='affine_simple_average'),\n        ModuleInput(constructor_input=FunctionInput(10, 1e-3, 0.3, False),\n                    forward_input=FunctionInput(make_input((4, 10))),\n                    desc='not_affine'),\n        ModuleInput(constructor_input=FunctionInput(10, 1e-3, 0.3, True, False),\n                    forward_input=FunctionInput(make_input((4, 10))),\n                    desc='not_tracking_stats'),\n        ModuleInput(constructor_input=FunctionInput(5, 1e-3, 0.3, False),\n                    forward_input=FunctionInput(make_input((4, 5, 3))),\n                    desc='3d_input_not_affine'),\n        ModuleInput(constructor_input=FunctionInput(5, 1e-3, 0.3, False),\n                    forward_input=FunctionInput(make_input((0, 5, 9))),\n                    desc='zero_batch')]\n\n\ndef module_inputs_torch_nn_BatchNorm2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6)))),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, None),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='2d_simple_average'),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, 0.8),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='momentum'),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, 0.8, False),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='not_affine'),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, 0.8, True, False),\n                    forward_input=FunctionInput(make_input((2, 3, 6, 6))),\n                    desc='not_tracking_stats'),\n        ModuleInput(constructor_input=FunctionInput(5, 1e-3, 0.3, False),\n                    forward_input=FunctionInput(make_input((0, 5, 2, 2))),\n                    desc='zero_batch')]\n\n\ndef module_inputs_torch_nn_BatchNorm3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(3,),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4)))),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, None),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='3d_simple_average'),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, 0.7),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='momentum'),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, 0.7, False),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='not_affine'),\n        ModuleInput(constructor_input=FunctionInput(3, 1e-3, 0.7, True, False),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 4, 4))),\n                    desc='not_tracking_stats'),\n        ModuleInput(constructor_input=FunctionInput(5, 1e-3, 0.3, False),\n                    forward_input=FunctionInput(make_input((0, 5, 2, 2, 2))),\n                    desc='zero_batch')]\n\n\ndef module_inputs_torch_nn_ConvNd(module_info, device, dtype, requires_grad, training, **kwargs):\n    N = kwargs['N']\n    lazy = kwargs.get('lazy', False)\n    transposed = kwargs.get('transposed', False)\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    conv_kwargs_list = [{}] if transposed else [{}, {'padding': 'same'}]\n    kernel_size, C_in, C_out = 3, 4, 5\n    input_no_batch_shape = (C_in,) + tuple(i + 3 for i in range(N))\n    input_batch_shape = (2,) + input_no_batch_shape\n    return [\n        ModuleInput(constructor_input=(FunctionInput(C_out, kernel_size, **conv_kwargs) if lazy else\n                                       FunctionInput(C_in, C_out, kernel_size, **conv_kwargs)),\n                    forward_input=FunctionInput(make_input(\n                        input_batch_shape if with_batch else input_no_batch_shape)),\n                    desc=('' if with_batch else 'no_batch_dim'),\n                    reference_fn=(None if with_batch else no_batch_dim_reference_fn))\n        for with_batch, conv_kwargs in itertools.product([True, False], conv_kwargs_list)\n    ]\n\n\ndef module_inputs_torch_nn_ELU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(alpha=2.),\n                    forward_input=FunctionInput(make_input((3, 2, 5))),\n                    reference_fn=lambda m, p, i: torch.where(i >= 0, i, 2 * (i.exp() - 1))),\n        ModuleInput(constructor_input=FunctionInput(alpha=2.),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((3,))),\n                    desc='no_batch_dim',\n                    reference_fn=no_batch_dim_reference_fn),\n        ModuleInput(constructor_input=FunctionInput(alpha=2.),\n                    forward_input=FunctionInput(make_input((2, 3, 2, 5))),\n                    desc='4d_input')]\n\n\ndef module_inputs_torch_nn_CELU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(alpha=2.),\n                    forward_input=FunctionInput(make_input((3, 2, 5))),\n                    reference_fn=lambda m, p, i: torch.where(i >= 0, i, 2. * ((.5 * i).exp() - 1))),\n        ModuleInput(constructor_input=FunctionInput(alpha=2.),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=lambda m, p, i: torch.where(i >= 0, i, 2. * ((.5 * i).exp() - 1)),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(alpha=2.),\n                    forward_input=FunctionInput(make_input((3,))),\n                    desc='no_batch_dim',\n                    reference_fn=no_batch_dim_reference_fn)]\n\n\ndef module_inputs_torch_nn_GLU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((5, 6)))),\n        ModuleInput(constructor_input=FunctionInput(1),\n                    forward_input=FunctionInput(make_input((5, 6, 7))),\n                    desc='dim'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((4,))),\n                    desc='no_batch_dim',\n                    reference_fn=no_batch_dim_reference_fn)]\n\n\ndef module_inputs_torch_nn_GELU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput('none'),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=lambda m, p, x, *_: x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0))),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput('none'),\n                    forward_input=FunctionInput(make_input((3, 2, 5))),\n                    reference_fn=lambda m, p, x, *_: x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((3,))),\n                    desc='no_batch_dim',\n                    reference_fn=no_batch_dim_reference_fn)]\n\n\ndef module_inputs_torch_nn_ReLU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                    desc='channels_last_mem_format'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 3, 4, 5))),\n                    desc='channels_last_3d_mem_format')]\n\n\ndef module_inputs_torch_nn_ReLU6(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                    desc='channels_last_mem_format'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 3, 4, 5))),\n                    desc='channels_last_3d_mem_format')]\n\n\ndef module_inputs_torch_nn_LeakyReLU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((3, 2, 5)))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(0.5),\n                    forward_input=FunctionInput(make_input((3, 2, 5))),\n                    desc='with_negval'),\n        ModuleInput(constructor_input=FunctionInput(0.0),\n                    forward_input=FunctionInput(make_input((10, 10))),\n                    desc='with_zero_negval'),\n        ModuleInput(constructor_input=FunctionInput(0.5),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='with_negval_scalar')]\n\n\ndef module_inputs_torch_nn_PReLU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                    reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                    desc='1d'),\n        ModuleInput(constructor_input=FunctionInput(3),\n                    forward_input=FunctionInput(make_input((2, 3, 4))),\n                    reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                    desc='1d_multiparam'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                    reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                    desc='2d'),\n        ModuleInput(constructor_input=FunctionInput(3),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                    reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                    desc='2d_multiparam'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                    reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                    desc='3d'),\n        ModuleInput(constructor_input=FunctionInput(3),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5, 6))),\n                    reference_fn=lambda m, p, i: torch.clamp(i, min=0) + torch.clamp(i, max=0) * p[0][0],\n                    desc='3d_multiparam')]\n\n\ndef module_inputs_torch_nn_SELU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((3, 2, 5)))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar')]\n\n\ndef module_inputs_torch_nn_SiLU(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=lambda m, p, x, *_: x * torch.sigmoid(x),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((5, 6, 7))),\n                    reference_fn=lambda m, p, x, *_: x * torch.sigmoid(x))]\n\n\ndef module_inputs_torch_nn_Softmax(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(1),\n                    forward_input=FunctionInput(make_input((10, 20))),\n                    reference_fn=lambda m, p, i: torch.exp(i).div(torch.exp(i).sum(1, True).expand(10, 20))),\n        ModuleInput(constructor_input=FunctionInput(0),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=lambda m, p, i: torch.exp(i).div(torch.exp(i).sum(0, True)),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(-1),\n                    forward_input=FunctionInput(make_input((4, 5))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Softmax2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((1, 3, 10, 20))),\n                    reference_fn=lambda m, p, i: torch.exp(i).div(torch.exp(i).sum(1, False))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((3, 4, 5))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_LogSoftmax(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(1),\n                    forward_input=FunctionInput(make_input((10, 20))),\n                    reference_fn=lambda m, p, i: torch.exp(i).div_(torch.exp(i).sum(1, True).expand(10, 20)).log_()),\n        ModuleInput(constructor_input=FunctionInput(1),\n                    forward_input=FunctionInput(make_input((1, 3, 10, 20))),\n                    reference_fn=lambda m, p, i: torch.exp(i).div_(torch.exp(i).sum(1, False)).log_(),\n                    desc='multiparam'),\n        ModuleInput(constructor_input=FunctionInput(0),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=lambda m, p, i: torch.exp(i).div_(torch.exp(i).sum(0, False)).log_(),\n                    desc='multiparam_scalar'),\n        ModuleInput(constructor_input=FunctionInput(-1),\n                    forward_input=FunctionInput(make_input((4, 5))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Softmin(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(1),\n                    forward_input=FunctionInput(make_input((10, 20)))),\n        ModuleInput(constructor_input=FunctionInput(1),\n                    forward_input=FunctionInput(make_input((2, 3, 5, 10))),\n                    desc='multidim'),\n        ModuleInput(constructor_input=FunctionInput(0),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(-1),\n                    forward_input=FunctionInput(make_input((3, 4, 10))),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Softplus(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((10, 20))),\n                    reference_fn=lambda m, p, i: torch.log(1 + torch.exp(i))),\n        ModuleInput(constructor_input=FunctionInput(2),\n                    forward_input=FunctionInput(make_input((10, 20))),\n                    reference_fn=lambda m, p, i: 1. / 2. * torch.log(1 + torch.exp(2 * i)),\n                    desc='beta'),\n        ModuleInput(constructor_input=FunctionInput(2, -100),\n                    forward_input=FunctionInput(make_input((10, 20))),\n                    reference_fn=(\n                        lambda m, p, i: ((i * 2) > -100).type_as(i) * i\n                        + ((i * 2) <= -100).type_as(i) * 1. / 2. * torch.log(1 + torch.exp(2 * i))),\n                    desc='beta_threshold'),\n        ModuleInput(constructor_input=FunctionInput(2, -100),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=(\n                        lambda m, p, i: ((i * 2) > -100).type_as(i) * i\n                        + ((i * 2) <= -100).type_as(i) * 1. / 2. * torch.log(1 + torch.exp(2 * i))),\n                    desc='beta_threshold_scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Softshrink(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((3, 2, 5)))),\n        ModuleInput(constructor_input=FunctionInput(1,),\n                    forward_input=FunctionInput(make_input((3, 2, 5))),\n                    desc='lambda'),\n        ModuleInput(constructor_input=FunctionInput(1,),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='lambda_scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Softsign(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((3, 2, 5))),\n                    reference_fn=lambda m, p, i: i.div(1 + torch.abs(i))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=lambda m, p, i: i.div(1 + torch.abs(i)),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Tanh(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5)))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\n\ndef module_inputs_torch_nn_Tanhshrink(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5)))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Threshold(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(2., 1.),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                    desc='threshold_value'),\n        ModuleInput(constructor_input=FunctionInput(2., 10.),\n                    forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n                    desc='large_value'),\n        ModuleInput(constructor_input=FunctionInput(2., 1.),\n                    forward_input=FunctionInput(make_input(())),\n                    desc='threshold_value_scalar'),\n        ModuleInput(constructor_input=FunctionInput(2., 1.),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_Mish(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((5, 6, 7))),\n                    reference_fn=lambda m, p, i: i * torch.tanh(F.softplus(i))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(())),\n                    reference_fn=lambda m, p, i: i * torch.tanh(F.softplus(i)),\n                    desc='scalar'),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(4)),\n                    reference_fn=no_batch_dim_reference_fn,\n                    desc='no_batch_dim')]\n\n\ndef module_inputs_torch_nn_L1Loss(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input((2, 3, 4)),\n                                                make_input((2, 3, 4))),\n                    reference_fn=lambda m, p, i, t: 1. / i.numel() * sum((a - b).abs().sum()\n                                                                         for a, b in zip(i, t))),\n        ModuleInput(constructor_input=FunctionInput(),\n                    forward_input=FunctionInput(make_input(()), make_input(())),\n                    reference_fn=lambda m, p, i, t: 1. / i.numel() * (i - t).abs().sum(),\n                    desc='scalar')] + generate_regression_criterion_inputs(make_input)\n\n\ndef module_inputs_torch_nn_CrossEntropyLoss(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    make_target = partial(make_tensor, device=device, dtype=torch.long, requires_grad=False)\n    make_weight = partial(make_tensor, device=device, dtype=dtype, requires_grad=False)\n\n    reductions = ['sum', 'mean', 'none']\n    samples = []\n    # Samples below are for validating the no-batch-dim support.\n    for reduction in reductions:\n        samples.append(\n            ModuleInput(constructor_input=FunctionInput(reduction=reduction),\n                        forward_input=FunctionInput(make_input((9,)), make_target((), low=0, high=9)),\n                        reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True))\n        )\n        samples.append(\n            ModuleInput(constructor_input=FunctionInput(reduction=reduction, weight=make_weight((9,))),\n                        forward_input=FunctionInput(make_input((9,)), make_target((), low=0, high=9)),\n                        reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True))\n        )\n        samples.append(\n            ModuleInput(constructor_input=FunctionInput(reduction=reduction, label_smoothing=0.5),\n                        forward_input=FunctionInput(make_input((9,)), make_target((), low=0, high=9)),\n                        reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True))\n        )\n        samples.append(\n            ModuleInput(constructor_input=FunctionInput(reduction=reduction, label_smoothing=0.5,\n                                                        weight=make_weight((9,))),\n                        forward_input=FunctionInput(make_input((9,)), make_target((), low=0, high=9)),\n                        reference_fn=partial(no_batch_dim_reference_fn, is_criterion=True))\n        )\n\n    return samples\n\n\ndef module_inputs_torch_nn_GroupNorm(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(3, 6, 1e-3),\n            forward_input=FunctionInput(make_input((4, 6, 5))),\n            desc='1d_affine'),\n        ModuleInput(\n            constructor_input=FunctionInput(3, 12, 1e-3),\n            forward_input=FunctionInput(make_input((4, 12))),\n            desc='1d_affine_GN'),\n        ModuleInput(\n            constructor_input=FunctionInput(1, 6, 1e-3),\n            forward_input=FunctionInput(make_input((150, 6))),\n            desc='1d_affine_large_batch'),\n        ModuleInput(\n            constructor_input=FunctionInput(5, 5, 1e-3, False),\n            forward_input=FunctionInput(make_input((4, 5, 5))),\n            desc='1d_no_affine_IN'),\n        ModuleInput(\n            constructor_input=FunctionInput(1, 10, 1e-3, False),\n            forward_input=FunctionInput(make_input((4, 10))),\n            desc='1d_no_affine_LN'),\n        ModuleInput(\n            constructor_input=FunctionInput(3, 6, 1e-3),\n            forward_input=FunctionInput(make_input((4, 6, 2, 3))),\n            desc='2d_affine'),\n        ModuleInput(\n            constructor_input=FunctionInput(3, 6, 1e-3),\n            forward_input=FunctionInput(make_input((4, 6, 28, 28))),\n            desc='2d_affine_large_feature'),\n        ModuleInput(\n            constructor_input=FunctionInput(3, 51, 1e-5, False),\n            forward_input=FunctionInput(make_input((2, 51, 28, 28))),\n            desc='2d_no_affine_large_feature'),\n        ModuleInput(\n            constructor_input=FunctionInput(3, 3, 1e-3, False),\n            forward_input=FunctionInput(make_input((4, 3, 2, 3))),\n            desc='2d_no_affine_IN'),\n        ModuleInput(\n            constructor_input=FunctionInput(1, 3, 1e-3, False),\n            forward_input=FunctionInput(make_input((4, 3, 2, 3))),\n            desc='2d_no_affine_LN'),\n    ]\n\n\ndef module_inputs_torch_nn_Hardshrink(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(2.),\n            forward_input=FunctionInput(make_input((4, 3, 2, 4))),\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(2.),\n            forward_input=FunctionInput(make_input(())),\n            desc='scalar',\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(4)),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='no_batch_dim',\n        )\n    ]\n\n\ndef module_inputs_torch_nn_Hardswish(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(4)),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='no_batch_dim',\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input((2, 3, 2, 5))),\n            desc='4d_input')\n    ]\n\n\ndef module_inputs_torch_nn_Hardtanh(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input((3, 2, 5))),\n            reference_fn=lambda m, p, i: i.clamp(-1, 1),\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(())),\n            reference_fn=lambda m, p, i: i.clamp(-1, 1),\n            desc='scalar',\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(4)),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='no_batch_dim',\n        )\n    ]\n\n\ndef module_inputs_torch_nn_InstanceNormNd(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    lazy = kwargs.get('lazy', False)\n    N = kwargs['N']\n    num_features, eps, momentum, affine, track_running_stats = 3, 1e-3, 0.3, False, True\n    input_no_batch_shape_dict = {1: (3, 15), 2: (3, 6, 6), 3: (3, 4, 4, 4)}\n    input_no_batch_shape = input_no_batch_shape_dict[N]\n    input_batch_shape = (4,) + input_no_batch_shape\n\n    return [\n        ModuleInput(\n            constructor_input=(\n                FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n            ),\n            forward_input=FunctionInput(make_input(input_batch_shape))),\n        ModuleInput(\n            constructor_input=(\n                FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n            ),\n            forward_input=FunctionInput(make_input(input_batch_shape)),\n            desc='tracking_stats'),\n        ModuleInput(\n            constructor_input=(\n                FunctionInput(eps, momentum) if lazy else FunctionInput(num_features, eps, momentum)\n            ),\n            forward_input=FunctionInput(make_input(input_no_batch_shape)),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='tracking_stats_no_batch_dim'),\n        ModuleInput(\n            constructor_input=(\n                FunctionInput(eps, momentum, affine, track_running_stats) if lazy else\n                FunctionInput(num_features, eps, momentum, affine, track_running_stats)\n            ),\n            forward_input=FunctionInput(make_input(input_no_batch_shape)),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='no_batch_dim')\n    ]\n\ndef module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput([5], 1e-3),\n            forward_input=FunctionInput(make_input((4, 5, 5))),\n            desc='1d_elementwise_affine'),\n        ModuleInput(\n            constructor_input=FunctionInput([5], 1e-3),\n            forward_input=FunctionInput(make_input((128, 5, 5))),\n            desc='1d_elementwise_affine_large_batch'),\n        ModuleInput(\n            constructor_input=FunctionInput([5], 1e-3, False),\n            forward_input=FunctionInput(make_input((4, 5, 5))),\n            desc='1d_no_elementwise_affine'),\n        ModuleInput(\n            constructor_input=FunctionInput([2, 2, 5], 1e-3),\n            forward_input=FunctionInput(make_input((4, 2, 2, 5))),\n            desc='3d_elementwise_affine'),\n        ModuleInput(\n            constructor_input=FunctionInput([2, 2, 5], 1e-3, False),\n            forward_input=FunctionInput(make_input((4, 2, 2, 5))),\n            desc='3d_no_elementwise_affine'),\n        ModuleInput(\n            constructor_input=FunctionInput([5], 1e-3),\n            forward_input=FunctionInput(make_input((0, 5))),\n            desc='1d_empty_elementwise_affine'),\n    ]\n\n\ndef module_inputs_torch_nn_LocalResponseNorm(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(3,),\n            forward_input=FunctionInput(make_input((1, 5, 7))),\n            desc='1d'),\n        ModuleInput(\n            constructor_input=FunctionInput(2,),\n            forward_input=FunctionInput(make_input((1, 5, 7, 7))),\n            desc='2d_uneven_pad'),\n        ModuleInput(\n            constructor_input=FunctionInput(1, 1., 0.5, 2.),\n            forward_input=FunctionInput(make_input((1, 5, 7, 7, 7))),\n            desc='3d_custom_params'),\n    ]\n\n\ndef module_inputs_torch_nn_LPPool1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1.5, 2),\n            forward_input=FunctionInput(make_input((1, 3, 7))),\n            desc='norm'),\n        ModuleInput(\n            constructor_input=FunctionInput(2, 2, 3),\n            forward_input=FunctionInput(make_input((1, 3, 7)))),\n        ModuleInput(\n            constructor_input=FunctionInput(2, 2, 3),\n            forward_input=FunctionInput(make_input((3, 7))),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='no_batch_dim'),\n    ]\n\n\n\ndef module_inputs_torch_nn_LPPool2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(2, 2, 2),\n            forward_input=FunctionInput(make_input((1, 3, 7, 7)))),\n        ModuleInput(\n            constructor_input=FunctionInput(1.5, 2),\n            forward_input=FunctionInput(make_input((1, 3, 7, 7))),\n            desc='norm'),\n    ]\n\n\ndef module_inputs_torch_nn_MaxPool1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(4),\n            forward_input=FunctionInput(make_input((2, 10, 4))),\n            desc='3d_input'),\n        ModuleInput(\n            constructor_input=FunctionInput(4, 4),\n            forward_input=FunctionInput(make_input((2, 10, 4))),\n            desc='stride'),\n        ModuleInput(\n            constructor_input=FunctionInput(4, return_indices=True),\n            forward_input=FunctionInput(make_input((2, 10, 4))),\n            desc='return_indices'),\n    ]\n\n\ndef module_inputs_torch_nn_MaxPool2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n            forward_input=FunctionInput(make_input((3, 7, 7))),\n            desc='3d_input'),\n        ModuleInput(\n            constructor_input=FunctionInput((3, 3), (2, 2), (1, 1)),\n            forward_input=FunctionInput(make_input((1, 3, 7, 7))),\n            desc='4d_input'),\n        ModuleInput(\n            constructor_input=FunctionInput((3, 3), (2, 2), (1, 1), return_indices=True),\n            forward_input=FunctionInput(make_input((1, 3, 7, 7))),\n            desc='return_indices'),\n    ]\n\ndef module_inputs_torch_nn_MaxPool3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput((2, 2, 2)),\n            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5)))),\n        ModuleInput(\n            constructor_input=FunctionInput(2, (2, 2, 2)),\n            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n            desc='stride'),\n        ModuleInput(\n            constructor_input=FunctionInput(2, 2, (1, 1, 1)),\n            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n            desc='stride_padding'),\n        ModuleInput(\n            constructor_input=FunctionInput(2, 2, (1, 1, 1), return_indices=True),\n            forward_input=FunctionInput(make_input((2, 3, 5, 5, 5))),\n            desc='return_indices'),\n    ]\n\n\ndef module_inputs_torch_nn_FractionalMaxPool2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    def make_random_samples():\n        return torch.empty((1, 3, 2), dtype=torch.double, device=device).uniform_()\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n            desc='ratio'),\n        ModuleInput(\n            constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((1, 3, 7, 6))),\n            desc='size'),\n        ModuleInput(\n            constructor_input=FunctionInput(\n                2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n            ),\n            forward_input=FunctionInput(make_input((1, 3, 5, 7))),\n            desc='ratio_return_indices'),\n        ModuleInput(\n            constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((3, 5, 7))),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='ratio_no_batch_dim'),\n        ModuleInput(\n            constructor_input=FunctionInput((2, 3), output_size=(4, 3), _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((3, 7, 6))),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='size_no_batch_dim'),\n    ]\n\n\ndef module_inputs_torch_nn_FractionalMaxPool3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    def make_random_samples():\n        return torch.empty((2, 4, 3), dtype=torch.double, device=device).uniform_()\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n            desc='ratio'),\n        ModuleInput(\n            constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((2, 4, 7, 7, 7))),\n            desc='size'),\n        ModuleInput(\n            constructor_input=FunctionInput((4, 2, 3), output_size=(10, 3, 2), _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((2, 4, 16, 7, 5))),\n            desc='asymsize'),\n        ModuleInput(\n            constructor_input=FunctionInput(\n                2, output_ratio=0.5, _random_samples=make_random_samples(), return_indices=True\n            ),\n            forward_input=FunctionInput(make_input((2, 4, 5, 5, 5))),\n            desc='ratio_return_indices'),\n        ModuleInput(\n            constructor_input=FunctionInput(2, output_ratio=0.5, _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((4, 5, 5, 5))),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='ratio_no_batch_dim'),\n        ModuleInput(\n            constructor_input=FunctionInput((2, 2, 2), output_size=(4, 4, 4), _random_samples=make_random_samples()),\n            forward_input=FunctionInput(make_input((4, 7, 7, 7))),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='size_no_batch_dim'),\n    ]\n\n\ndef module_inputs_torch_nn_Sigmoid(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(())),\n            desc='scalar'\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(4)),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='no_batch_dim',\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n            desc='channels_last_mem_format'\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input((2, 3, 3, 4, 5))),\n            desc='channels_last_3d_mem_format'\n        )\n    ]\n\n\ndef module_inputs_torch_nn_LogSigmoid(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(())),\n            reference_fn=lambda m, p, i: i.sigmoid().log(),\n            desc='scalar'\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input((2, 3, 4))),\n            reference_fn=lambda m, p, i: i.sigmoid().log(),\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(),\n            forward_input=FunctionInput(make_input(4)),\n            reference_fn=no_batch_dim_reference_fn,\n            desc='no_batch_dim',\n        ),\n    ]\n\n\ndef module_inputs_torch_nn_TransformerEncoder(module_info, device, dtype, requires_grad, training, **kwargs):\n    # Reuse the TransformerEncoderLayer samples since the forward args are nearly the same.\n    samples = []\n    for layer_module_input in module_inputs_torch_nn_TransformerEncoderLayer(\n            None, device, dtype, requires_grad, training):\n        # Construct a TransformerEncoderLayer object to pass to TransformerEncoder.\n        l_args, l_kwargs = (layer_module_input.constructor_input.args,\n                            layer_module_input.constructor_input.kwargs)\n        l_kwargs['device'] = device\n        l_kwargs['dtype'] = dtype\n        encoder_layer = torch.nn.TransformerEncoderLayer(*l_args, **l_kwargs)\n        num_layers = 2\n        # Note: TransformerEncoderLayer takes a \"src_mask\" while\n        # TransformerEncoder takes a \"mask\"; rename kwarg appropriately.\n        forward_input = layer_module_input.forward_input\n        if 'src_mask' in forward_input.kwargs:\n            forward_input.kwargs['mask'] = forward_input.kwargs['src_mask']\n            del forward_input.kwargs['src_mask']\n        samples.append(ModuleInput(\n            constructor_input=FunctionInput(encoder_layer, num_layers),\n            forward_input=forward_input,\n            desc=layer_module_input.desc\n        ))\n    return samples\n\ndef module_inputs_torch_nn_TransformerEncoderLayer(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    samples = [\n        ModuleInput(\n            constructor_input=FunctionInput(4, 2, 16, 0.0),\n            forward_input=FunctionInput(\n                make_input((2, 3, 4))\n            ),\n            desc='relu_activation'\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(4, 2, 8, 0.0, F.gelu),\n            forward_input=FunctionInput(\n                make_input((2, 3, 4))\n            ),\n            desc='gelu_activation'\n        ), ]\n\n    # Samples below are for validating the no-batch-dim support.\n    key_padding_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool))\n    attn_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool).expand((3, 3)))\n    for src_mask, src_key_padding_mask, norm_first in itertools.product(attn_masks, key_padding_masks, (True, False)):\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(d_model=4, nhead=2, dim_feedforward=8,\n                                                dropout=0.0, batch_first=True, norm_first=norm_first),\n                forward_input=FunctionInput(\n                    make_input((3, 4)), src_mask=src_mask, src_key_padding_mask=src_key_padding_mask\n                ),\n                reference_fn=partial(no_batch_dim_reference_fn,\n                                     batch_first=True, kwargs_to_batchify={'src_key_padding_mask': 0}),\n                desc='no_batch_dim_batch_first'\n            ))\n\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(4, 2, 8, dropout=0.0, batch_first=False, norm_first=norm_first),\n                forward_input=FunctionInput(\n                    make_input((3, 4)), src_mask=src_mask, src_key_padding_mask=src_key_padding_mask\n                ),\n                reference_fn=partial(no_batch_dim_reference_fn,\n                                     batch_first=False, kwargs_to_batchify={'src_key_padding_mask': 0}),\n                desc='no_batch_dim'\n            ))\n\n    def fast_path_reference_fn(module, parameters, *args, **kwargs):\n        assert not module.training\n        module = module.train(True)\n        output = module(*args, **kwargs)\n        module = module.train(False)\n        return output\n\n    if not training:\n        for norm_first in (True, False):\n            samples.append(\n                ModuleInput(\n                    constructor_input=FunctionInput(4, 2, 8, dropout=0.0, batch_first=True, norm_first=norm_first),\n                    forward_input=FunctionInput(\n                        make_input((2, 3, 4)),\n                    ),\n                    reference_fn=fast_path_reference_fn,\n                    desc=\"fast_path_norm_first\" if norm_first else \"fast_path\"\n                )\n            )\n\n    return samples\n\n\ndef module_inputs_torch_nn_TransformerDecoderLayer(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    samples = [\n        ModuleInput(\n            constructor_input=FunctionInput(4, 2, 16, 0.0),\n            forward_input=FunctionInput(\n                make_input((2, 3, 4)), make_input((2, 3, 4))\n            ),\n            desc='relu_activation'\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(4, 2, 8, 0.0, F.gelu),\n            forward_input=FunctionInput(\n                make_input((2, 3, 4)), make_input((2, 3, 4))\n            ),\n            desc='gelu_activation'\n        ), ]\n\n    # Samples below are for validating the no-batch-dim support.\n    key_padding_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool))\n    attn_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool).expand((3, 3)))\n    for tgt_mask, tgt_key_padding_mask, norm_first in itertools.product(attn_masks, key_padding_masks, (True, False)):\n        # Using same mask for tgt and memory\n        memory_mask = tgt_mask\n        memory_key_padding_mask = tgt_key_padding_mask\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(d_model=4, nhead=2, dim_feedforward=8,\n                                                dropout=0.0, batch_first=True, norm_first=norm_first),\n                forward_input=FunctionInput(\n                    make_input((3, 4)), make_input((3, 4)), tgt_mask=tgt_mask, memory_mask=memory_mask,\n                    tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n                ),\n                reference_fn=partial(no_batch_dim_reference_fn,\n                                     batch_first=True,\n                                     kwargs_to_batchify={'tgt_key_padding_mask': 0, 'memory_key_padding_mask': 0}),\n                desc='no_batch_dim_batch_first'\n            ))\n\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(4, 2, 8, dropout=0.0, batch_first=False, norm_first=norm_first),\n                forward_input=FunctionInput(\n                    make_input((3, 4)), make_input((3, 4)), tgt_mask=tgt_mask, memory_mask=memory_mask,\n                    tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n                ),\n                reference_fn=partial(no_batch_dim_reference_fn,\n                                     batch_first=False,\n                                     kwargs_to_batchify={'tgt_key_padding_mask': 0, 'memory_key_padding_mask': 0}),\n                desc='no_batch_dim'\n            ))\n\n    return samples\n\n\ndef module_inputs_torch_nn_Transformer(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    samples = []\n    # Samples below are for validating the no-batch-dim support.\n    key_padding_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool))\n    attn_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool).expand((3, 3)))\n    for mask, key_padding_mask, norm_first in itertools.product(attn_masks, key_padding_masks, (True, False)):\n        # Using same mask for tgt and memory\n        src_mask , tgt_mask = (mask,) * 2\n        src_key_padding_mask, tgt_key_padding_mask = (key_padding_mask,) * 2\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(d_model=4, nhead=2, dim_feedforward=8,\n                                                num_encoder_layers=1, num_decoder_layers=1,\n                                                dropout=0.0, batch_first=True, norm_first=norm_first),\n                forward_input=FunctionInput(\n                    make_input((3, 4)), make_input((3, 4)), tgt_mask=tgt_mask, src_mask=src_mask,\n                    tgt_key_padding_mask=tgt_key_padding_mask, src_key_padding_mask=src_key_padding_mask\n                ),\n                reference_fn=partial(no_batch_dim_reference_fn,\n                                     batch_first=True,\n                                     kwargs_to_batchify={'tgt_key_padding_mask': 0, 'src_key_padding_mask': 0}),\n                desc='no_batch_dim_batch_first'\n            ))\n\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(d_model=4, nhead=2, dim_feedforward=8,\n                                                num_encoder_layers=1, num_decoder_layers=1,\n                                                dropout=0.0, batch_first=False, norm_first=norm_first),\n                forward_input=FunctionInput(\n                    make_input((3, 4)), make_input((3, 4)), tgt_mask=tgt_mask, src_mask=src_mask,\n                    tgt_key_padding_mask=tgt_key_padding_mask, src_key_padding_mask=src_key_padding_mask\n                ),\n                reference_fn=partial(no_batch_dim_reference_fn,\n                                     batch_first=False,\n                                     kwargs_to_batchify={'tgt_key_padding_mask': 0, 'src_key_padding_mask': 0}),\n                desc='no_batch_dim'\n            ))\n\n    return samples\n\n\ndef module_inputs_torch_nn_Embedding(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_empty = partial(torch.empty, device=device, dtype=torch.long, requires_grad=False)\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(num_embeddings=4, embedding_dim=3),\n            forward_input=FunctionInput(make_empty(2, 3).random_(4))\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(num_embeddings=4, embedding_dim=3),\n            forward_input=FunctionInput(make_empty(1, 512).random_(4).expand(7, 512)),\n            desc='discontiguous'\n        ),\n    ]\n\n\ndef module_inputs_torch_nn_MultiheadAttention(module_info, device, dtype, requires_grad, training, **kwargs):\n    # Currently all samples below are for validating the no-batch-dim support.\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    samples = []\n    bool_vals = (True, False)\n    key_padding_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool))\n    attn_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool).expand((3, 3, 3)))\n    products = itertools.product(bool_vals, bool_vals, bool_vals, key_padding_masks, attn_masks)\n    for bias, add_bias_kv, add_zero_attn, key_padding_mask, attn_mask in products:\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(embed_dim=3, num_heads=3, batch_first=True,\n                                                bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn),\n                forward_input=FunctionInput(make_input((3, 3)), make_input((3, 3)), make_input((3, 3)),\n                                            key_padding_mask=key_padding_mask, attn_mask=attn_mask),\n                reference_fn=no_batch_dim_reference_mha,\n            )\n        )\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(embed_dim=3, num_heads=3, batch_first=False,\n                                                bias=bias, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn),\n                forward_input=FunctionInput(make_input((3, 3)), make_input((3, 3)), make_input((3, 3)),\n                                            key_padding_mask=key_padding_mask, attn_mask=attn_mask),\n                reference_fn=partial(no_batch_dim_reference_mha, batch_first=False),\n            )\n        )\n\n    return samples\n\n\ndef module_inputs_torch_nn_RNN_GRU_Cell(module_info, device, dtype, requires_grad, training, **kwargs):\n    # Currently all samples below are for validating the no-batch-dim support.\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    samples = [\n        ModuleInput(\n            constructor_input=FunctionInput(5, 10),\n            forward_input=FunctionInput(make_input(5), make_input(10)),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(5, 10, bias=True),\n            forward_input=FunctionInput(make_input(5), make_input(10)),\n            reference_fn=no_batch_dim_reference_fn,\n        )\n    ]\n\n    is_rnn = kwargs.get('is_rnn', False)\n    if is_rnn:\n        # RNN also supports `nonlinearity` argument.\n        # `tanh` is the default, so we check with `relu`\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(5, 10, bias=True, nonlinearity='relu'),\n                forward_input=FunctionInput(make_input(5), make_input(10)),\n                reference_fn=no_batch_dim_reference_fn,\n            )\n        )\n\n    return samples\n\n\ndef module_inputs_torch_nn_LSTMCell(module_info, device, dtype, requires_grad, training, **kwargs):\n    # Currently all samples below are for validating the no-batch-dim support.\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    samples = (\n        ModuleInput(\n            constructor_input=FunctionInput(5, 10),\n            forward_input=FunctionInput(make_input(5), (make_input(10), make_input(10))),\n            reference_fn=no_batch_dim_reference_lstmcell,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput(5, 10, bias=True),\n            forward_input=FunctionInput(make_input(5), (make_input(10), make_input(10))),\n            reference_fn=no_batch_dim_reference_lstmcell,\n        ),\n    )\n\n    return samples\n\ndef make_packed_sequence(inp, batch_sizes):\n    required_grad = inp.requires_grad\n    inp.requires_grad_(False)  # user won't have access to inp so won't be able to get its grads\n    seq = pack_padded_sequence(inp, batch_sizes)\n    seq.data.requires_grad_(required_grad)\n    return seq\n\n\ndef module_inputs_torch_nn_RNN_GRU(module_info, device, dtype, requires_grad, training, with_packed_sequence=False, **kwargs):\n    # Currently all samples below are for validating the no-batch-dim support.\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    is_rnn = kwargs['is_rnn']\n    nonlinearity = ('relu', 'tanh')\n    bias = (False, True)\n    batch_first = (False, True)\n    bidirectional = (False, True)\n\n    samples = []\n    if is_rnn:\n        prod_gen = product(nonlinearity, bias, batch_first, bidirectional)\n    else:\n        prod_gen = product(bias, batch_first, bidirectional)\n\n    for args in prod_gen:\n        if is_rnn:\n            nl, b, b_f, bidir = args\n        else:\n            b, b_f, bidir = args\n\n        cons_args = {'input_size': 2, 'hidden_size': 2, 'num_layers': 2,\n                     'batch_first': b_f, 'bias': b, 'bidirectional': bidir}\n        cons_args_hidden = {'input_size': 2, 'hidden_size': 3, 'num_layers': 2,\n                            'batch_first': b_f, 'bias': b, 'bidirectional': bidir}\n\n        if is_rnn:\n            cons_args['nonlinearity'] = nl\n            cons_args_hidden['nonlinearity'] = nl\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(**cons_args),\n                forward_input=FunctionInput(make_input((3, 2))),\n                reference_fn=partial(no_batch_dim_reference_rnn_gru, batch_first=b_f),\n            )\n        )\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(**cons_args_hidden),\n                forward_input=FunctionInput(make_input((3, 2)), make_input((4 if bidir else 2, 3))),\n                reference_fn=partial(no_batch_dim_reference_rnn_gru, batch_first=b_f),\n            )\n        )\n        if with_packed_sequence:\n            samples.append(\n                ModuleInput(\n                    constructor_input=FunctionInput(**cons_args),\n                    forward_input=FunctionInput(make_packed_sequence(make_input((5, 2, 2)), torch.tensor([5, 3]))),\n                    reference_fn=partial(no_batch_dim_reference_rnn_gru, batch_first=b_f),\n                )\n            )\n            samples.append(\n                ModuleInput(\n                    constructor_input=FunctionInput(**cons_args),\n                    forward_input=FunctionInput(make_packed_sequence(make_input((5, 5, 2)), torch.tensor([5, 3, 3, 2, 2]))),\n                    reference_fn=partial(no_batch_dim_reference_rnn_gru, batch_first=b_f),\n                )\n            )\n\n    return samples\n\n\ndef module_inputs_torch_nn_LSTM(module_info, device, dtype, requires_grad, training, **kwargs):\n    # Currently all samples below are for validating the no-batch-dim support.\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    bias = (False, True)\n    batch_first = (False, True)\n    bidirectional = (False, True)\n    proj_sizes = (0, 2)\n\n    samples = []\n    prod_gen = product(bias, batch_first, bidirectional, proj_sizes)\n\n    for args in prod_gen:\n        b, b_f, bidir, proj_size = args\n        hidden_size = 3\n        cons_args = {'input_size': 2, 'hidden_size': hidden_size, 'num_layers': 2, 'proj_size': proj_size,\n                     'batch_first': b_f, 'bias': b, 'bidirectional': bidir}\n        cons_args_hidden = {'input_size': 2, 'hidden_size': hidden_size, 'num_layers': 2, 'proj_size': proj_size,\n                            'batch_first': b_f, 'bias': b, 'bidirectional': bidir}\n\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(**cons_args),\n                forward_input=FunctionInput(make_input((2, 2))),\n                reference_fn=partial(no_batch_dim_reference_lstm, batch_first=b_f),\n            )\n        )\n\n        h_out = proj_size if proj_size > 0 else hidden_size\n        hx = (make_input((4 if bidir else 2, h_out)), make_input((4 if bidir else 2, hidden_size)))\n        samples.append(\n            ModuleInput(\n                constructor_input=FunctionInput(**cons_args_hidden),\n                forward_input=FunctionInput(make_input((3, 2)), hx),\n                reference_fn=partial(no_batch_dim_reference_lstm, batch_first=b_f),\n            )\n        )\n\n\n    return samples\n\n\n\ndef module_inputs_torch_nn_ReflectionPad1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((2, 3))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2)),\n            forward_input=FunctionInput(make_input((2, 3, 4))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ReflectionPad2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4, 5))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 3, 4)),\n            forward_input=FunctionInput(make_input((3, 4, 5, 6))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ReflectionPad3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n            reference_fn=no_batch_dim_reference_fn\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 1, 2, 1, 2)),\n            forward_input=FunctionInput(make_input((3, 3, 3, 3, 3))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ReplicationPad1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4))),\n            reference_fn=no_batch_dim_reference_fn\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2)),\n            forward_input=FunctionInput(make_input((3, 4, 5))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ReplicationPad2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4, 5))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 3, 4)),\n            forward_input=FunctionInput(make_input((3, 4, 5, 6))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ReplicationPad3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4, 5, 6))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 3, 4, 5, 6)),\n            forward_input=FunctionInput(make_input((3, 4, 5, 6, 7))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ZeroPad1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2)),\n            forward_input=FunctionInput(make_input((3, 4, 5))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ZeroPad2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((1, 2, 3))),\n            reference_fn=no_batch_dim_reference_fn\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 3, 4)),\n            forward_input=FunctionInput(make_input((1, 2, 3, 4))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ZeroPad3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4, 5, 6))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 3, 4, 5, 6)),\n            forward_input=FunctionInput(make_input((1, 2, 3, 4, 5))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ConstantPad1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1, 2),\n            forward_input=FunctionInput(make_input((3, 4))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2), 3),\n            forward_input=FunctionInput(make_input((3, 4, 5))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ConstantPad2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1, 3),\n            forward_input=FunctionInput(make_input((3, 4, 5))),\n            reference_fn=no_batch_dim_reference_fn\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 3, 4), 5),\n            forward_input=FunctionInput(make_input((1, 2, 3, 4))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_ConstantPad3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1, 3),\n            forward_input=FunctionInput(make_input((3, 4, 5, 6))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 3, 4, 5, 6), 7),\n            forward_input=FunctionInput(make_input((1, 2, 1, 2, 1))),\n        ),\n    ]\n\ndef module_inputs_torch_nn_CircularPad1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    def padding1d_circular_ref(inp, pad):\n        r\"\"\" input:\n                [[[0., 1., 2.],\n                  [3., 4., 5.]]]\n                pad: (1, 2)\n                output:\n                    [[[2., 0., 1., 2., 0., 1.],\n                      [5., 3., 4., 5., 3., 4.]]]\n            \"\"\"\n        return torch.cat([inp[:, :, -pad[0]:], inp, inp[:, :, :pad[1]]], dim=2)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4))),\n            reference_fn=no_batch_dim_reference_fn\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2)),\n            forward_input=FunctionInput(make_input((1, 2, 3))),\n            reference_fn=lambda m, p, i: padding1d_circular_ref(i, m.padding),\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((3, 1)),\n            forward_input=FunctionInput(make_input((1, 2, 3))),\n            reference_fn=lambda m, p, i: padding1d_circular_ref(i, m.padding),\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((3, 3)),\n            forward_input=FunctionInput(make_input((1, 2, 3))),\n            reference_fn=lambda m, p, i: padding1d_circular_ref(i, m.padding),\n        ),\n    ]\n\ndef module_inputs_torch_nn_CircularPad2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    def padding2d_circular_ref(inp, pad):\n        r\"\"\"input:\n                [[[[0., 1., 2],\n                   [3., 4., 5.]]]]\n                pad: (1, 2, 2, 1)\n        output:\n            [[[[2., 0., 1., 2., 0., 1.],\n               [5., 3., 4., 5., 3., 4.],\n               [2., 0., 1., 2., 0., 1.],\n               [5., 3., 4., 5., 3., 4.],\n               [2., 0., 1., 2., 0., 1.]]]]\n        \"\"\"\n        inp = torch.cat([inp[:, :, -pad[2]:], inp, inp[:, :, :pad[3]]], dim=2)\n        return torch.cat([inp[:, :, :, -pad[0]:], inp, inp[:, :, :, :pad[1]]], dim=3)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4, 5))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 2, 1)),\n            forward_input=FunctionInput(make_input((1, 1, 2, 3))),\n            reference_fn=lambda m, p, i: padding2d_circular_ref(i, m.padding),\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((2, 3, 2, 2)),\n            forward_input=FunctionInput(make_input((1, 1, 2, 3))),\n            reference_fn=lambda m, p, i: padding2d_circular_ref(i, m.padding),\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((3, 3, 3, 1)),\n            forward_input=FunctionInput(make_input((1, 1, 3, 3))),\n            reference_fn=lambda m, p, i: padding2d_circular_ref(i, m.padding),\n        ),\n    ]\n\ndef module_inputs_torch_nn_CircularPad3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n\n    def padding3d_circular_ref(inp, pad):\n        r\"\"\"input:\n                [[[[[ 0.,  1.,  2.],\n                    [ 3.,  4.,  5.]],\n                   [[ 6.,  7.,  8.],\n                    [ 9., 10., 11.]]]]]\n            pad: (1, 2, 2, 1, 1, 2)\n            output: [[[[[ 8.,  6.,  7.,  8.,  6.,  7.],\n                        [11.,  9., 10., 11.,  9., 10.],\n                        [ 8.,  6.,  7.,  8.,  6.,  7.],\n                        [11.,  9., 10., 11.,  9., 10.],\n                        [ 8.,  6.,  7.,  8.,  6.,  7.]],\n\n                       [[ 2.,  0.,  1.,  2.,  0.,  1.],\n                        [ 5.,  3.,  4.,  5.,  3.,  4.],\n                        [ 2.,  0.,  1.,  2.,  0.,  1.],\n                        [ 5.,  3.,  4.,  5.,  3.,  4.],\n                        [ 2.,  0.,  1.,  2.,  0.,  1.]],\n\n                       [[ 8.,  6.,  7.,  8.,  6.,  7.],\n                        [11.,  9., 10., 11.,  9., 10.],\n                        [ 8.,  6.,  7.,  8.,  6.,  7.],\n                        [11.,  9., 10., 11.,  9., 10.],\n                        [ 8.,  6.,  7.,  8.,  6.,  7.]],\n\n                       [[ 2.,  0.,  1.,  2.,  0.,  1.],\n                        [ 5.,  3.,  4.,  5.,  3.,  4.],\n                        [ 2.,  0.,  1.,  2.,  0.,  1.],\n                        [ 5.,  3.,  4.,  5.,  3.,  4.],\n                        [ 2.,  0.,  1.,  2.,  0.,  1.]],\n\n                       [[ 8.,  6.,  7.,  8.,  6.,  7.],\n                        [11.,  9., 10., 11.,  9., 10.],\n                        [ 8.,  6.,  7.,  8.,  6.,  7.],\n                        [11.,  9., 10., 11.,  9., 10.],\n                        [ 8.,  6.,  7.,  8.,  6.,  7.]]]]]\n        \"\"\"\n        inp = torch.cat([inp[:, :, -pad[4]:], inp, inp[:, :, :pad[5]]], dim=2)\n        inp = torch.cat([inp[:, :, :, -pad[2]:], inp, inp[:, :, :, :pad[3]]], dim=3)\n        return torch.cat([inp[:, :, :, :, -pad[0]:], inp, inp[:, :, :, :, :pad[1]]], dim=4)\n\n    return [\n        ModuleInput(\n            constructor_input=FunctionInput(1),\n            forward_input=FunctionInput(make_input((3, 4, 5, 6))),\n            reference_fn=no_batch_dim_reference_fn,\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((1, 2, 1, 2, 1, 2)),\n            forward_input=FunctionInput(make_input((1, 1, 2, 2, 3))),\n            reference_fn=lambda m, p, i: padding3d_circular_ref(i, m.padding)\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((3, 2, 2, 1, 1, 2)),\n            forward_input=FunctionInput(make_input((1, 1, 2, 2, 3))),\n            reference_fn=lambda m, p, i: padding3d_circular_ref(i, m.padding)\n        ),\n        ModuleInput(\n            constructor_input=FunctionInput((3, 3, 2, 1, 2, 2)),\n            forward_input=FunctionInput(make_input((1, 1, 2, 2, 3))),\n            reference_fn=lambda m, p, i: padding3d_circular_ref(i, m.padding)\n        ),\n    ]\n\n\n# All these operators share similar issues on cuDNN and MIOpen\nrnn_gru_lstm_module_info_decorators = (\n    # RuntimeError: Batching rule not implemented for aten::_cudnn_rnn_backward.\n    # We could not generate a fallback\n    DecorateInfo(\n        unittest.expectedFailure, \"TestModule\", \"test_grad\",\n        active_if=(TEST_CUDNN and not TEST_WITH_ROCM), device_type='cuda'\n    ),\n    # NotImplementedError: the derivative for '_cudnn_rnn_backward' is not implemented.\n    # Double backwards is not supported for CuDNN RNNs due to limitations in the CuDNN API\n    DecorateInfo(\n        unittest.expectedFailure, \"TestModule\", \"test_gradgrad\",\n        active_if=(TEST_CUDNN and not TEST_WITH_ROCM), device_type='cuda'\n    ),\n    # CUDNN GRU doesn't accept non-contiguous hx\n    DecorateInfo(\n        unittest.expectedFailure, \"TestModule\", \"test_non_contiguous_tensors\",\n        active_if=(TEST_CUDNN and not TEST_WITH_ROCM), device_type='cuda'\n    ),\n    # MIOPEN GRU doesn't accept non-contiguous hx (this is dispatched to miopen only for float).\n    DecorateInfo(\n        unittest.expectedFailure, \"TestModule\", \"test_non_contiguous_tensors\",\n        active_if=(TEST_CUDNN and TEST_WITH_ROCM), dtypes=(torch.float,), device_type='cuda'\n    ),\n    DecorateInfo(\n        skipCUDAVersionIn([(11, 7)]), \"TestExpandedWeightModule\", \"test_module\",\n        device_type='cuda'\n    ),\n    DecorateInfo(\n        skipCUDAVersionIn([(11, 7)]), \"TestDecomp\", \"test_rnn_decomp_module\",\n        device_type='cuda'\n    )\n)\n\n# Start of module error inputs functions.\n\ndef module_error_inputs_torch_nn_RNN_GRU_Cell(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    samples = [\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 11), make_input(3, 20)),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"input has inconsistent input_size: got 11 expected 10\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 10), make_input(3, 21)),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"hidden0 has inconsistent hidden_size: got 21, expected 20\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 10), make_input(5, 20)),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"Input batch size 3 doesn't match hidden0 batch size 5\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 10), make_input(3, 1, 1, 20)),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=ValueError,\n            error_regex=\"Expected hidden to be 1D or 2D, got 4D instead\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20, 'relu'),\n                forward_input=FunctionInput(make_input(3, 10), make_input(3, 21)),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"hidden0 has inconsistent hidden_size: got 21, expected 20\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20, 'tanh'),\n                forward_input=FunctionInput(make_input(3, 10), make_input(3, 21)),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"hidden0 has inconsistent hidden_size: got 21, expected 20\"\n        ),\n    ]\n    return samples\n\ndef module_error_inputs_torch_nn_LSTMCell(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n    samples = [\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 11), (make_input(3, 20), make_input(3, 20))),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"input has inconsistent input_size: got 11 expected 10\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 10), (make_input(3, 21), make_input(3, 21))),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"hidden0 has inconsistent hidden_size: got 21, expected 20\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 10), (make_input(5, 20), make_input(5, 20))),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=RuntimeError,\n            error_regex=\"Input batch size 3 doesn't match hidden0 batch size 5\"\n        ),\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(10, 20),\n                forward_input=FunctionInput(make_input(3, 10), (make_input(3, 1, 1, 20), make_input(3, 1, 1, 20))),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=ValueError,\n            error_regex=\"Expected hx\\\\[0\\\\] to be 1D or 2D, got 4D instead\"\n        ),\n    ]\n    return samples\n\n\n\ndef module_error_inputs_torch_nn_Pad1d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    is_constant = kwargs.get('is_constant', False)\n\n    return [\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(1, 3) if is_constant else FunctionInput(3),\n                forward_input=FunctionInput(make_input((2, 3, 4, 5))),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=ValueError,\n            error_regex=r\"expected 2D or 3D input \\(got 4D input\\)\",\n\n        ),\n    ]\n\ndef module_error_inputs_torch_nn_Pad2d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    is_constant = kwargs.get('is_constant', False)\n\n    return [\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(1, 3) if is_constant else FunctionInput(3),\n                forward_input=FunctionInput(make_input((2, 3))),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=ValueError,\n            error_regex=r\"expected 3D or 4D input \\(got 2D input\\)\",\n\n        ),\n    ]\n\ndef module_error_inputs_torch_nn_Pad3d(module_info, device, dtype, requires_grad, training, **kwargs):\n    make_input = partial(make_tensor, device=device, dtype=dtype, requires_grad=requires_grad)\n\n    is_constant = kwargs.get('is_constant', False)\n\n    return [\n        ErrorModuleInput(\n            ModuleInput(\n                constructor_input=FunctionInput(1, 3) if is_constant else FunctionInput(3),\n                forward_input=FunctionInput(make_input((2, 3))),\n            ),\n            error_on=ModuleErrorEnum.FORWARD_ERROR,\n            error_type=ValueError,\n            error_regex=r\"expected 4D or 5D input \\(got 2D input\\)\",\n\n        ),\n    ]\n\n\n# Database of ModuleInfo entries in alphabetical order.\nmodule_db: List[ModuleInfo] = [\n    ModuleInfo(torch.nn.AdaptiveAvgPool1d,\n               module_inputs_func=module_inputs_torch_nn_AdaptiveAvgPool1d,\n               skips=(\n                   # Fails on MPS backend if input/output sizes are not divisible\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.AdaptiveAvgPool2d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_inputs_func=module_inputs_torch_nn_AdaptiveAvgPool2d,\n               skips=(\n                   # Fails on MPS backend if input/output sizes are not divisible\n                   DecorateInfo(skipMPS),\n                   # Fails on backward check if output size is 1x1\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                   ),)\n               ),\n    ModuleInfo(torch.nn.AdaptiveAvgPool3d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_inputs_func=module_inputs_torch_nn_AdaptiveAvgPool3d,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.AdaptiveMaxPool1d,\n               module_inputs_func=module_inputs_torch_nn_AdaptiveMaxPool1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.AdaptiveMaxPool2d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_inputs_func=module_inputs_torch_nn_AdaptiveMaxPool2d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.AdaptiveMaxPool3d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_inputs_func=module_inputs_torch_nn_AdaptiveMaxPool3d,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.AvgPool1d,\n               module_inputs_func=module_inputs_torch_nn_AvgPool1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.AvgPool2d,\n               module_inputs_func=module_inputs_torch_nn_AvgPool2d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # The difference between channels last backward and\n                   # channels first backward of AvgPool2d on CUDA is too large\n                   # See https://github.com/pytorch/pytorch/issues/107201\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                       device_type='cuda',\n                   ),),\n               ),\n    ModuleInfo(torch.nn.AvgPool3d,\n               module_inputs_func=module_inputs_torch_nn_AvgPool3d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   # No channels_last support for AvgPool1d as it does not take 4D inputs\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.BatchNorm1d,\n               train_and_eval_differ=True,\n               module_inputs_func=module_inputs_torch_nn_BatchNorm1d,\n               skips=(\n                   # test fails on MPS backend and is being investigated.\n                   # See https://github.com/pytorch/pytorch/issues/100914\n                   DecorateInfo(skipMPS),\n                   # tracking here rather than in the list in test_aotdispatch.py as eval mode passes\n                   # RuntimeError: tried to get Double out of SymInt\n                   DecorateInfo(\n                       unittest.expectedFailure, 'TestEagerFusionModuleInfo',\n                       'test_aot_autograd_symbolic_module_exhaustive',\n                       active_if=lambda p: p['training']\n                   ),\n                   # torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default\n                   DecorateInfo(\n                       unittest.expectedFailure, 'TestEagerFusionModuleInfo',\n                       'test_aot_autograd_module_exhaustive',\n                       active_if=lambda p: p['training']\n                   ))\n               ),\n    ModuleInfo(torch.nn.BatchNorm2d,\n               train_and_eval_differ=True,\n               module_inputs_func=module_inputs_torch_nn_BatchNorm2d,\n               skips=(\n                   # test fails on MPS backend and is being investigated.\n                   # See https://github.com/pytorch/pytorch/issues/100914\n                   DecorateInfo(skipMPS),\n                   # tracking here rather than in the list in test_aotdispatch.py as eval mode passes\n                   # RuntimeError: tried to get Double out of SymInt\n                   DecorateInfo(\n                       unittest.expectedFailure, 'TestEagerFusionModuleInfo',\n                       'test_aot_autograd_symbolic_module_exhaustive',\n                       active_if=lambda p: p['training']\n                   ),\n                   # torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default\n                   DecorateInfo(\n                       unittest.expectedFailure, 'TestEagerFusionModuleInfo',\n                       'test_aot_autograd_module_exhaustive',\n                       active_if=lambda p: p['training']\n                   ),)\n               ),\n    ModuleInfo(torch.nn.BatchNorm3d,\n               train_and_eval_differ=True,\n               module_inputs_func=module_inputs_torch_nn_BatchNorm3d,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   # tracking here rather than in the list in test_aotdispatch.py as eval mode passes\n                   # RuntimeError: tried to get Double out of SymInt\n                   DecorateInfo(\n                       unittest.expectedFailure, 'TestEagerFusionModuleInfo',\n                       'test_aot_autograd_symbolic_module_exhaustive',\n                       active_if=lambda p: p['training']\n                   ),\n                   # torch._subclasses.fake_tensor.DataDependentOutputException: aten._local_scalar_dense.default\n                   DecorateInfo(\n                       unittest.expectedFailure, 'TestEagerFusionModuleInfo',\n                       'test_aot_autograd_module_exhaustive',\n                       active_if=lambda p: p['training']\n                   ),)\n               ),\n    ModuleInfo(torch.nn.CELU,\n               module_inputs_func=module_inputs_torch_nn_CELU,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.Conv1d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=1, lazy=False),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64])\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.Conv2d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=2, lazy=False),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\",\n                                device_type='cuda', dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\",\n                                device_type='mps', dtypes=[torch.float32]),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.Conv3d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=3, lazy=False),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 8005\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=8005), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # Conv3d is not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\"),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.ConvTranspose1d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=1, lazy=False, transposed=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               dtypes=floating_and_complex_types_and(torch.chalf),\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   DecorateInfo(skipIfMps, 'TestModule',\n                                dtypes=complex_types_and(torch.chalf, torch.float64, torch.complex128)),\n                   # Not implmented for chalf on CPU\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_forward',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_memory_format',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule',\n                                'test_if_train_and_eval_modes_differ', dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_non_contiguous_tensors',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_cpu_gpu_parity',\n                                dtypes=(torch.chalf,), device_type='cuda'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_multiple_device_transfer',\n                                dtypes=(torch.chalf,), device_type='cuda'),\n                   # Ref: https://github.com/pytorch/pytorch/issues/73502\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_pickle', dtypes=(torch.chalf,)),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.ConvTranspose2d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=2, lazy=False, transposed=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               dtypes=floating_and_complex_types_and(torch.chalf),\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   DecorateInfo(skipIfMps, 'TestModule',\n                                dtypes=complex_types_and(torch.chalf, torch.float64, torch.complex128)),\n                   # Fails on backward check because ViewAsRealBackward apply contiguous for grad\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_memory_format',\n                                dtypes=(torch.complex32, torch.complex64, torch.complex128)),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\", device_type='cuda',\n                                dtypes=[torch.float64, torch.complex128]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\",\n                                device_type='mps', dtypes=[torch.float32]),\n                   # Not implemented for chalf on CPU\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_forward',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_memory_format',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule',\n                                'test_if_train_and_eval_modes_differ', dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_non_contiguous_tensors',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_cpu_gpu_parity',\n                                dtypes=(torch.chalf,), device_type='cuda'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_multiple_device_transfer',\n                                dtypes=(torch.chalf,), device_type='cuda'),\n                   # Ref: https://github.com/pytorch/pytorch/issues/73502\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_pickle', dtypes=(torch.chalf,)),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.ConvTranspose3d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=3, lazy=False, transposed=True),\n               dtypes=floating_and_complex_types_and(torch.chalf),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 8005\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=8005), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # ConvTranspose3d is not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\"),\n                   # These fail only on ROCm\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\", device_type='cuda',\n                                dtypes=[torch.complex32, torch.complex64], active_if=TEST_WITH_ROCM),\n                   # Not implmented for chalf on CPU\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_forward',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_memory_format',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule',\n                                'test_if_train_and_eval_modes_differ', dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_non_contiguous_tensors',\n                                dtypes=(torch.chalf,), device_type='cpu'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_cpu_gpu_parity',\n                                dtypes=(torch.chalf,), device_type='cuda'),\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_multiple_device_transfer',\n                                dtypes=(torch.chalf,), device_type='cuda'),\n                   # Ref: https://github.com/pytorch/pytorch/issues/73502\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_pickle', dtypes=(torch.chalf,)),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(precisionOverride({torch.complex64: 1e-04}), 'TestModule', 'test_cpu_gpu_parity'),\n               )),\n    ModuleInfo(torch.nn.ELU,\n               module_inputs_func=module_inputs_torch_nn_ELU,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.FractionalMaxPool2d,\n               module_inputs_func=module_inputs_torch_nn_FractionalMaxPool2d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.FractionalMaxPool3d,\n               module_inputs_func=module_inputs_torch_nn_FractionalMaxPool3d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.L1Loss,\n               module_inputs_func=module_inputs_torch_nn_L1Loss,\n               skips=(\n                   # No channels_last support for loss functions.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.LazyConv1d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=1, lazy=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # Lazy modules don't currently play well with ModuleInfo tests on the meta device.\n                   # See https://github.com/pytorch/pytorch/issues/70505 for more info.\n                   DecorateInfo(skipMeta),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.LazyConv2d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=2, lazy=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # Lazy modules don't currently play well with ModuleInfo tests on the meta device.\n                   # See https://github.com/pytorch/pytorch/issues/70505 for more info.\n                   DecorateInfo(skipMeta),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\",\n                                device_type='cuda', dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\",\n                                device_type='mps', dtypes=[torch.float32]),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.LazyConv3d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=3, lazy=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 8005\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=8005), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # Lazy modules don't currently play well with ModuleInfo tests on the meta device.\n                   # See https://github.com/pytorch/pytorch/issues/70505 for more info.\n                   DecorateInfo(skipMeta),\n                   # LazyConv3d is not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\"),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.LazyConvTranspose1d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=1, lazy=True, transposed=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # Lazy modules don't currently play well with ModuleInfo tests on the meta device.\n                   # See https://github.com/pytorch/pytorch/issues/70505 for more info.\n                   DecorateInfo(skipMeta),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.LazyConvTranspose2d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=2, lazy=True, transposed=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 7603\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=7603), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # Lazy modules don't currently play well with ModuleInfo tests on the meta device.\n                   # See https://github.com/pytorch/pytorch/issues/70505 for more info.\n                   DecorateInfo(skipMeta),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\", device_type='cuda',\n                                dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\",\n                                device_type='mps', dtypes=[torch.float32]),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.LazyConvTranspose3d,\n               module_inputs_func=partial(module_inputs_torch_nn_ConvNd, N=3, lazy=True, transposed=True),\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               module_memformat_affects_out=True,\n               skips=(\n                   # channels_last support on cuda requires cudnn >= 8005\n                   DecorateInfo(skipCUDAIfCudnnVersionLessThan(version=8005), 'TestModule', 'test_memory_format'),\n                   # Failure on ROCM for float32 issue #70125\n                   DecorateInfo(skipCUDAIfRocm, 'TestModule', 'test_memory_format', dtypes=[torch.float32]),\n                   # Lazy modules don't currently play well with ModuleInfo tests on the meta device.\n                   # See https://github.com/pytorch/pytorch/issues/70505 for more info.\n                   DecorateInfo(skipMeta),\n                   # LazyConvTranspose3d is not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   # This was wrongly being skipped before and needs investigation.\n                   # See https://github.com/pytorch/pytorch/issues/80247\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\"),\n               ),\n               decorators=(\n                   DecorateInfo(precisionOverride({torch.float32: 1e-04}), 'TestModule', 'test_memory_format'),\n               )),\n    ModuleInfo(torch.nn.Linear,\n               module_inputs_func=module_inputs_torch_nn_Linear,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # No channels_last support for Linear currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.Bilinear,\n               module_inputs_func=module_inputs_torch_nn_Bilinear,\n               decorators=[\n                   DecorateInfo(\n                       toleranceOverride({\n                           torch.float32: tol(atol=1e-4, rtol=1e-4),\n                           torch.float64: tol(atol=1e-4, rtol=1e-4)}),\n                       'TestModule', 'test_forward', device_type='cpu')\n               ],\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # No channels_last support for Bilinear currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.LPPool1d,\n               module_inputs_func=module_inputs_torch_nn_LPPool1d,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_grad'),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_gradgrad'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.LPPool2d,\n               module_inputs_func=module_inputs_torch_nn_LPPool2d,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_grad'),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_gradgrad'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails on backward check on MPS\n                   # See https://github.com/pytorch/pytorch/issues/107214\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                       device_type='mps',\n                   ),)\n               ),\n    ModuleInfo(torch.nn.MaxPool1d,\n               module_inputs_func=module_inputs_torch_nn_MaxPool1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.MaxPool2d,\n               module_inputs_func=module_inputs_torch_nn_MaxPool2d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.MaxPool3d,\n               module_inputs_func=module_inputs_torch_nn_MaxPool3d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.NLLLoss,\n               module_inputs_func=module_inputs_torch_nn_NLLLoss,\n               skips=(\n                   # No channels_last support for loss functions.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.GaussianNLLLoss,\n               module_inputs_func=module_inputs_torch_nn_GaussianNLLLoss,\n               skips=(\n                   # No channels_last support for loss functions.\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)),\n    ModuleInfo(torch.nn.CrossEntropyLoss,\n               module_inputs_func=module_inputs_torch_nn_CrossEntropyLoss,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.GELU,\n               module_inputs_func=module_inputs_torch_nn_GELU,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.GLU,\n               module_inputs_func=module_inputs_torch_nn_GLU,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.GroupNorm,\n               module_inputs_func=module_inputs_torch_nn_GroupNorm,\n               dtypes=get_all_fp_dtypes(include_bfloat16=True, include_half=False),\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64, torch.bfloat16]),\n                   # Tracking at https://github.com/pytorch/pytorch/issues/98089\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_cpu_gpu_parity'),\n                   # No channels_last support for GroupNorm currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), \"TestModule\", \"test_grad\",\n                                active_if=TEST_WITH_ROCM, device_type='cuda'),)\n               ),\n    ModuleInfo(torch.nn.Hardshrink,\n               module_inputs_func=module_inputs_torch_nn_Hardshrink,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),),\n               ),\n    ModuleInfo(torch.nn.Hardswish,\n               module_inputs_func=module_inputs_torch_nn_Hardswish,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails on backward check on MPS\n                   # See https://github.com/pytorch/pytorch/issues/107214\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                       device_type='mps',\n                   ),),\n               supports_gradgrad=False),\n    ModuleInfo(torch.nn.Hardtanh,\n               module_inputs_func=module_inputs_torch_nn_Hardtanh,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),),\n               ),\n    ModuleInfo(torch.nn.InstanceNorm1d,\n               module_inputs_func=partial(module_inputs_torch_nn_InstanceNormNd, N=1),\n               train_and_eval_differ=True,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # No channels_last support for InstanceNorm1d currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.InstanceNorm2d,\n               module_inputs_func=partial(module_inputs_torch_nn_InstanceNormNd, N=2),\n               train_and_eval_differ=True,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # No channels_last support for InstanceNorm2d currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.InstanceNorm3d,\n               module_inputs_func=partial(module_inputs_torch_nn_InstanceNormNd, N=3),\n               train_and_eval_differ=True,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),\n                   # No channels_last support for InstanceNorm3d currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.LocalResponseNorm,\n               module_inputs_func=module_inputs_torch_nn_LocalResponseNorm,\n               skips=(\n                   # uses avg_pool3d which is not supported on MPS backend\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.LayerNorm,\n               module_inputs_func=module_inputs_torch_nn_LayerNorm,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # No channels_last support for LayerNorm currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    # TransformerEncoder takes the same inputs as TransformerEncoderLayer\n    ModuleInfo(torch.nn.TransformerEncoder,\n               train_and_eval_differ=True,\n               module_inputs_func=module_inputs_torch_nn_TransformerEncoder,\n               decorators=[\n                   # Not implemented for SDPA backward derivative\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_gradgrad',\n                                device_type='cpu'),\n               ],\n               skips=(\n                   # No channels_last support for TransformerEncoderLayer currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   # Doesn't support device / dtype kwargs directly because it is just a\n                   # container of TransformerEncoderLayers.\n                   DecorateInfo(unittest.expectedFailure, 'TestModule', 'test_factory_kwargs'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.TransformerEncoderLayer,\n               train_and_eval_differ=True,\n               module_inputs_func=module_inputs_torch_nn_TransformerEncoderLayer,\n               decorators=[\n                   DecorateInfo(toleranceOverride({torch.float32: tol(atol=1e-4, rtol=1e-4)}),\n                                'TestModule', 'test_non_contiguous_tensors',\n                                device_type='cpu', active_if=IS_WINDOWS),\n                   # Not implemented for SDPA backward derivative\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_gradgrad',\n                                device_type='cpu'),\n               ],\n               skips=(\n                   # No channels_last support for TransformerEncoderLayer currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.TransformerDecoderLayer,\n               module_inputs_func=module_inputs_torch_nn_TransformerDecoderLayer,\n               decorators=[\n                   # Not implemented for SDPA backward derivative\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_gradgrad',\n                                device_type='cpu'),\n               ],\n               skips=(\n                   # No channels_last support for TransformerDecoderLayer currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.Transformer,\n               module_inputs_func=module_inputs_torch_nn_Transformer,\n               decorators=[\n                   # Not implemented for SDPA backward derivative\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_gradgrad',\n                                device_type='cpu'),\n               ],\n               skips=(\n                   # No channels_last support for Transformer currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.MultiheadAttention,\n               train_and_eval_differ=True,\n               module_inputs_func=module_inputs_torch_nn_MultiheadAttention,\n               skips=(\n                   # No channels_last support for MultiheadAttention currently.\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.Embedding,\n               module_inputs_func=module_inputs_torch_nn_Embedding,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ReLU,\n               module_inputs_func=module_inputs_torch_nn_ReLU,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails on backward check on MPS\n                   # See https://github.com/pytorch/pytorch/issues/107214\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                       device_type='mps',\n                   ),)\n               ),\n    ModuleInfo(torch.nn.LeakyReLU,\n               module_inputs_func=module_inputs_torch_nn_LeakyReLU,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ReLU6,\n               module_inputs_func=module_inputs_torch_nn_ReLU6,\n               skips=(\n                   # test fails on MPS backend and is being investigated.\n                   # See https://github.com/pytorch/pytorch/issues/100914\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.PReLU,\n               module_inputs_func=module_inputs_torch_nn_PReLU,\n               skips=(\n                   # test fails on MPS backend and is being investigated.\n                   # See https://github.com/pytorch/pytorch/issues/100914\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.RNNCell,\n               module_inputs_func=partial(module_inputs_torch_nn_RNN_GRU_Cell, is_rnn=True),\n               module_error_inputs_func=module_error_inputs_torch_nn_RNN_GRU_Cell,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.GRUCell,\n               module_inputs_func=module_inputs_torch_nn_RNN_GRU_Cell,\n               module_error_inputs_func=module_error_inputs_torch_nn_RNN_GRU_Cell,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.LSTMCell,\n               module_inputs_func=module_inputs_torch_nn_LSTMCell,\n               module_error_inputs_func=module_error_inputs_torch_nn_LSTMCell,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.Sigmoid,\n               module_inputs_func=module_inputs_torch_nn_Sigmoid,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails on backward check on MPS\n                   # See https://github.com/pytorch/pytorch/issues/107214\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                       device_type='mps',\n                   ),)\n               ),\n    ModuleInfo(torch.nn.LogSigmoid,\n               module_inputs_func=module_inputs_torch_nn_LogSigmoid,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.SiLU,\n               module_inputs_func=module_inputs_torch_nn_SiLU,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.Softmax,\n               module_inputs_func=module_inputs_torch_nn_Softmax,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.Softmax2d,\n               module_inputs_func=module_inputs_torch_nn_Softmax2d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # no channels last support for Softmax2d currently\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.LogSoftmax,\n               module_inputs_func=module_inputs_torch_nn_LogSoftmax,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # no channels last support for LogSoftmax currently\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.Softmin,\n               module_inputs_func=module_inputs_torch_nn_Softmin,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # no channels last support for Softmin currently\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format'),)\n               ),\n    ModuleInfo(torch.nn.Softplus,\n               module_inputs_func=module_inputs_torch_nn_Softplus,\n               skips=(\n                   # test fails on MPS backend and is being investigated.\n                   # See https://github.com/pytorch/pytorch/issues/100914\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.Softshrink,\n               module_inputs_func=module_inputs_torch_nn_Softshrink,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.Softsign,\n               module_inputs_func=module_inputs_torch_nn_Softsign,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.Tanh,\n               module_inputs_func=module_inputs_torch_nn_Tanh,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails on backward check on MPS\n                   # See https://github.com/pytorch/pytorch/issues/107214\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                       device_type='mps',\n                   ),)\n               ),\n    ModuleInfo(torch.nn.Tanhshrink,\n               module_inputs_func=module_inputs_torch_nn_Tanhshrink,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails on backward check on MPS\n                   # See https://github.com/pytorch/pytorch/issues/107214\n                   DecorateInfo(\n                       unittest.expectedFailure,\n                       'TestModule',\n                       'test_memory_format',\n                       active_if=lambda p: p['training'],\n                       device_type='mps',\n                   ),)\n               ),\n    ModuleInfo(torch.nn.Threshold,\n               module_inputs_func=module_inputs_torch_nn_Threshold,\n               skips=(\n                   # test fails on MPS backend and is being investigated.\n                   # See https://github.com/pytorch/pytorch/issues/100914\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.Mish,\n               module_inputs_func=module_inputs_torch_nn_Mish,\n               skips=(\n                   # not supported on MPS backend\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.RNN,\n               train_and_eval_differ=True,\n               module_inputs_func=partial(module_inputs_torch_nn_RNN_GRU, is_rnn=True),\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),),\n               decorators=rnn_gru_lstm_module_info_decorators\n               ),\n    ModuleInfo(torch.nn.GRU,\n               train_and_eval_differ=True,\n               module_inputs_func=partial(module_inputs_torch_nn_RNN_GRU, is_rnn=False),\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),),\n               decorators=rnn_gru_lstm_module_info_decorators),\n    ModuleInfo(torch.nn.LSTM,\n               train_and_eval_differ=True,\n               module_inputs_func=module_inputs_torch_nn_LSTM,\n               skips=(\n                   # LSTM with projections is not currently supported with MPS\n                   DecorateInfo(skipMPS),),\n               decorators=rnn_gru_lstm_module_info_decorators),\n    ModuleInfo(torch.nn.ReflectionPad1d,\n               module_inputs_func=module_inputs_torch_nn_ReflectionPad1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ReflectionPad2d,\n               module_inputs_func=module_inputs_torch_nn_ReflectionPad2d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='cuda'),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='mps'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ReflectionPad3d,\n               module_inputs_func=module_inputs_torch_nn_ReflectionPad3d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='cuda'),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='mps'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ReplicationPad1d,\n               module_inputs_func=module_inputs_torch_nn_ReplicationPad1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ReplicationPad2d,\n               module_inputs_func=module_inputs_torch_nn_ReplicationPad2d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='cuda'),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='mps'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ReplicationPad3d,\n               module_inputs_func=module_inputs_torch_nn_ReplicationPad3d,\n               gradcheck_nondet_tol=GRADCHECK_NONDET_TOL,\n               skips=(\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='cuda'),\n                   DecorateInfo(unittest.skip(\"Skipped!\"), 'TestModule', 'test_memory_format',\n                                device_type='mps'),\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.SELU,\n               module_inputs_func=module_inputs_torch_nn_SELU,\n               skips=(\n                   # test fails on MPS backend and is being investigated.\n                   # See https://github.com/pytorch/pytorch/issues/100914\n                   DecorateInfo(skipMPS),)\n               ),\n    ModuleInfo(torch.nn.ZeroPad1d,\n               module_inputs_func=module_inputs_torch_nn_ZeroPad1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ZeroPad2d,\n               module_inputs_func=module_inputs_torch_nn_ZeroPad2d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\", device_type='mps'),)\n               ),\n    ModuleInfo(torch.nn.ZeroPad3d,\n               module_inputs_func=module_inputs_torch_nn_ZeroPad3d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\", device_type='mps'),)\n               ),\n    ModuleInfo(torch.nn.CircularPad1d,\n               module_inputs_func=module_inputs_torch_nn_CircularPad1d,\n               module_error_inputs_func=module_error_inputs_torch_nn_Pad1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.CircularPad2d,\n               module_inputs_func=module_inputs_torch_nn_CircularPad2d,\n               module_error_inputs_func=module_error_inputs_torch_nn_Pad2d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.CircularPad3d,\n               module_inputs_func=module_inputs_torch_nn_CircularPad3d,\n               module_error_inputs_func=module_error_inputs_torch_nn_Pad3d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\"),)\n               ),\n    ModuleInfo(torch.nn.ConstantPad1d,\n               module_inputs_func=module_inputs_torch_nn_ConstantPad1d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),)\n               ),\n    ModuleInfo(torch.nn.ConstantPad2d,\n               module_inputs_func=module_inputs_torch_nn_ConstantPad2d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\", device_type='mps'),)\n               ),\n    ModuleInfo(torch.nn.ConstantPad3d,\n               module_inputs_func=module_inputs_torch_nn_ConstantPad3d,\n               skips=(\n                   DecorateInfo(skipIfMps, 'TestModule', dtypes=[torch.float64]),\n                   # Fails with channels last test on MPS backend\n                   DecorateInfo(unittest.expectedFailure, \"TestModule\", \"test_memory_format\", device_type='mps'),)\n               )\n]\n",
      "torch/_VF.py": "\"\"\"\nThis makes the functions in torch._C._VariableFunctions available as\n    torch._VF.<funcname>\nwithout mypy being able to find them.\n\nA subset of those functions are mapped to ATen functions in\ntorch/jit/_builtins.py\n\nSee https://github.com/pytorch/pytorch/issues/21478 for the reason for\nintroducing torch._VF\n\n\"\"\"\nimport sys\nimport types\n\nimport torch\n\n\nclass VFModule(types.ModuleType):\n    vf: types.ModuleType\n\n    def __init__(self, name):\n        super().__init__(name)\n        self.vf = torch._C._VariableFunctions\n\n    def __getattr__(self, attr):\n        return getattr(self.vf, attr)\n\n\nsys.modules[__name__] = VFModule(__name__)\n",
      "torch/__config__.py": "import torch\n\n\ndef show():\n    \"\"\"\n    Return a human-readable string with descriptions of the\n    configuration of PyTorch.\n    \"\"\"\n    return torch._C._show_config()\n\n\n# TODO: In principle, we could provide more structured version/config\n# information here. For now only CXX_FLAGS is exposed, as Timer\n# uses them.\ndef _cxx_flags():\n    \"\"\"Returns the CXX_FLAGS used when building PyTorch.\"\"\"\n    return torch._C._cxx_flags()\n\n\ndef parallel_info():\n    r\"\"\"Returns detailed string with parallelization settings\"\"\"\n    return torch._C._parallel_info()\n",
      "torch/__future__.py": "\"\"\"\nThis global flag controls whether to assign new tensors to the parameters\ninstead of changing the existing parameters in-place when converting an `nn.Module`\nusing the following methods:\n1. `module.cuda()` / `.cpu()` (for moving `module` between devices)\n2. `module.float()` / `.double()` / `.half()` (for converting `module` to a different dtype)\n3. `module.to()` / `.type()` (for changing `module`'s device or dtype)\n4. `module._apply(fn)` (for generic functions applied to `module`)\n\nDefault: False\n\"\"\"\n_overwrite_module_params_on_conversion = False\n\n\ndef set_overwrite_module_params_on_conversion(value):\n    global _overwrite_module_params_on_conversion\n    _overwrite_module_params_on_conversion = value\n\n\ndef get_overwrite_module_params_on_conversion():\n    return _overwrite_module_params_on_conversion\n",
      "torch/__init__.py": "\nr\"\"\"\nThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\n\nIt has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0.\n\"\"\"\n\nimport math\nimport os\nimport sys\nimport platform\nimport textwrap\nimport ctypes\nimport inspect\n\n# multipy/deploy is setting this import before importing torch, this is the most\n# reliable way we have to detect if we're running within deploy.\n# https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137\ndef _running_with_deploy():\n    return sys.modules.get(\"torch._meta_registrations\", None) is object\n\nfrom ._utils import _import_dotted_name, classproperty\nfrom ._utils_internal import get_file_path, prepare_multiprocessing_environment, \\\n    USE_RTLD_GLOBAL_WITH_LIBTORCH, USE_GLOBAL_DEPS\n\n# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\nif _running_with_deploy():\n    __version__ = \"torch-deploy-1.8\"\nelse:\n    from .torch_version import __version__ as __version__\n\nfrom typing import Any, Callable, Dict, Optional, Set, Tuple, Type, TYPE_CHECKING, Union, List\nimport builtins\n\n__all__ = [\n    'typename', 'is_tensor', 'is_storage',\n    'set_default_tensor_type', 'set_default_device',\n    'set_rng_state', 'get_rng_state', 'manual_seed', 'initial_seed', 'seed',\n    'save', 'load', 'set_printoptions', 'chunk', 'split', 'stack', 'matmul',\n    'no_grad', 'enable_grad', 'rand', 'randn', 'inference_mode',\n    'DoubleStorage', 'FloatStorage', 'LongStorage', 'IntStorage',\n    'ShortStorage', 'CharStorage', 'ByteStorage', 'BoolStorage',\n    'TypedStorage', 'UntypedStorage',\n    'DoubleTensor', 'FloatTensor', 'LongTensor', 'IntTensor',\n    'ShortTensor', 'CharTensor', 'ByteTensor', 'BoolTensor', 'Tensor',\n    'lobpcg', 'use_deterministic_algorithms',\n    'are_deterministic_algorithms_enabled',\n    'is_deterministic_algorithms_warn_only_enabled',\n    'set_deterministic_debug_mode', 'get_deterministic_debug_mode',\n    'set_float32_matmul_precision', 'get_float32_matmul_precision',\n    'set_warn_always', 'is_warn_always_enabled', 'SymInt', 'SymFloat',\n    'SymBool', 'sym_not',\n    'sym_int', 'sym_float', 'sym_max', 'sym_min', 'compile', 'vmap',\n    'export',\n]\n\n################################################################################\n# Load the extension module\n################################################################################\n\nif sys.platform == 'win32':\n    pfiles_path = os.getenv('ProgramFiles', 'C:\\\\Program Files')\n    py_dll_path = os.path.join(sys.exec_prefix, 'Library', 'bin')\n    th_dll_path = os.path.join(os.path.dirname(__file__), 'lib')\n\n    # When users create a virtualenv that inherits the base environment,\n    # we will need to add the corresponding library directory into\n    # DLL search directories. Otherwise, it will rely on `PATH` which\n    # is dependent on user settings.\n    if sys.exec_prefix != sys.base_exec_prefix:\n        base_py_dll_path = os.path.join(sys.base_exec_prefix, 'Library', 'bin')\n    else:\n        base_py_dll_path = ''\n\n    dll_paths = list(filter(os.path.exists, [th_dll_path, py_dll_path, base_py_dll_path]))\n\n    if all(not os.path.exists(os.path.join(p, 'nvToolsExt64_1.dll')) for p in dll_paths):\n        nvtoolsext_dll_path = os.path.join(\n            os.getenv('NVTOOLSEXT_PATH', os.path.join(pfiles_path, 'NVIDIA Corporation', 'NvToolsExt')), 'bin', 'x64')\n    else:\n        nvtoolsext_dll_path = ''\n\n    from .version import cuda as cuda_version\n    import glob\n    if cuda_version and all(not glob.glob(os.path.join(p, 'cudart64*.dll')) for p in dll_paths):\n        cuda_version_1 = cuda_version.replace('.', '_')\n        cuda_path_var = 'CUDA_PATH_V' + cuda_version_1\n        default_path = os.path.join(pfiles_path, 'NVIDIA GPU Computing Toolkit', 'CUDA', 'v' + cuda_version)\n        cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), 'bin')\n    else:\n        cuda_path = ''\n\n    dll_paths.extend(filter(os.path.exists, [nvtoolsext_dll_path, cuda_path]))\n\n    kernel32 = ctypes.WinDLL('kernel32.dll', use_last_error=True)\n    with_load_library_flags = hasattr(kernel32, 'AddDllDirectory')\n    prev_error_mode = kernel32.SetErrorMode(0x0001)\n\n    kernel32.LoadLibraryW.restype = ctypes.c_void_p\n    if with_load_library_flags:\n        kernel32.LoadLibraryExW.restype = ctypes.c_void_p\n\n    for dll_path in dll_paths:\n        os.add_dll_directory(dll_path)\n\n    try:\n        ctypes.CDLL('vcruntime140.dll')\n        ctypes.CDLL('msvcp140.dll')\n        ctypes.CDLL('vcruntime140_1.dll')\n    except OSError:\n        print('''Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe''')\n\n    dlls = glob.glob(os.path.join(th_dll_path, '*.dll'))\n    path_patched = False\n    for dll in dlls:\n        is_loaded = False\n        if with_load_library_flags:\n            res = kernel32.LoadLibraryExW(dll, None, 0x00001100)\n            last_error = ctypes.get_last_error()\n            if res is None and last_error != 126:\n                err = ctypes.WinError(last_error)\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n            elif res is not None:\n                is_loaded = True\n        if not is_loaded:\n            if not path_patched:\n                os.environ['PATH'] = ';'.join(dll_paths + [os.environ['PATH']])\n                path_patched = True\n            res = kernel32.LoadLibraryW(dll)\n            if res is None:\n                err = ctypes.WinError(ctypes.get_last_error())\n                err.strerror += f' Error loading \"{dll}\" or one of its dependencies.'\n                raise err\n\n    kernel32.SetErrorMode(prev_error_mode)\n\n\ndef _preload_cuda_deps(lib_folder, lib_name):\n    \"\"\"Preloads cuda deps if they could not be found otherwise.\"\"\"\n    # Should only be called on Linux if default path resolution have failed\n    assert platform.system() == 'Linux', 'Should only be called on Linux'\n    import glob\n    lib_path = None\n    for path in sys.path:\n        nvidia_path = os.path.join(path, 'nvidia')\n        if not os.path.exists(nvidia_path):\n            continue\n        candidate_lib_paths = glob.glob(os.path.join(nvidia_path, lib_folder, 'lib', lib_name))\n        if candidate_lib_paths and not lib_path:\n            lib_path = candidate_lib_paths[0]\n        if lib_path:\n            break\n    if not lib_path:\n        raise ValueError(f\"{lib_name} not found in the system path {sys.path}\")\n    ctypes.CDLL(lib_path)\n\n\n# See Note [Global dependencies]\ndef _load_global_deps() -> None:\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return\n\n    lib_name = 'libtorch_global_deps' + ('.dylib' if platform.system() == 'Darwin' else '.so')\n    here = os.path.abspath(__file__)\n    lib_path = os.path.join(os.path.dirname(here), 'lib', lib_name)\n\n    try:\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n    except OSError as err:\n        # Can only happen for wheel with cuda libs as PYPI deps\n        # As PyTorch is not purelib, but nvidia-*-cu11 is\n        cuda_libs: Dict[str, str] = {\n            'cublas': 'libcublas.so.*[0-9]',\n            'cudnn': 'libcudnn.so.*[0-9]',\n            'cuda_nvrtc': 'libnvrtc.so.*[0-9].*[0-9]',\n            'cuda_runtime': 'libcudart.so.*[0-9].*[0-9]',\n            'cuda_cupti': 'libcupti.so.*[0-9].*[0-9]',\n            'cufft': 'libcufft.so.*[0-9]',\n            'curand': 'libcurand.so.*[0-9]',\n            'cusolver': 'libcusolver.so.*[0-9]',\n            'cusparse': 'libcusparse.so.*[0-9]',\n            'nccl': 'libnccl.so.*[0-9]',\n            'nvtx': 'libnvToolsExt.so.*[0-9]',\n        }\n        is_cuda_lib_err = [lib for lib in cuda_libs.values() if(lib.split('.')[0] in err.args[0])]\n        if not is_cuda_lib_err:\n            raise err\n        for lib_folder, lib_name in cuda_libs.items():\n            _preload_cuda_deps(lib_folder, lib_name)\n        ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)\n\n\nif (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv('TORCH_USE_RTLD_GLOBAL')) and \\\n        (_running_with_deploy() or platform.system() != 'Windows'):\n    # Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a\n    # few circumstances:\n    #\n    #   1. You're in a build environment (e.g., fbcode) where\n    #      libtorch_global_deps is not available, but you still need\n    #      to get mkl to link in with RTLD_GLOBAL or it will just\n    #      not work.\n    #\n    #   2. You're trying to run PyTorch under UBSAN and you need\n    #      to ensure that only one copy of libtorch is loaded, so\n    #      vptr checks work properly\n    #\n    # If you're using this setting, you must verify that all the libraries\n    # you load consistently use the same libstdc++, or you may have\n    # mysterious segfaults.\n    #\n    old_flags = sys.getdlopenflags()\n    sys.setdlopenflags(os.RTLD_GLOBAL | os.RTLD_LAZY)\n    from torch._C import *  # noqa: F403\n    sys.setdlopenflags(old_flags)\n    del old_flags\n\nelse:\n    # Easy way.  You want this most of the time, because it will prevent\n    # C++ symbols from libtorch clobbering C++ symbols from other\n    # libraries, leading to mysterious segfaults.\n    #\n    # If building in an environment where libtorch_global_deps isn't available\n    # like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will\n    # want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False\n    #\n    # See Note [Global dependencies]\n    if USE_GLOBAL_DEPS:\n        _load_global_deps()\n    from torch._C import *  # noqa: F403\n\n# Appease the type checker; ordinarily this binding is inserted by the\n# torch._C module initialization code in C\nif TYPE_CHECKING:\n    import torch._C as _C\n\nclass SymInt:\n    \"\"\"\n    Like an int (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return builtins.bool(self != 0)\n\n    def __int__(self):\n        return self.node.int_()\n\n    def __index__(self):\n        return self.node.int_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_float__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return str(self.node)\n\nclass SymFloat:\n    \"\"\"\n    Like an float (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n\n    def __eq__(self, other: object) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __lt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __gt__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __le__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __ge__(self, other) -> builtins.bool:\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_max__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_min__(self, other):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __sym_int__(self):\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\nclass SymBool:\n    \"\"\"\n    Like an bool (including magic methods), but redirects all operations on the\n    wrapped node. This is used in particular to symbolically record operations\n    in the symbolic shape workflow.\n\n    Unlike regular bools, regular boolean operators will force extra guards instead\n    of symbolically evaluate.  Use the bitwise operators instead to handle this.\n    \"\"\"\n\n    def __init__(self, node):\n        # This field MUST be named node; C++ binding code assumes that this\n        # class has a field named node that stores SymNode\n        self.node = node\n\n    def __bool__(self):\n        return self.node.bool_()\n\n    def __int__(self):\n        return builtins.int(self.node.bool_())\n\n    # Magic methods installed by torch.fx.experimental.symbolic_shapes\n    def __and__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __or__(self, other) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    # We very carefully define __sym_not__, and not a number of other\n    # plausible alternatives:\n    #\n    #   - We do not override __not__ because this is not a real magic\n    #     method; you cannot override the meaning of the not builtin in\n    #     Python.  We use the name 'sym_not' to clarify that in user code you\n    #     cannot use the builtin not or operator.not_ or operator.__not__ and\n    #     hit this magic method; you must use our custom sym_not operator.\n    #\n    #   - We do not override the __invert__ method because SymBool is\n    #     meant to be usable in situations where bool is expected.  However,\n    #     bitwise negation ~a does the wrong thing with booleans (because\n    #     bool is a subclass of int, so ~1 = -2 which is not falseish.)\n    #     This would be a giant footgun, so we get around it by defining\n    #     our own operator.  Note that bitwise and/or do the right thing,\n    #     so we reuse the conventional operators there for readability.\n    #\n    def __sym_not__(self) -> \"SymBool\":\n        raise AssertionError(\"type stub not overridden\")\n\n    def __repr__(self):\n        return self.node.str()\n\ndef sym_not(a):\n    r\"\"\" SymInt-aware utility for logical negation.\n\n    Args:\n        a (SymBool or bool): Object to negate\n    \"\"\"\n    if hasattr(a, '__sym_not__'):\n        return a.__sym_not__()\n    return not a\n\ndef sym_float(a):\n    r\"\"\" SymInt-aware utility for float casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymFloat):\n        return a\n    elif hasattr(a, '__sym_float__'):\n        return a.__sym_float__()\n    return py_float(a)  # type: ignore[operator]\n\n\ndef sym_int(a):\n    r\"\"\" SymInt-aware utility for int casting.\n\n    Args:\n        a (SymInt, SymFloat, or object): Object to cast\n    \"\"\"\n    if isinstance(a, SymInt):\n        return a\n    elif isinstance(a, SymFloat):\n        return math.floor(a) if a >= 0 else math.ceil(a)  # type: ignore[arg-type, call-overload]\n    return py_int(a)  # type: ignore[operator]\n\ndef sym_max(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_max__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        # NB: If you actually care about preserving output type exactly\n        # if you do something like max(0, 0.0), it is NOT sound to treat\n        # min/max as commutative\n        return b.__sym_max__(a)\n    return builtins.max(a, b)  # type: ignore[operator]\n\ndef sym_min(a, b):\n    \"\"\" SymInt-aware utility for max().\"\"\"\n    if isinstance(a, (SymInt, SymFloat)):\n        return a.__sym_min__(b)\n    elif isinstance(b, (SymInt, SymFloat)):\n        return b.__sym_min__(a)\n    return builtins.min(a, b)  # type: ignore[operator]\n\n# Check to see if we can load C extensions, and if not provide some guidance\n# on what the problem might be.\ntry:\n    # _initExtension is chosen (arbitrarily) as a sentinel.\n    from torch._C import _initExtension\nexcept ImportError:\n    import torch._C as _C_for_compiled_check\n\n    # The __file__ check only works for Python 3.7 and above.\n    if _C_for_compiled_check.__file__ is None:\n        raise ImportError(textwrap.dedent('''\n            Failed to load PyTorch C extensions:\n                It appears that PyTorch has loaded the `torch/_C` folder\n                of the PyTorch repository rather than the C extensions which\n                are expected in the `torch._C` namespace. This can occur when\n                using the `install` workflow. e.g.\n                    $ python setup.py install && python -c \"import torch\"\n\n                This error can generally be solved using the `develop` workflow\n                    $ python setup.py develop && python -c \"import torch\"  # This should succeed\n                or by running Python from a different directory.\n            ''').strip()) from None\n    raise  # If __file__ is not None the cause is unknown, so just re-raise.\n\nfor name in dir(_C):\n    if name[0] != '_' and not name.endswith('Base'):\n        __all__.append(name)\n        obj = getattr(_C, name)\n        if (isinstance(obj, Callable) or inspect.isclass(obj)):  # type: ignore[arg-type]\n            if (obj.__module__ != 'torch'):\n                # TODO: fix their module from C++ side\n                if name not in ['DisableTorchFunctionSubclass', 'DisableTorchFunction', 'Generator']:\n                    obj.__module__ = 'torch'\n\nif not TYPE_CHECKING:\n    # issue 38137 and python issue 43367. Submodules of a C extension are\n    # non-standard, and attributes of those submodules cannot be pickled since\n    # pickle expect to be able to import them as \"from _C.sub import attr\"\n    # which fails with \"_C is not a package\n    for attr in dir(_C):\n        candidate = getattr(_C, attr)\n        if type(candidate) is type(_C):\n            # submodule\n            if f'torch._C.{attr}' not in sys.modules:\n                sys.modules[f'torch._C.{attr}'] = candidate\n\n\n################################################################################\n# Define basic utilities\n################################################################################\n\n\ndef typename(o):\n    if isinstance(o, torch.Tensor):\n        return o.type()\n\n    module = ''\n    class_name = ''\n    if hasattr(o, '__module__') and o.__module__ != 'builtins' \\\n            and o.__module__ != '__builtin__' and o.__module__ is not None:\n        module = o.__module__ + '.'\n\n    if hasattr(o, '__qualname__'):\n        class_name = o.__qualname__\n    elif hasattr(o, '__name__'):\n        class_name = o.__name__\n    else:\n        class_name = o.__class__.__name__\n\n    return module + class_name\n\n\ndef is_tensor(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch tensor.\n\n    Note that this function is simply doing ``isinstance(obj, Tensor)``.\n    Using that ``isinstance`` check is better for typechecking with mypy,\n    and more explicit - so it's recommended to use that instead of\n    ``is_tensor``.\n\n    Args:\n        obj (Object): Object to test\n    Example::\n\n        >>> x = torch.tensor([1, 2, 3])\n        >>> torch.is_tensor(x)\n        True\n\n    \"\"\"\n    return isinstance(obj, torch.Tensor)\n\n\ndef is_storage(obj):\n    r\"\"\"Returns True if `obj` is a PyTorch storage object.\n\n    Args:\n        obj (Object): Object to test\n    \"\"\"\n    return type(obj) in _storage_classes\n\n\n_GLOBAL_DEVICE_CONTEXT = None\n\ndef set_default_device(device):\n    \"\"\"Sets the default ``torch.Tensor`` to be allocated on ``device``.  This\n    does not affect factory function calls which are called with an explicit\n    ``device`` argument.  Factory calls will be performed as if they\n    were passed ``device`` as an argument.\n\n    To only temporarily change the default device instead of setting it\n    globally, use ``with torch.device(device):`` instead.\n\n    The default device is initially ``cpu``.  If you set the default tensor\n    device to another device (e.g., ``cuda``) without a device index, tensors\n    will be allocated on whatever the current device for the device type,\n    even after :func:`torch.cuda.set_device` is called.\n\n    .. warning::\n\n        This function imposes a slight performance cost on every Python\n        call to the torch API (not just factory functions).  If this\n        is causing problems for you, please comment on\n        https://github.com/pytorch/pytorch/issues/92701\n\n    Args:\n        device (device or string): the device to set as default\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"requires cuda, changes global state\")\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cpu')\n        >>> torch.set_default_device('cuda')  # current device is 0\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=0)\n        >>> torch.set_default_device('cuda:1')\n        >>> torch.tensor([1.2, 3]).device\n        device(type='cuda', index=1)\n\n    \"\"\"\n    global _GLOBAL_DEVICE_CONTEXT\n    if _GLOBAL_DEVICE_CONTEXT is not None:\n        _GLOBAL_DEVICE_CONTEXT.__exit__(None, None, None)\n    if device is None:\n        _GLOBAL_DEVICE_CONTEXT = None\n        return\n    from torch.utils._device import DeviceContext\n    _GLOBAL_DEVICE_CONTEXT = DeviceContext(device)\n    _GLOBAL_DEVICE_CONTEXT.__enter__()\n\n\ndef set_default_tensor_type(t):\n    r\"\"\"Sets the default ``torch.Tensor`` type to floating point tensor type\n    ``t``. This type will also be used as default floating point type for\n    type inference in :func:`torch.tensor`.\n\n    The default floating point tensor type is initially ``torch.FloatTensor``.\n\n    Args:\n        t (type or string): the floating point tensor type or its name\n\n    Example::\n\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\n        torch.float32\n        >>> torch.set_default_tensor_type(torch.DoubleTensor)\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n\n    \"\"\"\n    if isinstance(t, str):\n        t = _import_dotted_name(t)\n    _C._set_default_tensor_type(t)\n\n\ndef set_default_dtype(d):\n    r\"\"\"\n\n    Sets the default floating point dtype to :attr:`d`. Supports torch.float32\n    and torch.float64 as inputs. Other dtypes may be accepted without complaint\n    but are not supported and are unlikely to work as expected.\n\n    When PyTorch is initialized its default floating point dtype is torch.float32,\n    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like\n    type inference. The default floating point dtype is used to:\n\n    1. Implicitly determine the default complex dtype. When the default floating point\n       type is float32 the default complex dtype is complex64, and when the default\n       floating point type is float64 the default complex type is complex128.\n    2. Infer the dtype for tensors constructed using Python floats or complex Python\n       numbers. See examples below.\n    3. Determine the result of type promotion between bool and integer tensors and\n       Python floats and complex Python numbers.\n\n    Args:\n        d (:class:`torch.dtype`): the floating point dtype to make the default.\n                                  Either torch.float32 or torch.float64.\n\n    Example:\n        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n        >>> # initial default for floating point is torch.float32\n        >>> # Python floats are interpreted as float32\n        >>> torch.tensor([1.2, 3]).dtype\n        torch.float32\n        >>> # initial default for floating point is torch.complex64\n        >>> # Complex Python numbers are interpreted as complex64\n        >>> torch.tensor([1.2, 3j]).dtype\n        torch.complex64\n\n        >>> torch.set_default_dtype(torch.float64)\n\n        >>> # Python floats are now interpreted as float64\n        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n        torch.float64\n        >>> # Complex Python numbers are now interpreted as complex128\n        >>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\n        torch.complex128\n\n    \"\"\"\n    _C._set_default_dtype(d)\n\ndef use_deterministic_algorithms(mode: builtins.bool, *, warn_only: builtins.bool = False) -> None:\n    r\"\"\" Sets whether PyTorch operations must use \"deterministic\"\n    algorithms. That is, algorithms which, given the same input, and when\n    run on the same software and hardware, always produce the same output.\n    When enabled, operations will use deterministic algorithms when available,\n    and if only nondeterministic algorithms are available they will throw a\n    :class:`RuntimeError` when called.\n\n    .. note:: This setting alone is not always enough to make an application\n        reproducible. Refer to :ref:`reproducibility` for more information.\n\n    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative\n        interface for this feature.\n\n    The following normally-nondeterministic operations will act\n    deterministically when ``mode=True``:\n\n        * :class:`torch.nn.Conv1d` when called on CUDA tensor\n        * :class:`torch.nn.Conv2d` when called on CUDA tensor\n        * :class:`torch.nn.Conv3d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor\n        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor\n        * :func:`torch.bmm` when called on sparse-dense CUDA tensors\n        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor\n          and the index is a list of tensors\n        * :func:`torch.Tensor.index_put` with ``accumulate=False``\n        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU\n          tensor\n        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor\n        * :func:`torch.gather` when called on a CUDA tensor that requires grad\n        * :func:`torch.index_add` when called on CUDA tensor\n        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor\n        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor\n        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor\n        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='sum'`` or ``reduce='mean'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_`, when called with a tensor that is not\n          quantized, sets new elements to a known value.  Floating point or\n          complex values are set to NaN. Integer values are set to the maximum\n          value.\n        * :func:`torch.empty`, :func:`torch.empty_like`, :func:`torch.empty_strided`,\n          and :func:`torch.empty_permuted` will fill the output tensor with a known\n          value. Floating point or complex dtype tensors are filled with NaN. Integer\n          dtype tensors are filled with the maximum value.\n\n    The following normally-nondeterministic operations will throw a\n    :class:`RuntimeError` when ``mode=True``:\n\n        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.MaxUnpool1d`\n        * :class:`torch.nn.MaxUnpool2d`\n        * :class:`torch.nn.MaxUnpool3d`\n        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor\n          and one of the following modes is used:\n\n          - ``linear``\n          - ``bilinear``\n          - ``bicubic``\n          - ``trilinear``\n\n        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor\n        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor\n        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when\n          ``mode='max'``\n        * :func:`torch.Tensor.put_` when ``accumulate=False``\n        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor\n        * :func:`torch.histc` when called on a CUDA tensor\n        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``\n          tensor is given\n        * :func:`torch.kthvalue` with called on a CUDA tensor\n        * :func:`torch.median` with indices output when called on a CUDA tensor\n        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor\n        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex\n        * :func:`torch.Tensor.scatter_reduce` when ``reduce='prod'`` and called on CUDA tensor\n        * :func:`torch.Tensor.resize_` when called with a quantized tensor\n\n    A handful of CUDA operations are nondeterministic if the CUDA version is\n    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``\n    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more\n    details: `<https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility>`_\n    If one of these environment variable configurations is not set, a :class:`RuntimeError`\n    will be raised from these operations when called with CUDA tensors:\n\n        * :func:`torch.mm`\n        * :func:`torch.mv`\n        * :func:`torch.bmm`\n\n    Note that deterministic operations tend to have worse performance than\n    nondeterministic operations.\n\n    .. note::\n\n        This flag does not detect or prevent nondeterministic behavior caused\n        by calling an inplace operation on a tensor with an internal memory\n        overlap or by giving such a tensor as the :attr:`out` argument for an\n        operation. In these cases, multiple writes of different data may target\n        a single memory location, and the order of writes is not guaranteed.\n\n    Args:\n        mode (:class:`bool`): If True, makes potentially nondeterministic\n            operations switch to a deterministic algorithm or throw a runtime\n            error. If False, allows nondeterministic operations.\n\n    Keyword args:\n        warn_only (:class:`bool`, optional): If True, operations that do not\n            have a deterministic implementation will throw a warning instead of\n            an error. Default: ``False``\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> torch.use_deterministic_algorithms(True)\n\n        # Forward mode nondeterministic error\n        >>> torch.randn(10, device='cuda').kthvalue(0)\n        ...\n        RuntimeError: kthvalue CUDA does not have a deterministic implementation...\n\n        # Backward mode nondeterministic error\n        >>> torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()\n        ...\n        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...\n    \"\"\"\n    _C._set_deterministic_algorithms(mode, warn_only=warn_only)\n\ndef are_deterministic_algorithms_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is turned on. Refer to\n    :func:`torch.use_deterministic_algorithms` documentation for more details.\n    \"\"\"\n    return _C._get_deterministic_algorithms()\n\ndef is_deterministic_algorithms_warn_only_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global deterministic flag is set to warn only.\n    Refer to :func:`torch.use_deterministic_algorithms` documentation for more\n    details.\n    \"\"\"\n    return _C._get_deterministic_algorithms_warn_only()\n\ndef set_deterministic_debug_mode(debug_mode: Union[builtins.int, str]) -> None:\n    r\"\"\"Sets the debug mode for deterministic operations.\n\n    .. note:: This is an alternative interface for\n        :func:`torch.use_deterministic_algorithms`. Refer to that function's\n        documentation for details about affected operations.\n\n    Args:\n        debug_mode(str or int): If \"default\" or 0, don't error or warn on\n            nondeterministic operations. If \"warn\" or 1, warn on\n            nondeterministic operations. If \"error\" or 2, error on\n            nondeterministic operations.\n    \"\"\"\n\n    # NOTE: builtins.int is used here because int in this scope resolves\n    # to torch.int\n    if not isinstance(debug_mode, (builtins.int, str)):\n        raise TypeError(f'debug_mode must be str or int, but got {type(debug_mode)}')\n\n    if isinstance(debug_mode, str):\n        if debug_mode == 'default':\n            debug_mode = 0\n        elif debug_mode == 'warn':\n            debug_mode = 1\n        elif debug_mode == 'error':\n            debug_mode = 2\n        else:\n            raise RuntimeError(\n                'invalid value of debug_mode, expected one of `default`, '\n                f'`warn`, `error`, but got {debug_mode}')\n\n    if debug_mode == 0:\n        _C._set_deterministic_algorithms(False)\n    elif debug_mode == 1:\n        _C._set_deterministic_algorithms(True, warn_only=True)\n    elif debug_mode == 2:\n        _C._set_deterministic_algorithms(True)\n    else:\n        raise RuntimeError(\n            'invalid value of debug_mode, expected 0, 1, or 2, '\n            f'but got {debug_mode}')\n\ndef get_deterministic_debug_mode() -> builtins.int:\n    r\"\"\"Returns the current value of the debug mode for deterministic\n    operations. Refer to :func:`torch.set_deterministic_debug_mode`\n    documentation for more details.\n    \"\"\"\n\n    if _C._get_deterministic_algorithms():\n        if _C._get_deterministic_algorithms_warn_only():\n            return 1\n        else:\n            return 2\n    else:\n        return 0\n\ndef get_float32_matmul_precision() -> builtins.str:\n    r\"\"\"Returns the current value of float32 matrix multiplication precision. Refer to\n    :func:`torch.set_float32_matmul_precision` documentation for more details.\n    \"\"\"\n    return _C._get_float32_matmul_precision()\n\ndef set_float32_matmul_precision(precision: str) -> None:\n    r\"\"\"Sets the internal precision of float32 matrix multiplications.\n\n    Running float32 matrix multiplications in lower precision may significantly increase\n    performance, and in some programs the loss of precision has a negligible impact.\n\n    Supports three settings:\n\n        * \"highest\", float32 matrix multiplications use the float32 datatype (24 mantissa\n          bits) for internal computations.\n        * \"high\", float32 matrix multiplications either use the TensorFloat32 datatype (10\n          mantissa bits) or treat each float32 number as the sum of two bfloat16 numbers\n          (approximately 16 mantissa bits), if the appropriate fast matrix multiplication\n          algorithms are available.  Otherwise float32 matrix multiplications are computed\n          as if the precision is \"highest\".  See below for more information on the bfloat16\n          approach.\n        * \"medium\", float32 matrix multiplications use the bfloat16 datatype (8 mantissa\n          bits) for internal computations, if a fast matrix multiplication algorithm\n          using that datatype internally is available. Otherwise float32\n          matrix multiplications are computed as if the precision is \"high\".\n\n    When using \"high\" precision, float32 multiplications may use a bfloat16-based algorithm\n    that is more complicated than simply truncating to some smaller number mantissa bits\n    (e.g. 10 for TensorFloat32, 8 for bfloat16).  Refer to [Henry2019]_ for a complete\n    description of this algorithm.  To briefly explain here, the first step is to realize\n    that we can perfectly encode a single float32 number as the sum of three bfloat16\n    numbers (because float32 has 24 mantissa bits while bfloat16 has 8, and both have the\n    same number of exponent bits).  This means that the product of two float32 numbers can\n    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade\n    accuracy for speed by dropping some of these products.  The \"high\" precision algorithm\n    specifically keeps only the three most significant products, which conveniently excludes\n    all of the products involving the last 8 mantissa bits of either input.  This means that\n    we can represent our inputs as the sum of two bfloat16 numbers rather than three.\n    Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than\n    float32 ones, it's faster to do three multiplications and 2 additions with bfloat16\n    precision than it is to do a single multiplication with float32 precision.\n\n    .. [Henry2019] http://arxiv.org/abs/1904.06376\n\n    .. note::\n\n        This does not change the output dtype of float32 matrix multiplications,\n        it controls how the internal computation of the matrix multiplication is performed.\n\n    .. note::\n\n        This does not change the precision of convolution operations. Other flags,\n        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution\n        operations.\n\n    .. note::\n\n        This flag currently only affects one native device type: CUDA.\n        If \"high\" or \"medium\" are set then the TensorFloat32 datatype will be used\n        when computing float32 matrix multiplications, equivalent to setting\n        `torch.backends.cuda.matmul.allow_tf32 = True`. When \"highest\" (the default)\n        is set then the float32 datatype is used for internal computations, equivalent\n        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.\n\n    Args:\n        precision(str): can be set to \"highest\" (default), \"high\", or \"medium\" (see above).\n\n    \"\"\"\n    _C._set_float32_matmul_precision(precision)\n\ndef set_warn_always(b: builtins.bool) -> None:\n    r\"\"\"When this flag is False (default) then some PyTorch warnings may only\n    appear once per process. This helps avoid excessive warning information.\n    Setting it to True causes these warnings to always appear, which may be\n    helpful when debugging.\n\n    Args:\n        b (:class:`bool`): If True, force warnings to always be emitted\n                           If False, set to the default behaviour\n    \"\"\"\n    _C._set_warnAlways(b)\n\ndef is_warn_always_enabled() -> builtins.bool:\n    r\"\"\"Returns True if the global warn_always flag is turned on. Refer to\n    :func:`torch.set_warn_always` documentation for more details.\n    \"\"\"\n    return _C._get_warnAlways()\n\n################################################################################\n# Define error checking functions\n################################################################################\n\n# These error checking functions must be kept consistent with their C++\n# equivalents. Their C++ equivalents are mentioned where applicable.\n\ndef _check_with(error_type, cond: Union[builtins.bool, SymBool], message: Callable[[], str]):\n    if not isinstance(cond, (builtins.bool, torch.SymBool)):\n        raise TypeError(f'cond must be a bool, but got {type(cond)}')\n\n    if torch.fx.experimental.symbolic_shapes.expect_true(cond):\n        return\n\n    # error_type must be a subclass of Exception and not subclass of Warning\n    assert issubclass(error_type, Exception) and not issubclass(error_type, Warning)\n\n    if message is None:\n        message_evaluated = (\n            'Expected cond to be True, but got False. (Could this error '\n            'message be improved? If so, please report an enhancement request '\n            'to PyTorch.)')\n\n    else:\n        if not callable(message):\n            raise TypeError('message must be a callable')\n\n        message_evaluated = str(message())\n\n    raise error_type(message_evaluated)\n\ndef _check(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(RuntimeError, cond, message)\n\ndef _check_index(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``IndexError``\n\n    C++ equivalent: ``TORCH_CHECK_INDEX``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(IndexError, cond, message)\n\ndef _check_value(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``ValueError``\n\n    C++ equivalent: ``TORCH_CHECK_VALUE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(ValueError, cond, message)\n\ndef _check_type(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``TypeError``\n\n    C++ equivalent: ``TORCH_CHECK_TYPE``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(TypeError, cond, message)\n\ndef _check_not_implemented(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``NotImplementedError``\n\n    C++ equivalent: ``TORCH_CHECK_NOT_IMPLEMENTED``\n\n    Args:\n        cond (:class:`bool`): If False, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_with(NotImplementedError, cond, message)\n\ndef _check_tensor_all_with(error_type, cond, message=None):\n    if not torch.is_tensor(cond):\n        raise TypeError(f'cond must be a tensor, but got {type(cond)}')\n\n    if not cond.dtype == torch.bool:\n        raise TypeError(\n            f'cond tensor must have dtype torch.bool, but got {cond.dtype}')\n\n    _check_with(error_type, cond._is_all_true().item(), message)\n\n# C++ equivalent: `TORCH_CHECK_TENSOR_ALL`\ndef _check_tensor_all(cond, message=None):\n    r\"\"\"Throws error containing an optional message if the specified condition\n    is False.\n\n    Error type: ``RuntimeError``\n\n    C++ equivalent: ``TORCH_CHECK_TENSOR_ALL``\n\n    Args:\n        cond (:class:`torch.Tensor`): Tensor of dtype ``torch.bool``. If any\n            element is ``False``, throw error\n\n        message (Callable, optional): Callable that returns either a string or\n            an object that has a ``__str__()`` method to be used as the error\n            message. Default: ``None``\n    \"\"\"\n    _check_tensor_all_with(RuntimeError, cond, message)\n\n################################################################################\n# Define numeric constants\n################################################################################\n\n# For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and\n# NumPy consistency (https://numpy.org/devdocs/reference/constants.html)\nfrom math import e , nan , inf , pi\n__all__.extend(['e', 'pi', 'nan', 'inf'])\n\n################################################################################\n# Define Storage and Tensor classes\n################################################################################\n\nfrom ._tensor import Tensor\nfrom .storage import _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n\n# NOTE: New <type>Storage classes should never be added. When adding a new\n# dtype, use torch.storage.TypedStorage directly.\n\nclass ByteStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.uint8\n\nclass DoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.double\n\nclass FloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.float\n\nclass HalfStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.half\n\nclass LongStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.long\n\nclass IntStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int\n\nclass ShortStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.short\n\nclass CharStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.int8\n\nclass BoolStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bool\n\nclass BFloat16Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.bfloat16\n\nclass ComplexDoubleStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cdouble\n\nclass ComplexFloatStorage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.cfloat\n\nclass QUInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint8\n\nclass QInt8Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint8\n\nclass QInt32Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.qint32\n\nclass QUInt4x2Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint4x2\n\nclass QUInt2x4Storage(_LegacyStorage):\n    @classproperty\n    def dtype(self):\n        _warn_typed_storage_removal()\n        return self._dtype\n\n    @classproperty\n    def _dtype(self):\n        return torch.quint2x4\n\n_storage_classes = {\n    UntypedStorage, DoubleStorage, FloatStorage, LongStorage, IntStorage,\n    ShortStorage, CharStorage, ByteStorage, HalfStorage, BoolStorage,\n    QUInt8Storage, QInt8Storage, QInt32Storage, BFloat16Storage,\n    ComplexFloatStorage, ComplexDoubleStorage, QUInt4x2Storage, QUInt2x4Storage,\n    TypedStorage\n}\n\n# The _tensor_classes set is initialized by the call to _C._initialize_tensor_type_bindings()\n_tensor_classes: Set[Type] = set()\n\n# If you edit these imports, please update torch/__init__.py.in as well\nfrom .random import set_rng_state, get_rng_state, manual_seed, initial_seed, seed\nfrom .serialization import save, load\nfrom ._tensor_str import set_printoptions\n\n################################################################################\n# Initialize extension\n################################################################################\n\ndef manager_path():\n    if _running_with_deploy() or platform.system() == 'Windows':\n        return b\"\"\n    path = get_file_path('torch', 'bin', 'torch_shm_manager')\n    prepare_multiprocessing_environment(get_file_path('torch'))\n    if not os.path.exists(path):\n        raise RuntimeError(\"Unable to find torch_shm_manager at \" + path)\n    return path.encode('utf-8')\n\nfrom torch.amp import autocast\n\n# Initializing the extension shadows the built-in python float / int classes;\n# store them for later use by SymInt / SymFloat.\npy_float = float\npy_int = int\n\n# Shared memory manager needs to know the exact location of manager executable\n_C._initExtension(manager_path())\ndel manager_path\n\n# Appease the type checker: it can't deal with direct setting of globals().\n# Note that we will see \"too many\" functions when reexporting this way; there\n# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\n# so that this import is good enough\nif TYPE_CHECKING:\n    # Some type signatures pulled in from _VariableFunctions here clash with\n    # signatures already imported. For now these clashes are ignored; see\n    # PR #43339 for details.\n    from torch._C._VariableFunctions import *  # type: ignore[assignment, misc] # noqa: F403\n    # Fixup segment_reduce visibility\n    _segment_reduce = segment_reduce\n    del segment_reduce\n\n# Ops not to be exposed in `torch` namespace,\n# mostly helper ops.\nPRIVATE_OPS = (\n    'unique_dim',\n)\n\nfor name in dir(_C._VariableFunctions):\n    if name.startswith('__') or name in PRIVATE_OPS:\n        continue\n    obj = getattr(_C._VariableFunctions, name)\n    obj.__module__ = 'torch'\n    # Hide some APIs that should not be public\n    if name == \"segment_reduce\":\n        # TODO: Once the undocumented FC window is passed, remove the line bellow\n        globals()[name] = obj\n        name = \"_\" + name\n    globals()[name] = obj\n    if not name.startswith(\"_\"):\n        __all__.append(name)\n\n\n\n################################################################################\n# Import TorchDynamo's lazy APIs to avoid circular dependenices\n################################################################################\n\n# needs to be before from .functional import * to avoid circular dependencies\nfrom ._compile import _disable_dynamo\n\n################################################################################\n# Import interface functions defined in Python\n################################################################################\n\n# needs to be after the above ATen bindings so we can overwrite from Python side\nfrom .functional import *  # noqa: F403\n\n\n################################################################################\n# Remove unnecessary members\n################################################################################\n\ndel _StorageBase\ndel _LegacyStorage\n\n################################################################################\n# Define _assert\n################################################################################\n\n# needs to be before the submodule imports to avoid circular dependencies\ndef _assert(condition, message):\n    r\"\"\"A wrapper around Python's assert which is symbolically traceable.\n    \"\"\"\n    from .overrides import has_torch_function, handle_torch_function\n\n    if type(condition) is not torch.Tensor and has_torch_function((condition,)):\n        return handle_torch_function(_assert, (condition,), condition, message)\n    assert condition, message\n\n################################################################################\n# Import most common subpackages\n################################################################################\n\n# Use the redundant form so that type checkers know that these are a part of\n# the public API. The \"regular\" import lines are there solely for the runtime\n# side effect of adding to the imported module's members for other users.\nfrom torch import cuda as cuda\nfrom torch import cpu as cpu\nfrom torch import mps as mps\nfrom torch import autograd as autograd\nfrom torch.autograd import (\n    no_grad as no_grad,\n    enable_grad as enable_grad,\n    set_grad_enabled as set_grad_enabled,\n    inference_mode as inference_mode,\n)\nfrom torch import fft as fft\nfrom torch import futures as futures\nfrom torch import _awaits as _awaits\nfrom torch import nested as nested\nfrom torch import nn as nn\nfrom torch.signal import windows as windows\nfrom torch import optim as optim\nimport torch.optim._multi_tensor\nfrom torch import multiprocessing as multiprocessing\nfrom torch import sparse as sparse\nfrom torch import special as special\nimport torch.utils.backcompat\nfrom torch import jit as jit\nfrom torch import linalg as linalg\nfrom torch import hub as hub\nfrom torch import random as random\nfrom torch import distributions as distributions\nfrom torch import testing as testing\nfrom torch import backends as backends\nimport torch.utils.data\nfrom torch import __config__ as __config__\nfrom torch import __future__ as __future__\nfrom torch import profiler as profiler\n\n# Quantized, sparse, AO, etc. should be last to get imported, as nothing\n# is expected to depend on them.\nfrom torch import ao as ao\n# nn.quant* depends on ao -- so should be after those.\nimport torch.nn.quantizable\nimport torch.nn.quantized\nimport torch.nn.qat\nimport torch.nn.intrinsic\n\n_C._init_names(list(torch._storage_classes))\n\n# attach docstrings to torch and tensor functions\nfrom . import _torch_docs, _tensor_docs, _storage_docs\ndel _torch_docs, _tensor_docs, _storage_docs\n\n\ndef compiled_with_cxx11_abi() -> builtins.bool:\n    r\"\"\"Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\"\"\"\n    return _C._GLIBCXX_USE_CXX11_ABI\n\n\n# Import the ops \"namespace\"\nfrom torch._ops import ops\nfrom torch._classes import classes\n\n# quantization depends on torch.fx\n# Import quantization\nfrom torch import quantization as quantization\n\n# Import the quasi random sampler\nfrom torch import quasirandom as quasirandom\n\n# If you are seeing this, it means that this call site was not checked if\n# the memory format could be preserved, and it was switched to old default\n# behaviour of contiguous\nlegacy_contiguous_format = contiguous_format\n\n# Register fork handler to initialize OpenMP in child processes (see gh-28389)\nfrom torch.multiprocessing._atfork import register_after_fork\nregister_after_fork(torch.get_num_threads)\ndel register_after_fork\n\n# Import tools that require fully imported torch (for applying\n# torch.jit.script as a decorator, for instance):\nfrom ._lobpcg import lobpcg as lobpcg\n\n# These were previously defined in native_functions.yaml and appeared on the\n# `torch` namespace, but we moved them to c10 dispatch to facilitate custom\n# class usage. We add these lines here to preserve backward compatibility.\nquantized_lstm = torch.ops.aten.quantized_lstm\nquantized_gru = torch.ops.aten.quantized_gru\n\nfrom torch.utils.dlpack import from_dlpack, to_dlpack\n\n# Import experimental masked operations support. See\n# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\n# information.\nfrom . import masked\n\n# Import removed ops with error message about removal\nfrom ._linalg_utils import (  # type: ignore[misc]\n    matrix_rank,\n    eig,\n    solve,\n    lstsq,\n)\nfrom ._linalg_utils import _symeig as symeig  # type: ignore[misc]\n\nclass _TorchCompileInductorWrapper:\n    compiler_name = \"inductor\"\n\n    def __init__(self, mode, options, dynamic):\n        self.config: Dict[str, Any] = dict()\n        self.dynamic = dynamic\n        self.apply_mode(mode)\n        self.apply_options(options)\n\n        # FIXME: CUPTI Lazy Re-init and CUDA Graph crashes with CUDA 11.\n        if self.config.get(\"triton.cudagraphs\", False):\n            os.environ[\"DISABLE_CUPTI_LAZY_REINIT\"] = \"1\"\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileInductorWrapper) and\n                self.config == other.config and\n                self.dynamic == other.dynamic)\n\n    def apply_mode(self, mode: Optional[str]):\n        if mode is None or mode == \"default\":\n            pass\n        elif mode in (\"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"):\n            from torch._inductor import list_mode_options\n            self.apply_options(list_mode_options(mode, self.dynamic))\n        else:\n            raise RuntimeError(\n                f\"Unrecognized mode={mode}, should be one of: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs\"\n            )\n\n    def apply_options(self, options: Optional[Dict[str, Any]]):\n        if not options:\n            return\n\n        from torch._inductor import config\n        current_config: Dict[str, Any] = config.to_dict()  # type: ignore[attr-defined]\n\n        for key, val in options.items():\n            attr_name = key.replace(\"-\", \"_\")\n            if attr_name not in current_config:\n                raise RuntimeError(\n                    f\"Unexpected optimization option {key}, known options are {list(current_config.keys())}\"\n                )\n            if type(val) is not type(current_config[attr_name]):\n                val_type_str = type(val).__name__\n                expected_type_str = type(current_config[attr_name]).__name__\n                raise RuntimeError(\n                    f\"Unexpected type of attr {key}, got {val_type_str} should be {expected_type_str}\"\n                )\n            self.config[attr_name] = val\n\n    def __call__(self, model_, inputs_):\n        from torch._inductor.compile_fx import compile_fx\n\n        return compile_fx(model_, inputs_, config_patches=self.config)\n\n    def get_compiler_config(self):\n        from torch._inductor.compile_fx import get_patched_config_dict\n        return get_patched_config_dict(config_patches=self.config)\n\n    def reset(self):\n        from torch._inductor import config\n        if \"triton.cudagraphs\" in self.config or config.triton.cudagraphs:\n            if self.config.get(\"triton.cudagraphs\", True):\n                from torch._inductor.cudagraph_trees import reset_cudagraph_trees\n                reset_cudagraph_trees()\n\nclass _TorchCompileWrapper:\n    def __init__(self, backend, mode, options, dynamic):\n        from torch._dynamo.backends.registry import lookup_backend\n\n        if isinstance(backend, str):\n            self.compiler_name = backend\n        elif hasattr(backend, \"__name__\"):\n            self.compiler_name = backend.__name__\n        else:\n            self.compiler_name = str(backend)\n        self.dynamic = dynamic\n        self.compiler_fn = lookup_backend(backend)\n        self.kwargs = {}\n        # only pass the args if they non-empty\n        if mode and mode != \"default\":\n            self.kwargs[\"mode\"] = mode\n        if options:\n            self.kwargs[\"options\"] = options\n\n    def __eq__(self, other):\n        return (isinstance(other, _TorchCompileWrapper) and\n                self.compiler_fn == other.compiler_fn and\n                self.kwargs == other.kwargs and\n                self.dynamic == other.dynamic)\n\n    def __call__(self, model_, inputs_):\n        return self.compiler_fn(model_, inputs_, **self.kwargs)\n\n\ndef compile(model: Optional[Callable] = None, *,\n            fullgraph: builtins.bool = False,\n            dynamic: Optional[builtins.bool] = None,\n            backend: Union[str, Callable] = \"inductor\",\n            mode: Union[str, None] = None,\n            options: Optional[Dict[str, Union[str, builtins.int, builtins.bool]]] = None,\n            disable: builtins.bool = False) -> Callable:\n    \"\"\"\n    Optimizes given model/function using TorchDynamo and specified backend.\n\n    Concretely, for every frame executed within the compiled region, we will attempt\n    to compile it and cache the compiled result on the code object for future\n    use.  A single frame may be compiled multiple times if previous compiled\n    results are not applicable for subsequent calls (this is called a \"guard\n    failure), you can use TORCH_LOGS=guards to debug these situations.\n    Multiple compiled results can be associated with a frame up to\n    ``torch._dynamo.config.cache_size_limit``, which defaults to 64; at which\n    point we will fall back to eager.  Note that compile caches are per\n    *code object*, not frame; if you dynamically create multiple copies of a\n    function, they will all share the same code cache.\n\n    Args:\n       model (Callable): Module/function to optimize\n       fullgraph (bool): Whether it is ok to break model into several subgraphs\n       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\n        to generate a kernel that is as dynamic as possible to avoid recompilations when\n        sizes change.  This may not always work as some operations/optimizations will\n        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\n        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\n        By default (None), we automatically detect if dynamism has occurred and compile a more\n        dynamic kernel upon recompile.\n       backend (str or Callable): backend to be used\n\n        - \"inductor\" is the default backend, which is a good balance between performance and overhead\n\n        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\n\n        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\n\n        - To register an out-of-tree custom backend: https://pytorch.org/docs/main/compile/custom-backends.html\n       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\n\n        - \"default\" is the default mode, which is a good balance between performance and overhead\n\n        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\n          useful for small batches.  Reduction of overhead can come at the cost of more memory\n          usage, as we will cache the workspace memory required for the invocation so that we\n          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\n          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\n          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\n          to debug.\n\n        - \"max-autotune\" is a mode that leverages Triton based matrix multiplications and convolutions\n          It enables CUDA graphs by default.\n\n        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\n\n        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\n\n       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\n\n        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\n\n        - `max_autotune` which will profile to pick the best matmul configuration\n\n        - `fallback_random` which is useful when debugging accuracy issues\n\n        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\n\n        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\n\n        - `trace.enabled` which is the most useful debugging flag to turn on\n\n        - `trace.graph_diagram` which will show you a picture of your graph after fusion\n\n        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\n       disable (bool): Turn torch.compile() into a no-op for testing\n\n    Example::\n\n        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n        def foo(x):\n            return torch.sin(x) + torch.cos(x)\n\n    \"\"\"\n    _C._log_api_usage_once(\"torch.compile\")\n    # Temporary until we get proper support for python 3.12\n    if sys.version_info >= (3, 12):\n        raise RuntimeError(\"Dynamo is not supported on Python 3.12+\")\n\n    # Decorator mode\n    if model is None:\n        def fn(model: Callable):\n            if model is None:\n                raise RuntimeError(\"Model can't be None\")\n            return compile(model,\n                           fullgraph=fullgraph,\n                           dynamic=dynamic,\n                           backend=backend,\n                           mode=mode,\n                           options=options,\n                           disable=disable)\n        return fn\n\n    if mode is not None and options is not None:\n        raise RuntimeError(\"Either mode or options can be specified, but both can't be specified at the same time.\")\n    if mode is None and options is None:\n        mode = \"default\"\n    if backend == \"inductor\":\n        backend = _TorchCompileInductorWrapper(mode, options, dynamic)\n    else:\n        backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n\n    return torch._dynamo.optimize(backend=backend, nopython=fullgraph, dynamic=dynamic, disable=disable)(model)\n\n\nfrom torch import export as export\n\n\ndef _register_device_module(device_type, module):\n    r\"\"\"Register an external runtime module of the specific :attr:`device_type`\n    supported by torch.\n\n    After the :attr:`module` is registered correctly, the user can refer\n    the external runtime module as part of torch with attribute torch.xxx.\n    \"\"\"\n    # Make sure the device_type represent a supported device type for torch.\n    device_type = torch.device(device_type).type\n    m = sys.modules[__name__]\n    if hasattr(m, device_type):\n        raise RuntimeError(f\"The runtime module of '{device_type}' has already \"\n                           f\"been registered with '{getattr(m, device_type)}'\")\n    setattr(m, device_type, module)\n    torch_module_name = '.'.join([__name__, device_type])\n    sys.modules[torch_module_name] = module\n\n# expose return_types\nfrom . import return_types\nfrom . import library\nif not TYPE_CHECKING:\n    from . import _meta_registrations\n\n# Enable CUDA Sanitizer\nif 'TORCH_CUDA_SANITIZER' in os.environ:\n    import torch.cuda._sanitizer as csan\n\n    csan.enable_cuda_sanitizer()\n\n# Populate magic methods on SymInt and SymFloat\nimport torch.fx.experimental.symbolic_shapes\n\nfrom torch import func as func\nfrom torch.func import vmap\n\n\n# The function _sparse_coo_tensor_unsafe is removed from PyTorch\n# Python API (v. 1.13), here we temporarily provide its replacement\n# with a deprecation warning.\n# TODO: remove the function for PyTorch v 1.15.\ndef _sparse_coo_tensor_unsafe(*args, **kwargs):\n    import warnings\n    warnings.warn('torch._sparse_coo_tensor_unsafe is deprecated, '\n                  'use torch.sparse_coo_tensor(..., check_invariants=False) instead.')\n    kwargs['check_invariants'] = False\n    return torch.sparse_coo_tensor(*args, **kwargs)\n\n# Register MPS specific decomps\ntorch.backends.mps._init()\n\nif not _running_with_deploy():\n    from torch import compiler as compiler\n\n    class _TritonLibrary:\n        lib = torch.library.Library(\"triton\", \"DEF\")\n        ops_table: Dict[Tuple[str, str], Callable] = {}\n\n        @classmethod\n        def registerOp(cls, op_key, full_schema, op_impl, dispatch_key):\n            if (op_key, dispatch_key) not in cls.ops_table:\n                cls.lib.define(full_schema)\n                cls.lib.impl(\"triton::\" + op_key, op_impl, dispatch_key)\n                cls.ops_table[(op_key, dispatch_key)] = op_impl\n\n            return cls.ops_table[(op_key, dispatch_key)]\n\n\n# Deprecated attributes\n_deprecated_attrs = {\n    \"has_mps\": torch.backends.mps.is_built,\n    \"has_cuda\": torch.backends.cuda.is_built,\n    \"has_cudnn\": torch.backends.cudnn.is_available,\n    \"has_mkldnn\": torch.backends.mkldnn.is_available,\n}\n\nif TYPE_CHECKING:\n    # Import the following modules during type checking to enable code intelligence features,\n    # such as auto-completion in tools like pylance, even when these modules are not explicitly\n    # imported in user code.\n    from torch import _dynamo as _dynamo\n    from torch import _inductor as _inductor\n    from torch import onnx as onnx\n\n_lazy_modules = {\n    \"_dynamo\",\n    \"_inductor\",\n    \"_export\",\n    # ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit\n    \"onnx\",\n}\n\ndef __getattr__(name):\n    # Deprecated attrs\n    replacement = _deprecated_attrs.get(name)\n    if replacement is not None:\n        import warnings\n        warnings.warn(f\"'{name}' is deprecated, please use '{replacement.__module__}.{replacement.__name__}()'\", stacklevel=2)\n        return replacement()\n\n    # Lazy modules\n    if name in _lazy_modules:\n        import importlib\n        return importlib.import_module(f\".{name}\", __name__)\n\n    raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")\n\nfrom . import _logging\n_logging._init_logs()\n",
      "torch/_appdirs.py": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# Copyright (c) 2005-2010 ActiveState Software Inc.\n# Copyright (c) 2013 Eddy Petri\u0219or\n\n# flake8: noqa\n\n\"\"\"\nThis file is directly from\nhttps://github.com/ActiveState/appdirs/blob/3fe6a83776843a46f20c2e5587afcffe05e03b39/appdirs.py\n\nThe license of https://github.com/ActiveState/appdirs copied below:\n\n\n# This is the MIT license\n\nCopyright (c) 2010 ActiveState Software Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\nCLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\"\"\"\n\n\"\"\"Utilities for determining application-specific dirs.\n\nSee <https://github.com/ActiveState/appdirs> for details and usage.\n\"\"\"\n# Dev Notes:\n# - MSDN on where to store app data files:\n#   http://support.microsoft.com/default.aspx?scid=kb;en-us;310294#XSLTH3194121123120121120120\n# - Mac OS X: http://developer.apple.com/documentation/MacOSX/Conceptual/BPFileSystem/index.html\n# - XDG spec for Un*x: https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html\n\n__version__ = \"1.4.4\"\n__version_info__ = tuple(int(segment) for segment in __version__.split(\".\"))\n\n\nimport os\nimport sys\n\nunicode = str\n\nif sys.platform.startswith(\"java\"):\n    import platform\n\n    os_name = platform.java_ver()[3][0]\n    if os_name.startswith(\"Windows\"):  # \"Windows XP\", \"Windows 7\", etc.\n        system = \"win32\"\n    elif os_name.startswith(\"Mac\"):  # \"Mac OS X\", etc.\n        system = \"darwin\"\n    else:  # \"Linux\", \"SunOS\", \"FreeBSD\", etc.\n        # Setting this to \"linux2\" is not ideal, but only Windows or Mac\n        # are actually checked for and the rest of the module expects\n        # *sys.platform* style strings.\n        system = \"linux2\"\nelse:\n    system = sys.platform\n\n\ndef user_data_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user data directories are:\n        Mac OS X:               ~/Library/Application Support/<AppName>\n        Unix:                   ~/.local/share/<AppName>    # or in $XDG_DATA_HOME, if defined\n        Win XP (not roaming):   C:\\Documents and Settings\\<username>\\Application Data\\<AppAuthor>\\<AppName>\n        Win XP (roaming):       C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\n        Win 7  (not roaming):   C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\n        Win 7  (roaming):       C:\\Users\\<username>\\AppData\\Roaming\\<AppAuthor>\\<AppName>\n\n    For Unix, we follow the XDG spec and support $XDG_DATA_HOME.\n    That means, by default \"~/.local/share/<AppName>\".\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        const = roaming and \"CSIDL_APPDATA\" or \"CSIDL_LOCAL_APPDATA\"\n        path = os.path.normpath(_get_win_folder(const))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Application Support/\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_DATA_HOME\", os.path.expanduser(\"~/.local/share\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef site_data_dir(appname=None, appauthor=None, version=None, multipath=False):\n    r\"\"\"Return full path to the user-shared data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"multipath\" is an optional parameter only applicable to *nix\n            which indicates that the entire list of data dirs should be\n            returned. By default, the first item from XDG_DATA_DIRS is\n            returned, or '/usr/local/share/<AppName>',\n            if XDG_DATA_DIRS is not set\n\n    Typical site data directories are:\n        Mac OS X:   /Library/Application Support/<AppName>\n        Unix:       /usr/local/share/<AppName> or /usr/share/<AppName>\n        Win XP:     C:\\Documents and Settings\\All Users\\Application Data\\<AppAuthor>\\<AppName>\n        Vista:      (Fail! \"C:\\ProgramData\" is a hidden *system* directory on Vista.)\n        Win 7:      C:\\ProgramData\\<AppAuthor>\\<AppName>   # Hidden, but writeable on Win 7.\n\n    For Unix, this is using the $XDG_DATA_DIRS[0] default.\n\n    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        path = os.path.normpath(_get_win_folder(\"CSIDL_COMMON_APPDATA\"))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"/Library/Application Support\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        # XDG default for $XDG_DATA_DIRS\n        # only first, if multipath is False\n        path = os.getenv(\n            \"XDG_DATA_DIRS\", os.pathsep.join([\"/usr/local/share\", \"/usr/share\"])\n        )\n        pathlist = [\n            os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)\n        ]\n        if appname:\n            if version:\n                appname = os.path.join(appname, version)\n            pathlist = [os.sep.join([x, appname]) for x in pathlist]\n\n        if multipath:\n            path = os.pathsep.join(pathlist)\n        else:\n            path = pathlist[0]\n        return path\n\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_config_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific config dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user config directories are:\n        Mac OS X:               ~/Library/Preferences/<AppName>\n        Unix:                   ~/.config/<AppName>     # or in $XDG_CONFIG_HOME, if defined\n        Win *:                  same as user_data_dir\n\n    For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME.\n    That means, by default \"~/.config/<AppName>\".\n    \"\"\"\n    if system == \"win32\":\n        path = user_data_dir(appname, appauthor, None, roaming)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Preferences/\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_CONFIG_HOME\", os.path.expanduser(\"~/.config\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef site_config_dir(appname=None, appauthor=None, version=None, multipath=False):\n    r\"\"\"Return full path to the user-shared data dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"multipath\" is an optional parameter only applicable to *nix\n            which indicates that the entire list of config dirs should be\n            returned. By default, the first item from XDG_CONFIG_DIRS is\n            returned, or '/etc/xdg/<AppName>', if XDG_CONFIG_DIRS is not set\n\n    Typical site config directories are:\n        Mac OS X:   same as site_data_dir\n        Unix:       /etc/xdg/<AppName> or $XDG_CONFIG_DIRS[i]/<AppName> for each value in\n                    $XDG_CONFIG_DIRS\n        Win *:      same as site_data_dir\n        Vista:      (Fail! \"C:\\ProgramData\" is a hidden *system* directory on Vista.)\n\n    For Unix, this is using the $XDG_CONFIG_DIRS[0] default, if multipath=False\n\n    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.\n    \"\"\"\n    if system == \"win32\":\n        path = site_data_dir(appname, appauthor)\n        if appname and version:\n            path = os.path.join(path, version)\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"/Library/Preferences\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        # XDG default for $XDG_CONFIG_DIRS\n        # only first, if multipath is False\n        path = os.getenv(\"XDG_CONFIG_DIRS\", \"/etc/xdg\")\n        pathlist = [\n            os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)\n        ]\n        if appname:\n            if version:\n                appname = os.path.join(appname, version)\n            pathlist = [os.sep.join([x, appname]) for x in pathlist]\n\n        if multipath:\n            path = os.pathsep.join(pathlist)\n        else:\n            path = pathlist[0]\n    return path\n\n\ndef user_cache_dir(appname=None, appauthor=None, version=None, opinion=True):\n    r\"\"\"Return full path to the user-specific cache dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"opinion\" (boolean) can be False to disable the appending of\n            \"Cache\" to the base app data dir for Windows. See\n            discussion below.\n\n    Typical user cache directories are:\n        Mac OS X:   ~/Library/Caches/<AppName>\n        Unix:       ~/.cache/<AppName> (XDG default)\n        Win XP:     C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\\Cache\n        Vista:      C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\\Cache\n\n    On Windows the only suggestion in the MSDN docs is that local settings go in\n    the `CSIDL_LOCAL_APPDATA` directory. This is identical to the non-roaming\n    app data dir (the default returned by `user_data_dir` above). Apps typically\n    put cache data somewhere *under* the given dir here. Some examples:\n        ...\\Mozilla\\Firefox\\Profiles\\<ProfileName>\\Cache\n        ...\\Acme\\SuperApp\\Cache\\1.0\n    OPINION: This function appends \"Cache\" to the `CSIDL_LOCAL_APPDATA` value.\n    This can be disabled with the `opinion=False` option.\n    \"\"\"\n    if system == \"win32\":\n        if appauthor is None:\n            appauthor = appname\n        path = os.path.normpath(_get_win_folder(\"CSIDL_LOCAL_APPDATA\"))\n        if appname:\n            if appauthor is not False:\n                path = os.path.join(path, appauthor, appname)\n            else:\n                path = os.path.join(path, appname)\n            if opinion:\n                path = os.path.join(path, \"Cache\")\n    elif system == \"darwin\":\n        path = os.path.expanduser(\"~/Library/Caches\")\n        if appname:\n            path = os.path.join(path, appname)\n    else:\n        path = os.getenv(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_state_dir(appname=None, appauthor=None, version=None, roaming=False):\n    r\"\"\"Return full path to the user-specific state dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"roaming\" (boolean, default False) can be set True to use the Windows\n            roaming appdata directory. That means that for users on a Windows\n            network setup for roaming profiles, this user data will be\n            sync'd on login. See\n            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>\n            for a discussion of issues.\n\n    Typical user state directories are:\n        Mac OS X:  same as user_data_dir\n        Unix:      ~/.local/state/<AppName>   # or in $XDG_STATE_HOME, if defined\n        Win *:     same as user_data_dir\n\n    For Unix, we follow this Debian proposal <https://wiki.debian.org/XDGBaseDirectorySpecification#state>\n    to extend the XDG spec and support $XDG_STATE_HOME.\n\n    That means, by default \"~/.local/state/<AppName>\".\n    \"\"\"\n    if system in [\"win32\", \"darwin\"]:\n        path = user_data_dir(appname, appauthor, None, roaming)\n    else:\n        path = os.getenv(\"XDG_STATE_HOME\", os.path.expanduser(\"~/.local/state\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\ndef user_log_dir(appname=None, appauthor=None, version=None, opinion=True):\n    r\"\"\"Return full path to the user-specific log dir for this application.\n\n        \"appname\" is the name of application.\n            If None, just the system directory is returned.\n        \"appauthor\" (only used on Windows) is the name of the\n            appauthor or distributing body for this application. Typically\n            it is the owning company name. This falls back to appname. You may\n            pass False to disable it.\n        \"version\" is an optional version path element to append to the\n            path. You might want to use this if you want multiple versions\n            of your app to be able to run independently. If used, this\n            would typically be \"<major>.<minor>\".\n            Only applied when appname is present.\n        \"opinion\" (boolean) can be False to disable the appending of\n            \"Logs\" to the base app data dir for Windows, and \"log\" to the\n            base cache dir for Unix. See discussion below.\n\n    Typical user log directories are:\n        Mac OS X:   ~/Library/Logs/<AppName>\n        Unix:       ~/.cache/<AppName>/log  # or under $XDG_CACHE_HOME if defined\n        Win XP:     C:\\Documents and Settings\\<username>\\Local Settings\\Application Data\\<AppAuthor>\\<AppName>\\Logs\n        Vista:      C:\\Users\\<username>\\AppData\\Local\\<AppAuthor>\\<AppName>\\Logs\n\n    On Windows the only suggestion in the MSDN docs is that local settings\n    go in the `CSIDL_LOCAL_APPDATA` directory. (Note: I'm interested in\n    examples of what some windows apps use for a logs dir.)\n\n    OPINION: This function appends \"Logs\" to the `CSIDL_LOCAL_APPDATA`\n    value for Windows and appends \"log\" to the user cache dir for Unix.\n    This can be disabled with the `opinion=False` option.\n    \"\"\"\n    if system == \"darwin\":\n        path = os.path.join(os.path.expanduser(\"~/Library/Logs\"), appname)\n    elif system == \"win32\":\n        path = user_data_dir(appname, appauthor, version)\n        version = False\n        if opinion:\n            path = os.path.join(path, \"Logs\")\n    else:\n        path = user_cache_dir(appname, appauthor, version)\n        version = False\n        if opinion:\n            path = os.path.join(path, \"log\")\n    if appname and version:\n        path = os.path.join(path, version)\n    return path\n\n\nclass AppDirs(object):\n    \"\"\"Convenience wrapper for getting application dirs.\"\"\"\n\n    def __init__(\n        self, appname=None, appauthor=None, version=None, roaming=False, multipath=False\n    ):\n        self.appname = appname\n        self.appauthor = appauthor\n        self.version = version\n        self.roaming = roaming\n        self.multipath = multipath\n\n    @property\n    def user_data_dir(self):\n        return user_data_dir(\n            self.appname, self.appauthor, version=self.version, roaming=self.roaming\n        )\n\n    @property\n    def site_data_dir(self):\n        return site_data_dir(\n            self.appname, self.appauthor, version=self.version, multipath=self.multipath\n        )\n\n    @property\n    def user_config_dir(self):\n        return user_config_dir(\n            self.appname, self.appauthor, version=self.version, roaming=self.roaming\n        )\n\n    @property\n    def site_config_dir(self):\n        return site_config_dir(\n            self.appname, self.appauthor, version=self.version, multipath=self.multipath\n        )\n\n    @property\n    def user_cache_dir(self):\n        return user_cache_dir(self.appname, self.appauthor, version=self.version)\n\n    @property\n    def user_state_dir(self):\n        return user_state_dir(self.appname, self.appauthor, version=self.version)\n\n    @property\n    def user_log_dir(self):\n        return user_log_dir(self.appname, self.appauthor, version=self.version)\n\n\n# ---- internal support stuff\n\n\ndef _get_win_folder_from_registry(csidl_name):\n    \"\"\"This is a fallback technique at best. I'm not sure if using the\n    registry for this guarantees us the correct answer for all CSIDL_*\n    names.\n    \"\"\"\n    import winreg as _winreg\n\n    shell_folder_name = {\n        \"CSIDL_APPDATA\": \"AppData\",\n        \"CSIDL_COMMON_APPDATA\": \"Common AppData\",\n        \"CSIDL_LOCAL_APPDATA\": \"Local AppData\",\n    }[csidl_name]\n\n    key = _winreg.OpenKey(\n        _winreg.HKEY_CURRENT_USER,\n        r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\",\n    )\n    dir, type = _winreg.QueryValueEx(key, shell_folder_name)\n    return dir\n\n\ndef _get_win_folder_with_pywin32(csidl_name):\n    from win32com.shell import shell, shellcon\n\n    dir = shell.SHGetFolderPath(0, getattr(shellcon, csidl_name), 0, 0)\n    # Try to make this a unicode path because SHGetFolderPath does\n    # not return unicode strings when there is unicode data in the\n    # path.\n    try:\n        dir = unicode(dir)\n\n        # Downgrade to short path name if have highbit chars. See\n        # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n        has_high_char = False\n        for c in dir:\n            if ord(c) > 255:\n                has_high_char = True\n                break\n        if has_high_char:\n            try:\n                import win32api\n\n                dir = win32api.GetShortPathName(dir)\n            except ImportError:\n                pass\n    except UnicodeError:\n        pass\n    return dir\n\n\ndef _get_win_folder_with_ctypes(csidl_name):\n    import ctypes\n\n    csidl_const = {\n        \"CSIDL_APPDATA\": 26,\n        \"CSIDL_COMMON_APPDATA\": 35,\n        \"CSIDL_LOCAL_APPDATA\": 28,\n    }[csidl_name]\n\n    buf = ctypes.create_unicode_buffer(1024)\n    ctypes.windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)\n\n    # Downgrade to short path name if have highbit chars. See\n    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n    has_high_char = False\n    for c in buf:\n        if ord(c) > 255:\n            has_high_char = True\n            break\n    if has_high_char:\n        buf2 = ctypes.create_unicode_buffer(1024)\n        if ctypes.windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):\n            buf = buf2\n\n    return buf.value\n\n\ndef _get_win_folder_with_jna(csidl_name):\n    import array\n\n    from com.sun import jna\n    from com.sun.jna.platform import win32\n\n    buf_size = win32.WinDef.MAX_PATH * 2\n    buf = array.zeros(\"c\", buf_size)\n    shell = win32.Shell32.INSTANCE\n    shell.SHGetFolderPath(\n        None,\n        getattr(win32.ShlObj, csidl_name),\n        None,\n        win32.ShlObj.SHGFP_TYPE_CURRENT,\n        buf,\n    )\n    dir = jna.Native.toString(buf.tostring()).rstrip(\"\\0\")\n\n    # Downgrade to short path name if have highbit chars. See\n    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.\n    has_high_char = False\n    for c in dir:\n        if ord(c) > 255:\n            has_high_char = True\n            break\n    if has_high_char:\n        buf = array.zeros(\"c\", buf_size)\n        kernel = win32.Kernel32.INSTANCE\n        if kernel.GetShortPathName(dir, buf, buf_size):\n            dir = jna.Native.toString(buf.tostring()).rstrip(\"\\0\")\n\n    return dir\n\n\nif system == \"win32\":\n    try:\n        import win32com.shell\n\n        _get_win_folder = _get_win_folder_with_pywin32\n    except ImportError:\n        try:\n            from ctypes import windll\n\n            _get_win_folder = _get_win_folder_with_ctypes\n        except ImportError:\n            try:\n                import com.sun.jna\n\n                _get_win_folder = _get_win_folder_with_jna\n            except ImportError:\n                _get_win_folder = _get_win_folder_from_registry\n\n\n# ---- self test code\n\nif __name__ == \"__main__\":\n    appname = \"MyApp\"\n    appauthor = \"MyCompany\"\n\n    props = (\n        \"user_data_dir\",\n        \"user_config_dir\",\n        \"user_cache_dir\",\n        \"user_state_dir\",\n        \"user_log_dir\",\n        \"site_data_dir\",\n        \"site_config_dir\",\n    )\n\n    print(f\"-- app dirs {__version__} --\")\n\n    print(\"-- app dirs (with optional 'version')\")\n    dirs = AppDirs(appname, appauthor, version=\"1.0\")\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (without optional 'version')\")\n    dirs = AppDirs(appname, appauthor)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (without optional 'appauthor')\")\n    dirs = AppDirs(appname)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n\n    print(\"\\n-- app dirs (with disabled 'appauthor')\")\n    dirs = AppDirs(appname, appauthor=False)\n    for prop in props:\n        print(f\"{prop}: {getattr(dirs, prop)}\")\n"
    },
    "patchgt": {
      "torch/nn/modules/normalization.py": "@@ -189,7 +189,8 @@ def __init__(self, normalized_shape: _shape_t, eps: float = 1e-5, elementwise_af\n     def reset_parameters(self) -> None:\n         if self.elementwise_affine:\n             init.ones_(self.weight)\n-            init.zeros_(self.bias)\n+            if self.bias is not None:\n+                init.zeros_(self.bias)\n \n     def forward(self, input: Tensor) -> Tensor:\n         return F.layer_norm(",
      "torch/testing/_internal/common_modules.py": "@@ -1414,6 +1414,10 @@ def module_inputs_torch_nn_LayerNorm(module_info, device, dtype, requires_grad,\n             constructor_input=FunctionInput([5], 1e-3),\n             forward_input=FunctionInput(make_input((0, 5))),\n             desc='1d_empty_elementwise_affine'),\n+        ModuleInput(\n+            constructor_input=FunctionInput([2, 2, 5], 1e-3, elementwise_affine=True, bias=False),\n+            forward_input=FunctionInput(make_input((4, 2, 2, 5))),\n+            desc='3d_elementwise_affine_no_bias'),\n     ]\n \n \n@@ -1809,15 +1813,16 @@ def module_inputs_torch_nn_Transformer(module_info, device, dtype, requires_grad\n     # Samples below are for validating the no-batch-dim support.\n     key_padding_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool))\n     attn_masks = (None, torch.tensor([False, False, True], device=device, dtype=torch.bool).expand((3, 3)))\n-    for mask, key_padding_mask, norm_first in itertools.product(attn_masks, key_padding_masks, (True, False)):\n+    for mask, key_padding_mask, norm_first, bias in \\\n+            itertools.product(attn_masks, key_padding_masks, (True, False), (True, False)):\n         # Using same mask for tgt and memory\n         src_mask , tgt_mask = (mask,) * 2\n         src_key_padding_mask, tgt_key_padding_mask = (key_padding_mask,) * 2\n         samples.append(\n             ModuleInput(\n                 constructor_input=FunctionInput(d_model=4, nhead=2, dim_feedforward=8,\n                                                 num_encoder_layers=1, num_decoder_layers=1,\n-                                                dropout=0.0, batch_first=True, norm_first=norm_first),\n+                                                dropout=0.0, batch_first=True, norm_first=norm_first, bias=bias),\n                 forward_input=FunctionInput(\n                     make_input((3, 4)), make_input((3, 4)), tgt_mask=tgt_mask, src_mask=src_mask,\n                     tgt_key_padding_mask=tgt_key_padding_mask, src_key_padding_mask=src_key_padding_mask"
    },
    "repo": "pytorch",
    "pr_number": 108078
  }
]